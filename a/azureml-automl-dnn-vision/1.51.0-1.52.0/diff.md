# Comparing `tmp/azureml_automl_dnn_vision-1.51.0-py3-none-any.whl.zip` & `tmp/azureml_automl_dnn_vision-1.52.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,194 +1,195 @@
-Zip file size: 435572 bytes, number of entries: 192
--rw-rw-rw-  2.0 fat      299 b- defN 23-May-24 03:26 azureml/__init__.py
--rw-rw-rw-  2.0 fat      311 b- defN 23-May-24 03:26 azureml/automl/__init__.py
--rw-rw-rw-  2.0 fat      315 b- defN 23-May-24 03:26 azureml/automl/dnn/__init__.py
--rw-rw-rw-  2.0 fat      682 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/__init__.py
--rw-rw-rw-  2.0 fat      226 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/__init__.py
--rw-rw-rw-  2.0 fat    14072 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/runner.py
--rw-rw-rw-  2.0 fat      229 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/common/__init__.py
--rw-rw-rw-  2.0 fat    27028 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/common/classification_utils.py
--rw-rw-rw-  2.0 fat     7587 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/common/constants.py
--rw-rw-rw-  2.0 fat     2573 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/common/transforms.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/inference/__init__.py
--rw-rw-rw-  2.0 fat    23903 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/inference/score.py
--rw-rw-rw-  2.0 fat      210 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/__init__.py
--rw-rw-rw-  2.0 fat      398 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/read/__init__.py
--rw-rw-rw-  2.0 fat     4919 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/read/dataloader.py
--rw-rw-rw-  2.0 fat    22399 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/read/dataset_wrappers.py
--rw-rw-rw-  2.0 fat     5507 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/read/utils.py
--rw-rw-rw-  2.0 fat      237 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/write/__init__.py
--rw-rw-rw-  2.0 fat     2613 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/write/featurize_script.py
--rw-rw-rw-  2.0 fat     6433 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/write/score_script.py
--rw-rw-rw-  2.0 fat      973 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/io/write/score_script_utils.py
--rw-rw-rw-  2.0 fat      429 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/models/__init__.py
--rw-rw-rw-  2.0 fat    11491 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/models/base_model_wrapper.py
--rw-rw-rw-  2.0 fat    30503 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/models/classification_model_wrappers.py
--rw-rw-rw-  2.0 fat      221 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/trainer/__init__.py
--rw-rw-rw-  2.0 fat     1203 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/trainer/criterion.py
--rw-rw-rw-  2.0 fat    38271 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/classification/trainer/train.py
--rw-rw-rw-  2.0 fat    85645 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/NOTICE
--rw-rw-rw-  2.0 fat      263 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/__init__.py
--rw-rw-rw-  2.0 fat     1903 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/aml_dataset_base_wrapper.py
--rw-rw-rw-  2.0 fat    18726 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/artifacts_utils.py
--rw-rw-rw-  2.0 fat     2025 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/average_meter.py
--rw-rw-rw-  2.0 fat     2162 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/base_model_factory.py
--rw-rw-rw-  2.0 fat     1231 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/base_model_settings.py
--rw-rw-rw-  2.0 fat    22576 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/constants.py
--rw-rw-rw-  2.0 fat     2826 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/data_utils.py
--rw-rw-rw-  2.0 fat     3184 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/dataloaders.py
--rw-rw-rw-  2.0 fat    18976 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/dataset_helper.py
--rw-rw-rw-  2.0 fat    21108 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/distributed_utils.py
--rw-rw-rw-  2.0 fat      672 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/errors.py
--rw-rw-rw-  2.0 fat     2379 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/exceptions.py
--rw-rw-rw-  2.0 fat     2528 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/logging_utils.py
--rw-rw-rw-  2.0 fat    25968 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/model_export_utils.py
--rw-rw-rw-  2.0 fat    14730 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/parameters.py
--rw-rw-rw-  2.0 fat    10208 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/prediction_dataset.py
--rw-rw-rw-  2.0 fat    37122 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/pretrained_model_utilities.py
--rw-rw-rw-  2.0 fat     3200 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/sku_validation.py
--rw-rw-rw-  2.0 fat     9887 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/system_meter.py
--rw-rw-rw-  2.0 fat     5477 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/tiling_dataset_element.py
--rw-rw-rw-  2.0 fat     6399 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/tiling_utils.py
--rw-rw-rw-  2.0 fat     1039 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/torch_utils.py
--rw-rw-rw-  2.0 fat     2994 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/training_state.py
--rw-rw-rw-  2.0 fat    70480 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/utils.py
--rw-rw-rw-  2.0 fat      237 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/mlflow/__init__.py
--rw-rw-rw-  2.0 fat     6364 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py
--rw-rw-rw-  2.0 fat      272 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/trainer/__init__.py
--rw-rw-rw-  2.0 fat     7720 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/trainer/lrschedule.py
--rw-rw-rw-  2.0 fat     3336 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/trainer/lrschedule_parameters.py
--rw-rw-rw-  2.0 fat     9241 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/trainer/optimize.py
--rw-rw-rw-  2.0 fat     4337 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/common/trainer/optimize_parameters.py
--rw-rw-rw-  2.0 fat      229 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/explainability/__init__.py
--rw-rw-rw-  2.0 fat     2638 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/explainability/constants.py
--rw-rw-rw-  2.0 fat    16285 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/explainability/methods.py
--rw-rw-rw-  2.0 fat     8627 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/explainability/utils.py
--rw-rw-rw-  2.0 fat     6437 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/explainability/xrai_utils.py
--rw-rw-rw-  2.0 fat      321 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/metrics/__init__.py
--rw-rw-rw-  2.0 fat     6643 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/metrics/automl_classification_metrics.py
--rw-rw-rw-  2.0 fat     4480 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/metrics/classification_metrics.py
--rw-rw-rw-  2.0 fat      240 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/__init__.py
--rw-rw-rw-  2.0 fat    20822 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/runner.py
--rw-rw-rw-  2.0 fat      243 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/__init__.py
--rw-rw-rw-  2.0 fat     9636 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/augmentations.py
--rw-rw-rw-  2.0 fat     7356 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/boundingbox.py
--rw-rw-rw-  2.0 fat     3009 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/coco_eval_box_converter.py
--rw-rw-rw-  2.0 fat    11089 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/constants.py
--rw-rw-rw-  2.0 fat    11945 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/masktools.py
--rw-rw-rw-  2.0 fat    27385 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py
--rw-rw-rw-  2.0 fat     1632 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/od_training_state.py
--rw-rw-rw-  2.0 fat     4891 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/parameters.py
--rw-rw-rw-  2.0 fat    26057 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/common/tiling_helper.py
--rw-rw-rw-  2.0 fat      238 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/__init__.py
--rw-rw-rw-  2.0 fat     4605 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/dataset_wrappers.py
--rw-rw-rw-  2.0 fat    40467 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/datasets.py
--rw-rw-rw-  2.0 fat     6110 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/loaders.py
--rw-rw-rw-  2.0 fat    12574 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/object_annotation.py
--rw-rw-rw-  2.0 fat     4985 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/tiling_distributed_sampler.py
--rw-rw-rw-  2.0 fat    11231 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/data/utils.py
--rw-rw-rw-  2.0 fat      240 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/eval/__init__.py
--rw-rw-rw-  2.0 fat     8380 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/eval/cocotools.py
--rw-rw-rw-  2.0 fat    21506 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py
--rw-rw-rw-  2.0 fat    17354 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py
--rw-rw-rw-  2.0 fat    14609 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/eval/object_detection_instance_segmentation_evaluator.py
--rw-rw-rw-  2.0 fat     5220 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/eval/utils.py
--rw-rw-rw-  2.0 fat      239 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/models/__init__.py
--rw-rw-rw-  2.0 fat    14422 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/models/base_model_wrapper.py
--rw-rw-rw-  2.0 fat     5102 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/models/detection.py
--rw-rw-rw-  2.0 fat    14240 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/models/instance_segmentation_model_wrappers.py
--rw-rw-rw-  2.0 fat    28844 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/models/object_detection_model_wrappers.py
--rw-rw-rw-  2.0 fat      238 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/trainer/__init__.py
--rw-rw-rw-  2.0 fat     3651 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/trainer/criterion.py
--rw-rw-rw-  2.0 fat    25255 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/trainer/train.py
--rw-rw-rw-  2.0 fat      257 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/writers/__init__.py
--rw-rw-rw-  2.0 fat    15885 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/writers/score.py
--rw-rw-rw-  2.0 fat     5041 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/writers/score_script.py
--rw-rw-rw-  2.0 fat     1411 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection/writers/score_script_utils.py
--rw-rw-rw-  2.0 fat      241 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/__init__.py
--rw-rw-rw-  2.0 fat    14496 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/runner.py
--rw-rw-rw-  2.0 fat      243 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/common/__init__.py
--rw-rw-rw-  2.0 fat     7128 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/common/constants.py
--rw-rw-rw-  2.0 fat     1382 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/common/od_yolo_training_state.py
--rw-rw-rw-  2.0 fat      254 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/data/__init__.py
--rw-rw-rw-  2.0 fat    12848 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/data/datasets.py
--rw-rw-rw-  2.0 fat     9783 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/data/utils.py
--rw-rw-rw-  2.0 fat      245 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/eval/__init__.py
--rw-rw-rw-  2.0 fat     4430 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/eval/yolo_evaluator.py
--rw-rw-rw-  2.0 fat      239 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/__init__.py
--rw-rw-rw-  2.0 fat     6097 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/common.py
--rw-rw-rw-  2.0 fat     9858 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/yolo.py
--rw-rw-rw-  2.0 fat    11041 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/yolo_wrapper.py
--rw-rw-rw-  2.0 fat     1501 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0l.yaml
--rw-rw-rw-  2.0 fat     1503 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0m.yaml
--rw-rw-rw-  2.0 fat     1503 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0s.yaml
--rw-rw-rw-  2.0 fat     1503 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0x.yaml
--rw-rw-rw-  2.0 fat      238 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/trainer/__init__.py
--rw-rw-rw-  2.0 fat    28280 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/trainer/train.py
--rw-rw-rw-  2.0 fat      242 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/utils/__init__.py
--rw-rw-rw-  2.0 fat     3060 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/utils/ema.py
--rw-rw-rw-  2.0 fat     3799 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/utils/torch_utils.py
--rw-rw-rw-  2.0 fat    41566 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/utils/utils.py
--rw-rw-rw-  2.0 fat      257 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/writers/__init__.py
--rw-rw-rw-  2.0 fat    14388 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/writers/score.py
--rw-rw-rw-  2.0 fat     3292 b- defN 23-May-24 03:26 azureml/automl/dnn/vision/object_detection_yolo/writers/score_script.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-24 03:26 tests/__init__.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-24 03:26 tests/classification_tests/__init__.py
--rw-rw-rw-  2.0 fat      426 b- defN 23-May-24 03:26 tests/classification_tests/aml_dataset_mock.py
--rw-rw-rw-  2.0 fat      915 b- defN 23-May-24 03:26 tests/classification_tests/conftest.py
--rw-rw-rw-  2.0 fat    14353 b- defN 23-May-24 03:26 tests/classification_tests/test_classification_local_run.py
--rw-rw-rw-  2.0 fat     5320 b- defN 23-May-24 03:26 tests/classification_tests/test_classification_resume_local_run.py
--rw-rw-rw-  2.0 fat     3457 b- defN 23-May-24 03:26 tests/classification_tests/test_classification_trainer.py
--rw-rw-rw-  2.0 fat      906 b- defN 23-May-24 03:26 tests/classification_tests/test_classification_utils.py
--rw-rw-rw-  2.0 fat    11342 b- defN 23-May-24 03:26 tests/classification_tests/test_classification_xai_methods.py
--rw-rw-rw-  2.0 fat     2169 b- defN 23-May-24 03:26 tests/classification_tests/test_dataloaders.py
--rw-rw-rw-  2.0 fat    19599 b- defN 23-May-24 03:26 tests/classification_tests/test_dataset_wrappers.py
--rw-rw-rw-  2.0 fat    13726 b- defN 23-May-24 03:26 tests/classification_tests/test_inference_model_wrapper.py
--rw-rw-rw-  2.0 fat    19780 b- defN 23-May-24 03:26 tests/classification_tests/test_model_wrappers.py
--rw-rw-rw-  2.0 fat    11694 b- defN 23-May-24 03:26 tests/classification_tests/test_prediction_dataset.py
--rw-rw-rw-  2.0 fat     3067 b- defN 23-May-24 03:26 tests/classification_tests/test_pretrained_model_factory.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-24 03:26 tests/common/__init__.py
--rw-rw-rw-  2.0 fat     1237 b- defN 23-May-24 03:26 tests/common/aml_dataset_mock.py
--rw-rw-rw-  2.0 fat     5116 b- defN 23-May-24 03:26 tests/common/run_mock.py
--rw-rw-rw-  2.0 fat    10438 b- defN 23-May-24 03:26 tests/common/test_aml_dataset_helper.py
--rw-rw-rw-  2.0 fat    12795 b- defN 23-May-24 03:26 tests/common/test_artifacts_utils.py
--rw-rw-rw-  2.0 fat    57289 b- defN 23-May-24 03:26 tests/common/test_common_methods.py
--rw-rw-rw-  2.0 fat    16927 b- defN 23-May-24 03:26 tests/common/test_distributed_utils.py
--rw-rw-rw-  2.0 fat    10683 b- defN 23-May-24 03:26 tests/common/test_model_export_utils.py
--rw-rw-rw-  2.0 fat     6720 b- defN 23-May-24 03:26 tests/common/test_pretrained_model_utilities.py
--rw-rw-rw-  2.0 fat     6606 b- defN 23-May-24 03:26 tests/common/test_runner_default_args.py
--rw-rw-rw-  2.0 fat     5624 b- defN 23-May-24 03:26 tests/common/test_training_state.py
--rw-rw-rw-  2.0 fat     6315 b- defN 23-May-24 03:26 tests/common/utils.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-24 03:26 tests/object_detection_tests/__init__.py
--rw-rw-rw-  2.0 fat      430 b- defN 23-May-24 03:26 tests/object_detection_tests/aml_dataset_mock.py
--rw-rw-rw-  2.0 fat      711 b- defN 23-May-24 03:26 tests/object_detection_tests/conftest.py
--rw-rw-rw-  2.0 fat     5755 b- defN 23-May-24 03:26 tests/object_detection_tests/test_augmentations.py
--rw-rw-rw-  2.0 fat     4323 b- defN 23-May-24 03:26 tests/object_detection_tests/test_coco_eval_box_converter.py
--rw-rw-rw-  2.0 fat     1604 b- defN 23-May-24 03:26 tests/object_detection_tests/test_cocotools.py
--rw-rw-rw-  2.0 fat     9815 b- defN 23-May-24 03:26 tests/object_detection_tests/test_dataset_wrappers.py
--rw-rw-rw-  2.0 fat    73835 b- defN 23-May-24 03:26 tests/object_detection_tests/test_datasets.py
--rw-rw-rw-  2.0 fat    78443 b- defN 23-May-24 03:26 tests/object_detection_tests/test_incremental_voc_evaluator.py
--rw-rw-rw-  2.0 fat     6193 b- defN 23-May-24 03:26 tests/object_detection_tests/test_maskutils.py
--rw-rw-rw-  2.0 fat    21122 b- defN 23-May-24 03:26 tests/object_detection_tests/test_metric_computation_utils.py
--rw-rw-rw-  2.0 fat    15744 b- defN 23-May-24 03:26 tests/object_detection_tests/test_model_wrappers.py
--rw-rw-rw-  2.0 fat    14068 b- defN 23-May-24 03:26 tests/object_detection_tests/test_object_annotation.py
--rw-rw-rw-  2.0 fat    18676 b- defN 23-May-24 03:26 tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py
--rw-rw-rw-  2.0 fat     4296 b- defN 23-May-24 03:26 tests/object_detection_tests/test_object_detection_local_run.py
--rw-rw-rw-  2.0 fat    21358 b- defN 23-May-24 03:26 tests/object_detection_tests/test_object_detection_utils.py
--rw-rw-rw-  2.0 fat     2728 b- defN 23-May-24 03:26 tests/object_detection_tests/test_pretrained_model_factory.py
--rw-rw-rw-  2.0 fat     5550 b- defN 23-May-24 03:26 tests/object_detection_tests/test_scoring.py
--rw-rw-rw-  2.0 fat     5677 b- defN 23-May-24 03:26 tests/object_detection_tests/test_secondary_model_wrappers.py
--rw-rw-rw-  2.0 fat     5050 b- defN 23-May-24 03:26 tests/object_detection_tests/test_tiling_distributed_sampler.py
--rw-rw-rw-  2.0 fat    49608 b- defN 23-May-24 03:26 tests/object_detection_tests/test_tiling_helper.py
--rw-rw-rw-  2.0 fat     9531 b- defN 23-May-24 03:26 tests/object_detection_tests/test_tiling_utils.py
--rw-rw-rw-  2.0 fat     1280 b- defN 23-May-24 03:26 tests/object_detection_tests/test_trainer_criterion.py
--rw-rw-rw-  2.0 fat     4133 b- defN 23-May-24 03:26 tests/object_detection_tests/test_yolo_trainer_train.py
--rw-rw-rw-  2.0 fat     3710 b- defN 23-May-24 03:26 tests/object_detection_tests/test_yolo_utils.py
--rw-rw-rw-  2.0 fat      773 b- defN 23-May-24 03:26 tests/object_detection_tests/test_yolo_wrapper.py
--rw-rw-rw-  2.0 fat     1088 b- defN 23-May-24 03:26 tests/object_detection_tests/utils.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-May-24 03:33 azureml_automl_dnn_vision-1.51.0.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1932 b- defN 23-May-24 03:33 azureml_automl_dnn_vision-1.51.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-May-24 03:33 azureml_automl_dnn_vision-1.51.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        1 b- defN 23-May-24 03:33 azureml_automl_dnn_vision-1.51.0.dist-info/namespace_packages.txt
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-24 03:33 azureml_automl_dnn_vision-1.51.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    21710 b- defN 23-May-24 03:33 azureml_automl_dnn_vision-1.51.0.dist-info/RECORD
-192 files, 1857453 bytes uncompressed, 399334 bytes compressed:  78.5%
+Zip file size: 441751 bytes, number of entries: 193
+-rw-rw-rw-  2.0 fat      299 b- defN 23-Jun-26 15:32 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      311 b- defN 23-Jun-26 15:32 azureml/automl/__init__.py
+-rw-rw-rw-  2.0 fat      315 b- defN 23-Jun-26 15:32 azureml/automl/dnn/__init__.py
+-rw-rw-rw-  2.0 fat      682 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/__init__.py
+-rw-rw-rw-  2.0 fat      226 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/__init__.py
+-rw-rw-rw-  2.0 fat    14072 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/runner.py
+-rw-rw-rw-  2.0 fat      229 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/__init__.py
+-rw-rw-rw-  2.0 fat    27028 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/classification_utils.py
+-rw-rw-rw-  2.0 fat     7587 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/constants.py
+-rw-rw-rw-  2.0 fat     2573 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/transforms.py
+-rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/inference/__init__.py
+-rw-rw-rw-  2.0 fat    23903 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/inference/score.py
+-rw-rw-rw-  2.0 fat      210 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/__init__.py
+-rw-rw-rw-  2.0 fat      398 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/__init__.py
+-rw-rw-rw-  2.0 fat     4919 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/dataloader.py
+-rw-rw-rw-  2.0 fat    22399 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat     5507 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/utils.py
+-rw-rw-rw-  2.0 fat      237 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/__init__.py
+-rw-rw-rw-  2.0 fat     2613 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/featurize_script.py
+-rw-rw-rw-  2.0 fat     6433 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/score_script.py
+-rw-rw-rw-  2.0 fat      973 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/score_script_utils.py
+-rw-rw-rw-  2.0 fat      429 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/models/__init__.py
+-rw-rw-rw-  2.0 fat    11491 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/models/base_model_wrapper.py
+-rw-rw-rw-  2.0 fat    30503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/models/classification_model_wrappers.py
+-rw-rw-rw-  2.0 fat      221 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     1203 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/trainer/criterion.py
+-rw-rw-rw-  2.0 fat    38271 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/trainer/train.py
+-rw-rw-rw-  2.0 fat    85645 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/NOTICE
+-rw-rw-rw-  2.0 fat      263 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/__init__.py
+-rw-rw-rw-  2.0 fat     1903 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/aml_dataset_base_wrapper.py
+-rw-rw-rw-  2.0 fat    19557 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/artifacts_utils.py
+-rw-rw-rw-  2.0 fat     2025 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/average_meter.py
+-rw-rw-rw-  2.0 fat     2162 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/base_model_factory.py
+-rw-rw-rw-  2.0 fat     1231 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/base_model_settings.py
+-rw-rw-rw-  2.0 fat    22734 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/constants.py
+-rw-rw-rw-  2.0 fat     2826 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/data_utils.py
+-rw-rw-rw-  2.0 fat     3184 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/dataloaders.py
+-rw-rw-rw-  2.0 fat    18976 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/dataset_helper.py
+-rw-rw-rw-  2.0 fat    21108 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/distributed_utils.py
+-rw-rw-rw-  2.0 fat      672 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/errors.py
+-rw-rw-rw-  2.0 fat     2379 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/exceptions.py
+-rw-rw-rw-  2.0 fat     2528 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/logging_utils.py
+-rw-rw-rw-  2.0 fat    25039 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/model_export_utils.py
+-rw-rw-rw-  2.0 fat    14730 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/parameters.py
+-rw-rw-rw-  2.0 fat    10208 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/prediction_dataset.py
+-rw-rw-rw-  2.0 fat    37122 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/pretrained_model_utilities.py
+-rw-rw-rw-  2.0 fat     3200 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/sku_validation.py
+-rw-rw-rw-  2.0 fat     9887 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/system_meter.py
+-rw-rw-rw-  2.0 fat     5477 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/tiling_dataset_element.py
+-rw-rw-rw-  2.0 fat     6399 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/tiling_utils.py
+-rw-rw-rw-  2.0 fat     1039 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/torch_utils.py
+-rw-rw-rw-  2.0 fat     2994 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/training_state.py
+-rw-rw-rw-  2.0 fat    73199 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/utils.py
+-rw-rw-rw-  2.0 fat      237 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat     8165 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py
+-rw-rw-rw-  2.0 fat      272 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     7720 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/lrschedule.py
+-rw-rw-rw-  2.0 fat     3336 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/lrschedule_parameters.py
+-rw-rw-rw-  2.0 fat     9241 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/optimize.py
+-rw-rw-rw-  2.0 fat     4337 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/optimize_parameters.py
+-rw-rw-rw-  2.0 fat      229 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/__init__.py
+-rw-rw-rw-  2.0 fat     2638 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/constants.py
+-rw-rw-rw-  2.0 fat    16285 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/methods.py
+-rw-rw-rw-  2.0 fat     8627 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/utils.py
+-rw-rw-rw-  2.0 fat     6437 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/xrai_utils.py
+-rw-rw-rw-  2.0 fat      321 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/metrics/__init__.py
+-rw-rw-rw-  2.0 fat     6643 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/metrics/automl_classification_metrics.py
+-rw-rw-rw-  2.0 fat     4480 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/metrics/classification_metrics.py
+-rw-rw-rw-  2.0 fat      240 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/__init__.py
+-rw-rw-rw-  2.0 fat    20822 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/runner.py
+-rw-rw-rw-  2.0 fat      243 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/__init__.py
+-rw-rw-rw-  2.0 fat     9636 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/augmentations.py
+-rw-rw-rw-  2.0 fat     7356 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/boundingbox.py
+-rw-rw-rw-  2.0 fat     3009 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/coco_eval_box_converter.py
+-rw-rw-rw-  2.0 fat    11089 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/constants.py
+-rw-rw-rw-  2.0 fat    11945 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/masktools.py
+-rw-rw-rw-  2.0 fat    28085 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py
+-rw-rw-rw-  2.0 fat     1632 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/od_training_state.py
+-rw-rw-rw-  2.0 fat     4891 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/parameters.py
+-rw-rw-rw-  2.0 fat    26057 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/tiling_helper.py
+-rw-rw-rw-  2.0 fat      238 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/__init__.py
+-rw-rw-rw-  2.0 fat     4605 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    40467 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/datasets.py
+-rw-rw-rw-  2.0 fat     6110 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/loaders.py
+-rw-rw-rw-  2.0 fat    12574 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/object_annotation.py
+-rw-rw-rw-  2.0 fat     4985 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/tiling_distributed_sampler.py
+-rw-rw-rw-  2.0 fat    11231 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/utils.py
+-rw-rw-rw-  2.0 fat      240 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/__init__.py
+-rw-rw-rw-  2.0 fat     8380 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/cocotools.py
+-rw-rw-rw-  2.0 fat    22337 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py
+-rw-rw-rw-  2.0 fat    19785 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py
+-rw-rw-rw-  2.0 fat    14609 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/object_detection_instance_segmentation_evaluator.py
+-rw-rw-rw-  2.0 fat     5220 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/utils.py
+-rw-rw-rw-  2.0 fat      239 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/__init__.py
+-rw-rw-rw-  2.0 fat    14422 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/base_model_wrapper.py
+-rw-rw-rw-  2.0 fat     5102 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/detection.py
+-rw-rw-rw-  2.0 fat    14240 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/instance_segmentation_model_wrappers.py
+-rw-rw-rw-  2.0 fat    28844 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/object_detection_model_wrappers.py
+-rw-rw-rw-  2.0 fat      238 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     3651 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/trainer/criterion.py
+-rw-rw-rw-  2.0 fat    25255 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/trainer/train.py
+-rw-rw-rw-  2.0 fat      257 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/__init__.py
+-rw-rw-rw-  2.0 fat    15885 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/score.py
+-rw-rw-rw-  2.0 fat     5041 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/score_script.py
+-rw-rw-rw-  2.0 fat     1411 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/score_script_utils.py
+-rw-rw-rw-  2.0 fat      241 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/__init__.py
+-rw-rw-rw-  2.0 fat    14496 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/runner.py
+-rw-rw-rw-  2.0 fat      243 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/common/__init__.py
+-rw-rw-rw-  2.0 fat     7128 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/common/constants.py
+-rw-rw-rw-  2.0 fat     1382 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/common/od_yolo_training_state.py
+-rw-rw-rw-  2.0 fat      254 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/data/__init__.py
+-rw-rw-rw-  2.0 fat    12848 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/data/datasets.py
+-rw-rw-rw-  2.0 fat     9783 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/data/utils.py
+-rw-rw-rw-  2.0 fat      245 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/eval/__init__.py
+-rw-rw-rw-  2.0 fat     4430 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/eval/yolo_evaluator.py
+-rw-rw-rw-  2.0 fat      239 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/__init__.py
+-rw-rw-rw-  2.0 fat     6097 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/common.py
+-rw-rw-rw-  2.0 fat     9858 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolo.py
+-rw-rw-rw-  2.0 fat    11041 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolo_wrapper.py
+-rw-rw-rw-  2.0 fat     1501 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0l.yaml
+-rw-rw-rw-  2.0 fat     1503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0m.yaml
+-rw-rw-rw-  2.0 fat     1503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0s.yaml
+-rw-rw-rw-  2.0 fat     1503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0x.yaml
+-rw-rw-rw-  2.0 fat      238 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/trainer/__init__.py
+-rw-rw-rw-  2.0 fat    28280 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/trainer/train.py
+-rw-rw-rw-  2.0 fat      242 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3060 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/ema.py
+-rw-rw-rw-  2.0 fat     3799 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/torch_utils.py
+-rw-rw-rw-  2.0 fat    41566 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/utils.py
+-rw-rw-rw-  2.0 fat      257 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/writers/__init__.py
+-rw-rw-rw-  2.0 fat    14388 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/writers/score.py
+-rw-rw-rw-  2.0 fat     3292 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/writers/score_script.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/__init__.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/classification_tests/__init__.py
+-rw-rw-rw-  2.0 fat      426 b- defN 23-Jun-26 15:32 tests/classification_tests/aml_dataset_mock.py
+-rw-rw-rw-  2.0 fat      915 b- defN 23-Jun-26 15:32 tests/classification_tests/conftest.py
+-rw-rw-rw-  2.0 fat    14353 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_local_run.py
+-rw-rw-rw-  2.0 fat     5320 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_resume_local_run.py
+-rw-rw-rw-  2.0 fat     3457 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_trainer.py
+-rw-rw-rw-  2.0 fat      906 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_utils.py
+-rw-rw-rw-  2.0 fat    11342 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_xai_methods.py
+-rw-rw-rw-  2.0 fat     2169 b- defN 23-Jun-26 15:32 tests/classification_tests/test_dataloaders.py
+-rw-rw-rw-  2.0 fat    19599 b- defN 23-Jun-26 15:32 tests/classification_tests/test_dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    13726 b- defN 23-Jun-26 15:32 tests/classification_tests/test_inference_model_wrapper.py
+-rw-rw-rw-  2.0 fat    19780 b- defN 23-Jun-26 15:32 tests/classification_tests/test_model_wrappers.py
+-rw-rw-rw-  2.0 fat    11694 b- defN 23-Jun-26 15:32 tests/classification_tests/test_prediction_dataset.py
+-rw-rw-rw-  2.0 fat     3067 b- defN 23-Jun-26 15:32 tests/classification_tests/test_pretrained_model_factory.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/common/__init__.py
+-rw-rw-rw-  2.0 fat     1237 b- defN 23-Jun-26 15:32 tests/common/aml_dataset_mock.py
+-rw-rw-rw-  2.0 fat     5177 b- defN 23-Jun-26 15:32 tests/common/run_mock.py
+-rw-rw-rw-  2.0 fat    10438 b- defN 23-Jun-26 15:32 tests/common/test_aml_dataset_helper.py
+-rw-rw-rw-  2.0 fat    16892 b- defN 23-Jun-26 15:32 tests/common/test_artifacts_utils.py
+-rw-rw-rw-  2.0 fat    61650 b- defN 23-Jun-26 15:32 tests/common/test_common_methods.py
+-rw-rw-rw-  2.0 fat    16927 b- defN 23-Jun-26 15:32 tests/common/test_distributed_utils.py
+-rw-rw-rw-  2.0 fat     4295 b- defN 23-Jun-26 15:32 tests/common/test_mlflow_model_wrapper.py
+-rw-rw-rw-  2.0 fat    10586 b- defN 23-Jun-26 15:32 tests/common/test_model_export_utils.py
+-rw-rw-rw-  2.0 fat     6720 b- defN 23-Jun-26 15:32 tests/common/test_pretrained_model_utilities.py
+-rw-rw-rw-  2.0 fat     6606 b- defN 23-Jun-26 15:32 tests/common/test_runner_default_args.py
+-rw-rw-rw-  2.0 fat     5624 b- defN 23-Jun-26 15:32 tests/common/test_training_state.py
+-rw-rw-rw-  2.0 fat     6315 b- defN 23-Jun-26 15:32 tests/common/utils.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/object_detection_tests/__init__.py
+-rw-rw-rw-  2.0 fat      430 b- defN 23-Jun-26 15:32 tests/object_detection_tests/aml_dataset_mock.py
+-rw-rw-rw-  2.0 fat      711 b- defN 23-Jun-26 15:32 tests/object_detection_tests/conftest.py
+-rw-rw-rw-  2.0 fat     5755 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_augmentations.py
+-rw-rw-rw-  2.0 fat     4323 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_coco_eval_box_converter.py
+-rw-rw-rw-  2.0 fat     1604 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_cocotools.py
+-rw-rw-rw-  2.0 fat     9815 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    73835 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_datasets.py
+-rw-rw-rw-  2.0 fat   100617 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_incremental_voc_evaluator.py
+-rw-rw-rw-  2.0 fat     6193 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_maskutils.py
+-rw-rw-rw-  2.0 fat    26704 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_metric_computation_utils.py
+-rw-rw-rw-  2.0 fat    15744 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_model_wrappers.py
+-rw-rw-rw-  2.0 fat    14068 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_annotation.py
+-rw-rw-rw-  2.0 fat    20800 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py
+-rw-rw-rw-  2.0 fat     4296 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_detection_local_run.py
+-rw-rw-rw-  2.0 fat    25852 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_detection_utils.py
+-rw-rw-rw-  2.0 fat     2728 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_pretrained_model_factory.py
+-rw-rw-rw-  2.0 fat     5550 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_scoring.py
+-rw-rw-rw-  2.0 fat     5677 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_secondary_model_wrappers.py
+-rw-rw-rw-  2.0 fat     5050 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_tiling_distributed_sampler.py
+-rw-rw-rw-  2.0 fat    49608 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_tiling_helper.py
+-rw-rw-rw-  2.0 fat     9531 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_tiling_utils.py
+-rw-rw-rw-  2.0 fat     1280 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_trainer_criterion.py
+-rw-rw-rw-  2.0 fat     4133 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_yolo_trainer_train.py
+-rw-rw-rw-  2.0 fat     3710 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_yolo_utils.py
+-rw-rw-rw-  2.0 fat      773 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_yolo_wrapper.py
+-rw-rw-rw-  2.0 fat     1088 b- defN 23-Jun-26 15:32 tests/object_detection_tests/utils.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1932 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        1 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/namespace_packages.txt
+-rw-rw-rw-  2.0 fat       14 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    21809 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/RECORD
+193 files, 1913185 bytes uncompressed, 405355 bytes compressed:  78.8%
```

## zipnote {}

```diff
@@ -456,14 +456,17 @@
 
 Filename: tests/common/test_common_methods.py
 Comment: 
 
 Filename: tests/common/test_distributed_utils.py
 Comment: 
 
+Filename: tests/common/test_mlflow_model_wrapper.py
+Comment: 
+
 Filename: tests/common/test_model_export_utils.py
 Comment: 
 
 Filename: tests/common/test_pretrained_model_utilities.py
 Comment: 
 
 Filename: tests/common/test_runner_default_args.py
@@ -552,26 +555,26 @@
 
 Filename: tests/object_detection_tests/test_yolo_wrapper.py
 Comment: 
 
 Filename: tests/object_detection_tests/utils.py
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.51.0.dist-info/LICENSE.txt
+Filename: azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.51.0.dist-info/METADATA
+Filename: azureml_automl_dnn_vision-1.52.0.dist-info/METADATA
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.51.0.dist-info/WHEEL
+Filename: azureml_automl_dnn_vision-1.52.0.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.51.0.dist-info/namespace_packages.txt
+Filename: azureml_automl_dnn_vision-1.52.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.51.0.dist-info/top_level.txt
+Filename: azureml_automl_dnn_vision-1.52.0.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.51.0.dist-info/RECORD
+Filename: azureml_automl_dnn_vision-1.52.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/automl/dnn/vision/common/artifacts_utils.py

```diff
@@ -18,15 +18,15 @@
 from azureml.automl.dnn.vision.common.exceptions import AutoMLVisionValidationException, \
     AutoMLVisionRuntimeUserException
 from azureml.automl.dnn.vision.common.model_export_utils import prepare_model_export
 from azureml.automl.dnn.vision.common.torch_utils import intersect_dicts
 from azureml.automl.dnn.vision.common.trainer.lrschedule import BaseLRSchedulerWrapper
 from azureml.automl.dnn.vision.common.training_state import TrainingState
 from azureml.automl.dnn.vision.common.utils import logger, \
-    _set_train_run_properties, _distill_run_from_experiment
+    _set_train_run_properties, _distill_run_from_experiment, should_log_metrics_to_parent
 from azureml.automl.dnn.vision.object_detection.common.constants import ModelNames
 from azureml.automl.dnn.vision.object_detection.models.object_detection_model_wrappers \
     import BaseObjectDetectionModelWrapper
 from azureml.core.run import Run, _OfflineRun
 from typing import Union, Any, Dict, Optional, List, Tuple
 
 
@@ -79,14 +79,17 @@
     with open(label_file_path, 'w') as f:
         json.dump(labels, f)
 
     _set_train_run_properties(run, model_wrapper.model_name, best_metric)
 
     folder_name = os.path.basename(output_dir)
     run.upload_folder(name=folder_name, path=output_dir)
+    parent_run = should_log_metrics_to_parent(run)
+    if parent_run:
+        parent_run.upload_folder(name=folder_name, path=output_dir)
     model_settings.update(model_wrapper.inference_settings)
     prepare_model_export(run=run,
                          output_dir=output_dir,
                          task_type=task_type,
                          model_settings=model_settings,
                          save_as_mlflow=save_as_mlflow,
                          is_yolo=is_yolo)
@@ -96,21 +99,34 @@
     """Uploads the model checkpoints to workspace.
 
     :param run: azureml run object
     :type run: azureml.core.Run
     :param model_location: Location of saved model file
     :type model_location: str
     """
-
+    # Flag to indicate if checkpoint upload succeeds to either run or pipeline run
+    # In either case, we need to remove the model from model_location, else upload_folder would fail
+    file_upload_successful = False
     try:
         run.upload_files(names=[model_location],
                          paths=[model_location])
+        file_upload_successful = True
     except Exception as e:
         logger.error('Error in uploading the checkpoint: {}'.format(e))
-    else:
+
+    parent_run = should_log_metrics_to_parent(run)
+    if parent_run:
+        try:
+            parent_run.upload_files(names=[model_location],
+                                    paths=[model_location])
+            file_upload_successful = True
+        except Exception as e:
+            logger.error('Error in uploading the checkpoint to pipeline run: {}'.format(e))
+
+    if file_upload_successful:
         # This is a workaround and needs to be done because currently upload_folder
         # doesn't handle overwrite correctly. upload_folder API is failing with checkpoints
         # already exist error when the entire train_artifacts folder is uploaded (at end of training).
         # WI to track this bug - https://msdata.visualstudio.com/Vienna/_workitems/edit/1948166
         os.remove(model_location)
```

## azureml/automl/dnn/vision/common/constants.py

```diff
@@ -5,14 +5,15 @@
 """Constants for the package."""
 
 import os
 from typing import Any, Dict
 from urllib.parse import urljoin
 
 import torch
+from mlflow.types import DataType
 from azureml.automl.core.automl_utils import get_automl_resource_url
 
 
 class ArtifactLiterals:
     """Filenames for artifacts."""
     FEATURIZE_SCRIPT = 'featurize_script.py'
     LABEL_FILE_NAME = 'labels.json'
@@ -81,14 +82,16 @@
     WEIGHTED = 'weighted'
     MEAN_AVERAGE_PRECISION = 'mean_average_precision'
     CONFUSION_MATRIX = 'confusion_matrix'
     AUTOML_CLASSIFICATION_EVAL_METRICS = 'automl_classification_eval_metrics'
     AUTOML_CLASSIFICATION_TRAIN_METRICS = 'automl_classification_train_metrics'
     COCO_METRICS = 'coco_metrics'
     PER_LABEL_METRICS = 'per_label_metrics'
+    PRECISIONS_PER_SCORE_THRESHOLD = 'precisions_per_score_threshold'
+    RECALLS_PER_SCORE_THRESHOLD = 'recalls_per_score_threshold'
     IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = 'image_level_binary_classifier_metrics'
     CONFUSION_MATRICES_PER_SCORE_THRESHOLD = 'confusion_matrices_per_score_threshold'
     CLASS_NAME = 'class_name'
     CLASS_LABELS = 'class_labels'
     MATRIX = 'matrix'
     DATA = 'data'
     AVERAGE = 'average'
@@ -385,25 +388,24 @@
                          "If you are already running on a compute with gpu devices, please check to make sure " \
                          "your nvidia drivers are compatible with torch version {}."
 
 
 class MLFlowSchemaLiterals:
     """MLFlow model signature related schema"""
     INPUT_IMAGE_KEY = 'image_base64'
-    INPUT_COLUMN_IMAGE_DATA_TYPE = 'string'
+    INPUT_COLUMN_IMAGE_DATA_TYPE = DataType.binary
     INPUT_COLUMN_IMAGE = 'image'
     # INPUT_COLUMN_XAI_DATA_TYPE = 'boolean'
     # INPUT_COLUMN_XAI_PARAMETERS_DATA_TYPE = 'string'
-    OUTPUT_COLUMN_DATA_TYPE = 'string'
-    OUTPUT_COLUMN_FILENAME = 'filename'
+    OUTPUT_COLUMN_DATA_TYPE = DataType.string
     OUTPUT_COLUMN_PROBS = 'probs'
     OUTPUT_COLUMN_LABELS = 'labels'
     OUTPUT_COLUMN_BOXES = 'boxes'
-    OUTPUT_COLUMN_XAI_VISUALIZATIONS_DATA_TYPE = 'string'
-    OUTPUT_COLUMN_XAI_ATTRIBUTIONS_DATA_TYPE = 'string'
+    OUTPUT_COLUMN_XAI_VISUALIZATIONS_DATA_TYPE = DataType.string
+    OUTPUT_COLUMN_XAI_ATTRIBUTIONS_DATA_TYPE = DataType.string
 
 
 class MLFlowDefaultParameters:
     """MLFlow default parameters"""
     DEFAULT_SAVE_MLFLOW = True
```

## azureml/automl/dnn/vision/common/model_export_utils.py

```diff
@@ -37,16 +37,14 @@
 from azureml.core.conda_dependencies import CondaDependencies
 from azureml.core.run import Run, _OfflineRun
 
 from mlflow.models.signature import ModelSignature
 from mlflow.types.schema import ColSpec, Schema
 
 
-TORCH_CU113_REPOSITORY = "https://download.pytorch.org/whl/cu113"
-
 logger = get_logger(__name__)
 
 
 def _get_scoring_method(task_type: str, is_yolo: Optional[bool] = False) -> Callable[..., None]:
     """
     Return scoring method to be used at the inference time for the input task type.
 
@@ -83,29 +81,25 @@
     """
 
     input_schema = Schema([ColSpec(MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE_DATA_TYPE,
                                    MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE)])
     if task_type in shared_constants.Tasks.ALL_IMAGE_CLASSIFICATION:
 
         output_schema = Schema([ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_DATA_TYPE,
-                                        MLFlowSchemaLiterals.OUTPUT_COLUMN_FILENAME),
-                                ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_DATA_TYPE,
                                         MLFlowSchemaLiterals.OUTPUT_COLUMN_PROBS),
                                 ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_DATA_TYPE,
                                         MLFlowSchemaLiterals.OUTPUT_COLUMN_LABELS),
                                 ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_XAI_VISUALIZATIONS_DATA_TYPE,
                                         XAIPredictionLiterals.VISUALIZATIONS_KEY_NAME),
                                 ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_XAI_ATTRIBUTIONS_DATA_TYPE,
                                         XAIPredictionLiterals.ATTRIBUTIONS_KEY_NAME)])
 
     # for object detection and instance segmentation mlflow signature remains same
     else:
         output_schema = Schema([ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_DATA_TYPE,
-                                        MLFlowSchemaLiterals.OUTPUT_COLUMN_FILENAME),
-                                ColSpec(MLFlowSchemaLiterals.OUTPUT_COLUMN_DATA_TYPE,
                                         MLFlowSchemaLiterals.OUTPUT_COLUMN_BOXES)])
 
     return ModelSignature(inputs=input_schema, outputs=output_schema)
 
 
 def prepare_model_export(run: Run, output_dir: str, task_type: str,
                          model_settings: Optional[Dict[str, Any]] = {},
@@ -221,20 +215,14 @@
     conda_deps.add_channel('pytorch')
     conda_deps.add_channel("anaconda")
 
     # Renames environment to 'project environment' instead
     # using the default generated name
     conda_deps._conda_dependencies['name'] = 'project_environment'
 
-    # TODO: replace the following hack with a systematic solution. Add extra index url to the conda_env_v_1_0_0.yml
-    # file because the torch and torchvision package versions used by the vision package as of 4/4/2023 are not on
-    # the PyPi repository. They are installed via `pip install --extra-index-url` in the Dockerfile and the
-    # conda_env_v_1_0_0.yml file must follow suit and use --extra-index-url too.
-    conda_deps.set_pip_option("--extra-index-url {}".format(TORCH_CU113_REPOSITORY))
-
     return conda_deps.serialize_to_string()
 
 
 def _get_scoring_file(run: Run, task_type: str, model_settings: Optional[Dict[str, Any]] = {},
                       is_yolo: Optional[bool] = False) -> str:
     """
     Return scoring file to be used at the inference time.
```

## azureml/automl/dnn/vision/common/utils.py

```diff
@@ -115,14 +115,17 @@
     properties_to_add = {
         RunPropertyLiterals.PIPELINE_SCORE: best_metric,
         AutoMLInferenceArtifactIDs.ModelName: model_id,
         "runTemplate": "automl_child",
         "run_algorithm": model_name
     }
     run.add_properties(properties_to_add)
+    parent_run = should_log_metrics_to_parent(run)
+    if parent_run:
+        parent_run.add_properties(properties_to_add)
 
 
 def _get_default_device() -> torch.device:
     return torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
 
 
 class AzureAutoMLSettingsStub:
@@ -668,14 +671,15 @@
     """
 
     # The pycocotools package prints all scores from coco metrics, so printing coco_metrics would be redundant.
     METRICS_EXCLUDED_FROM_PRINTING = [MetricsLiterals.COCO_METRICS]
 
     # A select set of metrics need custom logging.
     METRICS_WITH_CUSTOM_LOGGING = [
+        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD, MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD,
         MetricsLiterals.PER_LABEL_METRICS, MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS,
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD
     ]
 
     if azureml_run is None:
         raise AutoMLVisionTrainingException("Cannot log metrics to Run History since azureml_run is None",
                                             has_pii=False)
@@ -738,64 +742,107 @@
     :type metrics: Dict[str,Any]
     :param azureml_run: The run object.
     :type azureml_run: azureml.core.run
     :param class_names: List of class names
     :type class_names: List[str]
     """
 
+    UNDEFINED_VALUE = -1.0
     MISSED_CLASS_NAME = "Missed"
     NO_VALUE = "N/A"
 
-    for metric_name in metrics:
-        metric_name_stem = metric_name[:-len("_train")] if metric_name.endswith("_train") else metric_name
+    def _add_train_suffix(name: str, is_train: bool) -> str:
+        """Add relevant suffix to metric name if computed on training set."""
+        return name + ("_train" if is_train else "")
+
+    def _log_pr_curve(
+            class_name: Optional[str], precisions_per_score_threshold: Dict[float, float],
+            recalls_per_score_threshold: Dict[float, float], is_train: bool
+    ) -> None:
+        # Log under a name that includes a. the _train suffix if present in the original metric name and b. the class
+        # name if the PR curve is class specific.
+        logged_metric_name = _add_train_suffix("pr_curve", is_train) + ("" if class_name is None else "_" + class_name)
+
+        # Go through sorted score thresholds and log the corresponding precision and recall values.
+        sorted_score_thresholds = sorted(
+            set(precisions_per_score_threshold.keys()).union(recalls_per_score_threshold.keys()), reverse=True
+        )
+        for st in sorted_score_thresholds:
+            # Skip score thresholds with invalid precision or recall values.
+            p = precisions_per_score_threshold.get(st, UNDEFINED_VALUE)
+            r = recalls_per_score_threshold.get(st, UNDEFINED_VALUE)
+            if (p != UNDEFINED_VALUE) and (r != UNDEFINED_VALUE):
+                azureml_run.log_row(logged_metric_name, recall=r, precision=p, score_threshold=st)
+
+    def _log_cm(
+            confusion_matrices_per_score_threshold: Dict[float, List[List[Any]]], is_train: bool
+    ) -> None:
+        # Go through sorted score thresholds and log the corresponding confusion matrices.
+        sorted_score_thresholds = sorted(confusion_matrices_per_score_threshold.keys())
+        for st in sorted_score_thresholds:
+            # Skip empty confusion matrices.
+            cm = confusion_matrices_per_score_threshold[st]
+            if len(cm) == 0:
+                continue
+
+            # Format the confusion matrix in scikit format, ie append a row to make it square and add a class. The
+            # matrix ends up being (C+1)x(C+1), with the last class called 'Missed'.
+            fcm = {
+                "schema_type": "confusion_matrix",
+                "schema_version": "1.0.0",
+                "data": {
+                    "class_labels": class_names + [MISSED_CLASS_NAME],
+                    "matrix": cm + [[NO_VALUE for _ in range(len(cm[0]))]]
+                }
+            }
 
-        if metric_name_stem == MetricsLiterals.PER_LABEL_METRICS:
+            # Log under a name that includes a. the _train suffix if present in the original metric name and b. the
+            # score threshold.
+            logged_metric_name = _add_train_suffix("confusion_matrix", is_train) + "_score_threshold_{}".format(st)
+            azureml_run.log_confusion_matrix(logged_metric_name, fcm)
+
+    for is_train in [False, True]:
+        # Global PR curve.
+        metric_name1 = _add_train_suffix(MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD, is_train)
+        metric_name2 = _add_train_suffix(MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD, is_train)
+        if (metric_name1 in metrics) and (metric_name2 in metrics):
+            # Log table with score thresholds, precisions and recalls (global, regardless of class).
+            _log_pr_curve(None, metrics[metric_name1], metrics[metric_name2], is_train)
+
+        # Per class precision, recall, AP and PR curve.
+        metric_name = _add_train_suffix(MetricsLiterals.PER_LABEL_METRICS, is_train)
+        if metric_name in metrics:
             for class_index, class_metrics in metrics[metric_name].items():
+                # Log table with precision, recall and AP for the current class.
                 azureml_run.log_row(
                     metric_name, class_name=class_names[class_index],
                     precision=class_metrics[MetricsLiterals.PRECISION], recall=class_metrics[MetricsLiterals.RECALL],
                     average_precision=class_metrics[MetricsLiterals.AVERAGE_PRECISION]
                 )
 
-        if metric_name_stem == MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS:
+                # Log table with score thresholds, precisions and recalls for the current class.
+                _log_pr_curve(
+                    class_names[class_index], class_metrics[MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD],
+                    class_metrics[MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD], is_train
+                )
+
+        # Image level precision, recall and AP.
+        metric_name = _add_train_suffix(MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, is_train)
+        if metric_name in metrics:
             image_level_metrics = metrics[metric_name]
             azureml_run.log_row(
                 metric_name, precision=image_level_metrics[MetricsLiterals.PRECISION],
                 recall=image_level_metrics[MetricsLiterals.RECALL],
                 average_precision=image_level_metrics[MetricsLiterals.AVERAGE_PRECISION]
             )
 
-        if metric_name_stem == MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD:
-            confusion_matrices_per_score_threshold = metrics[metric_name]
-
-            # Go through sorted score thresholds and log the corresponding confusion matrices.
-            sorted_score_thresholds = sorted(confusion_matrices_per_score_threshold.keys())
-            for st in sorted_score_thresholds:
-                # Skip empty confusion matrices.
-                cm = confusion_matrices_per_score_threshold[st]
-                if len(cm) == 0:
-                    continue
-
-                # Format the confusion matrix in scikit format, ie append a row to make it square and add a class. The
-                # matrix ends up being (C+1)x(C+1), with the last class called 'Missed'.
-                fcm = {
-                    "schema_type": "confusion_matrix",
-                    "schema_version": "1.0.0",
-                    "data": {
-                        "class_labels": class_names + [MISSED_CLASS_NAME],
-                        "matrix": cm + [[NO_VALUE for _ in range(len(cm[0]))]]
-                    }
-                }
-
-                # Log confusion matrix under a name that includes a. the _train suffix if present in the original
-                # metric name and b. the score threshold.
-                logged_metric_name = "confusion_matrix{}_score_threshold_{}".format(
-                    "_train" if metric_name.endswith("_train") else "", st
-                )
-                azureml_run.log_confusion_matrix(logged_metric_name, fcm)
+        # Confusion matrices.
+        metric_name = _add_train_suffix(MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD, is_train)
+        if metric_name in metrics:
+            _log_cm(metrics[metric_name], is_train)
 
 
 def log_verbose_metrics_to_rh(train_time: float, epoch_time: AverageMeter,
                               train_sys_meter: SystemMeter, valid_sys_meter: SystemMeter,
                               azureml_run: Run) -> None:
     """Logs verbose metrics to run history at the end of training.
 
@@ -1463,15 +1510,15 @@
         return
 
     stream_image_files = advanced_settings.get(SettingsLiterals.STREAM_IMAGE_FILES)
     automl_settings[SettingsLiterals.STREAM_IMAGE_FILES] = True if stream_image_files is True else False
     automl_settings[SettingsLiterals.APPLY_AUTOML_TRAIN_AUGMENTATIONS] = bool(
         advanced_settings.get(SettingsLiterals.APPLY_AUTOML_TRAIN_AUGMENTATIONS, True))
     automl_settings[SettingsLiterals.APPLY_MOSAIC_FOR_YOLO] = bool(
-        advanced_settings.get(SettingsLiterals.APPLY_MOSAIC_FOR_YOLO , True))
+        advanced_settings.get(SettingsLiterals.APPLY_MOSAIC_FOR_YOLO, True))
 
 
 def set_run_traits(azureml_run: Run, settings: Dict[str, Any]) -> None:
     """Sets traits on the AzureML run. Run traits are propagted to cold path logs. App Insights and Kusto logs are
     only stored for 30 days, but cold path logs are stored for a longer period of time. Setting traits enables
     querying run characteristics over a longer window of time.
```

## azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py

```diff
@@ -11,15 +11,15 @@
 
 import pandas as pd
 from azureml.automl.core.shared import logging_utilities
 from azureml.automl.dnn.vision.common.constants import MLFlowSchemaLiterals
 from azureml.automl.dnn.vision.common.logging_utils import get_logger
 from azureml.automl.dnn.vision.common.utils import strtobool
 from azureml.automl.dnn.vision.explainability.constants import (
-    ExplainabilityDefaults, ExplainabilityLiterals)
+    ExplainabilityDefaults, ExplainabilityLiterals, XAIPredictionLiterals)
 
 import mlflow
 
 logger = get_logger(__name__)
 
 
 class MLFlowImagesModelWrapper(mlflow.pyfunc.PythonModel):
@@ -61,24 +61,59 @@
                 self._task_type, context.artifacts["model"], **self._model_settings
             )
         except Exception as e:
             logger.warning("Failed to load the the model.")
             logging_utilities.log_traceback(e, logger)
             raise
 
-    def _decode_base64_img(self, img: pd.Series) -> pd.Series:
-        """This method decodes input data from base64 string format.
+    @staticmethod
+    def _process_image(img: pd.Series) -> pd.Series:
+        """This method decodes input data from binary or base64 string format.
         https://github.com/mlflow/mlflow/blob/master/examples/flower_classifier/image_pyfunc.py
 
-        :param img: pandas series with image in base64 string format.
+        :param img: pandas series with image in binary or base64 string format.
         :type img: pd.Series
         :return: decoded image in pandas series format.
         :rtype: Pandas Series
         """
-        return pd.Series(base64.b64decode(img[0]))
+        if isinstance(img[0], bytes):
+            return pd.Series(img[0])
+        elif isinstance(img[0], str):
+            try:
+                return pd.Series(base64.b64decode(img[0]))
+            except ValueError:
+                raise ValueError("The provided image string cannot be decoded."
+                                 "Expected format is bytes or base64 string.")
+        else:
+            raise ValueError(f"Image received in {type(img[0])} format which is not supported."
+                             "Expected format is bytes or base64 string.")
+
+    @staticmethod
+    def _process_result(result: str, explain: bool) -> dict:
+        """
+        This method formats the output of the predict function
+
+        :param result: inference result as json string
+        :type result: string
+        :param explain: whether or not model explainability is requested
+        :type explain: bool
+        :return: result record to report to user
+        :rtype: dict
+        """
+        result_dict = json.loads(result)
+        keep_keys = [MLFlowSchemaLiterals.OUTPUT_COLUMN_PROBS, MLFlowSchemaLiterals.OUTPUT_COLUMN_BOXES,
+                     MLFlowSchemaLiterals.OUTPUT_COLUMN_LABELS]
+        if explain:
+            keep_keys.append(XAIPredictionLiterals.VISUALIZATIONS_KEY_NAME)
+            keep_keys.append(XAIPredictionLiterals.ATTRIBUTIONS_KEY_NAME)
+        return_dict = {}
+        for key in keep_keys:
+            if key in result_dict:
+                return_dict[key] = result_dict[key]
+        return return_dict
 
     def predict(
         self, context: mlflow.pyfunc.PythonModelContext, input_data: pd.DataFrame
     ) -> pd.DataFrame:
         """This method performs inference on the input data.
 
         :param context: Mlflow context containing artifacts that the model can use for inference.
@@ -93,21 +128,24 @@
         from azureml.automl.dnn.vision.common.model_export_utils import (
             create_temp_file, run_inference_batch)
 
         # whether the rows in dataframe are dictionaries (for xai) or base64 strings (for just scoring)
         dict_input_format = False
         # Read explainability parameters if available in dictionary at input_data first row
         xai_params = {}
-        if isinstance(input_data.loc[0, MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE], str):
+        record = input_data.loc[0, MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE]
+
+        # Acceptable inputs are only string (str) and binary (bytes)
+        if isinstance(record, str):
             try:
                 xai_params = json.loads(input_data.loc[0, MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE])
                 dict_input_format = True
             except Exception:
                 logger.info("input data format isn't for XAI.")
-        else:
+        elif not isinstance(record, bytes):  # if not str or bytes, raise error for incompatible format
             logger.info("input data format is incompatible.")
             raise ValueError("incompatible input format")
 
         def get_json_dict(x: pd.Series) -> pd.Series:
             return json.loads(x[0])[MLFlowSchemaLiterals.INPUT_IMAGE_KEY]
 
         if dict_input_format:
@@ -116,15 +154,15 @@
             input_data.loc[:, MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE] = input_data.loc[
                 :, [MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE]
             ].apply(axis=1, func=get_json_dict)
 
         # Decode the base64 image column
         decoded_images = input_data.loc[
             :, [MLFlowSchemaLiterals.INPUT_COLUMN_IMAGE]
-        ].apply(axis=1, func=self._decode_base64_img)
+        ].apply(axis=1, func=MLFlowImagesModelWrapper._process_image)
 
         model_explainability = xai_params.get(
             ExplainabilityLiterals.MODEL_EXPLAINABILITY, ExplainabilityDefaults.MODEL_EXPLAINABILITY
         )
         model_explainability = bool(strtobool(str(model_explainability)))
 
         xai_parameters = xai_params.get(ExplainabilityLiterals.XAI_PARAMETERS, {})
@@ -138,9 +176,10 @@
             result = run_inference_batch(
                 self._model,
                 image_path_list,
                 self._scoring_method,
                 model_explainability=model_explainability,
                 **xai_parameters
             )
-        process_result: Callable[[str], Any] = lambda x: json.loads(x)
-        return pd.DataFrame(map(process_result, result))
+
+        return pd.DataFrame(map(MLFlowImagesModelWrapper._process_result,
+                                result, [model_explainability] * len(result)))
```

## azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py

```diff
@@ -261,77 +261,88 @@
 
 
 def _update_with_voc_metrics(current_metrics, cumulative_per_label_metrics, voc_metrics, is_train=False):
     """
     Update the current metrics and the cumulative metrics according to the VOC metrics dictionary.
     """
 
-    MetricInformation = namedtuple("MetricInformation", ["type", "is_base", "updates_current"])
+    # Information controling how each VOC metric updates the current and cumulative metrics.
+    MetricInformation = namedtuple(
+        "MetricInformation", ["type", "group", "updates_current"], defaults=[None, None, True]
+    )
     METRIC_INFORMATION_BY_NAME = {
-        MetricsLiterals.PRECISION: MetricInformation("scalar", is_base=True, updates_current=True),
-        MetricsLiterals.RECALL: MetricInformation("scalar", is_base=True, updates_current=True),
-        MetricsLiterals.AVERAGE_PRECISION: MetricInformation("scalar", is_base=True, updates_current=False),
-        MetricsLiterals.PER_LABEL_METRICS: MetricInformation("per_label", is_base=False, updates_current=True),
-        MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: MetricInformation(
-            "image_level", is_base=False, updates_current=True
-        ),
+        MetricsLiterals.PRECISION: MetricInformation("scalar", "base"),
+        MetricsLiterals.RECALL: MetricInformation("scalar", "base"),
+        MetricsLiterals.AVERAGE_PRECISION: MetricInformation("scalar", "base", updates_current=False),
+        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: MetricInformation("dictionary", "extended_base"),
+        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: MetricInformation("dictionary", "extended_base"),
+        MetricsLiterals.PER_LABEL_METRICS: MetricInformation("per_label", "extra"),
+        MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: MetricInformation("image_level", "extra"),
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: MetricInformation(
-            "per_score_threshold", is_base=False, updates_current=True
-        )
+            "matrix_per_score_threshold", "extra"
+        ),
     }
 
-    def _alter_name(name):
+    def _add_suffix(name):
+        """Add suffix to metric name if computed on training set."""
+
         return name + ("_train" if is_train else "")
 
-    def _round(x, value_type):
-        if value_type == "scalar":
-            if isinstance(x, torch.Tensor):
-                x = x.item()
-            return round(x, 5)
+    def _select_and_round(m, m_type):
+        """Select metric fields to be logged and round their numeric values."""
+
+        if m_type == "scalar":
+            if isinstance(m, torch.Tensor):
+                m = m.item()
+            return round(m, 5)
+
+        if m_type == "dictionary":
+            return {_select_and_round(k, "scalar"): _select_and_round(v, "scalar") for k, v in m.items()}
 
-        if value_type == "per_label":
+        if m_type == "per_label":
             return {
                 label_index: {
-                    metric_name: _round(metrics[metric_name], "scalar")
+                    metric_name: _select_and_round(metrics[metric_name], metric_information.type)
                     for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items()
-                    if metric_information.is_base
+                    if metric_information.group in {"base", "extended_base"}
                 }
-                for label_index, metrics in x.items()
+                for label_index, metrics in m.items()
             }
 
-        if value_type == "image_level":
+        if m_type == "image_level":
             return {
-                metric_name: _round(metric_value, "scalar")
-                for metric_name, metric_value in x.items()
+                metric_name: _select_and_round(m[metric_name], metric_information.type)
+                for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items()
+                if metric_information.group == "base"
             }
 
-        if value_type == "per_score_threshold":
+        if m_type == "matrix_per_score_threshold":
             return {
-                _round(score_threshold, "scalar"): [
-                    [_round(column, "scalar") for column in row] for row in confusion_matrix
+                _select_and_round(score_threshold, "scalar"): [
+                    [_select_and_round(column, "scalar") for column in row] for row in confusion_matrix
                 ]
-                for score_threshold, confusion_matrix in x.items()
+                for score_threshold, confusion_matrix in m.items()
             }
 
     # Set the current metrics: precision, recall, per label, image level and confusion matrix metrics.
     for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items():
         if metric_information.updates_current and (metric_name in voc_metrics):
-            altered_metric_name = _alter_name(metric_name)
+            metric_name_with_suffix = _add_suffix(metric_name)
             metric_value = voc_metrics[metric_name]
-            current_metrics[altered_metric_name] = _round(metric_value, metric_information.type)
+            current_metrics[metric_name_with_suffix] = _select_and_round(metric_value, metric_information.type)
 
     # Update the cumulative per-label metrics. Use label index instead of label name due to pii.
-    for label_index, metrics in current_metrics[_alter_name(MetricsLiterals.PER_LABEL_METRICS)].items():
+    for label_index, metrics in current_metrics[_add_suffix(MetricsLiterals.PER_LABEL_METRICS)].items():
         # If entry does not exist, initialize to empty dictionary.
         if label_index not in cumulative_per_label_metrics:
             cumulative_per_label_metrics[label_index] = {}
 
         # Go through base metrics: precision, recall and average precision.
         for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items():
-            if metric_information.is_base:
+            if metric_information.group == "base":
                 # If entry does not exist, initialize to empty list.
                 if metric_name not in cumulative_per_label_metrics[label_index]:
                     cumulative_per_label_metrics[label_index][metric_name] = []
 
                 # Accumulate current metric value.
                 cumulative_per_label_metrics[label_index][metric_name].append(metrics[metric_name])
```

## azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py

```diff
@@ -14,14 +14,17 @@
 from azureml.automl.dnn.vision.object_detection.eval.metric_computation_utils import calculate_confusion_matrices, \
     calculate_pr_metrics, match_objects, EPSILON, UNASSIGNED
 
 
 logger = get_logger(__name__)
 
 
+# TODO: refactor `compute_metrics()` and `_evaluate_image()` to reduce complexity and to abstract the accumulation
+# parts more.
+
 class IncrementalVocEvaluator:
     """
     Incremental VOC-style evaluation for object detection and instance segmentation.
 
     Suggested flow: make new object at beginning of evaluation, call `evaluate_batch()` after each batch, and
     eventually call `compute_metrics()` to get the final evaluation results.
     Users must specify whether the task is object detection or instance segmentation, the number of classes and the
@@ -178,71 +181,79 @@
         :rtype: dict with precision, recall, mean_average_precision, per_label_metrics, image_level_binary_classifier,
             confusion_matrices_per_score_thresholds keys
         """
 
         # Initialize the per class metrics to empty.
         metrics_per_class = {}
 
+        # Initialize the number of ground truth objects across all classes.
+        num_gt_objects = 0
+
         # Initialize the lists with all labels, scores and image indexes of predicted objects.
-        all_tp_fp_labels = []
-        all_scores = []
-        all_image_indexes = []
+        all_tp_fp_labels = [np.zeros((0,), dtype=np.uint8)]
+        all_scores = [np.zeros((0,))]
+        all_image_indexes = [np.zeros((0,), dtype=np.uint32)]
 
         # Go through each class and calculate the metrics for its objects (e.g. AP).
         for c in range(self._num_classes):
             # Get the labels and scores of predicted objects across all images.
             tp_fp_labels = np.concatenate(self._tp_fp_labels_per_class[c])
             scores = np.concatenate(self._scores_per_class[c])
 
             # Calculate metrics for the objects in the current class.
             metrics_per_class[c] = calculate_pr_metrics(
                 self._num_gt_objects_per_class[c], tp_fp_labels, scores, None,
                 self._use_voc_11_point_metric, self.UNDEFINED_METRIC_VALUE
             )
 
-            if self._task_is_detection:
-                # Accumulate the per class lists with all labels, scores and image indexes of predicted objects.
-                all_tp_fp_labels.extend(self._tp_fp_labels_per_class[c])
-                all_scores.extend(self._scores_per_class[c])
-                all_image_indexes.extend(self._image_indexes_per_class[c])
+            # Accumulate the number of ground truth objects for the current class.
+            num_gt_objects += self._num_gt_objects_per_class[c]
+
+            # Accumulate the lists with labels, scores and image indexes of predicted objects for the current class.
+            all_tp_fp_labels.extend(self._tp_fp_labels_per_class[c])
+            all_scores.extend(self._scores_per_class[c])
+            all_image_indexes.extend(self._image_indexes_per_class[c])
+
+        # Calculate metrics for all objects regardless of class.
+        metrics = calculate_pr_metrics(
+            num_gt_objects, np.concatenate(all_tp_fp_labels), np.concatenate(all_scores), None,
+            self._use_voc_11_point_metric, self.UNDEFINED_METRIC_VALUE
+        )
 
-        # Calculate the mean over all classes for the last precision, last recall and AP (=>mAP) metrics.
+        # Calculate the mean over all classes for the last precision, last recall and AP (-> mAP) metrics.
         object_level_metrics = {
             MetricsLiterals.PER_LABEL_METRICS: metrics_per_class,
             MetricsLiterals.PRECISION: self._calculate_metric_mean_over_classes(
                 metrics_per_class, MetricsLiterals.PRECISION
             ),
             MetricsLiterals.RECALL: self._calculate_metric_mean_over_classes(
                 metrics_per_class, MetricsLiterals.RECALL
             ),
             MetricsLiterals.MEAN_AVERAGE_PRECISION: self._calculate_metric_mean_over_classes(
                 metrics_per_class, MetricsLiterals.AVERAGE_PRECISION
-            )
+            ),
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: metrics[MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD],
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: metrics[MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD]
         }
 
         if self._task_is_detection:
             # Image level metrics and confusion matrices for object detection.
 
             # Calculate the image level metrics.
             image_level_metrics = {
                 MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: calculate_pr_metrics(
-                    self._num_images_with_gt_objects,
-                    np.concatenate([np.zeros((0,), dtype=np.uint8)] + all_tp_fp_labels),
-                    np.concatenate([np.zeros((0,))] + all_scores),
-                    np.concatenate([np.zeros((0,), dtype=np.uint32)] + all_image_indexes),
-                    False,
-                    self.UNDEFINED_METRIC_VALUE
+                    self._num_images_with_gt_objects, np.concatenate(all_tp_fp_labels), np.concatenate(all_scores),
+                    np.concatenate(all_image_indexes), False, self.UNDEFINED_METRIC_VALUE
                 )
             }
 
             # Calculate the confusion matrices at representative scores.
             confusion_matrix_metrics = {
                 MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: calculate_confusion_matrices(
-                    self._num_gt_objects_per_class,
-                    np.concatenate([np.zeros((0, 3), dtype=np.float32)] + self._all_matched_classes_and_scores)
+                    self._num_gt_objects_per_class, np.concatenate(self._all_matched_classes_and_scores)
                 )
             }
 
         else:
             # No image level metrics or confusion matrices for instance segmentation.
             image_level_metrics = {}
             confusion_matrix_metrics = {}
@@ -423,12 +434,14 @@
         :return: Mean metric value.
         :rtype: float
         """
 
         # Get the list of values of a metric across classes.
         values = [metrics_per_class[c][metric_name] for c in range(self._num_classes)]
 
-        # Calculate the mean of valid values.
+        # Calculate the mean of valid values. If no valid values, return undefined value.
         valid_values = [v for v in values if v != self.UNDEFINED_METRIC_VALUE]
+        if len(valid_values) == 0:
+            return self.UNDEFINED_METRIC_VALUE
         average_value = sum(valid_values) / (len(valid_values) + EPSILON)
 
         return average_value
```

## azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py

```diff
@@ -12,16 +12,18 @@
 from azureml.automl.dnn.vision.common.exceptions import AutoMLVisionSystemException
 from azureml.automl.dnn.vision.common.logging_utils import get_logger
 
 
 # Codes for TP, FP, other.
 _TP_CODE, _FP_CODE, _OTHER_CODE = 1, 0, 2
 
-# Score thresholds at which to compute confusion matrices.
-_SCORE_THRESHOLDS = [(i / 10.0) for i in range(10)]
+# Score thresholds at which to compute precision-recall pairs and confusion matrices. Since confusion matrices can be
+# very large, they are computed on a smaller/coarser set of thresholds than the precision-recall pairs.
+_SCORE_THRESHOLDS_FINE = [(i / 100.0) for i in range(100)]
+_SCORE_THRESHOLDS_COARSE = [(i / 10.0) for i in range(10)]
 
 # Code for predicted objects not assigned to ground truth objects.
 UNASSIGNED = -1
 
 # Constant to avoid division by 0.
 EPSILON = 1E-9
 
@@ -177,91 +179,103 @@
                 tp_fp_labels[predicted_index] = _OTHER_CODE
 
     return tp_fp_labels, predicted_assignment
 
 
 def calculate_pr_metrics(m, tp_fp_labels, scores, image_indexes, use_voc_11_point_metric, undefined_value):
     """
-    Calculate AP, highest recall and precision at highest recall given TP/FP labels, scores and image indexes.
+    Calculate metrics related to the PR curve, eg AP.
 
-    If the image indexes are set to `None`, then the regular object level metrics are computed. If valid image indexes
-    are provided, then the image level metrics are computed.
+    Calculate average precision (area under PR curve), highest recall and precision at the highest recall, and
+    precision-recall values for fixed score thresholds from TP/FP labels, scores and image indexes.
+    If image indexes are provided, then the image level metrics are computed. If no image indexes are provided
+    (parameter set to `None`), then the regular object level metrics are computed.
 
     :param m: Number of ground truth objects/images with ground truth objects.
     :type m: int
     :param tp_fp_labels: Labels for predicted objects.
     :type tp_fp_labels: numpy.ndarray
     :param scores: Scores for predicted objects.
     :type scores: numpy.ndarray
     :param image_indexes: The indexes of the images the predicted objects belong to.
     :type image_indexes: Optional[numpy.ndarray]
     :param use_voc_11_point_metric: Whether to use the 11 point computation style.
     :type use_voc_11_point_metric: bool
     :param undefined_value: Value to use when a metric is undefined.
     :type undefined_value: float
-    :return: AP, highest recall, precision@highest recall.
-    :rtype: dict with precision, recall, AP
+    :return: AP, highest recall, precision@highest recall, precision-recall values at fixed thresholds.
+    :rtype: dict with AP, highest recall, precision@highest recall, precision-recall pairs
     """
 
     if image_indexes is None:
         # Get the number of predicted objects.
         n = len(tp_fp_labels)
     else:
         # Get the number of images with predicted objects.
         n = len(np.unique(image_indexes))
 
     # If there are no ground truth objects/images with ground truth objects and no predicted objects/images with
-    # predicted objects, AP, precision and recall are undefined.
+    # predicted objects, then the AP, precision and recall, and precision-recall pairs are undefined.
+    undefined_value_per_score_threshold = {st: undefined_value for st in _SCORE_THRESHOLDS_FINE}
     if (m == 0) and (n == 0):
         return {
             MetricsLiterals.AVERAGE_PRECISION: undefined_value,
             MetricsLiterals.PRECISION: undefined_value,
-            MetricsLiterals.RECALL: undefined_value
+            MetricsLiterals.RECALL: undefined_value,
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
         }
     # If there are no ground truth objects/images with ground truth objects but predicted objects/images with predicted
-    # objects exist, AP and recall are undefined and precision is 0.
+    # objects exist, then the AP and recall are undefined, precision is 0, and precision-recall pairs are 0-undefined.
+    zero_per_score_threshold = {st: 0.0 for st in _SCORE_THRESHOLDS_FINE}
     if m == 0:
         return {
             MetricsLiterals.AVERAGE_PRECISION: undefined_value,
             MetricsLiterals.PRECISION: 0.0,
-            MetricsLiterals.RECALL: undefined_value
+            MetricsLiterals.RECALL: undefined_value,
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: zero_per_score_threshold,
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
         }
     # If ground truth objects/images with ground truth objects exist but there are no predicted objects/images with
-    # predicted objects, AP and recall are 0 and precision is undefined.
+    # predicted objects, then the AP and recall are 0, precision is undefined, and precision-recall pairs are
+    # undefined-0.
     if n == 0:
         return {
             MetricsLiterals.AVERAGE_PRECISION: 0.0,
             MetricsLiterals.PRECISION: undefined_value,
-            MetricsLiterals.RECALL: 0.0
+            MetricsLiterals.RECALL: 0.0,
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: zero_per_score_threshold,
         }
 
-    # Get the predictions in decreasing order by score.
-    indexes_scores_decreasing = np.argsort(scores)[::-1]
-    labels_sorted_by_score_desc = tp_fp_labels[indexes_scores_decreasing]
+    # Get the indexes that sort the scores and the prediction labels sorted by scores.
+    indexes_scores_inc = np.argsort(scores)
+    indexes_scores_dec = indexes_scores_inc[::-1]
+    labels_sorted_by_score_dec = tp_fp_labels[indexes_scores_dec]
 
     if image_indexes is None:
         # Count the true positive and the false positive objects for each score threshold.
-        cum_tp = np.cumsum(labels_sorted_by_score_desc == _TP_CODE)
-        cum_fp = np.cumsum(labels_sorted_by_score_desc == _FP_CODE)
+        cum_tp = np.cumsum(labels_sorted_by_score_dec == _TP_CODE)
+        cum_fp = np.cumsum(labels_sorted_by_score_dec == _FP_CODE)
 
     else:
         # Get the image indexes in decreasing order by score.
-        image_indexes_sorted_by_score_desc = image_indexes[indexes_scores_decreasing]
+        image_indexes_sorted_by_score_dec = image_indexes[indexes_scores_dec]
 
         # Count the true positive and the false positive images for each score threshold. This has to be done in a for
         # loop with custom logic to obtain the image counts from object level information.
 
         # Initialize the counts to all zero and sets of images to empty.
         cum_tp = np.zeros((len(image_indexes),))
         cum_fp = np.zeros((len(image_indexes),))
         tp_images = set()
         fp_images = set()
 
         # Go through each score threshold and incrementally update the sets of true positive and false positive images.
-        for k, (c, i) in enumerate(zip(labels_sorted_by_score_desc, image_indexes_sorted_by_score_desc)):
+        for k, (c, i) in enumerate(zip(labels_sorted_by_score_dec, image_indexes_sorted_by_score_dec)):
             # Update the sets of true positives and false positives: if at least one true positive object exists in an
             # image, then the image becomes a true positive. If only false positive objects are present, then the image
             # is a false positive.
             if c == _TP_CODE:
                 tp_images.add(i)
                 fp_images.discard(i)
             elif c == _FP_CODE:
@@ -277,19 +291,34 @@
 
     # Calculate the area under the PR curve.
     if use_voc_11_point_metric:
         average_precision = _map_score_voc_11_point_metric(precisions, recalls)
     else:
         average_precision = _map_score_voc_auc(precisions, recalls)
 
+    # Get the precision and recall values at the thresholds in the fine set. Each threshold is binary searched in the
+    # sorted set of scores, and the precision and recall values for the score greater or equal to the threshold are
+    # used. If the threshold is greater than all the scores, the precision is set to undefined and the recall to zero.
+    threshold_indexes = len(scores) - 1 - np.searchsorted(
+        scores, _SCORE_THRESHOLDS_FINE, side="left", sorter=indexes_scores_inc
+    )
+    precisions_at_thresholds = np.concatenate((precisions, [undefined_value]))[threshold_indexes]
+    recalls_at_thresholds = np.concatenate((recalls, [0.0]))[threshold_indexes]
+
     # TODO: add F1 score.
     return {
         MetricsLiterals.AVERAGE_PRECISION: average_precision,
         MetricsLiterals.PRECISION: precisions[-1],
-        MetricsLiterals.RECALL: recalls[-1]
+        MetricsLiterals.RECALL: recalls[-1],
+        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
+            st: precisions_at_thresholds[i] for i, st in enumerate(_SCORE_THRESHOLDS_FINE)
+        },
+        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+            st: recalls_at_thresholds[i] for i, st in enumerate(_SCORE_THRESHOLDS_FINE)
+        }
     }
 
 
 def calculate_confusion_matrices(num_gt_objects_per_class, matched_classes_and_scores):
     """Calculate the confusion matrices at fixed score thresholds.
 
     The confusion matrix is of size Cx(C+1) where C is the number of classes. The element at (i,j) where j<=C
@@ -320,15 +349,15 @@
     # Sort the matches in descending order of scores.
     indexes_scores_decreasing = np.argsort(matched_classes_and_scores[:, 2])
     matched_classes_and_scores = matched_classes_and_scores[indexes_scores_decreasing, :]
 
     # Go through the score thresholds in descending order and through the matches in descending order of score.
     confusion_matrices_per_score_threshold = {}
     score_index = len(matched_classes_and_scores) - 1
-    for score_threshold in sorted(_SCORE_THRESHOLDS, reverse=True):
+    for score_threshold in sorted(_SCORE_THRESHOLDS_COARSE, reverse=True):
         # Update the confusion matrix with the matches with score >= the current score threshold.
         while score_index >= 0:
             # Get the ground truth object class, predicted object class and score.
             gt_class, predicted_class, score = matched_classes_and_scores[score_index]
             gt_class, predicted_class = int(gt_class), int(predicted_class)
 
             # The matches with scores less than the score threshold do not count towards the confusion matrix for the
```

## tests/common/run_mock.py

```diff
@@ -33,14 +33,17 @@
 
     def get_file_names(self):
         return []
 
     def download_file(self):
         return
 
+    def upload_files(self, names, paths):
+        return
+
 
 class ExperimentMock:
 
     def __init__(self, ws):
         self.workspace = ws
```

## tests/common/test_artifacts_utils.py

```diff
@@ -269,7 +269,84 @@
         _test(True, False, False)
 
         # Has checkpoint, but no best model checkpoint
         _test(True, True, False)
 
         # Has checkpoint, best model checkpoint
         _test(True, True, True)
+
+    @patch("tests.common.run_mock.RunMock.upload_files")
+    @patch("azureml.automl.dnn.vision.common.artifacts_utils.should_log_metrics_to_parent")
+    @patch("os.remove")
+    def test_upload_model_checkpoint(self, remove_file_mock, should_log_metrics_mock, upload_files_mock):
+        model_location = "train_artifacts/9_model.pt"
+
+        run = RunMock("exp")
+        pipeline_run = RunMock("exp")
+
+        # Non-pipeline run. Model upload successful
+        should_log_metrics_mock.return_value = None
+        artifacts_utils.upload_model_checkpoint(run, model_location)
+        upload_files_mock.assert_called_once_with(names=[model_location], paths=[model_location])
+        remove_file_mock.assert_called_once()
+
+        should_log_metrics_mock.reset_mock(return_value=True)
+        upload_files_mock.reset_mock()
+        remove_file_mock.reset_mock()
+
+        # Pipeline run. Model upload successful
+        should_log_metrics_mock.return_value = pipeline_run
+        artifacts_utils.upload_model_checkpoint(run, model_location)
+        upload_files_mock.assert_has_calls([call(names=[model_location], paths=[model_location]),
+                                            call(names=[model_location], paths=[model_location])])
+        remove_file_mock.assert_called_once()
+
+    @patch("tests.common.run_mock.RunMock.upload_files")
+    @patch("azureml.automl.dnn.vision.common.artifacts_utils.should_log_metrics_to_parent")
+    @patch("os.remove")
+    def test_upload_model_checkpoint_exception_handling(self, remove_file_mock, should_log_metrics_mock,
+                                                        upload_files_mock):
+        model_location = "train_artifacts/9_model.pt"
+
+        run = RunMock("exp")
+        pipeline_run = RunMock("exp")
+
+        # Non-pipeline run. Exception in model upload. model_location should not be deleted.
+        should_log_metrics_mock.return_value = None
+        upload_files_mock.side_effect = RuntimeError("dummy upload exception")
+        artifacts_utils.upload_model_checkpoint(run, model_location)
+        upload_files_mock.assert_called_once_with(names=[model_location], paths=[model_location])
+        remove_file_mock.assert_not_called()
+
+        should_log_metrics_mock.reset_mock(return_value=True)
+        upload_files_mock.reset_mock(side_effect=True)
+        remove_file_mock.reset_mock()
+
+        # Pipeline run. Model upload failed in child run and pipeline run. model_location should not be deleted.
+        should_log_metrics_mock.return_value = pipeline_run
+        upload_files_mock.side_effect = RuntimeError("dummy upload exception")
+        artifacts_utils.upload_model_checkpoint(run, model_location)
+        upload_files_mock.assert_has_calls([call(names=[model_location], paths=[model_location]),
+                                            call(names=[model_location], paths=[model_location])])
+        remove_file_mock.assert_not_called()
+
+        # Pipeline run. Model upload failed in child run, but successful in pipeline run.
+        # model_location should be deleted.
+        should_log_metrics_mock.reset_mock(return_value=True)
+        upload_files_mock.reset_mock(side_effect=True)
+        remove_file_mock.reset_mock()
+
+        should_log_metrics_mock.return_value = pipeline_run
+        upload_files_mock.side_effect = [RuntimeError("dummy upload exception"), None]
+        artifacts_utils.upload_model_checkpoint(run, model_location)
+        remove_file_mock.assert_called_once()
+
+        # Pipeline run. Model upload succeeded in child run, but failed in pipeline run.
+        # model_location should be deleted.
+        should_log_metrics_mock.reset_mock(return_value=True)
+        upload_files_mock.reset_mock(side_effect=True)
+        remove_file_mock.reset_mock()
+
+        should_log_metrics_mock.return_value = pipeline_run
+        upload_files_mock.side_effect = [None, RuntimeError("dummy upload exception")]
+        artifacts_utils.upload_model_checkpoint(run, model_location)
+        remove_file_mock.assert_called_once()
```

## tests/common/test_common_methods.py

```diff
@@ -482,33 +482,56 @@
         if 'corrupt' in image_url and not use_cv2:
             return
         assert img is None, image_url
     else:
         assert img is not None, image_url
 
 
+@patch("azureml.automl.dnn.vision.common.utils.should_log_metrics_to_parent")
 @mock.patch('azureml.automl.dnn.vision.common.utils._get_model_name')
-def test_set_train_run_properties(mock_fun):
+def test_set_train_run_properties(mock_fun, should_log_metrics_mock):
     ds_mock = DatastoreMock('some_ds')
     ws_mock = WorkspaceMock(ds_mock)
     exp_mock = ExperimentMock(ws_mock)
     run_mock = RunMock(exp_mock)
     model_name = "some_model_name"
     best_metric = 95
+
+    # Non pipeline run
+    should_log_metrics_mock.return_value = None
     _set_train_run_properties(run_mock, model_name, best_metric)
 
     run_properties = run_mock.properties
 
     mock_fun.assert_called_once_with(run_mock.id)
     assert run_properties['runTemplate'] == 'automl_child'
     assert run_properties['run_algorithm'] == model_name
     assert run_properties[RunPropertyLiterals.PIPELINE_SCORE] == best_metric
     assert run_properties[AutoMLInferenceArtifactIDs.ModelName] is not None
     assert AutoMLInferenceArtifactIDs.ModelName in run_properties
 
+    mock_fun.reset_mock()
+    should_log_metrics_mock.reset_mock(return_value=True)
+
+    # Pipeline run
+    run_mock = RunMock(exp_mock)
+    pipeline_run_mock = RunMock(exp_mock)
+    should_log_metrics_mock.return_value = pipeline_run_mock
+    _set_train_run_properties(run_mock, model_name, best_metric)
+
+    run_properties = run_mock.properties
+    pipeline_run_properties = pipeline_run_mock.properties
+    mock_fun.assert_called_once_with(run_mock.id)
+    assert run_properties == pipeline_run_properties
+    assert pipeline_run_properties['runTemplate'] == 'automl_child'
+    assert pipeline_run_properties['run_algorithm'] == model_name
+    assert pipeline_run_properties[RunPropertyLiterals.PIPELINE_SCORE] == best_metric
+    assert pipeline_run_properties[AutoMLInferenceArtifactIDs.ModelName] is not None
+    assert AutoMLInferenceArtifactIDs.ModelName in run_properties
+
 
 def test_round_numeric_values():
     assert utils.round_numeric_values({}, 3) == {}
     assert utils.round_numeric_values({"a": 1.11111}, 2)["a"] == 1.11
     assert utils.round_numeric_values({"a": 1.11111}, 3)["a"] == 1.111
     assert utils.round_numeric_values({"a": 1.11111}, 4)["a"] == 1.1111
 
@@ -998,51 +1021,90 @@
     mock_run.return_value = None
     mock_log_row.return_value = None
 
     metrics = {
         MetricsLiterals.PRECISION: 0.7,
         MetricsLiterals.RECALL: 0.8,
         MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.9,
+        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.25: 0.33, 0.5: 0.5, 0.75: 0.66},
+        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.25: 0.66, 0.5: 0.5, 0.75: 0.33},
         MetricsLiterals.PER_LABEL_METRICS: {
-            0: {"precision": 0.1, "recall": 0.2, "average_precision": 0.3},
-            1: {"precision": 0.2, "recall": 0.3, "average_precision": 0.4},
-            2: {"precision": 0.3, "recall": 0.4, "average_precision": 0.5},
+            0: {
+                "precision": 0.1, "recall": 0.2, "average_precision": 0.3,
+                "precisions_per_score_threshold": {0.1: 1.0, 0.15: 1.0},
+                "recalls_per_score_threshold": {0.1: 1.0, 0.15: 1.0},
+            },
+            1: {
+                "precision": 0.2, "recall": 0.3, "average_precision": 0.4,
+                "precisions_per_score_threshold": {0.2: 1.0, 0.25: 1.0},
+                "recalls_per_score_threshold": {0.2: 1.0, 0.25: 1.0},
+            },
+            2: {
+                "precision": 0.3, "recall": 0.4, "average_precision": 0.5,
+                "precisions_per_score_threshold": {0.3: 1.0, 0.35: 1.0},
+                "recalls_per_score_threshold": {0.3: 1.0, 0.35: 1.0},
+            },
         },
         MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
             "precision": 0.5, "recall": 0.6, "average_precision": 0.7
         },
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
             0.1: [[3, 2, 3, 2], [4, 3, 4, 3], [5, 4, 5, 4]],
             0.2: [[2, 2, 3, 3], [3, 3, 4, 4], [4, 4, 5, 5]],
             0.3: [[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]],
         }
     }
     if include_training:
+        metrics[MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD + "_train"] = {0.15: 0.33, 0.4: 0.5, 0.65: 0.66}
+        metrics[MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD + "_train"] = {0.15: 0.66, 0.4: 0.5, 0.65: 0.33}
         metrics[MetricsLiterals.PER_LABEL_METRICS + "_train"] = {
-            0: {"precision": 0.2, "recall": 0.3, "average_precision": 0.4},
-            1: {"precision": 0.3, "recall": 0.4, "average_precision": 0.5},
-            2: {"precision": 0.4, "recall": 0.5, "average_precision": 0.6},
+            0: {
+                "precision": 0.2, "recall": 0.3, "average_precision": 0.4,
+                "precisions_per_score_threshold": {0.1: 0.5, 0.15: 0.5},
+                "recalls_per_score_threshold": {0.1: 0.5, 0.15: 0.5},
+            },
+            1: {
+                "precision": 0.3, "recall": 0.4, "average_precision": 0.5,
+                "precisions_per_score_threshold": {0.2: 0.5, 0.25: 0.5},
+                "recalls_per_score_threshold": {0.2: 0.5, 0.25: 0.5}
+            },
+            2: {
+                "precision": 0.4, "recall": 0.5, "average_precision": 0.6,
+                "precisions_per_score_threshold": {0.3: 0.5, 0.35: 0.5},
+                "recalls_per_score_threshold": {0.3: 0.5, 0.35: 0.5},
+            },
         }
         metrics[MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS + "_train"] = {
             "precision": 0.6, "recall": 0.7, "average_precision": 0.8
         }
         metrics[MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD + "_train"] = {
             0.1: [[4, 3, 4, 3], [5, 4, 5, 4], [6, 5, 6, 5]],
             0.2: [[3, 3, 4, 4], [4, 4, 5, 5], [5, 5, 6, 6]],
             0.3: [[2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]],
         }
 
     utils.log_detailed_object_detection_metrics(metrics, mock_run, ["dog", "cat", "axolotl"])
 
+    global_pr_calls = [
+        call("pr_curve", recall=0.66, precision=0.33, score_threshold=0.25),
+        call("pr_curve", recall=0.5, precision=0.5, score_threshold=0.5),
+        call("pr_curve", recall=0.33, precision=0.66, score_threshold=0.75)
+    ]
     per_label_calls = [
         call(MetricsLiterals.PER_LABEL_METRICS, class_name="dog", precision=0.1, recall=0.2, average_precision=0.3),
+        call("pr_curve_dog", recall=1.0, precision=1.0, score_threshold=0.1),
+        call("pr_curve_dog", recall=1.0, precision=1.0, score_threshold=0.15),
         call(MetricsLiterals.PER_LABEL_METRICS, class_name="cat", precision=0.2, recall=0.3, average_precision=0.4),
+        call("pr_curve_cat", recall=1.0, precision=1.0, score_threshold=0.2),
+        call("pr_curve_cat", recall=1.0, precision=1.0, score_threshold=0.25),
         call(
             MetricsLiterals.PER_LABEL_METRICS, class_name="axolotl", precision=0.3, recall=0.4, average_precision=0.5
-        )
+        ),
+        call("pr_curve_axolotl", recall=1.0, precision=1.0, score_threshold=0.3),
+        call("pr_curve_axolotl", recall=1.0, precision=1.0, score_threshold=0.35),
     ]
     image_level_calls = [
         call(MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, precision=0.5, recall=0.6, average_precision=0.7)
     ]
     confusion_matrix_calls = [
         call(
             "confusion_matrix_score_threshold_0.1",
@@ -1075,27 +1137,38 @@
                     "class_labels": ["dog", "cat", "axolotl", "Missed"],
                     "matrix": [[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], ["N/A", "N/A", "N/A", "N/A"]]
                 }
             }
         )
     ]
     if include_training:
+        global_pr_calls += [
+            call("pr_curve_train", recall=0.66, precision=0.33, score_threshold=0.15),
+            call("pr_curve_train", recall=0.5, precision=0.5, score_threshold=0.4),
+            call("pr_curve_train", recall=0.33, precision=0.66, score_threshold=0.65)
+        ]
         per_label_calls += [
             call(
                 MetricsLiterals.PER_LABEL_METRICS + "_train",
                 class_name="dog", precision=0.2, recall=0.3, average_precision=0.4
             ),
+            call("pr_curve_train_dog", recall=0.5, precision=0.5, score_threshold=0.1),
+            call("pr_curve_train_dog", recall=0.5, precision=0.5, score_threshold=0.15),
             call(
                 MetricsLiterals.PER_LABEL_METRICS + "_train",
                 class_name="cat", precision=0.3, recall=0.4, average_precision=0.5
             ),
+            call("pr_curve_train_cat", recall=0.5, precision=0.5, score_threshold=0.2),
+            call("pr_curve_train_cat", recall=0.5, precision=0.5, score_threshold=0.25),
             call(
                 MetricsLiterals.PER_LABEL_METRICS + "_train",
                 class_name="axolotl", precision=0.4, recall=0.5, average_precision=0.6
-            )
+            ),
+            call("pr_curve_train_axolotl", recall=0.5, precision=0.5, score_threshold=0.3),
+            call("pr_curve_train_axolotl", recall=0.5, precision=0.5, score_threshold=0.35),
         ]
         image_level_calls += [
             call(
                 MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS + "_train",
                 precision=0.6, recall=0.7, average_precision=0.8
             )
         ]
@@ -1131,29 +1204,31 @@
                         "class_labels": ["dog", "cat", "axolotl", "Missed"],
                         "matrix": [[2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7], ["N/A", "N/A", "N/A", "N/A"]]
                     }
                 }
             )
         ]
 
-    mock_log_row.assert_has_calls(per_label_calls + image_level_calls, any_order=True)
+    mock_log_row.assert_has_calls(global_pr_calls + per_label_calls + image_level_calls, any_order=True)
     mock_log_confusion_matrix.assert_has_calls(confusion_matrix_calls, any_order=True)
 
 
 @mock.patch("azureml.core.run.Run.log_confusion_matrix")
 @mock.patch("azureml.core.run.Run.log_row")
 @mock.patch("azureml.core.run.Run")
 def test_detailed_object_detection_metrics_zero_classes(mock_run, mock_log_row, mock_log_confusion_matrix):
     mock_run.return_value = None
     mock_log_row.return_value = None
 
     metrics = {
-        MetricsLiterals.PRECISION: 0.0,
-        MetricsLiterals.RECALL: 0.0,
-        MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.0,
+        MetricsLiterals.PRECISION: -1.0,
+        MetricsLiterals.RECALL: -1.0,
+        MetricsLiterals.MEAN_AVERAGE_PRECISION: -1.0,
+        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {st / 100.0: -1.0 for st in range(100)},
+        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {st / 100.0: -1.0 for st in range(100)},
         MetricsLiterals.PER_LABEL_METRICS: {},
         MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
             "precision": -1.0, "recall": -1.0, "average_precision": -1.0
         },
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
             -1.0: []
         }
```

## tests/common/test_model_export_utils.py

```diff
@@ -101,15 +101,14 @@
             assert run_mock.properties == expected_properties
 
     def test_create_conda_env_file_content(self):
         run_mock = self._setup_mock_run()
         conda_file_content = model_utils._create_conda_env_file_content(run_mock)
         for package in inference.AutoMLVisionCondaPackagesList:
             assert package in conda_file_content
-        assert "--extra-index-url https://download.pytorch.org/whl/cu113" in conda_file_content
 
     @pytest.mark.parametrize('is_yolo', [True, False])
     @pytest.mark.parametrize('task_type', constants.Tasks.ALL_IMAGE)
     @pytest.mark.parametrize('model_settings', [{'test_settings': 'test_val'}, {'test_settings': None}, None])
     def test_get_scoring_file_od(self, is_yolo, task_type, model_settings):
         run_mock = self._setup_mock_run()
         score_file_content = model_utils._get_scoring_file(run_mock, task_type, model_settings, is_yolo)
```

## tests/object_detection_tests/test_incremental_voc_evaluator.py

```diff
@@ -9,29 +9,32 @@
 
 
 PRECISION, RECALL = MetricsLiterals.PRECISION, MetricsLiterals.RECALL
 AVERAGE_PRECISION, MEAN_AVERAGE_PRECISION = MetricsLiterals.AVERAGE_PRECISION, MetricsLiterals.MEAN_AVERAGE_PRECISION
 PER_LABEL_METRICS = MetricsLiterals.PER_LABEL_METRICS
 IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS
 CONFUSION_MATRICES_PER_SCORE_THRESHOLD = MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD
+PRECISIONS_PER_SCORE_THRESHOLD = MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD
+RECALLS_PER_SCORE_THRESHOLD = MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD
 
 
 def _check_metrics_keys(metrics, task_is_detection=True):
     expected_metrics_keys = {
-        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PER_LABEL_METRICS, IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS,
-        CONFUSION_MATRICES_PER_SCORE_THRESHOLD
+        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PRECISIONS_PER_SCORE_THRESHOLD, RECALLS_PER_SCORE_THRESHOLD,
+        PER_LABEL_METRICS, IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, CONFUSION_MATRICES_PER_SCORE_THRESHOLD
     } if task_is_detection else {
-        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PER_LABEL_METRICS
+        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PRECISIONS_PER_SCORE_THRESHOLD, RECALLS_PER_SCORE_THRESHOLD,
+        PER_LABEL_METRICS
     }
 
     assert set(metrics.keys()) == expected_metrics_keys
 
 
 def _check_valid_metric_value(metric_value):
-    assert (metric_value is None) or ((metric_value >= 0.0) and (metric_value <= 1.0))
+    assert (metric_value == -1.0) or ((metric_value >= 0.0) and (metric_value <= 1.0))
 
 
 def _make_random_objects(width, height, num_classes, num_boxes, is_ground_truth):
     xs = np.random.randint(0, width, size=(num_boxes, 2))
     ys = np.random.randint(0, height, size=(num_boxes, 2))
     boxes = np.concatenate(
         (
@@ -51,14 +54,21 @@
 
 def _xyxy2xywh(box):
     return [
         float(box[0]), float(box[1]), float(box[2]) - float(box[0]), float(box[3]) - float(box[1])
     ]
 
 
+def _image_level_base(metrics):
+    return {
+        k: v for k, v in metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS].items()
+        if k in {AVERAGE_PRECISION, PRECISION, RECALL}
+    }
+
+
 class TestIncrementalVocEvaluator:
     @staticmethod
     def _rle_mask_from_bbox(bbox, height, width):
         x1, y1, x2, y2 = bbox
         polygon = [[x1, y1, x2, y1, x2, y2, x1, y2, x1, y1]]
         rle_masks = masktools.convert_polygon_to_rle_masks(polygon, height, width)
         return rle_masks[0]
@@ -89,23 +99,35 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
+        assert metrics[PRECISION] == -1.0
+        assert metrics[RECALL] == -1.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
 
     def test_single_image_no_gt_one_pred(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([], dtype=bool)}]
         gt_objects_per_image = [
             {"boxes": np.zeros((0, 4)), "masks": None, "classes": np.zeros((0,)), "scores": None}
@@ -118,23 +140,36 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == -1.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
 
     def test_single_image_one_gt_no_pred1(self):
         # no predictions specified with empty prediction objects dictionary
 
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -147,23 +182,36 @@
         predicted_objects_per_image = [{}]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: -1.0, RECALL: approx(0.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == -1.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_no_pred2(self):
         # no predictions specified with prediction objects with empty boxes
 
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -181,23 +229,36 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: -1.0, RECALL: approx(0.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == -1.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_one_pred_perfect_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -213,23 +274,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 75 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_one_gt_one_pred_crowd(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([True])}]
         gt_objects_per_image = [
             {
@@ -245,23 +320,36 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == -1.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
 
     def test_single_image_one_gt_one_pred_different_class(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -277,23 +365,36 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_one_pred_insufficient_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -309,23 +410,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
+        precision_per_st = {st / 100.0: 0.0 if st <= 75 else -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_one_pred_sufficient_overlap(self):
         # Like insufficient above, but with lower IOU threshold that makes the overlap sufficient.
         ive = IncrementalVocEvaluator(True, 3, 0.25)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
@@ -342,23 +457,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 75 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_one_gt_one_pred_masks_small_overlap(self):
         ive = IncrementalVocEvaluator(False, 3, 0.1)
 
         # Two polygons roughly along the diagonals of a square.
         p1 = [0, 0, 35, 0, 200, 165, 200, 200, 165, 200, 0, 35, 0, 0]
         p2 = [200, 0, 165, 0, 0, 165, 0, 200, 35, 200, 200, 35, 200, 0]
@@ -378,23 +507,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(1.0) if st <= 50 else 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_one_gt_one_pred_masks_zero_overlap(self):
         ive = IncrementalVocEvaluator(False, 3, 0.1)
 
         # Two completely disjoint polygons, each consisting of two squares placed on a diagonal.
         # p1 p2
         # p2 p1
@@ -422,23 +565,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
+        precision_per_st = {st / 100.0: 0.0 if st <= 50 else -1.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
         assert metrics[PRECISION] == approx(0.0)
         assert metrics[RECALL] == approx(0.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_one_pred_not_clipped(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 300, "height": 300, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -454,23 +611,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
+        precision_per_st = {st / 100.0: 0.0 if st <= 75 else -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_one_pred_degenerate(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -486,23 +657,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
+        precision_per_st = {st / 100.0: 0.0 if st <= 50 else -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_two_gt_two_pred_good_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -528,23 +713,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(1.0) for st in range(100)}
+        recall_per_st = {st / 100.0: approx(1.0) if st <= 88 else approx(0.5) for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_two_gt_two_pred_good_overlap_different_class(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -570,23 +769,39 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5)},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
+        p1 = {st / 100.0: approx(1.0) for st in range(100)}
+        p2 = {st / 100.0: approx(0.5) if st <= 88 else approx(1.0) for st in range(100)}
+        recall_per_st = {st / 100.0: approx(0.5) for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.5)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p2
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_two_gt_two_pred_one_match(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -612,23 +827,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, pred_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5)},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(0.5) if st <= 75 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(0.5) if st <= 75 else 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.5)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_two_gt_two_pred_one_match_masks(self):
         ive = IncrementalVocEvaluator(False, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -654,23 +883,37 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, pred_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5)},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(0.5) if st <= 75 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(0.5) if st <= 75 else 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.5)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_1K_gt_1K_pred_random(self):
         ive = IncrementalVocEvaluator(True, 10, 0.5)
 
         meta_info_per_image = [{"width": 1600, "height": 1600, "iscrowd": np.array([False] * 1000)}]
 
         xs, ys = np.random.randint(0, 1600, size=(1000, 2)), np.random.randint(0, 1600, size=(1000, 2))
@@ -697,18 +940,26 @@
         for i in range(10):
             assert i in metrics[PER_LABEL_METRICS]
             m = metrics[PER_LABEL_METRICS][i]
 
             _check_valid_metric_value(m[AVERAGE_PRECISION])
             _check_valid_metric_value(m[PRECISION])
             _check_valid_metric_value(m[RECALL])
+            for precision_per_st in m[PRECISIONS_PER_SCORE_THRESHOLD].values():
+                _check_valid_metric_value(precision_per_st)
+            for recall_per_st in m[RECALLS_PER_SCORE_THRESHOLD].values():
+                _check_valid_metric_value(recall_per_st)
 
         _check_valid_metric_value(metrics[MEAN_AVERAGE_PRECISION])
         _check_valid_metric_value(metrics[PRECISION])
         _check_valid_metric_value(metrics[RECALL])
+        for precision_per_st in metrics[PRECISIONS_PER_SCORE_THRESHOLD].values():
+            _check_valid_metric_value(precision_per_st)
+        for recall_per_st in metrics[RECALLS_PER_SCORE_THRESHOLD].values():
+            _check_valid_metric_value(recall_per_st)
 
     def test_multi_image_one_gt_one_pred_good_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 480, "iscrowd": np.array([False])},
             {"width": 1280, "height": 960, "iscrowd": np.array([False])}
@@ -747,23 +998,36 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        o = {st / 100.0: approx(1.0) for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: o, RECALLS_PER_SCORE_THRESHOLD: o
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: o, RECALLS_PER_SCORE_THRESHOLD: o
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == o
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == o
 
     def test_multi_image_one_gt_one_pred_different_class(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 480, "iscrowd": np.array([False])},
             {"width": 1280, "height": 960, "iscrowd": np.array([False])}
@@ -810,23 +1074,36 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0},
-            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            1: {
+                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_multi_image_two_gt_two_pred_perfect_match(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 400, "height": 500, "iscrowd": np.array([False, False])},
             {"width": 200, "height": 300, "iscrowd": np.array([False, False])}
@@ -877,23 +1154,49 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+        p0 = {st / 100.0: approx(1.0) if st <= 80 else -1.0 for st in range(100)}
+        r0 = {st / 100.0: approx(1.0) if st <= 80 else 0.0 for st in range(100)}
+        p1 = {st / 100.0: approx(1.0) if st <= 90 else -1.0 for st in range(100)}
+        r1 = {st / 100.0: approx(1.0) if st <= 90 else 0.0 for st in range(100)}
+        p2 = {st / 100.0: approx(1.0) if st <= 70 else -1.0 for st in range(100)}
+        r2 = {st / 100.0: approx(1.0) if st <= 70 else 0.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 90 else -1.0 for st in range(100)}
+        recall_per_st = {
+            st / 100.0:
+                approx(1.0) if st <= 70
+                else approx(0.75) if st <= 80
+                else approx(0.5) if st <= 90
+                else 0.0
+            for st in range(100)
+        }
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p0, RECALLS_PER_SCORE_THRESHOLD: r0
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: r1
+            },
+            2: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p2, RECALLS_PER_SCORE_THRESHOLD: r2
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_multi_image_three_gt_three_pred_single_match(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 640, "iscrowd": np.array([False, False, False])},
             {"width": 6400, "height": 6400, "iscrowd": np.array([False, False, False])},
@@ -966,23 +1269,36 @@
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
         _13 = 1.0 / 3.0
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(_13) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
-            1: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
-            2: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
+            0: {
+                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            1: {
+                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            2: {
+                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(_13)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_multi_image_three_gt_three_pred_single_match_masks(self):
         ive = IncrementalVocEvaluator(False, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 640, "iscrowd": np.array([False, False, False])},
             {"width": 6400, "height": 6400, "iscrowd": np.array([False, False, False])},
@@ -1055,23 +1371,36 @@
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
         _13 = 1.0 / 3.0
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(_13) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
-            1: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
-            2: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
+            0: {
+                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            1: {
+                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            2: {
+                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(_13)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_set_from_one_other(self):
         # First and only evaluator.
         ive1 = IncrementalVocEvaluator(True, 3, 0.5)
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -1099,25 +1428,39 @@
         ive = IncrementalVocEvaluator(True, 3, 0.5)
         ive.set_from_others([ive1])
         metrics = ive.compute_metrics()
 
         # Check combined evaluator.
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(1.0) if st <= 50 else 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
+            },
+            1: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
         cm1 = [[1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]
         cm2 = [[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0]]
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
             st: cm1 if st <= 0.5 else cm2
@@ -1184,26 +1527,53 @@
         ive = IncrementalVocEvaluator(True, 3, 0.5)
         ive.set_from_others([ive1, ive2])
         metrics = ive.compute_metrics()
 
         # Check combined evaluator.
         _check_metrics_keys(metrics)
 
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(0.5), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)},
+        p0 = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        r0 = {st / 100.0: approx(1.0) if st <= 50 else 0.0 for st in range(100)}
+        p1 = {st / 100.0: approx(0.5) if st <= 25 else approx(1.0) if st <= 75 else -1.0 for st in range(100)}
+        r1 = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
+        p2 = {st / 100.0: 0.0 for st in range(100)}
+        r2 = {st / 100.0: 0.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p0, RECALLS_PER_SCORE_THRESHOLD: r0
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(0.5), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: r1
+            },
+            2: {
+                AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p2, RECALLS_PER_SCORE_THRESHOLD: r2
+            },
         }
 
         _23 = 2.0 / 3.0
+        precision_per_st = {
+            st / 100.0:
+                approx(3.0 / 5.0) if st <= 25
+                else approx(3.0 / 4.0) if st <= 50
+                else approx(1.0 / 2.0) if st <= 75
+                else 0.0 for st in range(100)
+        }
+        recall_per_st = {
+            st / 100.0: approx(3.0 / 4.0) if st <= 50 else approx(1.0 / 4.0) if st <= 75 else 0.0 for st in range(100)
+        }
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(_23)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(_23)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
         cm1 = [[2, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]]
         cm2 = [[0, 0, 0, 2], [0, 1, 0, 0], [0, 0, 0, 1]]
         cm3 = [[0, 0, 0, 2], [0, 0, 0, 1], [0, 0, 0, 1]]
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
@@ -1288,25 +1658,43 @@
         ive = IncrementalVocEvaluator(True, 3, 0.5)
         ive.set_from_others(ives)
         metrics = ive.compute_metrics()
 
         # Check combined evaluator.
         _check_metrics_keys(metrics)
 
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        p0 = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        r0 = {st / 100.0: approx(0.5) if st <= 50 else 0.0 for st in range(100)}
+        p1 = {st / 100.0: 0.0 if st <= 50 else -1.0 for st in range(100)}
+        r1 = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5)},
-            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)},
-            2: {AVERAGE_PRECISION: approx(-1.0), PRECISION: approx(-1.0), RECALL: approx(-1.0)},
+            0: {
+                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p0, RECALLS_PER_SCORE_THRESHOLD: r0
+            },
+            1: {
+                AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: r1
+            },
+            2: {
+                AVERAGE_PRECISION: approx(-1.0), PRECISION: approx(-1.0), RECALL: approx(-1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+            },
         }
 
+        precision_per_st = {st / 100.0: approx(1.0 / 3.0) if st <= 50 else -1.0 for st in range(100)}
+        recall_per_st = {st / 100.0: approx(1.0 / 3.0) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.25)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.25)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(1.0 / 9.0), PRECISION: approx(1.0 / 3.0), RECALL: approx(1.0 / 3.0)
         }
 
         cm1 = [[5, 5, 0, 0], [0, 0, 0, 5], [0, 0, 0, 0]]
         cm2 = [[0, 0, 0, 10], [0, 0, 0, 5], [0, 0, 0, 0]]
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
             st: cm1 if st <= 0.5 else cm2
@@ -1426,15 +1814,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
     def test_single_image_one_gt_two_pred_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -1461,15 +1849,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
     def test_single_image_one_gt_one_pred_no_match_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -1487,15 +1875,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)
         }
 
     def test_two_images_multi_gt_multi_pred_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
@@ -1533,15 +1921,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(0.25), PRECISION: approx(0.5), RECALL: approx(0.5)
         }
 
     def test_four_images_multi_gt_multi_pred_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
@@ -1603,15 +1991,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(1.0 / 6.0), PRECISION: approx(0.5), RECALL: approx(1.0 / 3.0)
         }
 
     @pytest.mark.parametrize("num_classes", [0, 1])
     def test_three_images_no_gt_no_pred(self, num_classes):
         ive = IncrementalVocEvaluator(True, num_classes, 0.5)
 
@@ -1629,27 +2017,30 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        if num_classes == 0:
-            assert metrics[PER_LABEL_METRICS] == {}
-        else:
-            assert metrics[PER_LABEL_METRICS] == {
-                i: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0}
-                for i in range(num_classes)
+        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
+        assert metrics[PER_LABEL_METRICS] == {
+            i: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
             }
+            for i in range(num_classes)
+        }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
-        assert metrics[PRECISION] == approx(0.0)
-        assert metrics[RECALL] == approx(0.0)
+        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
+        assert metrics[PRECISION] == -1.0
+        assert metrics[RECALL] == -1.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0
         }
 
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
             -1.0: [] if num_classes == 0 else [[0, 0]]
         }
 
@@ -1700,15 +2091,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
+        assert _image_level_base(metrics) == {
             AVERAGE_PRECISION: approx(0.5 / 9.0), PRECISION: approx(1.0 / 9.0), RECALL: approx(0.5)
         }
 
     def test_multi_image_random_gt_pred_image_level(self):
         np.random.seed(42)
 
         num_images, num_boxes_per_image = 40_000, 25
```

## tests/object_detection_tests/test_metric_computation_utils.py

```diff
@@ -5,27 +5,69 @@
 
 from pytest import approx
 
 from azureml.automl.dnn.vision.common.constants import MetricsLiterals
 from azureml.automl.dnn.vision.common.exceptions import AutoMLVisionSystemException
 from azureml.automl.dnn.vision.object_detection.common import masktools
 from azureml.automl.dnn.vision.object_detection.eval.metric_computation_utils import _map_score_voc_11_point_metric, \
-    _map_score_voc_auc, calculate_pr_metrics, calculate_confusion_matrices, match_objects
+    _map_score_voc_auc, _SCORE_THRESHOLDS_FINE, _SCORE_THRESHOLDS_COARSE, calculate_pr_metrics, \
+    calculate_confusion_matrices, match_objects
 
 
 PRECISION, RECALL = MetricsLiterals.PRECISION, MetricsLiterals.RECALL
 AVERAGE_PRECISION, MEAN_AVERAGE_PRECISION = MetricsLiterals.AVERAGE_PRECISION, MetricsLiterals.MEAN_AVERAGE_PRECISION
+PRECISIONS_PER_SCORE_THRESHOLD = MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD
+RECALLS_PER_SCORE_THRESHOLD = MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD
 
 
 def _xyxy2xywh(box):
     return [
         float(box[0]), float(box[1]), float(box[2]) - float(box[0]), float(box[3]) - float(box[1])
     ]
 
 
+def _fill_prpst_100_points(precisions_recalls_per_score_threshold):
+    new_precisions_per_score_threshold, new_recalls_per_score_threshold = {}, {}
+
+    precisions_per_score_threshold = {st: pr[0] for st, pr in precisions_recalls_per_score_threshold.items()}
+    recalls_per_score_threshold = {st: pr[1] for st, pr in precisions_recalls_per_score_threshold.items()}
+    for i in range(100):
+        score_threshold = i / 100.0
+
+        first_greater_equal_score_threshold = None
+        for st in precisions_recalls_per_score_threshold:
+            if st >= score_threshold:
+                if (first_greater_equal_score_threshold is None) or (st < first_greater_equal_score_threshold):
+                    first_greater_equal_score_threshold = st
+
+        new_precisions_per_score_threshold[score_threshold] = precisions_per_score_threshold.get(
+            first_greater_equal_score_threshold, -1.0
+        )
+        new_recalls_per_score_threshold[score_threshold] = recalls_per_score_threshold.get(
+            first_greater_equal_score_threshold, 0.0
+        )
+
+    return new_precisions_per_score_threshold, new_recalls_per_score_threshold
+
+
+def _check_prpst_equal(
+    precisions_per_score_threshold1, recalls_per_score_threshold1,
+    precisions_per_score_threshold2, recalls_per_score_threshold2
+):
+    score_thresholds1 = set(precisions_per_score_threshold1.keys()).union(recalls_per_score_threshold1.keys())
+    score_thresholds2 = set(precisions_per_score_threshold2.keys()).union(recalls_per_score_threshold2.keys())
+    assert score_thresholds1 == score_thresholds2
+
+    for st in score_thresholds1:
+        p1, r1 = precisions_per_score_threshold1[st], recalls_per_score_threshold1[st]
+        p2, r2 = precisions_per_score_threshold2[st], recalls_per_score_threshold2[st]
+        np.testing.assert_almost_equal(p1, p2, decimal=6)
+        np.testing.assert_almost_equal(r1, r2, decimal=6)
+
+
 def _fill_cmpst_10_points(num_gt_objects_per_class, confusion_matrices_per_score_threshold):
     num_classes = len(num_gt_objects_per_class)
 
     new_confusion_matrices_per_score_threshold = {}
     for i in range(10):
         score_threshold = i / 10.0
 
@@ -298,100 +340,163 @@
         tp_fp_labels = np.array([])
         scores = np.array([])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == -1.0
         assert metrics[PRECISION] == -1.0
         assert metrics[RECALL] == -1.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
 
-    def test_pr_metrics_one_gt_two_pred(self):
+    def test_pr_metrics_one_gt_no_pred(self):
+        num_gt_boxes = 1
+        tp_fp_labels = np.array([])
+        scores = np.array([])
+
+        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
+        assert metrics[AVERAGE_PRECISION] == 0.0
+        assert metrics[PRECISION] == -1.0
+        assert metrics[RECALL] == 0.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == {st: 0.0 for st in _SCORE_THRESHOLDS_FINE}
+
+    def test_pr_metrics_no_gt_one_pred(self):
+        num_gt_boxes = 0
+        tp_fp_labels = np.array([0])
+        scores = np.array([0.3])
+
+        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
+        assert metrics[AVERAGE_PRECISION] == -1.0
+        assert metrics[PRECISION] == 0.0
+        assert metrics[RECALL] == -1.0
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == {st: 0.0 for st in _SCORE_THRESHOLDS_FINE}
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
+
+    def test_pr_metrics_one_gt_two_pred1(self):
         num_gt_boxes = 1
         tp_fp_labels = np.array([0, 1])
         scores = np.array([0.3, 0.9])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(1.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.3: [0.5, 1.0], 0.9: [1.0, 1.0]})
+        )
 
     def test_pr_metrics_one_gt_two_pred_image_level1(self):
-        num_gt_boxes = 1
+        num_images_with_gt_boxes = 1
         tp_fp_labels = np.array([0, 1])
         scores = np.array([0.3, 0.9])
         image_indexes = np.array([0, 0])
 
-        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
+        metrics = calculate_pr_metrics(num_images_with_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.3: [1.0, 1.0], 0.9: [1.0, 1.0]})
+        )
 
     def test_pr_metrics_one_gt_two_pred_image_level2(self):
-        num_gt_boxes = 1
+        num_images_with_gt_boxes = 1
         tp_fp_labels = np.array([0, 1])
         scores = np.array([0.9, 0.3])
         image_indexes = np.array([0, 1])
 
-        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
+        metrics = calculate_pr_metrics(num_images_with_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(1.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.3: [0.5, 1.0], 0.9: [0.0, 0.0]})
+        )
 
     def test_pr_metrics_two_gt_two_pred(self):
         num_gt_boxes = 2
         tp_fp_labels = np.array([1, 1])
         scores = np.array([0.3, 0.9])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.3: [1.0, 1.0], 0.9: [1.0, 0.5]})
+        )
 
     def test_pr_metrics_two_gt_two_pred_image_level(self):
-        num_gt_boxes = 2
+        num_images_with_gt_boxes = 2
         tp_fp_labels = np.array([1, 1])
         scores = np.array([0.3, 0.9])
         image_indexes = np.array([0, 1])
 
-        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
+        metrics = calculate_pr_metrics(num_images_with_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.3: [1.0, 1.0], 0.9: [1.0, 0.5]})
+        )
 
     def test_pr_metrics_four_gt_one_pred_other(self):
         num_gt_boxes = 4
         tp_fp_labels = np.array([0, 1, 1, 2])
         scores = np.array([0.4, 0.2, 0.8, 0.6])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(0.25 * 1.0 + 0.25 * (2.0 / 3.0))
         assert metrics[PRECISION] == approx(2.0 / 3.0)
         assert metrics[RECALL] == approx(0.5)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.2: [2.0 / 3.0, 0.5], 0.4: [0.5, 0.25], 0.6: [1.0, 0.25], 0.8: [1.0, 0.25]})
+        )
 
     def test_pr_metrics_two_gt_two_pred_other(self):
         num_gt_boxes = 2
         tp_fp_labels = np.array([2, 2])
         scores = np.array([0.25, 0.75])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(0.0)
         assert metrics[PRECISION] == approx(0.0)
         assert metrics[RECALL] == approx(0.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points({0.25: [0.0, 0.0], 0.75: [0.0, 0.0]})
+        )
 
     def test_pr_metrics_11_point(self):
         num_gt_boxes = 3
 
         tp_fp_labels = np.array([1, 0, 1, 0, 1])
         scores = np.array([0.1, 0.2, 0.3, 0.4, 0.5])
 
         tp_fp_labels, scores = np.array(tp_fp_labels), np.array(scores)
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, True, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx((4 * 1.0 + 3 * (2.0 / 3.0) + 4 * 0.6) / 11.0)
         assert metrics[PRECISION] == approx(0.6)
         assert metrics[RECALL] == approx(1.0)
+        _check_prpst_equal(
+            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
+            *_fill_prpst_100_points(
+                {
+                    0.1: [0.6, 1.0], 0.2: [0.5, 2.0 / 3.0], 0.3: [2.0 / 3.0, 2.0 / 3.0], 0.4: [0.5, 1.0 / 3.0],
+                    0.5: [1.0, 1.0 / 3.0]
+                }
+            )
+        )
 
     def test_confusion_matrices_diagonal(self):
         confusion_matrices_per_score_threshold = calculate_confusion_matrices(
             [1, 1, 1], np.array([[0, 0, 0.5], [1, 1, 0.6], [2, 2, 0.7]])
         )
 
         _check_cmpst_equal(confusion_matrices_per_score_threshold, _fill_cmpst_10_points(
@@ -475,7 +580,10 @@
             assert (cm.shape == cm_min.shape) and (cm.shape == cm_max.shape)
             assert (cm >= cm_min).all() and (cm <= cm_max).all()
             if cm_previous is not None:
                 assert cm.shape == cm_previous.shape
                 assert (cm[:, :-1] <= cm_previous[:, :-1]).all()
                 assert (cm[:, -1] >= cm_previous[:, -1]).all()
             cm_previous = cm
+
+    def test_score_thresholds_relation(self):
+        assert set(_SCORE_THRESHOLDS_FINE).issuperset(set(_SCORE_THRESHOLDS_COARSE))
```

## tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py

```diff
@@ -17,14 +17,16 @@
 from azureml.automl.dnn.vision.object_detection_yolo.eval.yolo_evaluator import YoloEvaluator
 from azureml.automl.dnn.vision.object_detection_yolo.utils.utils import xyxy2xywh
 from tests.common.run_mock import ObjectDetectionDatasetMock
 
 
 PRECISION, RECALL = MetricsLiterals.PRECISION, MetricsLiterals.RECALL
 AVERAGE_PRECISION, MEAN_AVERAGE_PRECISION = MetricsLiterals.AVERAGE_PRECISION, MetricsLiterals.MEAN_AVERAGE_PRECISION
+PRECISIONS_PER_SCORE_THRESHOLD = MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD
+RECALLS_PER_SCORE_THRESHOLD = MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD
 PER_LABEL_METRICS = MetricsLiterals.PER_LABEL_METRICS
 IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS
 CONFUSION_MATRICES_PER_SCORE_THRESHOLD = MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD
 
 
 def convert_to_yolo_format(predicted_objects_per_image, gt_objects_per_image, meta_info_per_image):
     yolo_predtictions = []
@@ -68,15 +70,14 @@
                       "labels": torch.tensor([1])},
                      {"areas": [60000], "iscrowd": [0], "filename": "image_1.jpg",
                       "height": 640, "width": 480,
                       "original_width": 640, "original_height": 480
                       }
                       )]
     if eval_type == ValidationMetricType.COCO:
-
         dataset = ObjectDetectionDatasetMock(dataset_items, 3)
         dataset_wrapper = CommonObjectDetectionDatasetWrapper(dataset, DatasetProcessingType.IMAGES)
         val_index_map = dataset._classes
     else:
         dataset_wrapper = None
         val_index_map = ['1', '2', '3']
 
@@ -119,23 +120,36 @@
                              evaluator.val_metric_type,
                              evaluator.coco_index, ive,
                              metrics, {},
                              evaluator.coco_metric_time, evaluator.voc_metric_time,
                              evaluator.primary_metric, is_train=False)
 
     assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
-
     if eval_type == ValidationMetricType.VOC:
+        p = {st / 100.0: approx(1.0) if st <= 75 else -1.0 for st in range(100)}
+        r = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == r
 
+        u = {st / 100.0: -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
-            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
-            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            0: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: u, RECALLS_PER_SCORE_THRESHOLD: u,
+            },
+            1: {
+                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
+            2: {
+                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
+                PRECISIONS_PER_SCORE_THRESHOLD: u, RECALLS_PER_SCORE_THRESHOLD: u,
+            },
         }
 
         assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             PRECISION: approx(1.0), RECALL: approx(1.0), AVERAGE_PRECISION: approx(1.0)
         }
 
         cm1 = [[0, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0]]
@@ -257,23 +271,36 @@
                              evaluator.val_metric_type,
                              evaluator.coco_index, ive,
                              metrics, {},
                              evaluator.coco_metric_time, evaluator.voc_metric_time,
                              evaluator.primary_metric, is_train=False)
 
     _13 = 1.0 / 3.0
-    assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13, rel=1e-1)
+    assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13, rel=1e-2)
     if eval_type == ValidationMetricType.VOC:
+        p = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        r = {st / 100.0: approx(_13, abs=1e-5) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PRECISION] == approx(1.0)
-        assert metrics[RECALL] == approx(_13, rel=1e-3)
+        assert metrics[RECALL] == approx(_13, abs=1e-5)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == r
 
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
-            1: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
-            2: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
+            0: {
+                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
+            1: {
+                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
+            2: {
+                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
         }
 
         assert IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS not in metrics
         assert CONFUSION_MATRICES_PER_SCORE_THRESHOLD not in metrics
 
 
 @pytest.mark.parametrize("dataset_processing_type",
@@ -383,21 +410,34 @@
                              metrics, {},
                              evaluator.coco_metric_time, evaluator.voc_metric_time,
                              evaluator.primary_metric, is_train=False)
 
     _13 = 1.0 / 3.0
     assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13, rel=1e-2)
     if eval_type == ValidationMetricType.VOC:
+        p = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
+        r = {st / 100.0: approx(_13, abs=1e-5) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PRECISION] == approx(1.0)
-        assert metrics[RECALL] == approx(_13, rel=1e-3)
+        assert metrics[RECALL] == approx(_13, abs=1e-5)
+        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p
+        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == r
 
         assert metrics[PER_LABEL_METRICS] == {
-            0: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
-            1: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
-            2: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
+            0: {
+                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
+            1: {
+                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
+            2: {
+                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
+                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
+            },
         }
 
         assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             PRECISION: approx(1.0), RECALL: approx(1.0), AVERAGE_PRECISION: approx(1.0)
         }
 
         cm1 = [[1, 0, 0, 2], [0, 1, 0, 2], [0, 0, 1, 2]]
```

## tests/object_detection_tests/test_object_detection_utils.py

```diff
@@ -54,24 +54,30 @@
         current_metrics = {}
         cumulative_per_label_metrics = {}
 
         voc_metrics = {
             MetricsLiterals.PRECISION: 0.35,
             MetricsLiterals.RECALL: 0.65,
             MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.5,
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.6, 0.6: 0.7, 0.8: 0.8},
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.4, 0.6: 0.3, 0.8: 0.2},
             MetricsLiterals.PER_LABEL_METRICS: {
                 0: {
                     MetricsLiterals.PRECISION: 0.3,
                     MetricsLiterals.RECALL: 0.7,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.25
+                    MetricsLiterals.AVERAGE_PRECISION: 0.25,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.2, 0.3: 0.4},
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.5, 0.3: 0.6},
                 },
                 1: {
                     MetricsLiterals.PRECISION: 0.4,
                     MetricsLiterals.RECALL: 0.6,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.75
+                    MetricsLiterals.AVERAGE_PRECISION: 0.75,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.7, 0.3: 0.8},
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.9, 0.3: 1.0},
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.7,
                 MetricsLiterals.RECALL: 0.8,
                 MetricsLiterals.AVERAGE_PRECISION: 0.6
             },
@@ -84,24 +90,30 @@
             voc_metrics = _convert_numbers_to_tensors(voc_metrics)
 
         od_utils._update_with_voc_metrics(current_metrics, cumulative_per_label_metrics, voc_metrics)
 
         assert current_metrics == {
             MetricsLiterals.PRECISION: 0.35,
             MetricsLiterals.RECALL: 0.65,
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.6, 0.6: 0.7, 0.8: 0.8},
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.4, 0.6: 0.3, 0.8: 0.2},
             MetricsLiterals.PER_LABEL_METRICS: {
                 0: {
                     MetricsLiterals.PRECISION: 0.3,
                     MetricsLiterals.RECALL: 0.7,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.25
+                    MetricsLiterals.AVERAGE_PRECISION: 0.25,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.2, 0.3: 0.4},
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.5, 0.3: 0.6},
                 },
                 1: {
                     MetricsLiterals.PRECISION: 0.4,
                     MetricsLiterals.RECALL: 0.6,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.75
+                    MetricsLiterals.AVERAGE_PRECISION: 0.75,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.7, 0.3: 0.8},
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.9, 0.3: 1.0},
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.7,
                 MetricsLiterals.RECALL: 0.8,
                 MetricsLiterals.AVERAGE_PRECISION: 0.6
             },
@@ -142,30 +154,49 @@
         if data_type == "tensors":
             cumulative_per_label_metrics = _convert_numbers_to_tensors(cumulative_per_label_metrics)
 
         voc_metrics = {
             MetricsLiterals.PRECISION: 0.4287317322877335,
             MetricsLiterals.RECALL: 0.3727672265,
             MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.609,
+            # precisions per threshold missing but no crash
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+                0.25349234: 0.89432452, 0.49456456: 0.41, 0.694356456: 0.393245, 0.80000001: 0.22
+            },
             MetricsLiterals.PER_LABEL_METRICS: {
                 1: {
                     MetricsLiterals.PRECISION: 0.12321,
                     MetricsLiterals.RECALL: 0.456,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.55
+                    MetricsLiterals.AVERAGE_PRECISION: 0.55,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
+                        0.0001: 0.12345, 0.001: 0.1234, 0.01: 0.123, 0.1: 0.12, 1.0: 0.1
+                    },
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+                        0.0001: 0.745893534534534423, 0.001: 0.5, 0.01: 0.63, 0.1: 0.631, 1.0: 0.631001
+                    },
                 },
                 2: {
                     MetricsLiterals.PRECISION: 0.734253464575467,
                     MetricsLiterals.RECALL: 0.289534453,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.668
+                    MetricsLiterals.AVERAGE_PRECISION: 0.668,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
+                        0.1: 0.9534053, 0.2: 0.09453245, 0.5: 0.853451, 0.8: 0.84354, 0.9: 1.0
+                    },
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+                        0.1: 0.13, 0.2: 0.1313, 0.5: 0.131313, 0.8: 0.13131313, 0.9: 0.131313131313
+                    },
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.7539893453945,
                 MetricsLiterals.RECALL: 0.8549328534,
-                MetricsLiterals.AVERAGE_PRECISION: 0.6235478395734
+                MetricsLiterals.AVERAGE_PRECISION: 0.6235478395734,
+                # _update_with_voc_metrics() ignores precisions and recalls per score threshold at image level.
+                MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.2: 0.75, 0.3: 0.25},
+                MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.2: 0.25, 0.3: 0.75},
             },
             MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
                 0.85647: [[4, 0, 0, 0, 12], [0, 9, 0, 0, 7], [0, 0, 9, 0, 10], [0, 0, 0, 8, 8]],
                 0.85188: [[7, 0, 0, 0, 9], [0, 9, 0, 0, 7], [0, 0, 12, 0, 7], [0, 0, 0, 8, 8]],
                 0.84831: [[8, 0, 0, 0, 8], [0, 11, 0, 0, 5], [0, 0, 14, 0, 5], [0, 0, 0, 9, 7]],
                 0.84128: [[10, 0, 0, 0, 6], [0, 12, 0, 0, 4], [0, 0, 15, 0, 4], [0, 0, 0, 11, 5]],
                 0.83179: [[10, 0, 0, 0, 6], [0, 14, 0, 0, 2], [0, 0, 17, 0, 2], [0, 0, 0, 13, 3]]
@@ -175,24 +206,39 @@
             voc_metrics = _convert_numbers_to_tensors(voc_metrics)
 
         od_utils._update_with_voc_metrics(current_metrics, cumulative_per_label_metrics, voc_metrics)
 
         assert current_metrics == {
             MetricsLiterals.PRECISION: 0.42873,
             MetricsLiterals.RECALL: 0.37277,
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+                0.25349: 0.89432, 0.49456: 0.41, 0.69436: 0.39325, 0.8: 0.22
+            },
             MetricsLiterals.PER_LABEL_METRICS: {
                 1: {
                     MetricsLiterals.PRECISION: 0.12321,
                     MetricsLiterals.RECALL: 0.456,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.55
+                    MetricsLiterals.AVERAGE_PRECISION: 0.55,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
+                        0.0001: 0.12345, 0.001: 0.1234, 0.01: 0.123, 0.1: 0.12, 1.0: 0.1
+                    },
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+                        0.0001: 0.74589, 0.001: 0.5, 0.01: 0.63, 0.1: 0.631, 1.0: 0.631
+                    }
                 },
                 2: {
                     MetricsLiterals.PRECISION: 0.73425,
                     MetricsLiterals.RECALL: 0.28953,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.668
+                    MetricsLiterals.AVERAGE_PRECISION: 0.668,
+                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
+                        0.1: 0.95341, 0.2: 0.09453, 0.5: 0.85345, 0.8: 0.84354, 0.9: 1.0
+                    },
+                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
+                        0.1: 0.13, 0.2: 0.1313, 0.5: 0.13131, 0.8: 0.13131, 0.9: 0.13131
+                    }
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.75399,
                 MetricsLiterals.RECALL: 0.85493,
                 MetricsLiterals.AVERAGE_PRECISION: 0.62355
             },
@@ -335,18 +381,32 @@
     @mock.patch(od_utils.__name__ + '.IncrementalVocEvaluator.compute_metrics')
     def test_evaluate_and_log(self, mock_incremental_voc_evaluator_compute):
         # Set up mock objects
         metrics = {
             MetricsLiterals.PRECISION: 0.7,
             MetricsLiterals.RECALL: 0.8,
             MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.9,
+            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.5, 0.9: 0.5},
+            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.9, 0.9: 0.1},
             MetricsLiterals.PER_LABEL_METRICS: {
-                1: {'precision': 0.1, 'recall': 0.2, 'average_precision': 0.3},
-                2: {'precision': 0.2, 'recall': 0.3, 'average_precision': 0.4},
-                3: {'precision': 0.3, 'recall': 0.4, 'average_precision': 0.5},
+                1: {
+                    'precision': 0.1, 'recall': 0.2, 'average_precision': 0.3,
+                    'precisions_per_score_threshold': {0.2: 0.6, 1.0: 0.6},
+                    'recalls_per_score_threshold': {0.2: 1.0, 1.0: 0.2},
+                },
+                2: {
+                    'precision': 0.2, 'recall': 0.3, 'average_precision': 0.4,
+                    'precisions_per_score_threshold': {0.1: 0.5, 0.9: 0.5},
+                    'recalls_per_score_threshold': {0.1: 0.9, 0.9: 0.1},
+                },
+                3: {
+                    'precision': 0.3, 'recall': 0.4, 'average_precision': 0.5,
+                    'precisions_per_score_threshold': {0.01: 0.5, 0.09: 0.5},
+                    'recalls_per_score_threshold': {0.01: 0.9, 0.09: 0.1},
+                },
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.2,
                 MetricsLiterals.RECALL: 0.4,
                 MetricsLiterals.AVERAGE_PRECISION: 0.6
             },
             MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
@@ -369,22 +429,28 @@
         mock_incremental_voc_evaluator_compute.assert_called_once_with()
 
         # Validate properties contain only basic metric values
         properties = mock_run.properties
         assert properties[MetricsLiterals.PRECISION] == 0.7
         assert properties[MetricsLiterals.RECALL] == 0.8
         assert properties[MetricsLiterals.MEAN_AVERAGE_PRECISION] == 0.9
+        assert MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD not in properties
+        assert MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD not in properties
+        assert MetricsLiterals.PER_LABEL_METRICS not in properties
         assert MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS not in properties
         assert MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD not in properties
 
         # Validate metrics contain only basic metric values
         metrics = mock_run.metrics
         assert metrics[MetricsLiterals.PRECISION] == 0.7
         assert metrics[MetricsLiterals.RECALL] == 0.8
         assert metrics[MetricsLiterals.MEAN_AVERAGE_PRECISION] == 0.9
+        assert MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD not in metrics
+        assert MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD not in metrics
+        assert MetricsLiterals.PER_LABEL_METRICS not in metrics
         assert MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS not in metrics
         assert MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD not in metrics
 
 
 @pytest.mark.usefixtures('new_clean_dir')
 def test_score_validation_data(monkeypatch):
     def mock_fetch_model(run_id, device, model_settings):
```

## Comparing `azureml_automl_dnn_vision-1.51.0.dist-info/LICENSE.txt` & `azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_automl_dnn_vision-1.51.0.dist-info/METADATA` & `azureml_automl_dnn_vision-1.52.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 Metadata-Version: 2.1
 Name: azureml-automl-dnn-vision
-Version: 1.51.0
+Version: 1.52.0
 Summary: AutoML DNN Vision Models
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corp
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Requires-Python: >=3.7,<3.9
 Description-Content-Type: text/x-rst
-Requires-Dist: azureml-automl-core (~=1.51.0)
-Requires-Dist: azureml-dataset-runtime (~=1.51.0)
-Requires-Dist: azureml-core (~=1.51.0)
-Requires-Dist: azureml-telemetry (~=1.51.0)
-Requires-Dist: azureml-train-automl-client (~=1.51.0)
-Requires-Dist: azureml-train-automl-runtime (~=1.51.0)
-Requires-Dist: azureml-automl-runtime (~=1.51.0)
+Requires-Dist: azureml-automl-core (~=1.52.0)
+Requires-Dist: azureml-dataset-runtime (~=1.52.0)
+Requires-Dist: azureml-core (~=1.52.0)
+Requires-Dist: azureml-telemetry (~=1.52.0)
+Requires-Dist: azureml-train-automl-client (~=1.52.0)
+Requires-Dist: azureml-train-automl-runtime (~=1.52.0)
+Requires-Dist: azureml-automl-runtime (~=1.52.0)
 Requires-Dist: numpy (<=1.22.3,>=1.18.5)
 Requires-Dist: opencv-python-headless (==4.3.0.38)
 Requires-Dist: pillow (==9.0.1)
 Requires-Dist: pretrainedmodels (==0.7.4)
 Requires-Dist: psutil (==5.8.0)
 Requires-Dist: pycocotools (==2.0.4)
 Requires-Dist: pynvml (==8.0.4)
```

## Comparing `azureml_automl_dnn_vision-1.51.0.dist-info/RECORD` & `azureml_automl_dnn_vision-1.52.0.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -24,39 +24,39 @@
 azureml/automl/dnn/vision/classification/models/classification_model_wrappers.py,sha256=15jTGeX8vno9YEn-89kpVTKDktbcqEYIJcKqea6JHQ8,30503
 azureml/automl/dnn/vision/classification/trainer/__init__.py,sha256=PM5gJjO24-O5UiQomHY-Ug5e0OoGOPdjqf3Splr7wgE,221
 azureml/automl/dnn/vision/classification/trainer/criterion.py,sha256=pQ6zsYmZ2kPPh34_FGZMNA366JrNcwGED_NEc4JacVQ,1203
 azureml/automl/dnn/vision/classification/trainer/train.py,sha256=KxS5PdgE7z1Cs6vQVTi6aCTJpc4RKTPi4pTV3pnO7sA,38271
 azureml/automl/dnn/vision/common/NOTICE,sha256=HuhflQutvMLtIxOQzjD8EUgjSySzU1hmGw6GxvxFWJQ,85645
 azureml/automl/dnn/vision/common/__init__.py,sha256=hJxcB5ZxLIaRdoof-QBNH9rbcj-WGh3Azx7QzBGLYZs,263
 azureml/automl/dnn/vision/common/aml_dataset_base_wrapper.py,sha256=V7UYkdN1d5tIUij4DWyJnG9RS-jDX6fd7YWku99X8N4,1903
-azureml/automl/dnn/vision/common/artifacts_utils.py,sha256=sJ7x5fj_9OUCuhOhwTu2r4CwfGUNuoLt8ss8rGxzWzo,18726
+azureml/automl/dnn/vision/common/artifacts_utils.py,sha256=CkNcL9VK7Ks73fEgFNX6YTCuTYO5J3cjV0wfzEfEe0E,19557
 azureml/automl/dnn/vision/common/average_meter.py,sha256=jgLAM2T4sqzcYwkqyZPonA-PzjTo22u_FkgjSXELJz4,2025
 azureml/automl/dnn/vision/common/base_model_factory.py,sha256=jqIwxJ4Uc2Yew0a6B3Ju6y1JA7LcqyZeZbVHMCj4rvo,2162
 azureml/automl/dnn/vision/common/base_model_settings.py,sha256=vjFc2VjnItGtNFI6GO_5RyY8TTh3nFMRciIM-darIws,1231
-azureml/automl/dnn/vision/common/constants.py,sha256=3SdCGBYFfeLHZJ86yy06wh_MZQnt_KldpfdfqwxxnKA,22576
+azureml/automl/dnn/vision/common/constants.py,sha256=h8GkrNtXav9dX19zJDzJly8ONUB8ZZP1OAzC_WnJmeE,22734
 azureml/automl/dnn/vision/common/data_utils.py,sha256=qdg2SmmN-0BJkaut0XtjhNGbNm7zvhabolJcnxJbDxM,2826
 azureml/automl/dnn/vision/common/dataloaders.py,sha256=k3lTVbW_1r_9zJAamCN1as8N60yS-IuwEXY4e5v8GDc,3184
 azureml/automl/dnn/vision/common/dataset_helper.py,sha256=cwZxMPnH31tL-LpmCGFX5SbX07449x3W_UXabliIDMw,18976
 azureml/automl/dnn/vision/common/distributed_utils.py,sha256=Ut6zamPMIxCcDsS3SwT4lHyulCouy9vrshRcziGdQX4,21108
 azureml/automl/dnn/vision/common/errors.py,sha256=9knV7WI1paVWHLXGTSIa-AxYMq0s2RtrrsyEXxVbqH0,672
 azureml/automl/dnn/vision/common/exceptions.py,sha256=eDxs4p3tI7JZG4Fv60rIehayqAO751xELLPGozsWuig,2379
 azureml/automl/dnn/vision/common/logging_utils.py,sha256=LldRV06kWtaI16snaGVgyH6rpMhWAnTy2SHBex0_R6A,2528
-azureml/automl/dnn/vision/common/model_export_utils.py,sha256=cXcdWg7OicZpMviy4EW8bIw9srd5zIbSFpHFA0ijbQM,25968
+azureml/automl/dnn/vision/common/model_export_utils.py,sha256=RfvOk0fDGsjX1kdZnes3myJ37dldPuKgkfgEUqVZn2I,25039
 azureml/automl/dnn/vision/common/parameters.py,sha256=kNcFXvUD_GFMRi8TK0IQIYsZEOJuCiiv0VswSiAJU1g,14730
 azureml/automl/dnn/vision/common/prediction_dataset.py,sha256=WnVeCzap2tgFWMKOJ6vIC-opIJ0r6D2pvagijuxM3vs,10208
 azureml/automl/dnn/vision/common/pretrained_model_utilities.py,sha256=64IFy7zlMVfE68VVuzJXodG99pvcL5fgwnCLShoFj_s,37122
 azureml/automl/dnn/vision/common/sku_validation.py,sha256=RW_lZFsZFqhi3U8pkP57Vr1rnl7OQquwLy4hbjTkb88,3200
 azureml/automl/dnn/vision/common/system_meter.py,sha256=yOC8fQFD7Fu7-8GI3mXUnmwnTBHaAq-uHWRD1h9Gki0,9887
 azureml/automl/dnn/vision/common/tiling_dataset_element.py,sha256=Nrwmqy5qq5mkOljjEOnlMLMzGtVP91YXbZiWIIf8OmA,5477
 azureml/automl/dnn/vision/common/tiling_utils.py,sha256=JxnOSZkv2yS47rBLzFXszzUwm-CPIrZX3T2o26nrQOg,6399
 azureml/automl/dnn/vision/common/torch_utils.py,sha256=LgCIEUaJLj4vvGRPLFzW71G1kj5BdDW4hh2yYZpzhGk,1039
 azureml/automl/dnn/vision/common/training_state.py,sha256=u_hcK_Gkqqr0bO8EzHkiwJ39gEs8hT3QPxL1ySkUpuU,2994
-azureml/automl/dnn/vision/common/utils.py,sha256=CyAuAJSDnAvv9IAQnXb_hDTPU7eWGYFeXjqflMzDzA0,70480
+azureml/automl/dnn/vision/common/utils.py,sha256=XwZQbWRam-8yTjd14OfgS1HuN2rcsxBnmk48UkxFvc4,73199
 azureml/automl/dnn/vision/common/mlflow/__init__.py,sha256=TkLaDSO2l8DaHcSUqn9IeOq1sxv9QB7l-OlT_7b_-ao,237
-azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py,sha256=NwC3rmXWybAiFnFrtlTKOoUVfWrs6NrQfC_6vds89Jc,6364
+azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py,sha256=HqfiJs0SH8TRLgh4bsjhGWOKObHBQewoTYIqyBjY4Q0,8165
 azureml/automl/dnn/vision/common/trainer/__init__.py,sha256=pDiItIKTd0YnUIgh2SBUpb-biWbwdEIEg_GctE0c3MA,272
 azureml/automl/dnn/vision/common/trainer/lrschedule.py,sha256=6RKR9lJHBW4MSbLJ78MT7tvlKsU7K0I62vMSQZzaWFg,7720
 azureml/automl/dnn/vision/common/trainer/lrschedule_parameters.py,sha256=KUAaOi2O8RYulDBfW52947R_oFN0Hb0x1EEpc_5a81Q,3336
 azureml/automl/dnn/vision/common/trainer/optimize.py,sha256=I5-u1O0gIPsq-GCaHqpXTz1xdVve7-25ERL7dZbxW10,9241
 azureml/automl/dnn/vision/common/trainer/optimize_parameters.py,sha256=NPKOxN6pBqFvflPS6fc0qW_AlGasd90kuzPzsjpwqAI,4337
 azureml/automl/dnn/vision/explainability/__init__.py,sha256=Q4SUbhQCljP04FikmdQ9puWD4P0GmnFAWgYB2hD4ioY,229
 azureml/automl/dnn/vision/explainability/constants.py,sha256=tAjWtbcQkDDd3Eg9sptMqxNJVwQaMKQ67a3Q9SKOWWQ,2638
@@ -70,29 +70,29 @@
 azureml/automl/dnn/vision/object_detection/runner.py,sha256=K9NiDpZ-zS9-VDLlKHB9STLyk4XFW5hxkCVuQilD_N8,20822
 azureml/automl/dnn/vision/object_detection/common/__init__.py,sha256=i6TcRNfQxVjsDyzq2SQCeYdOvAvIQ2LX2Y0WpSpN12w,243
 azureml/automl/dnn/vision/object_detection/common/augmentations.py,sha256=umZSLiOSsOZv17EQ9uTI1X_EpmtA4uIosHfJS_VyIzQ,9636
 azureml/automl/dnn/vision/object_detection/common/boundingbox.py,sha256=WpftxN0Nvej0_BJxHWQdPkRKUx-dCBHhUUE7vD-caGo,7356
 azureml/automl/dnn/vision/object_detection/common/coco_eval_box_converter.py,sha256=d4_qrM0zaV-gqxBlHF55QXHyYv4572a-pCs_0MACUl0,3009
 azureml/automl/dnn/vision/object_detection/common/constants.py,sha256=02XBI1JWG0jt7siuj0wk8zwHo2KuF5bcUQ-ehaH_zi8,11089
 azureml/automl/dnn/vision/object_detection/common/masktools.py,sha256=bl9jPxVuNjpG7qdUd3jkhx_xABtctk1qUqHme6CJ1m0,11945
-azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py,sha256=MXfinruUNhUghnEHiUqzQMXzGTfwKhQkRhGnS5AcwT0,27385
+azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py,sha256=Hto8NudMbilgtTKpTuMzPjGhoP4RaP_wy513GR5MIFk,28085
 azureml/automl/dnn/vision/object_detection/common/od_training_state.py,sha256=E_SMar741zRj6JCzJUW_5zL3N9lUBJHbhOFtg-chX5o,1632
 azureml/automl/dnn/vision/object_detection/common/parameters.py,sha256=ylYngnLI8Q_3FEcL2xKM3ktgwAXvhkFGvqCXqOdxmh0,4891
 azureml/automl/dnn/vision/object_detection/common/tiling_helper.py,sha256=DtAPW_H0u8zLckTj7sSLAMEUHc77UgQ67iPijgJI6pQ,26057
 azureml/automl/dnn/vision/object_detection/data/__init__.py,sha256=XEFSLiDC0lAhxmYq4MkWsVLzAO95Onn0fXbvXR0JTJ8,238
 azureml/automl/dnn/vision/object_detection/data/dataset_wrappers.py,sha256=ItAowpQ1vWrvFc6xFv-dETaaX2XvnQzZtdcQXI3m99o,4605
 azureml/automl/dnn/vision/object_detection/data/datasets.py,sha256=fhM2dkiknUa3s9TjxiI2ZPWrBoaRbdINbW1h6N5JEII,40467
 azureml/automl/dnn/vision/object_detection/data/loaders.py,sha256=QtV3k6QrJw82n2pMv_vH-D968ZOG9MYlM4S6_zLucDg,6110
 azureml/automl/dnn/vision/object_detection/data/object_annotation.py,sha256=hYYGIg3cms8J_3RFZSj0ifRt0vlk-B3Mqeo4U3Og0js,12574
 azureml/automl/dnn/vision/object_detection/data/tiling_distributed_sampler.py,sha256=SGlKInQQUeFt5mRlaHYXiVLvi_rBumRKUZ7Rzk8m46w,4985
 azureml/automl/dnn/vision/object_detection/data/utils.py,sha256=bj13WOkTUz2zn3LpkPlxY0w1Fa65cgHay698hfbIXwM,11231
 azureml/automl/dnn/vision/object_detection/eval/__init__.py,sha256=z1te76w-UN8eUJa5Fn0NRQne-cdDNU1jEs0_ARQBH9c,240
 azureml/automl/dnn/vision/object_detection/eval/cocotools.py,sha256=onTV6-iVpOsox9RSoPpMzMBIlqUBsatAKyrXJcaucQI,8380
-azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py,sha256=xTTy1bnZpPcL6nA4zDtDRJPWrmwYYFfaLl-9HD5Ftjk,21506
-azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py,sha256=ASKw0j115uGAiMDXBdCtw8utllopAHry80PFCyOOagI,17354
+azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py,sha256=5GORFlvWX1oLKLrZT-NKIXRQe73tEMYGZMhOF3lwIBY,22337
+azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py,sha256=b-J-dg9PdBv2Qi-IJ8OX-flX2yP0ns0T8HDrzmYhZKY,19785
 azureml/automl/dnn/vision/object_detection/eval/object_detection_instance_segmentation_evaluator.py,sha256=zxdt7ITVTxTl6rFUqss8cTMiHoM0v0tB4tDITD2r7hA,14609
 azureml/automl/dnn/vision/object_detection/eval/utils.py,sha256=WnlKSB8zSJFPGS5TPQJ3CwkAWZ1mW4_RZbrUJ-28Dxk,5220
 azureml/automl/dnn/vision/object_detection/models/__init__.py,sha256=NhNRu71Vk3XkfvXsrb0rJ7UeyJsi7O3iyqEoyOVVzpk,239
 azureml/automl/dnn/vision/object_detection/models/base_model_wrapper.py,sha256=S8mrdf5ePgwK4fo8FvLXNbk9dgYNCwNCBD1AwmTgnxc,14422
 azureml/automl/dnn/vision/object_detection/models/detection.py,sha256=8hDkCsCiL_aG0fYf9bRuQo0JhIz2xiu7K9RIrmfv7G4,5102
 azureml/automl/dnn/vision/object_detection/models/instance_segmentation_model_wrappers.py,sha256=EZyDm19W90CX3EtJOEWvH8HTAb_s5K3aXnmG9limWMo,14240
 azureml/automl/dnn/vision/object_detection/models/object_detection_model_wrappers.py,sha256=4oiMFOiWKpop06ZBQrXrFwCeNsXq8lEnAv1eipmgEPQ,28844
@@ -143,50 +143,51 @@
 tests/classification_tests/test_dataset_wrappers.py,sha256=WLNtO5nyEnTm9Y-BINQU1rCSNas62ScDBckaq40emF0,19599
 tests/classification_tests/test_inference_model_wrapper.py,sha256=0N55QpDc3EONBy3oeZu-4dCr3dipvQNEd1ehvsBekBI,13726
 tests/classification_tests/test_model_wrappers.py,sha256=3NOrrSm_tWs00jzLhbXZOXPaet0HxOJCcLAc7cXjkwA,19780
 tests/classification_tests/test_prediction_dataset.py,sha256=u0Qax67k7DxM5Yrtvgd60xXF2ytsx_UUQzXwiAUyCw0,11694
 tests/classification_tests/test_pretrained_model_factory.py,sha256=vOwVPR1gNbVZV9aMajwTGdTPW9gccn0Hq9OjMWpICwE,3067
 tests/common/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/common/aml_dataset_mock.py,sha256=dCJETAPmt1VLTU3z8aBWz_rdMwYqc58jQS_YjQPrVkc,1237
-tests/common/run_mock.py,sha256=4m_xk_qRBI56AIntlIOlpZCOWUgF-AN-8czkH9KODRg,5116
+tests/common/run_mock.py,sha256=fKgIrdGAmZuwFRH2NyUl7L8lFEAhzqmqkCCcUztJ0Rk,5177
 tests/common/test_aml_dataset_helper.py,sha256=3g1vnARqj4RURkdO_J-AlM7TWh1HOkXtjaCOcOmKGN4,10438
-tests/common/test_artifacts_utils.py,sha256=5inn9XGxp21olRe9W33s20Hr7LzB1R33mkMp_H3OE9o,12795
-tests/common/test_common_methods.py,sha256=Cx1cUwPG_UTfPrNlGLOcL2pOS__G_0FuCO-XZBgDQ9w,57289
+tests/common/test_artifacts_utils.py,sha256=l1wda9R3VgbBlZv6wJgQgVBm3V8a-l4INvPbvKH-MP0,16892
+tests/common/test_common_methods.py,sha256=LB1weptqv2YfPzL4PTA7CP50oTvaFnJwomZWAI-J250,61650
 tests/common/test_distributed_utils.py,sha256=cs08Um-czHe52GTLjhgHlJIbx0it6Sag83Bz30ck0Gs,16927
-tests/common/test_model_export_utils.py,sha256=7CSOjxD9suSlCIBRZgUYU8a_NMSLBc-a-dDf1Q_YHH8,10683
+tests/common/test_mlflow_model_wrapper.py,sha256=w54CRk49-v5COAmf4u85PW_oBYrKoPDKfrSA3TzhARo,4295
+tests/common/test_model_export_utils.py,sha256=KNo1Gc4xsZOQ30O1sg-0xDqM9pxeoD_E8nyx-Ejna2M,10586
 tests/common/test_pretrained_model_utilities.py,sha256=9yxWrj7IGLVkxOUSCyTKeu8XEmNgIBSxxT5cS_mibCs,6720
 tests/common/test_runner_default_args.py,sha256=K9Rhnfdaz69hl_CZP27HhfOgF_IRKIzTKNV0Yo3IcaE,6606
 tests/common/test_training_state.py,sha256=efxSLfV72e0M6bZwpbIIs4F-1dzmmoGtJqXqul82bRc,5624
 tests/common/utils.py,sha256=ZZoXTNgem_jI2G8r2wTmoLjh1nVqw5F9XIoEjR6sY7Y,6315
 tests/object_detection_tests/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/object_detection_tests/aml_dataset_mock.py,sha256=YzZ57ivEwZGO-ziZRFBDPsGy5fr1gX3nuItOFm70ffQ,430
 tests/object_detection_tests/conftest.py,sha256=1xubLF0mMfJ72UeDWyk4ShG903gHqq0lV0_zuIflALc,711
 tests/object_detection_tests/test_augmentations.py,sha256=-VAEzx5CMsDG23KXN3n3EPQcloZac-W7TbePTKoVA1I,5755
 tests/object_detection_tests/test_coco_eval_box_converter.py,sha256=fvekV1MxfGCTiJwa1LPFtFbxMtZ1EUwoNZZjsvwa4YU,4323
 tests/object_detection_tests/test_cocotools.py,sha256=3MP6QNli7XcMwTz2f2zJC9nCA_efr0mrIgS8M0xA1IE,1604
 tests/object_detection_tests/test_dataset_wrappers.py,sha256=ulH_CqDLssrbnplqC4gSQq_U2Bp8CSooaFr4LWcGJZ8,9815
 tests/object_detection_tests/test_datasets.py,sha256=Wlu6Ui0C8K_Ypw_oc-0gLdGlp6l2bYMcM30zWjjVZrA,73835
-tests/object_detection_tests/test_incremental_voc_evaluator.py,sha256=9YjRZeGdHePLeNwWsz23fTRWEQDi-Cmy-LIbqLSyL-c,78443
+tests/object_detection_tests/test_incremental_voc_evaluator.py,sha256=WP8r0tTuauYcf0jxRgdDG6tjYTAbijCmTQ8BwShawno,100617
 tests/object_detection_tests/test_maskutils.py,sha256=Nheu3leLh9DA4JRqjXAXsjimXAbc0DIvc_z0yclUDuA,6193
-tests/object_detection_tests/test_metric_computation_utils.py,sha256=xAEx-25IDbuNW0aNiVhJD9REg6sFa6Ys-aTDLbkpL7E,21122
+tests/object_detection_tests/test_metric_computation_utils.py,sha256=Nb4msRrVoA3W_Yq8UUGfIYA2FDXlZWs5Vhk0cchv5vQ,26704
 tests/object_detection_tests/test_model_wrappers.py,sha256=Qj8Lalw4Y1rcy6jfpNCrlX6Y8InVbiVREQuVl5huCtQ,15744
 tests/object_detection_tests/test_object_annotation.py,sha256=sLAmwHX48qVJR-aOqVtncGE7o_gD-sk3KmXAYTl3Dgs,14068
-tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py,sha256=ySWdOn7-JJwgs53caYxqYvv7cVM9YjsoGRmgOYZAMDw,18676
+tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py,sha256=5VuoGzM3yTVM3S04fLfbBom-49GZqsivOqvBiqfmkAg,20800
 tests/object_detection_tests/test_object_detection_local_run.py,sha256=Z1KR2YIYrs578bvdCW8e59vvcn__-Jxx4JnuJTD5_mA,4296
-tests/object_detection_tests/test_object_detection_utils.py,sha256=o2X8gd80VZopMSdRs0eR-bHi8OeWrg435-anCWCZnO4,21358
+tests/object_detection_tests/test_object_detection_utils.py,sha256=ZaVgEipD2reJiQnI6pBXcLEXO2LmWs5PE9ZpjCXpEIw,25852
 tests/object_detection_tests/test_pretrained_model_factory.py,sha256=HJn3YXbiYIQvVjTf88t8WwGFQO20-RGbSIPygjgEQaY,2728
 tests/object_detection_tests/test_scoring.py,sha256=Wa_MGonEjrw5NhaaEiZB0SMhNqXf8xHm0koOoLLPCiI,5550
 tests/object_detection_tests/test_secondary_model_wrappers.py,sha256=Yuj5Zq1r3hAfxL_eb8M-nEUtVQGuSBY90AnUwrpgnmw,5677
 tests/object_detection_tests/test_tiling_distributed_sampler.py,sha256=t8GN8eenYvDjQ6dXDUgC3HR4xsXfTPzOJzsJTQQ3thM,5050
 tests/object_detection_tests/test_tiling_helper.py,sha256=BTkNsHimEWquGlsm-CvGX196vaGp0ViR8RwICVWIluU,49608
 tests/object_detection_tests/test_tiling_utils.py,sha256=wLE0JZLO6y08JB4rpp4gH0A4MMCGXfURFgGpXtgHEvs,9531
 tests/object_detection_tests/test_trainer_criterion.py,sha256=KqsUiG5OhcJcLOQSfpXmMcnWsu3aqJZ5bx72d_fxhbA,1280
 tests/object_detection_tests/test_yolo_trainer_train.py,sha256=D4R3C--NmdYLWBO-KFVBErl5VAmM2FreIcFrt3Quet8,4133
 tests/object_detection_tests/test_yolo_utils.py,sha256=2bi7M2Ty7300-pYX6KjbcrfFEqmWhnPoVtNtTRAsez0,3710
 tests/object_detection_tests/test_yolo_wrapper.py,sha256=AnUUAoT6julWm0AT0gM2GTuktDGB16kGwv4fcCER39I,773
 tests/object_detection_tests/utils.py,sha256=k4b7GmBViQFEi_LP3JFMa9kCmfeqEtD3y8oLHa9tH6o,1088
-azureml_automl_dnn_vision-1.51.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_automl_dnn_vision-1.51.0.dist-info/METADATA,sha256=XDb0xpZflaTbpPHGvwDPRfVIVxIE3oWncULRGkinnaE,1932
-azureml_automl_dnn_vision-1.51.0.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_automl_dnn_vision-1.51.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-azureml_automl_dnn_vision-1.51.0.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
-azureml_automl_dnn_vision-1.51.0.dist-info/RECORD,,
+azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_automl_dnn_vision-1.52.0.dist-info/METADATA,sha256=cXr2VSZytgythPDc--Dge-4rxhEFQ2d1RH9QeFalrgc,1932
+azureml_automl_dnn_vision-1.52.0.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_automl_dnn_vision-1.52.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+azureml_automl_dnn_vision-1.52.0.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
+azureml_automl_dnn_vision-1.52.0.dist-info/RECORD,,
```


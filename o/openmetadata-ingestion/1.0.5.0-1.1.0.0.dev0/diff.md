# Comparing `tmp/openmetadata-ingestion-1.0.5.0.tar.gz` & `tmp/openmetadata-ingestion-1.1.0.0.dev0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "openmetadata-ingestion-1.0.5.0.tar", last modified: Mon Jun 19 13:23:07 2023, max compression
+gzip compressed data, was "openmetadata-ingestion-1.1.0.0.dev0.tar", last modified: Mon Jun 26 12:17:36 2023, max compression
```

## Comparing `openmetadata-ingestion-1.0.5.0.tar` & `openmetadata-ingestion-1.1.0.0.dev0.tar`

### file list

```diff
@@ -1,1338 +1,1435 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.035946 openmetadata-ingestion-1.0.5.0/
--rw-r--r--   0 runner    (1001) docker     (123)    11356 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     2549 2023-06-19 13:23:07.035946 openmetadata-ingestion-1.0.5.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)      636 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      809 2023-06-19 13:23:07.035946 openmetadata-ingestion-1.0.5.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     9849 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.867942 openmetadata-ingestion-1.0.5.0/src/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.867942 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/
--rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.867942 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3561 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/hooks/openmetadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.867942 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3273 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/callback.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.867942 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/commons.py
--rw-r--r--   0 runner    (1001) docker     (123)     3938 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     4223 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/providers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2958 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/operator.py
--rw-r--r--   0 runner    (1001) docker     (123)    12899 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     4200 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/status.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      700 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2710 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/__version__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/antlr/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/antlr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1854 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/antlr/split_listener.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/automations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2349 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/automations/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/cli/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6524 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/backup.py
--rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/dataquality.py
--rw-r--r--   0 runner    (1001) docker     (123)     6215 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/db_dump.py
--rw-r--r--   0 runner    (1001) docker     (123)    13593 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/docker.py
--rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/ingest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1652 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/insight.py
--rw-r--r--   0 runner    (1001) docker     (123)     4451 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/openmetadata_dag_config_migration.py
--rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/openmetadata_imports_migration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1675 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/profile.py
--rw-r--r--   0 runner    (1001) docker     (123)     2882 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/restore.py
--rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cli/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/clients/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/clients/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6139 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/clients/aws_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     5201 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/clients/domo_client.py
--rw-r--r--   0 runner    (1001) docker     (123)    16228 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/cmd.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/config/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3222 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/config/common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13196 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.871942 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/helper/
--rw-r--r--   0 runner    (1001) docker     (123)      930 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/helper/data_insight_es_index.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2289 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/data_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     8331 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/entity_report_data_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)    13522 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/web_analytic_report_data_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/runner/
--rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/runner/kpi_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     5130 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/runner/run_result_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3708 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/api/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    24326 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/pandas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3894 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/sqlalchemy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6413 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     1673 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/test_suite_protocol.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/runner/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/runner/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1656 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/runner/core.py
--rw-r--r--   0 runner    (1001) docker     (123)      874 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/runner/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/base_test_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.875942 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.879943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3054 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2682 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2693 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2682 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2714 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     3966 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     2680 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     4510 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2808 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     3013 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (123)     3107 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (123)     2907 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (123)     2956 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.879943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1782 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1771 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1751 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1855 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1793 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1753 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.883943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1770 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1749 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1731 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1750 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1852 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1717 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1725 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (123)     2200 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.883943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/mixins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2890 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.883943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.883943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2313 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2302 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     2437 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (123)     3180 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     2677 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2419 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     3505 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.887943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1435 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1280 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     3095 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.887943 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     1427 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (123)     1475 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1369 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1289 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     2444 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)      954 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/validator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.851942 openmetadata-ingestion-1.0.5.0/src/metadata/examples/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.895943 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/
--rw-r--r--   0 runner    (1001) docker     (123)      353 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/airbyte.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      568 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/airflow.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/amundsen.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      557 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/athena.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      338 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/athena_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      460 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/athena_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      606 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/atlas.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      601 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/azuresql.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      894 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/bigquery.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/bigquery_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1336 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/bigquery_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      998 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/bigquery_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      553 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/clickhouse.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      325 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/clickhouse_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/clickhouse_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      378 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/dagster.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      413 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/data_insight.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      531 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/databricks.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      325 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/databricks_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      482 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/databricks_pipeline.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      627 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/databricks_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      607 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/datalake.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/datalake_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      436 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/db2.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/db2_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2136 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/dbt.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/deltalake.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      547 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/domodashboard.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      616 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/dynamodb.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/fivetran.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      506 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/glue.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      523 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/gluepipeline.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      367 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/hive.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      423 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/impala.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/kafka.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      415 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/kinesis.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      306 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/ldap_user_to_catalog.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      471 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/looker.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      455 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mariadb.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      452 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/metabase.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      699 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/migrate_source.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mlflow.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      744 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mode.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mssql.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      315 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mssql_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      594 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mssql_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      507 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mysql.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mysql_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      530 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/openmetadata.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      448 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/oracle.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      491 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/pinotdb.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      443 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/postgres.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      343 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/postgres_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      630 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/postgres_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      826 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/powerbi.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/presto.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      707 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/query_log_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      628 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/quicksight.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/redash.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      488 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/redpanda.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/redshift.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      319 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/redshift_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/redshift_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      646 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/redshift_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      481 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/sagemaker.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      505 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/salesforce.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/singlestore.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      676 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/snowflake.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      317 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/snowflake_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      620 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/snowflake_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      423 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/sqlite.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      651 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/superset.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      885 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/tableau.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      874 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/test_suite.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      547 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/trino.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      435 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/vertica.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.851942 openmetadata-ingestion-1.0.5.0/src/metadata/generated/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/
--rw-r--r--   0 runner    (1001) docker     (123)     8889 2023-06-19 13:22:49.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/EntityLinkLexer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-06-19 13:22:49.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/EntityLinkListener.py
--rw-r--r--   0 runner    (1001) docker     (123)     9288 2023-06-19 13:22:49.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/EntityLinkParser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2606 2023-06-19 13:22:49.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/FqnLexer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1210 2023-06-19 13:22:49.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/FqnListener.py
--rw-r--r--   0 runner    (1001) docker     (123)     6811 2023-06-19 13:22:49.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/FqnParser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.859942 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      948 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportData.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1263 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
--rw-r--r--   0 runner    (1001) docker     (123)     1094 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py
--rw-r--r--   0 runner    (1001) docker     (123)      958 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py
--rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEvent.py
--rw-r--r--   0 runner    (1001) docker     (123)      883 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventData.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventType/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventType/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      978 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py
--rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/analytics/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/analytics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.899943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/automations/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1483 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/automations/createWorkflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.903943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/createClassification.py
--rw-r--r--   0 runner    (1001) docker     (123)     1754 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/createTag.py
--rw-r--r--   0 runner    (1001) docker     (123)      525 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/loadTags.py
--rw-r--r--   0 runner    (1001) docker     (123)      959 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/createBot.py
--rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/createEventPublisherJob.py
--rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/createType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.903943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1350 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createChart.py
--rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createContainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDashboard.py
--rw-r--r--   0 runner    (1001) docker     (123)     1775 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDashboardDataModel.py
--rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDatabase.py
--rw-r--r--   0 runner    (1001) docker     (123)     1252 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDatabaseSchema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1541 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createGlossary.py
--rw-r--r--   0 runner    (1001) docker     (123)     2402 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createGlossaryTerm.py
--rw-r--r--   0 runner    (1001) docker     (123)     2446 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createMlModel.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createTable.py
--rw-r--r--   0 runner    (1001) docker     (123)      727 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createTableProfile.py
--rw-r--r--   0 runner    (1001) docker     (123)     2713 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createTopic.py
--rw-r--r--   0 runner    (1001) docker     (123)      424 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/restoreEntity.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.903943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.903943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/kpi/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.903943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      401 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/closeTask.py
--rw-r--r--   0 runner    (1001) docker     (123)      526 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/createPost.py
--rw-r--r--   0 runner    (1001) docker     (123)     1692 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/createThread.py
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/resolveTask.py
--rw-r--r--   0 runner    (1001) docker     (123)      749 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/threadCount.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.907943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      579 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/lineage/addLineage.py
--rw-r--r--   0 runner    (1001) docker     (123)      672 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/openMetadataServerVersion.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.907943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/policies/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1082 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/policies/createPolicy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.907943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createDashboardService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1163 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createDatabaseService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createMessagingService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createMetadataService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1160 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createMlModelService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1171 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createPipelineService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1232 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createStorageService.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.907943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/ingestionPipelines/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)      547 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/setOwner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.907943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      830 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/createRole.py
--rw-r--r--   0 runner    (1001) docker     (123)     2469 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/createTeam.py
--rw-r--r--   0 runner    (1001) docker     (123)     2082 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/createUser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.907943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1227 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createCustomMetric.py
--rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createTestCase.py
--rw-r--r--   0 runner    (1001) docker     (123)     1159 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createTestDefinition.py
--rw-r--r--   0 runner    (1001) docker     (123)      820 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createTestSuite.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/voteRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.911943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      339 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      462 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/basicLoginRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      886 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/changePasswordRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      437 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/createPersonalToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      365 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/emailRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/emailVerificationToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      347 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/generateToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      751 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/jwtAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      409 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/loginRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      626 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/logoutRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      683 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/passwordResetRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1027 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/passwordResetToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      956 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/personalAccessToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/refreshToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      623 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/registrationRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      476 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/revokePersonalToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      324 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/revokeToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      288 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/serviceTokenEnum.py
--rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/ssoAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      378 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/tokenRefreshRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.911943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/applicationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/authConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/authenticationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/authorizerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      363 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/changeEventConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      442 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      357 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/fernetConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      658 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1823 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      894 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      462 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      978 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     2133 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      436 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      885 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/dataInsightChart.py
--rw-r--r--   0 runner    (1001) docker     (123)     1933 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/kpi/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/kpi/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     2537 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/kpi/kpi.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py
--rw-r--r--   0 runner    (1001) docker     (123)      791 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py
--rw-r--r--   0 runner    (1001) docker     (123)      635 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py
--rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py
--rw-r--r--   0 runner    (1001) docker     (123)      831 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py
--rw-r--r--   0 runner    (1001) docker     (123)      843 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/email/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/email/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1250 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/email/emailRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1312 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/email/smtpSettings.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.915943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/automations/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/automations/testServiceConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/automations/workflow.py
--rw-r--r--   0 runner    (1001) docker     (123)     2055 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/bot.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.919943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/classification/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2864 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/classification/classification.py
--rw-r--r--   0 runner    (1001) docker     (123)     3342 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/classification/tag.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.919943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3144 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/chart.py
--rw-r--r--   0 runner    (1001) docker     (123)     4668 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/container.py
--rw-r--r--   0 runner    (1001) docker     (123)     3408 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/dashboard.py
--rw-r--r--   0 runner    (1001) docker     (123)     3335 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/dashboardDataModel.py
--rw-r--r--   0 runner    (1001) docker     (123)     3385 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/database.py
--rw-r--r--   0 runner    (1001) docker     (123)     3096 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/databaseSchema.py
--rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/glossary.py
--rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/glossaryTerm.py
--rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     6419 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/mlmodel.py
--rw-r--r--   0 runner    (1001) docker     (123)     6226 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/query.py
--rw-r--r--   0 runner    (1001) docker     (123)     2303 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    21396 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/table.py
--rw-r--r--   0 runner    (1001) docker     (123)     4555 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/topic.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.919943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/events/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/events/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1195 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/events/webhook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.919943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/feed/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/feed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5247 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/feed/thread.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.919943 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.923944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1821 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
--rw-r--r--   0 runner    (1001) docker     (123)     1497 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/rule.py
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/filters.py
--rw-r--r--   0 runner    (1001) docker     (123)     3173 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/policy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.923944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.923944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.923944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      935 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1492 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1700 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2025 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1310 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1785 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.927944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2482 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2555 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2796 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3446 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3411 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.931944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      531 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      518 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2272 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3741 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1690 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1449 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3346 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3384 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2665 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2748 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3107 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3326 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2616 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3341 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2233 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2736 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3937 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2785 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.931944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      935 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2551 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      901 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      729 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2364 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      389 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/saslMechanismType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.931944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5392 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.931944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      920 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      915 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      735 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1154 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1412 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1371 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      916 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2241 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1172 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py
--rw-r--r--   0 runner    (1001) docker     (123)      919 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1146 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1119 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2930 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py
--rw-r--r--   0 runner    (1001) docker     (123)     3955 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/dashboardService.py
--rw-r--r--   0 runner    (1001) docker     (123)     5873 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/databaseService.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/ingestionPipelines/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5984 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3623 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/messagingService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3475 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/metadataService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/mlmodelService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3973 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/pipelineService.py
--rw-r--r--   0 runner    (1001) docker     (123)      391 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/serviceType.py
--rw-r--r--   0 runner    (1001) docker     (123)     2977 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/storageService.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2564 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/role.py
--rw-r--r--   0 runner    (1001) docker     (123)     4409 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/team.py
--rw-r--r--   0 runner    (1001) docker     (123)     1755 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/teamHierarchy.py
--rw-r--r--   0 runner    (1001) docker     (123)     4058 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/user.py
--rw-r--r--   0 runner    (1001) docker     (123)     3392 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/type.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/entitiesCount.py
--rw-r--r--   0 runner    (1001) docker     (123)      951 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/servicesCount.py
--rw-r--r--   0 runner    (1001) docker     (123)      953 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.935944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/api/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1858 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/api/createEventSubscription.py
--rw-r--r--   0 runner    (1001) docker     (123)      501 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/dataInsightAlertConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/emailAlertConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      476 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/entitySpelFilters.py
--rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/eventFilterRule.py
--rw-r--r--   0 runner    (1001) docker     (123)     6298 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/eventSubscription.py
--rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.939944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2342 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)      469 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dataInsightPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1577 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1208 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.939944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      676 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
--rw-r--r--   0 runner    (1001) docker     (123)     1291 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      600 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      955 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      597 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1728 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1116 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1366 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.939944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/storage/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/storage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1361 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      797 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     5129 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.939944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/monitoring/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/monitoring/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/monitoring/eventMonitorProvider.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.939944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      579 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      700 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      636 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      669 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      757 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      494 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      677 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/accessTokenAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/awsCredentials.py
--rw-r--r--   0 runner    (1001) docker     (123)      894 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/azureCredentials.py
--rw-r--r--   0 runner    (1001) docker     (123)      589 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      822 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/gcsCredentials.py
--rw-r--r--   0 runner    (1001) docker     (123)     2136 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/gcsValues.py
--rw-r--r--   0 runner    (1001) docker     (123)     1113 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/githubCredentials.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/secrets/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/secrets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      938 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/securityConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/ssl/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/ssl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      520 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      727 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/settings/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/settings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2089 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/settings/settings.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/system/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/system/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3679 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/system/eventPublisherJob.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.943944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/customMetric.py
--rw-r--r--   0 runner    (1001) docker     (123)     2617 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/testCase.py
--rw-r--r--   0 runner    (1001) docker     (123)     3317 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/testDefinition.py
--rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/testSuite.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.947944 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/auditLog.py
--rw-r--r--   0 runner    (1001) docker     (123)     4213 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     4078 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/changeEvent.py
--rw-r--r--   0 runner    (1001) docker     (123)      870 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/collectionDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (123)      526 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvDocumentation.py
--rw-r--r--   0 runner    (1001) docker     (123)      422 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvErrorType.py
--rw-r--r--   0 runner    (1001) docker     (123)      942 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvFile.py
--rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvImportResult.py
--rw-r--r--   0 runner    (1001) docker     (123)      453 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/dailyCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1887 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/databaseConnectionConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     2457 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityHistory.py
--rw-r--r--   0 runner    (1001) docker     (123)     2938 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityLineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     1560 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityReference.py
--rw-r--r--   0 runner    (1001) docker     (123)     2337 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityRelationship.py
--rw-r--r--   0 runner    (1001) docker     (123)      579 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityUsage.py
--rw-r--r--   0 runner    (1001) docker     (123)      707 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/filterPattern.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/function.py
--rw-r--r--   0 runner    (1001) docker     (123)      262 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/include.py
--rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/jdbcConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/paging.py
--rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/profile.py
--rw-r--r--   0 runner    (1001) docker     (123)     1362 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/queryParserData.py
--rw-r--r--   0 runner    (1001) docker     (123)      739 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/reaction.py
--rw-r--r--   0 runner    (1001) docker     (123)      604 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     2580 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/tableQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/tableUsageCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/tagLabel.py
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/usageDetails.py
--rw-r--r--   0 runner    (1001) docker     (123)      413 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/usageRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      892 2023-06-19 13:22:48.000000 openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/votes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.947944 openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15214 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/action.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.947944 openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/utils/ometa_config_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.947944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.951944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/bulk_sink.py
--rw-r--r--   0 runner    (1001) docker     (123)      742 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/closeable.py
--rw-r--r--   0 runner    (1001) docker     (123)     1149 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    16436 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1937 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/sink.py
--rw-r--r--   0 runner    (1001) docker     (123)     2902 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/stage.py
--rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/status.py
--rw-r--r--   0 runner    (1001) docker     (123)     9295 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/topology_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)    12770 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.951944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/bulksink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/bulksink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12958 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/bulksink/metadata_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.951944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5040 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/builders.py
--rw-r--r--   0 runner    (1001) docker     (123)     2440 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/headers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1920 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/secrets.py
--rw-r--r--   0 runner    (1001) docker     (123)     1229 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/session.py
--rw-r--r--   0 runner    (1001) docker     (123)    13289 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/test_connections.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.951944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    16367 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    15543 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/sql_lineage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.955944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/custom_pydantic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/custom_types.py
--rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/delete_entity.py
--rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/encoders.py
--rw-r--r--   0 runner    (1001) docker     (123)     9201 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/es_documents.py
--rw-r--r--   0 runner    (1001) docker     (123)     1136 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/ometa_classification.py
--rw-r--r--   0 runner    (1001) docker     (123)      825 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/ometa_topic_data.py
--rw-r--r--   0 runner    (1001) docker     (123)      850 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/pipeline_status.py
--rw-r--r--   0 runner    (1001) docker     (123)      911 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/profile_data.py
--rw-r--r--   0 runner    (1001) docker     (123)      982 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/table_metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/tests_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     5298 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/topology.py
--rw-r--r--   0 runner    (1001) docker     (123)     1078 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/user.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.955944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14255 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/auth_provider.py
--rw-r--r--   0 runner    (1001) docker     (123)    10871 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/client_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3964 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/credentials.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.955944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     3639 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/es_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)    17774 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/glossary_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     4098 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5479 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)    13534 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/patch_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     4273 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4153 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2842 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/query_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)    15416 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2519 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/server_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2772 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/service_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     9812 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/table_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     7458 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/tests_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/topic_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2307 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/user_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/version_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1048 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    26655 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/ometa_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     4456 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/provider_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.955944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/processor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/processor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/processor/query_parser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.959944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    34025 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.959944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9905 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/container_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     8525 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1629 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     7376 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     8343 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     6018 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     8548 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/query_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)    10284 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     2474 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     4395 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     8763 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     4517 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1433 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1445 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     2219 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (123)    19119 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.959944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/connections.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.959944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16873 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/dashboard_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.959944 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/domodashboard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2167 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9578 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/columns.py
--rw-r--r--   0 runner    (1001) docker     (123)     2167 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    27746 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     4699 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/parser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5233 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1871 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    11144 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1981 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6133 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1781 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8971 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10656 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1810 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    13850 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     4151 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    12844 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      761 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2183 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2000 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    12390 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.963945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5787 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/api_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     5011 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5830 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/db_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1967 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     5890 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.967945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/
--rw-r--r--   0 runner    (1001) docker     (123)      955 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4721 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     4539 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    20239 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     4126 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     1185 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.967945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.967945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3243 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1655 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     6344 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     3307 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.967945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/azuresql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/azuresql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3044 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/azuresql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3240 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/azuresql/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.967945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5567 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1312 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    16904 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1420 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2647 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1336 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     8406 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     2301 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     3472 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/column_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)    16214 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/column_type_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    21420 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/common_db_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    14783 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/database_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7172 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    11373 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1006 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2257 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2436 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4668 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    29488 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     3807 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/db2/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/db2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1980 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/db2/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3210 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/db2/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dbt/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dbt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8519 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dbt/dbt_service.py
--rw-r--r--   0 runner    (1001) docker     (123)    39411 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dbt/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/deltalake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/deltalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5033 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/deltalake/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    14961 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/deltalake/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.971945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2177 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8608 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.975945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/druid/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/druid/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/druid/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/druid/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.975945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dynamodb/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dynamodb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dynamodb/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8901 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dynamodb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.975945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/glue/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/glue/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/glue/connection.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13385 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/glue/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.975945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3999 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    13447 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.979945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4095 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6883 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      742 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     4760 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/lineage_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.979945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mariadb/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mariadb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mariadb/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mariadb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.983945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1203 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     1742 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)      987 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     8105 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.983945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2570 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2064 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     5100 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.987945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4073 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5084 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     2540 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     8235 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.987945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/pinotdb/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/pinotdb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/pinotdb/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/pinotdb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.987945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2369 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     8781 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     5942 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     8777 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1034 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    10483 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.991945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3403 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6120 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.991945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1262 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     1498 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query_parser_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.991945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    17992 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     7553 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.991945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/salesforce/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/salesforce/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2029 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/salesforce/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    10086 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/salesforce/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)    44528 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sample_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3851 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sample_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.991945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/singlestore/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/singlestore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/singlestore/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/singlestore/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.995945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6442 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1248 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    13514 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3645 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     4088 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     7700 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    13318 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sql_column_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2970 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlalchemy_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.995945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlite/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlite/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlite/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.995945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9614 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      980 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     5727 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/usage_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.995945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1380 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    10925 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     4794 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/ldap_users.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.995945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11063 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/common_broker_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.995945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kafka/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kafka/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4626 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kafka/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kafka/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1839 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    10127 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     2464 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/messaging_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/redpanda/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/redpanda/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/redpanda/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1627 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/redpanda/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3513 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2551 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    20702 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     6272 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2451 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    19617 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     8224 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1792 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/openmetadata/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/openmetadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/openmetadata/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:06.999945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlflow/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1841 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8032 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/sagemaker/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7459 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1124 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3668 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9396 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4156 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4625 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    18953 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1763 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2456 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9412 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3549 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/databrickspipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1965 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8570 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.003945 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/domopipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2087 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6162 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1799 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7741 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/gluepipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1780 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6767 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5069 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2479 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8799 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     9064 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/pipeline_service.py
--rw-r--r--   0 runner    (1001) docker     (123)     1139 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/sqa_types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2706 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    16878 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     2319 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     4854 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/storage_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/stage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/stage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6315 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/stage/table_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/pandas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4718 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/pandas/pandas_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.007946 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/sqalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/sqalchemy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3717 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/mixins/sqalchemy/sqa_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/parsers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/parsers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8193 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/parsers/avro_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2730 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/parsers/json_schema_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/parsers/protobuf_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2304 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/parsers/schema_parsers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/pii/
--rw-r--r--   0 runner    (1001) docker     (123)      640 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/pii/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3294 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/pii/column_name_scanner.py
--rw-r--r--   0 runner    (1001) docker     (123)      854 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/pii/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     4385 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/pii/ner_scanner.py
--rw-r--r--   0 runner    (1001) docker     (123)     3821 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/pii/processor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2466 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/api/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    22502 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)    12042 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/pandas/pandas_profiler_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     7864 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/profiler_protocol.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)    19509 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/sqlalchemy/sqa_profiler_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.011946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.015946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1784 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/distinct_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/duplicate_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1621 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/ilike_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     1860 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/iqr.py
--rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/like_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/non_parametric_skew.py
--rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/null_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/unique_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/core.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.015946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/hybrid/
--rw-r--r--   0 runner    (1001) docker     (123)     8971 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/hybrid/histogram.py
--rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.019946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2587 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/column_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/column_names.py
--rw-r--r--   0 runner    (1001) docker     (123)     1530 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/count_in_set.py
--rw-r--r--   0 runner    (1001) docker     (123)     1713 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/distinct_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1580 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/ilike_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1549 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/like_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1903 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/max.py
--rw-r--r--   0 runner    (1001) docker     (123)     2126 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/max_length.py
--rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/mean.py
--rw-r--r--   0 runner    (1001) docker     (123)     1903 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/min.py
--rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/min_length.py
--rw-r--r--   0 runner    (1001) docker     (123)     1622 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/not_like_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2504 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/not_regexp_match_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/null_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2396 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/regexp_match_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1306 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/row_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/stddev.py
--rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/sum.py
--rw-r--r--   0 runner    (1001) docker     (123)     2703 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/unique_count.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.019946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/system/
--rw-r--r--   0 runner    (1001) docker     (123)    17396 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/system/system.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.019946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2995 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/first_quartile.py
--rw-r--r--   0 runner    (1001) docker     (123)     2845 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/median.py
--rw-r--r--   0 runner    (1001) docker     (123)     2986 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/third_quartile.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.019946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10789 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/converter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.019946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1521 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/concat.py
--rw-r--r--   0 runner    (1001) docker     (123)     1510 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/conn_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10286 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/datetime.py
--rw-r--r--   0 runner    (1001) docker     (123)     2074 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/length.py
--rw-r--r--   0 runner    (1001) docker     (123)     5429 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/median.py
--rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/modulo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3140 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/random_num.py
--rw-r--r--   0 runner    (1001) docker     (123)     3008 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/sum.py
--rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.023946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2119 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/bytea_to_string.py
--rw-r--r--   0 runner    (1001) docker     (123)     1442 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/custom_array.py
--rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/custom_timestamp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/hex_byte_string.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/uuid.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.023946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    20033 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/core.py
--rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/datalake_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2631 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     4823 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/handle_partition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     5539 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)    10964 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3007 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.023946 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2177 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (123)     3905 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/profiler/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.023946 openmetadata-ingestion-1.0.5.0/src/metadata/readers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/readers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      942 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/readers/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3182 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/readers/github.py
--rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/readers/local.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.023946 openmetadata-ingestion-1.0.5.0/src/metadata/timer/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/timer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/timer/repeated_timer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2548 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/timer/workflow_reporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.031946 openmetadata-ingestion-1.0.5.0/src/metadata/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/azure_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2132 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/class_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1647 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/client_version.py
--rw-r--r--   0 runner    (1001) docker     (123)     1115 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     4859 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/credentials.py
--rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/custom_thread_pool.py
--rw-r--r--   0 runner    (1001) docker     (123)    12278 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/dbt_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/dispatch.py
--rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/elasticsearch.py
--rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/entity_link.py
--rw-r--r--   0 runner    (1001) docker     (123)     7377 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/filters.py
--rw-r--r--   0 runner    (1001) docker     (123)    15342 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/fqn.py
--rw-r--r--   0 runner    (1001) docker     (123)     3247 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/gcs_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    10657 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7311 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/importer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4394 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/lru_cache.py
--rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/metadata_service_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     3559 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/partition.py
--rw-r--r--   0 runner    (1001) docker     (123)     3706 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/profiler_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3592 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/s3_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.031946 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/aws_based_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     2449 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/aws_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     2379 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1075 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/external_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/noop_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     3202 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/secrets_manager_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     1051 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/singleton.py
--rw-r--r--   0 runner    (1001) docker     (123)      992 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/sqa_like_column.py
--rw-r--r--   0 runner    (1001) docker     (123)     7185 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/sqa_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3088 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/sqlalchemy_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1788 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/ssl_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/tag_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/test_suite.py
--rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/time_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/timeout.py
--rw-r--r--   0 runner    (1001) docker     (123)      916 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/uuid_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    16918 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/utils/workflow_output_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.031946 openmetadata-ingestion-1.0.5.0/src/metadata/workflow/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/workflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3277 2023-06-19 13:21:31.000000 openmetadata-ingestion-1.0.5.0/src/metadata/workflow/workflow_status_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 13:23:07.035946 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     2549 2023-06-19 13:23:03.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    66336 2023-06-19 13:23:03.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-19 13:23:03.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      144 2023-06-19 13:23:03.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-19 13:22:40.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)     5943 2023-06-19 13:23:03.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       39 2023-06-19 13:23:03.000000 openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/
+-rw-r--r--   0 runner    (1001) docker     (123)    11356 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     2601 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)      636 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      809 2023-06-26 12:17:36.197330 openmetadata-ingestion-1.1.0.0.dev0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)    10159 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.049331 openmetadata-ingestion-1.1.0.0.dev0/src/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.049331 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/
+-rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.049331 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3561 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/hooks/openmetadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3273 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/callback.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/commons.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3938 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4223 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/providers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2958 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/operator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13481 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4200 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/status.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2710 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/__version__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/antlr/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/antlr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1854 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/antlr/split_listener.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/automations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2349 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/automations/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6524 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/backup.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/dataquality.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6215 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/db_dump.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13593 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/docker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/ingest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1652 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/insight.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4451 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/openmetadata_dag_config_migration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/openmetadata_imports_migration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1675 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2882 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/restore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/clients/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/clients/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6545 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/clients/aws_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5201 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/clients/domo_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16228 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cmd.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/config/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3222 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/config/common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13196 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.053331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/helper/
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/helper/data_insight_es_index.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2289 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8380 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/entity_report_data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13522 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/web_analytic_report_data_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/runner/
+-rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/runner/kpi_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5130 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/runner/run_result_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3708 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1356 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/api/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20417 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3848 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5939 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3197 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/test_suite_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3117 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/test_suite_interface_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/runner/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/runner/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1659 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/runner/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)      874 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/runner/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/source/
+-rw-r--r--   0 runner    (1001) docker     (123)     3729 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/source/base_test_suite_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/source/test_suite_source_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5483 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/base_test_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.057331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.061331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3054 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2682 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2693 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2682 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2714 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3966 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2680 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4510 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2808 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3013 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3107 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2907 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2956 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.061331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1782 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1771 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1751 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1855 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1793 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1753 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.065331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1770 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1749 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1731 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1750 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1852 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1717 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1725 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2200 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.065331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/mixins/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/mixins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2890 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.065331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.065331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2313 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2302 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2437 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3180 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3296 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2419 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3505 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.065331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1485 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1280 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3095 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.065331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1427 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1475 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1760 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1369 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1289 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2444 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)      954 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/validator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.037331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.077331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/airbyte.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1206 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/airflow.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1058 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/airflow_backend.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1607 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/airflow_postgres.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1123 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/amundsen.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/athena.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      964 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/athena_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/athena_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1193 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/atlas.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1227 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/azuresql.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      920 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/bigquery.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      947 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/bigquery_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2082 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/bigquery_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/bigquery_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1179 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/clickhouse.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      951 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/clickhouse_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1240 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/clickhouse_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/dagster.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1039 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/data_insight.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1185 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/databricks.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      951 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/databricks_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1108 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/databricks_pipeline.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1253 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/databricks_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1235 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/datalake_azure.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1646 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/datalake_gcs.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1206 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/datalake_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/datalake_s3.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1056 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/db2.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1649 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/db2_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     3505 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/dbt.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1105 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/deltalake.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1171 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/domodashboard.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/dynamodb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1016 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/fivetran.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1130 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/glue.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/gluepipeline.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      993 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/hive.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1049 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/impala.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/kafka.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1041 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/kinesis.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1090 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/looker.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1081 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mariadb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1078 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/metabase.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mlflow.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1363 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mode.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1065 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mongodb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1043 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mssql.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      941 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mssql_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1220 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mssql_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1151 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mysql.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1256 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mysql_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/openmetadata.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/oracle.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      939 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/oracle_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1054 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/oracle_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1117 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/pinotdb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/postgres.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      969 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/postgres_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1265 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/postgres_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1452 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/powerbi.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/presto.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1351 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/query_log_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1252 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/quicksight.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/redash.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1116 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/redpanda.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/redshift.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      945 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/redshift_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1663 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/redshift_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1272 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/redshift_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1107 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/sagemaker.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1131 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/salesforce.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1092 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/singlestore.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1296 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/snowflake.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      937 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/snowflake_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1240 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/snowflake_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/spline.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1043 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/sqlite.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1291 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/superset.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/tableau.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/test_suite.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1167 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/trino.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/vertica.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.037331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.077331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/
+-rw-r--r--   0 runner    (1001) docker     (123)     8889 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/EntityLinkLexer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/EntityLinkListener.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9288 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/EntityLinkParser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2606 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/FqnLexer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1210 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/FqnListener.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6811 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/FqnParser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8675 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/JdbcUriLexer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2181 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/JdbcUriListener.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16824 2023-06-26 12:17:18.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/JdbcUriParser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.041331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.077331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      948 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportData.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.077331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1263 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1094 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py
+-rw-r--r--   0 runner    (1001) docker     (123)      958 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEvent.py
+-rw-r--r--   0 runner    (1001) docker     (123)      883 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventData.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.077331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      978 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/analytics/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/analytics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/automations/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1483 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/automations/createWorkflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/createClassification.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1754 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/createTag.py
+-rw-r--r--   0 runner    (1001) docker     (123)      525 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/loadTags.py
+-rw-r--r--   0 runner    (1001) docker     (123)      959 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/createBot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/createEventPublisherJob.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/createType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1363 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createChart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createContainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2181 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDashboard.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1935 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDashboardDataModel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1910 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDatabase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDatabaseSchema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1541 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createGlossary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2430 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createGlossaryTerm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2446 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createMlModel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2163 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createTable.py
+-rw-r--r--   0 runner    (1001) docker     (123)      727 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createTableProfile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2713 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createTopic.py
+-rw-r--r--   0 runner    (1001) docker     (123)      424 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/restoreEntity.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.081331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      401 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/closeTask.py
+-rw-r--r--   0 runner    (1001) docker     (123)      526 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/createPost.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1705 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/createThread.py
+-rw-r--r--   0 runner    (1001) docker     (123)      421 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/resolveTask.py
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/threadCount.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.085331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      408 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/lineage/addLineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      672 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/openMetadataServerVersion.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.085331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/policies/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1082 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/policies/createPolicy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.085331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createDashboardService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1163 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createDatabaseService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createMessagingService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createMetadataService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1160 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createMlModelService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createPipelineService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1232 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createStorageService.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.085331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)      547 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/setOwner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.085331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      830 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/createRole.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2469 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/createTeam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2082 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/createUser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.085331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1227 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createCustomMetric.py
+-rw-r--r--   0 runner    (1001) docker     (123)      585 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createLogicalTestCases.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createTestCase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1159 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createTestDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1312 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createTestSuite.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/voteRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.089331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      339 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      462 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/basicLoginRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      886 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/changePasswordRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      437 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/createPersonalToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/emailRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/emailVerificationToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      347 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/generateToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      751 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/jwtAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      409 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/loginRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      626 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/logoutRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      683 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/passwordResetRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1027 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/passwordResetToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      956 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/personalAccessToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/refreshToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      623 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/registrationRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      476 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/revokePersonalToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/revokeToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      288 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/serviceTokenEnum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/ssoAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      378 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/tokenRefreshRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.089331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/applicationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/authConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/authenticationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/authorizerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      363 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/changeEventConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1690 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      442 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      843 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/extensionConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      357 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/fernetConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      658 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1823 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.093331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      894 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      451 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      462 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      978 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2623 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      683 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/slackAppConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      436 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      885 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.093331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1933 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.093331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2537 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/kpi.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py
+-rw-r--r--   0 runner    (1001) docker     (123)      791 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py
+-rw-r--r--   0 runner    (1001) docker     (123)      635 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py
+-rw-r--r--   0 runner    (1001) docker     (123)      843 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/email/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/email/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1250 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/email/emailRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1312 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/email/smtpSettings.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/automations/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/automations/testServiceConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/automations/workflow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2055 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/bot.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/classification/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2864 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/classification/classification.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3372 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/classification/tag.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3213 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/chart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4707 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/container.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3825 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/dashboard.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3576 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/dashboardDataModel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3834 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3640 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/databaseSchema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/glossary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5058 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/glossaryTerm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6478 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/mlmodel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6411 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2942 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2303 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22222 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/topic.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.097331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/events/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/events/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1195 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/events/webhook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.101331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/feed/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/feed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5260 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/feed/thread.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.101331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.101331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1643 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1821 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1497 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/rule.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/filters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/policy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.101331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.101331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.101331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      935 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1492 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1803 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2025 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1310 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1785 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.105331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2482 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2555 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2796 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3446 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.105331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/common/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      541 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      536 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3615 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.109331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      531 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2272 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3741 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1690 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1449 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3346 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3384 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2665 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.109331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mongoDB/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mongoDB/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1332 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mongoDB/mongoDBValues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1451 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mongoDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2856 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3165 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3765 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3705 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2616 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3341 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2233 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3105 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2736 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4204 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2785 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.109331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      935 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2551 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      901 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      729 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2364 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      389 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/saslMechanismType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.109331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5370 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.109331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      920 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      915 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      735 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.109331 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1154 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1412 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1371 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      916 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2241 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1018 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.113330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1172 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      919 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1146 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1119 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2930 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3985 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/dashboardService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6095 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/databaseService.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.113330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5984 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3653 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/messagingService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3505 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/metadataService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3420 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/mlmodelService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4094 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/pipelineService.py
+-rw-r--r--   0 runner    (1001) docker     (123)      391 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/serviceType.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3007 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/storageService.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.113330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/role.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4493 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/team.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1755 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/teamHierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4107 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/user.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3392 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/type.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.113330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/entitiesCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)      951 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/servicesCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.113330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.113330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/api/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1858 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/api/createEventSubscription.py
+-rw-r--r--   0 runner    (1001) docker     (123)      501 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/dataInsightAlertConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/emailAlertConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      476 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/entitySpelFilters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/eventFilterRule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6335 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/eventSubscription.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.117330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2342 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)      469 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dataInsightPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1577 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1208 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2121 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.117330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      676 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1291 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      955 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      597 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1116 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1922 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.117330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1361 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      797 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)      969 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5129 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.117330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/monitoring/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/monitoring/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/monitoring/eventMonitorProvider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.117330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.117330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      579 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      636 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      757 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      494 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.121330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      677 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/accessTokenAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/awsCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)      894 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/azureCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)      589 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      904 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/bitbucketCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)      822 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/gcpCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2154 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/gcpValues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1102 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/gitCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/githubCredentials.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.121330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/secrets/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/secrets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      298 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerClientLoader.py
+-rw-r--r--   0 runner    (1001) docker     (123)      938 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
+-rw-r--r--   0 runner    (1001) docker     (123)      560 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/securityConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.121330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/ssl/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/ssl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      520 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      727 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.121330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/settings/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/settings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2234 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/settings/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.121330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/system/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/system/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3679 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/system/eventPublisherJob.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.121330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3010 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/customMetric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2700 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/testCase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3445 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/testDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3395 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/testSuite.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.125330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/auditLog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4326 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4078 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/changeEvent.py
+-rw-r--r--   0 runner    (1001) docker     (123)      870 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/collectionDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      526 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvDocumentation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      422 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvErrorType.py
+-rw-r--r--   0 runner    (1001) docker     (123)      942 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvFile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvImportResult.py
+-rw-r--r--   0 runner    (1001) docker     (123)      453 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/dailyCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1887 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/databaseConnectionConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2457 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityHistory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3021 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityLineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1478 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityReference.py
+-rw-r--r--   0 runner    (1001) docker     (123)      807 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityReferenceList.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2337 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityRelationship.py
+-rw-r--r--   0 runner    (1001) docker     (123)      579 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityUsage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      707 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/filterPattern.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/function.py
+-rw-r--r--   0 runner    (1001) docker     (123)      262 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/include.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/jdbcConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/paging.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1362 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/queryParserData.py
+-rw-r--r--   0 runner    (1001) docker     (123)      739 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/reaction.py
+-rw-r--r--   0 runner    (1001) docker     (123)      604 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2580 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/tableQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/tableUsageCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/tagLabel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/usageDetails.py
+-rw-r--r--   0 runner    (1001) docker     (123)      413 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/usageRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      914 2023-06-26 12:17:17.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/votes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.125330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15870 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/action.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.125330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/utils/ometa_config_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.125330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.129330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/bulk_sink.py
+-rw-r--r--   0 runner    (1001) docker     (123)      742 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/closeable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1149 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16881 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1937 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/sink.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2902 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/stage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/status.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9256 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/topology_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12770 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.129330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/bulksink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/bulksink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12958 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/bulksink/metadata_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.129330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6525 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/builders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2440 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/headers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1920 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/secrets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1229 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/session.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13463 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/test_connections.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.129330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16703 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15543 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/sql_lineage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.129330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3665 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/custom_pydantic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/custom_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/delete_entity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/encoders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9245 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/es_documents.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1136 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/ometa_classification.py
+-rw-r--r--   0 runner    (1001) docker     (123)      825 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/ometa_topic_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      850 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/pipeline_status.py
+-rw-r--r--   0 runner    (1001) docker     (123)      911 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/profile_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      982 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/table_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1373 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/tests_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5298 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/topology.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1078 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/user.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.133330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14209 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/auth_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10030 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/client_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3964 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/credentials.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.133330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4825 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/es_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17774 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/glossary_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4098 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5479 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14629 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/patch_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4273 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4145 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2842 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/query_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15416 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2519 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/server_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2772 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/service_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9812 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/table_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10595 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/tests_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/topic_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2307 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/user_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/version_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1048 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26651 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/ometa_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4456 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/provider_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.133330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/processor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/processor/query_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.133330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34074 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.137330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9905 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/container_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8522 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1629 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7376 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8341 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6018 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8548 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/query_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10284 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2474 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4395 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8763 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4517 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1433 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1445 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2219 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20332 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.137330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/connections.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.137330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16819 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/dashboard_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.137330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2167 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9576 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.137330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/columns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1106 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/links.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32652 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4958 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.141330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5816 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1871 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12454 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.141330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6133 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1781 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9209 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.141330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11293 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1810 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25728 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5074 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.141330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12802 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      761 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.141330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2183 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2000 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11567 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.141330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6309 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/api_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5011 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6152 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/db_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1967 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6257 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.145330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/
+-rw-r--r--   0 runner    (1001) docker     (123)      955 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4721 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4539 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19812 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4166 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1185 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.145330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.145330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3243 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1655 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6344 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3307 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.145330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/azuresql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/azuresql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3044 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/azuresql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3240 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/azuresql/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5567 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1312 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17062 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1714 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2647 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1336 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8406 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2301 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3965 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/column_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16214 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/column_type_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20027 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/common_db_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8543 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/common_nosql_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14626 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/database_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8961 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5096 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/connection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/legacy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/legacy/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11429 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/legacy/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2050 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2078 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1419 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1006 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2257 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/query_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/unity_catalog/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/unity_catalog/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6734 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/unity_catalog/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17296 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/unity_catalog/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2436 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4668 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24172 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1430 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.149330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/db2/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/db2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1980 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/db2/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3210 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/db2/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15211 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9417 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_service.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5751 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36773 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1042 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/deltalake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/deltalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5033 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/deltalake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14961 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/deltalake/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2177 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8608 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/druid/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/druid/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/druid/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/druid/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8869 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/connection.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13849 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3999 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13447 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4095 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6883 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      742 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4760 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/lineage_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.153330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mariadb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mariadb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mariadb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mariadb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.157330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mongodb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mongodb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2752 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mongodb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3686 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mongodb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.157330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1203 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1742 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)      987 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8105 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.157330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2570 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2064 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5100 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.157330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4072 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1361 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4984 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3417 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1802 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1364 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7782 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.157330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.157330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2369 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8435 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6182 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8777 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1034 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10483 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3403 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6120 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      654 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1262 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1498 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query_parser_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1563 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7813 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9210 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12164 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/salesforce/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/salesforce/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2029 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/salesforce/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10054 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/salesforce/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47710 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3851 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sample_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/saphana/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/saphana/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5726 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/saphana/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2946 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/saphana/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/singlestore/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/singlestore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/singlestore/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/singlestore/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.161330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6567 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2243 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1248 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15519 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3784 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4088 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7700 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13318 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sql_column_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2970 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlalchemy_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlite/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlite/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlite/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9614 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      980 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5727 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/usage_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1380 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10925 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4794 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/ldap_users.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11063 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/common_broker_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4626 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1839 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10127 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2464 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/messaging_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1627 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.165330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3513 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2551 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17649 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6272 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2451 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19968 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5329 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1841 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8032 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7459 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1124 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3668 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9336 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4156 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4625 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20770 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1803 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4571 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8917 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2162 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3549 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.169330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1965 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8538 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2087 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6162 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1799 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7653 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1780 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6739 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5069 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2479 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8799 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8963 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/pipeline_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1807 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8237 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1338 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1139 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/sqa_types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2706 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17090 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2319 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4854 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/storage_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/stage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/stage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6315 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/stage/table_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4748 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/pandas/pandas_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.173330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/sqalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/sqalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3717 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/sqalchemy/sqa_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8193 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/avro_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2730 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/json_schema_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/protobuf_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2304 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/schema_parsers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/
+-rw-r--r--   0 runner    (1001) docker     (123)      640 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2628 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/column_name_scanner.py
+-rw-r--r--   0 runner    (1001) docker     (123)      854 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6267 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/ner_scanner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3821 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/processor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2466 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/api/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17736 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12030 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/pandas/profiler_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7864 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/profiler_protocol.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19894 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/sqlalchemy/profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1784 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/distinct_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/duplicate_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1621 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/ilike_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1860 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/iqr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/like_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/non_parametric_skew.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/null_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/unique_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6831 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/core.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.177330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/hybrid/
+-rw-r--r--   0 runner    (1001) docker     (123)     8971 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/hybrid/histogram.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.181330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2587 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/column_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/column_names.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1530 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/count_in_set.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1713 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/distinct_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1580 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/ilike_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1549 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/like_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1903 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/max.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2126 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/max_length.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/mean.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1903 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/min.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/min_length.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1622 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/not_like_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2504 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/not_regexp_match_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/null_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2396 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/regexp_match_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1306 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/row_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/stddev.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/sum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2703 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/unique_count.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.181330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/dml_operation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.181330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1644 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2869 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/redshift.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3322 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/snowflake.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13258 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/system.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.181330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2995 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/first_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2845 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/median.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2986 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/third_quartile.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.181330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10789 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1521 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/concat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1600 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/conn_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10286 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/datetime.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2106 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/length.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5468 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/median.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/modulo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3178 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/random_num.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3008 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/sum.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11321 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/table_metric_construct.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4504 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2119 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/bytea_to_string.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1442 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/custom_array.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/custom_timestamp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/hex_byte_string.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/uuid.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20186 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2631 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4823 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/handle_partition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/pandas/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5539 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10964 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/sqlalchemy/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3007 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2177 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3905 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8041 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/base_profiler_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.185330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/bigquery/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2336 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/bigquery/profiler_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/source/profiler_source_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.189330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1806 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/api_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)      942 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/bitbucket.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2349 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/credentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2727 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/github.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/local.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.189330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/timer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/timer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/timer/repeated_timer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2548 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/timer/workflow_reporter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2132 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/class_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1647 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/client_version.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4859 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/credentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/custom_thread_pool.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4161 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/avro_dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1623 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3960 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/csv_tsv_dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3349 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/datalake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3613 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/json_dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3589 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/parquet_dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3031 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/db_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/elasticsearch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/entity_link.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7377 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/filters.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15499 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/fqn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12452 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7311 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/importer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4394 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/lru_cache.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/metadata_service_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3559 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/partition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4151 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/profiler_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/aws_based_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2449 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/aws_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2379 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/client/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/client/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2995 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/client/loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1075 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/external_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/noop_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4459 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/secrets_manager_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1051 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/singleton.py
+-rw-r--r--   0 runner    (1001) docker     (123)      992 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/sqa_like_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7185 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/sqa_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3088 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/sqlalchemy_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1788 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/ssl_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4339 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/tag_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/test_suite.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/time_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/timeout.py
+-rw-r--r--   0 runner    (1001) docker     (123)      916 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/uuid_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16918 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/workflow_output_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/workflow/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/workflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3277 2023-06-26 12:16:04.000000 openmetadata-ingestion-1.1.0.0.dev0/src/metadata/workflow/workflow_status_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 12:17:36.193330 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     2601 2023-06-26 12:17:32.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    71294 2023-06-26 12:17:32.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-26 12:17:32.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      144 2023-06-26 12:17:32.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-26 12:17:09.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)     6137 2023-06-26 12:17:32.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       39 2023-06-26 12:17:32.000000 openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/top_level.txt
```

### Comparing `openmetadata-ingestion-1.0.5.0/LICENSE` & `openmetadata-ingestion-1.1.0.0.dev0/LICENSE`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/PKG-INFO` & `openmetadata-ingestion-1.1.0.0.dev0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: openmetadata-ingestion
-Version: 1.0.5.0
+Version: 1.1.0.0.dev0
 Summary: Ingestion Framework for OpenMetadata
 Home-page: https://open-metadata.org/
 Author: OpenMetadata Committers
 License: Apache License 2.0
 Project-URL: Documentation, https://docs.open-metadata.org/
 Project-URL: Source, https://github.com/open-metadata/OpenMetadata
 Requires-Python: >=3.7
@@ -40,14 +40,15 @@
 Provides-Extra: hive
 Provides-Extra: impala
 Provides-Extra: kafka
 Provides-Extra: kinesis
 Provides-Extra: ldap-users
 Provides-Extra: looker
 Provides-Extra: mlflow
+Provides-Extra: mongo
 Provides-Extra: mssql
 Provides-Extra: mssql-odbc
 Provides-Extra: mysql
 Provides-Extra: nifi
 Provides-Extra: okta
 Provides-Extra: oracle
 Provides-Extra: pinotdb
@@ -57,14 +58,15 @@
 Provides-Extra: pymssql
 Provides-Extra: quicksight
 Provides-Extra: redash
 Provides-Extra: redpanda
 Provides-Extra: redshift
 Provides-Extra: sagemaker
 Provides-Extra: salesforce
+Provides-Extra: sap-hana
 Provides-Extra: singlestore
 Provides-Extra: sklearn
 Provides-Extra: snowflake
 Provides-Extra: superset
 Provides-Extra: tableau
 Provides-Extra: trino
 Provides-Extra: vertica
```

### Comparing `openmetadata-ingestion-1.0.5.0/README.md` & `openmetadata-ingestion-1.1.0.0.dev0/README.md`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/setup.cfg` & `openmetadata-ingestion-1.1.0.0.dev0/setup.cfg`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/setup.py` & `openmetadata-ingestion-1.1.0.0.dev0/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,14 +40,16 @@
     "pandas": "pandas==1.3.5",
     "pyarrow": "pyarrow~=10.0",
     "pydomo": "pydomo~=0.3",
     "pymysql": "pymysql>=1.0.2",
     "pyodbc": "pyodbc>=4.0.35,<5",
     "scikit-learn": "scikit-learn~=1.0",  # Python 3.7 only goes up to 1.0.2
     "packaging": "packaging==21.3",
+    "azure-storage-blob": "azure-storage-blob~=12.14",
+    "azure-identity": "azure-identity~=1.12",
 }
 
 COMMONS = {
     "datalake": {
         VERSIONS["boto3"],
         VERSIONS["pandas"],
         VERSIONS["pyarrow"],
@@ -55,15 +57,15 @@
     },
     "hive": {
         "presto-types-parser>=0.0.2",
         "pyhive~=0.6",
     },
     "kafka": {
         VERSIONS["avro"],
-        "confluent_kafka==1.8.2",
+        "confluent_kafka==2.1.1",
         "fastavro>=1.2.0",
         # Due to https://github.com/grpc/grpc/issues/30843#issuecomment-1303816925
         # use >= v1.47.2 https://github.com/grpc/grpc/blob/v1.47.2/tools/distrib/python/grpcio_tools/grpc_version.py#L17
         VERSIONS[
             "grpc-tools"
         ],  # grpcio-tools already depends on grpcio. No need to add separately
         "protobuf",
@@ -138,20 +140,22 @@
         "dagster_graphql~=1.1",
     },
     "dbt": {
         "google-cloud",
         VERSIONS["boto3"],
         VERSIONS["google-cloud-storage"],
         "dbt-artifacts-parser",
+        VERSIONS["azure-storage-blob"],
+        VERSIONS["azure-identity"],
     },
     "db2": {"ibm-db-sa~=0.3"},
-    "databricks": {"sqlalchemy-databricks~=0.1"},
+    "databricks": {"sqlalchemy-databricks~=0.1", "databricks-sdk~=0.1"},
     "datalake-azure": {
-        "azure-storage-blob~=12.14",
-        "azure-identity~=1.12",
+        VERSIONS["azure-storage-blob"],
+        VERSIONS["azure-identity"],
         "adlfs>=2022.2.0",  # Python 3.7 does only support up to 2022.2.0
         *COMMONS["datalake"],
     },
     "datalake-gcs": {
         VERSIONS["google-cloud-storage"],
         "gcsfs==2022.11.0",
         *COMMONS["datalake"],
@@ -189,14 +193,15 @@
         "thrift-sasl~=0.4",
     },
     "kafka": {*COMMONS["kafka"]},
     "kinesis": {VERSIONS["boto3"]},
     "ldap-users": {"ldap3==2.9.1"},
     "looker": {"looker-sdk>=22.20.0", "lkml~=1.3"},
     "mlflow": {"mlflow-skinny~=1.30", "alembic~=1.10.2"},
+    "mongo": {"pymongo~=4.3", VERSIONS["pandas"]},
     "mssql": {"sqlalchemy-pytds~=0.3"},
     "mssql-odbc": {VERSIONS["pyodbc"]},
     "mysql": {VERSIONS["pymysql"]},
     "nifi": {},  # uses requests
     "okta": {"okta~=2.3"},
     "oracle": {"cx_Oracle>=8.3.0,<9", "oracledb~=1.2"},
     "pinotdb": {"pinotdb~=0.3"},
@@ -216,14 +221,15 @@
         # Going higher has memory and performance issues
         "sqlalchemy-redshift==0.8.12",
         "psycopg2-binary",
         VERSIONS["geoalchemy2"],
     },
     "sagemaker": {VERSIONS["boto3"]},
     "salesforce": {"simple_salesforce==1.11.4"},
+    "sap-hana": {"hdbcli", "sqlalchemy-hana"},
     "singlestore": {VERSIONS["pymysql"]},
     "sklearn": {VERSIONS["scikit-learn"]},
     "snowflake": {"snowflake-sqlalchemy~=1.4"},
     "superset": {},  # uses requests
     "tableau": {"tableau-api-lib~=0.1"},
     "trino": {"trino[sqlalchemy]"},
     "vertica": {"sqlalchemy-vertica[vertica-python]>=0.0.5"},
@@ -254,15 +260,15 @@
     # install dbt dependency
     "dbt-artifacts-parser",
 }
 
 build_options = {"includes": ["_cffi_backend"]}
 setup(
     name="openmetadata-ingestion",
-    version="1.0.5.0",
+    version="1.1.0.0.dev0",
     url="https://open-metadata.org/",
     author="OpenMetadata Committers",
     license="Apache License 2.0",
     description="Ingestion Framework for OpenMetadata",
     long_description=get_long_description(),
     long_description_content_type="text/markdown",
     python_requires=">=3.7",
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/__init__.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/hooks/openmetadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/hooks/openmetadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/backend.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/backend.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/callback.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/callback.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/commons.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/commons.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/loader.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/loader.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/config/providers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/providers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/operator.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/operator.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/runner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/runner.py`

 * *Files 6% similar despite different names*

```diff
@@ -43,15 +43,15 @@
     PipelineService,
     PipelineServiceType,
 )
 from metadata.generated.schema.type.entityLineage import EntitiesEdge, LineageDetails
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.pipeline.airflow.lineage_parser import XLets
-from metadata.utils.helpers import datetime_to_ts
+from metadata.utils.helpers import clean_uri, datetime_to_ts
 
 
 class SimpleEdge(BaseModel):
     """
     Simple Edge representation with FQN and id
     """
 
@@ -89,14 +89,16 @@
         self.service_name = service_name
         self.only_keep_dag_lineage = only_keep_dag_lineage
         self.max_status = max_status
 
         self.dag = dag
         self.xlets = xlets
 
+        self.host_port = conf.get("webserver", "base_url")
+
     def get_or_create_pipeline_service(self) -> PipelineService:
         """
         Fetch the Pipeline Service from OM. If it does not exist,
         create it.
         """
         service_entity: PipelineService = self.metadata.get_by_name(
             entity=PipelineService, fqn=self.service_name
@@ -107,38 +109,41 @@
 
         pipeline_service: PipelineService = self.metadata.create_or_update(
             CreatePipelineServiceRequest(
                 name=self.service_name,
                 serviceType=PipelineServiceType.Airflow,
                 connection=PipelineConnection(
                     config=AirflowConnection(
-                        hostPort=conf.get("webserver", "base_url"),
+                        hostPort=self.host_port,
                         connection=BackendConnection(),
                     ),
                 ),
             )
         )
 
         if pipeline_service is None:
             raise RuntimeError("Failed to create Airflow service.")
 
         return pipeline_service
 
     def get_task_url(self, task: "Operator"):
-        return f"/taskinstance/list/?flt1_dag_id_equals={self.dag.dag_id}&_flt_3_task_id={task.task_id}"
+        return (
+            f"{clean_uri(self.host_port)}/taskinstance/list/"
+            f"?flt1_dag_id_equals={self.dag.dag_id}&_flt_3_task_id={task.task_id}"
+        )
 
     def get_om_tasks(self) -> List[Task]:
         """
         Get all tasks from the DAG and map them to
         OpenMetadata Task Entities
         """
         return [
             Task(
                 name=task.task_id,
-                taskUrl=self.get_task_url(task),
+                sourceUrl=self.get_task_url(task),
                 taskType=task.task_type,
                 startDate=task.start_date.isoformat() if task.start_date else None,
                 endDate=task.end_date.isoformat() if task.end_date else None,
                 downstreamTasks=list(task.downstream_task_ids)
                 if task.downstream_task_ids
                 else None,
             )
@@ -149,15 +154,15 @@
         """
         Create the Pipeline Entity
         """
         self.dag.log.info("Creating or updating Pipeline Entity from DAG...")
         pipeline_request = CreatePipelineRequest(
             name=self.dag.dag_id,
             description=self.dag.description,
-            pipelineUrl=f"/tree?dag_id={self.dag.dag_id}",
+            sourceUrl=f"{clean_uri(self.host_port)}/tree?dag_id={self.dag.dag_id}",
             concurrency=self.dag.max_active_tasks,
             pipelineLocation=self.dag.fileloc,
             startDate=self.dag.start_date.isoformat() if self.dag.start_date else None,
             tasks=self.get_om_tasks(),
             service=pipeline_service.fullyQualifiedName,
         )
 
@@ -245,36 +250,44 @@
         Add the lineage from inlets and outlets
         """
 
         lineage_details = LineageDetails(
             pipeline=EntityReference(id=pipeline.id, type="pipeline")
         )
 
-        for fqn_in, fqn_out in zip(xlets.inlets, xlets.outlets):
-            table_in: Optional[Table] = self.metadata.get_by_name(
-                entity=Table, fqn=fqn_in
-            )
-            table_out: Optional[Table] = self.metadata.get_by_name(
-                entity=Table, fqn=fqn_out
-            )
-
-            if table_in and table_out:
-                try:
-                    lineage = AddLineageRequest(
-                        edge=EntitiesEdge(
-                            fromEntity=EntityReference(id=table_in.id, type="table"),
-                            toEntity=EntityReference(id=table_out.id, type="table"),
-                            lineageDetails=lineage_details,
-                        ),
-                    )
-                    self.metadata.add_lineage(lineage)
-                except AttributeError as err:
-                    self.dag.log.error(
-                        f"Error trying to compute lineage due to: {err}."
+        for from_fqn in xlets.inlets or []:
+            from_entity: Optional[Table] = self.metadata.get_by_name(
+                entity=Table, fqn=from_fqn
+            )
+            if from_entity:
+                for to_fqn in xlets.outlets or []:
+                    to_entity: Optional[Table] = self.metadata.get_by_name(
+                        entity=Table, fqn=to_fqn
                     )
+                    if to_entity:
+                        lineage = AddLineageRequest(
+                            edge=EntitiesEdge(
+                                fromEntity=EntityReference(
+                                    id=from_entity.id, type="table"
+                                ),
+                                toEntity=EntityReference(id=to_entity.id, type="table"),
+                                lineageDetails=lineage_details,
+                            )
+                        )
+                        self.metadata.add_lineage(lineage)
+                    else:
+                        self.dag.log.warning(
+                            f"Could not find Table [{to_fqn}] from "
+                            f"[{pipeline.fullyQualifiedName.__root__}] outlets"
+                        )
+            else:
+                self.dag.log.warning(
+                    f"Could not find Table [{from_fqn}] from "
+                    f"[{pipeline.fullyQualifiedName.__root__}] inlets"
+                )
 
     def clean_lineage(self, pipeline: Pipeline, xlets: XLets):
         """
         Clean the lineage nodes that are not part of xlets.
 
         We'll only clean up table nodes
         """
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/airflow_provider_openmetadata/lineage/status.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/airflow_provider_openmetadata/lineage/status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/__main__.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/__main__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/__version__.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/__version__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/antlr/split_listener.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/antlr/split_listener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/automations/runner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/automations/runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/backup.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/backup.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/dataquality.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/dataquality.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/db_dump.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/db_dump.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/docker.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/docker.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/ingest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/ingest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/insight.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/insight.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/openmetadata_dag_config_migration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/openmetadata_dag_config_migration.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/openmetadata_imports_migration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/openmetadata_imports_migration.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/profile.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/profile.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/restore.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/restore.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cli/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cli/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/clients/aws_client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/clients/aws_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -30,14 +30,15 @@
     CLOUDWATCH = "cloudwatch"
     DYNAMO_DB = "dynamodb"
     GLUE = "glue"
     SAGEMAKER = "sagemaker"
     KINESIS = "kinesis"
     QUICKSIGHT = "quicksight"
     ATHENA = "athena"
+    RDS = "rds"
 
 
 class AWSAssumeRoleException(Exception):
     """
     Exception class to handle assume role related issues
     """
 
@@ -71,15 +72,14 @@
             config.awsAccessKeyId,
             config.awsSecretAccessKey,
             config.awsSessionToken,
             config.awsRegion,
             config.profileName,
         )
         sts_client = session.client("sts")
-        resp = None
         if config.assumeRoleSourceIdentity:
             resp = sts_client.assume_role(
                 RoleArn=config.assumeRoleArn,
                 RoleSessionName=config.assumeRoleSessionName,
                 SourceIdentity=config.assumeRoleSourceIdentity,
             )
         else:
@@ -95,20 +95,25 @@
                 secretAccessKey=credentials.get("SecretAccessKey"),
                 sessionToken=credentials.get("SessionToken"),
             )
         return None
 
     @staticmethod
     def _get_session(
-        aws_access_key_id,
-        aws_secret_access_key,
-        aws_session_token,
-        aws_region,
+        aws_access_key_id: Optional[str],
+        aws_secret_access_key: Optional[CustomSecretStr],
+        aws_session_token: Optional[str],
+        aws_region: str,
         profile=None,
     ) -> Session:
+        """
+        The only required param for boto3 is the region.
+        The rest of credentials will have fallback strategies based on
+        https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials
+        """
         return Session(
             aws_access_key_id=aws_access_key_id,
             aws_secret_access_key=aws_secret_access_key.get_secret_value()
             if aws_secret_access_key
             else None,
             aws_session_token=aws_session_token,
             region_name=aws_region,
@@ -154,14 +159,17 @@
         session = self.create_session()
         if self.config.endPointURL is not None:
             return session.resource(
                 service_name=service_name, endpoint_url=self.config.endPointURL
             )
         return session.resource(service_name=service_name)
 
+    def get_rds_client(self):
+        return self.get_client(AWSServices.RDS.value)
+
     def get_s3_client(self):
         return self.get_client(AWSServices.S3.value)
 
     def get_cloudwatch_client(self):
         return self.get_client(AWSServices.CLOUDWATCH.value)
 
     def get_dynamo_client(self):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/clients/domo_client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/clients/domo_client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/cmd.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/cmd.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/config/common.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/config/common.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/api/workflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/api/workflow.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/helper/data_insight_es_index.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/helper/data_insight_es_index.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/data_processor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/data_processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/entity_report_data_processor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/entity_report_data_processor.py`

 * *Files 10% similar despite different names*

```diff
@@ -32,18 +32,16 @@
     databaseSchema,
     mlmodel,
     pipeline,
     table,
     topic,
 )
 from metadata.generated.schema.entity.teams.user import User
-from metadata.generated.schema.type.entityReference import (
-    EntityReference,
-    EntityReferenceList,
-)
+from metadata.generated.schema.type.entityReference import EntityReference
+from metadata.generated.schema.type.entityReferenceList import EntityReferenceList
 from metadata.utils.helpers import get_entity_tier_from_tags
 from metadata.utils.logger import data_insight_logger
 
 logger = data_insight_logger()
 
 ENTITIES = [
     chart.Chart,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/processor/web_analytic_report_data_processor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/processor/web_analytic_report_data_processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/runner/kpi_runner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/runner/kpi_runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/runner/run_result_registry.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/runner/run_result_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_insight/sink/metadata_rest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_insight/sink/metadata_rest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/api/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/common.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,44 +4,36 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Return types for TestSuite workflow execution.
-
-We need to define this class as we end up having
-multiple test cases per workflow.
+Common definitions for configuration management
 """
+from typing import Any, Optional, TypeVar
 
-from typing import List, Optional
+from pydantic import BaseModel
 
-from metadata.config.common import ConfigModel
-from metadata.generated.schema.tests.testCase import TestCaseParameterValue
-from metadata.generated.schema.type.basic import EntityLink
+from metadata.utils.logger import ingestion_logger
 
+T = TypeVar("T")
 
-class TestCaseDefinition(ConfigModel):
-    """Test case definition for the CLI"""
+logger = ingestion_logger()
 
-    name: str
-    description: Optional[str] = "Default suite description"
-    testDefinitionName: str
-    entityLink: EntityLink
-    parameterValues: Optional[List[TestCaseParameterValue]]
+# Allow types from the generated pydantic models
+Entity = TypeVar("Entity", bound=BaseModel)
 
 
-class TestSuiteDefinition(ConfigModel):
-    """definition for a test suite"""
+class ConfigModel(BaseModel):
+    class Config:
+        extra = "forbid"
 
-    name: str
-    description: Optional[str] = "Default test suite description"
-    testCases: List[TestCaseDefinition]
 
+class DynamicTypedConfig(ConfigModel):
+    type: str
+    config: Optional[Any]
 
-class TestSuiteProcessorConfig(ConfigModel):
-    """class for the processor config"""
 
-    testSuites: Optional[List[TestSuiteDefinition]] = None
+class WorkflowExecutionError(Exception):
+    """An error occurred when executing the workflow"""
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/api/workflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/api/workflow.py`

 * *Files 23% similar despite different names*

```diff
@@ -11,68 +11,59 @@
 
 """
 Workflow definition for the test suite
 """
 
 from __future__ import annotations
 
-import sys
 import traceback
 from copy import deepcopy
 from logging import Logger
-from typing import List, Optional, Set, Tuple
+from typing import List, Optional, cast
 
-from antlr4.error.Errors import ParseCancellationException
 from pydantic import BaseModel, ValidationError
-from sqlalchemy import MetaData
 
 from metadata.config.common import WorkflowExecutionError
 from metadata.data_quality.api.models import (
     TestCaseDefinition,
-    TestSuiteDefinition,
     TestSuiteProcessorConfig,
 )
-from metadata.data_quality.interface.pandas.pandas_test_suite_interface import (
-    PandasTestSuiteInterface,
+from metadata.data_quality.source.test_suite_source_factory import (
+    test_suite_source_factory,
 )
-from metadata.data_quality.interface.sqlalchemy.sqa_test_suite_interface import (
-    SQATestSuiteInterface,
-)
-from metadata.data_quality.runner.core import DataTestsRunner
 from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
 from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
-from metadata.generated.schema.entity.data.table import PartitionProfilerConfig, Table
-from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
-    DatalakeConnection,
-)
+from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
+from metadata.generated.schema.entity.services.connections.serviceConnection import (
+    ServiceConnection,
+)
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.ingestionPipelines.ingestionPipeline import (
     PipelineState,
 )
 from metadata.generated.schema.metadataIngestion.testSuitePipeline import (
     TestSuitePipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     OpenMetadataWorkflowConfig,
 )
 from metadata.generated.schema.tests.testCase import TestCase
+from metadata.generated.schema.tests.testDefinition import TestDefinition, TestPlatform
 from metadata.generated.schema.tests.testSuite import TestSuite
-from metadata.generated.schema.type.basic import FullyQualifiedEntityName
+from metadata.generated.schema.type.basic import EntityLink, FullyQualifiedEntityName
 from metadata.ingestion.api.parser import parse_workflow_config_gracefully
 from metadata.ingestion.api.processor import ProcessorStatus
 from metadata.ingestion.ometa.client_utils import create_ometa_client
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.profiler.api.models import ProfileSampleConfig
 from metadata.utils import entity_link
+from metadata.utils.fqn import split
 from metadata.utils.importer import get_sink
 from metadata.utils.logger import test_suite_logger
-from metadata.utils.partition import get_partition_details
 from metadata.utils.workflow_output_handler import print_test_suite_status
 from metadata.workflow.workflow_status_mixin import WorkflowStatusMixin
 
 logger: Logger = test_suite_logger()
 
 
 class TestCaseToCreate(BaseModel):
@@ -101,32 +92,37 @@
         Args:
             config: OM workflow configuration object
 
         Attributes:
             config: OM workflow configuration object
         """
         self.config = config
+        self.metadata_config: OpenMetadataConnection = (
+            self.config.workflowConfig.openMetadataServerConfig
+        )
+        self.metadata = create_ometa_client(self.metadata_config)
 
         self.source_config: TestSuitePipeline = self.config.source.sourceConfig.config
+        self.service: DatabaseService = self._retrieve_service()
+        self._retrieve_service_connection()
+
         self.processor_config: TestSuiteProcessorConfig = (
             TestSuiteProcessorConfig.parse_obj(
                 self.config.processor.dict().get("config")
             )
         )
 
-        self.metadata_config: OpenMetadataConnection = (
-            self.config.workflowConfig.openMetadataServerConfig
-        )
-        self.client = create_ometa_client(self.metadata_config)
-        self.metadata = OpenMetadata(self.metadata_config)
-
         self.set_ingestion_pipeline_status(state=PipelineState.running)
 
         self.status = ProcessorStatus()
 
+        self.table_entity: Optional[Table] = self._get_table_entity(
+            self.source_config.entityFullyQualifiedName.__root__
+        )
+
         if self.config.sink:
             self.sink = get_sink(
                 sink_type=self.config.sink.type,
                 sink_config=self.config.sink,
                 metadata_config=self.metadata_config,
                 from_="data_quality",
             )
@@ -146,456 +142,325 @@
             return cls(config)
         except ValidationError as err:
             logger.error(
                 f"Error trying to parse the Profiler Workflow configuration: {err}"
             )
             raise err
 
-    def _filter_test_cases_for_entity(
-        self, entity_fqn: str, test_cases: List[TestCase]
-    ) -> list[TestCase]:
-        """Filter test cases for specific entity"""
-        return [
-            test_case
-            for test_case in test_cases
-            if test_case.entityLink.__root__.split("::")[2].replace(">", "")
-            == entity_fqn
-        ]
-
-    def _get_unique_entities_from_test_cases(self, test_cases: List[TestCase]) -> Set:
-        """from a list of test cases extract unique table entities"""
-        entity_fqns = [
-            test_case.entityLink.__root__.split("::")[2].replace(">", "")
-            for test_case in test_cases
-        ]
-
-        return set(entity_fqns)
-
-    def _get_service_connection_from_test_case(self, entity_fqn: str):
-        """given an entityLink return the service connection
-
-        Args:
-            entity_fqn: entity link for the test case
-        """
-        service: DatabaseService = self.metadata.get_by_name(
-            entity=DatabaseService,
-            fqn=entity_fqn.split(".")[0],
-        )
-
-        if service:
-            service_connection_config = deepcopy(service.connection.config)
-            if hasattr(service_connection_config, "supportsDatabase"):
-                if (
-                    hasattr(
-                        service_connection_config,
-                        "database",
-                    )
-                    and not service_connection_config.database
-                ):
-                    service_connection_config.database = entity_fqn.split(".")[1]
-                if (
-                    hasattr(
-                        service_connection_config,
-                        "catalog",
-                    )
-                    and not service_connection_config.catalog
-                ):
-                    service_connection_config.catalog = entity_fqn.split(".")[1]
-            return service_connection_config
-
-        logger.error(f"Could not retrieve connection details for entity {entity_link}")
-        raise ValueError()
+    def _retrieve_service(self) -> DatabaseService:
+        """Get service object from source config `entityFullyQualifiedName`"""
+        fully_qualified_name = self.source_config.entityFullyQualifiedName.__root__
+        try:
+            service_name = split(fully_qualified_name)[0]
+        except IndexError as exc:
+            logger.debug(traceback.format_exc())
+            raise IndexError(
+                f"Could not retrieve service name from entity fully qualified name {fully_qualified_name}: {exc}"
+            )
+        try:
+            service = self.metadata.get_by_name(DatabaseService, service_name)
+            if not service:
+                raise ConnectionError(
+                    f"Could not retrieve service with name `{service_name}`. "
+                    "Typically caused by the `entityFullyQualifiedName` does not exists in OpenMetadata "
+                    "or the JWT Token is invalid."
+                )
+        except ConnectionError as exc:
+            raise exc
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(
+                f"Error getting service connection for service name [{service_name}]"
+                f" using the secrets manager provider [{self.metadata.config.secretsManagerProvider}]: {exc}"
+            )
+        return service
 
-    def _get_table_entity_from_test_case(self, entity_fqn: str):
-        """given an entityLink return the table entity
+    def _get_table_entity(self, entity_fqn: str) -> Optional[Table]:
+        """given an entity fqn return the table entity
 
         Args:
             entity_fqn: entity fqn for the test case
         """
         return self.metadata.get_by_name(
             entity=Table,
             fqn=entity_fqn,
-            fields=["tableProfilerConfig"],
+            fields=["tableProfilerConfig", "testSuite"],
         )
 
-    def _get_profile_sample(self, entity: Table) -> Optional[float]:
-        """Get profile sample
-
-        Args:
-            entity: table entity
-        """
-        if (
-            hasattr(entity, "tableProfilerConfig")
-            and hasattr(entity.tableProfilerConfig, "profileSample")
-            and entity.tableProfilerConfig.profileSample
-        ):
-            return ProfileSampleConfig(
-                profile_sample=entity.tableProfilerConfig.profileSample,
-                profile_sample_type=entity.tableProfilerConfig.profileSampleType,
-            )
-
-        return None
-
-    def _get_profile_query(self, entity: Table) -> Optional[str]:
-        """Get profile query
-
-        Args:
-            entity: table entity
-        """
-        if entity.tableProfilerConfig:
-            return entity.tableProfilerConfig.profileQuery
-
-        return None
-
-    def _get_partition_details(
-        self, entity: Table
-    ) -> Optional[PartitionProfilerConfig]:
-        """Get partition details
-
-        Args:
-            entity: table entity
-        """
-        return get_partition_details(entity)
-
-    def _create_runner_interface(self, entity_fqn: str):
-        """create the interface to execute test against sources"""
-        table_entity = self._get_table_entity_from_test_case(entity_fqn)
-        service_connection_config = self._get_service_connection_from_test_case(
-            entity_fqn
-        )
-        table_partition_config = None
-        profile_sample_config = None
-        table_sample_query = (
-            self._get_profile_query(table_entity)
-            if not self._get_profile_sample(table_entity)
-            else None
-        )
-        if not table_sample_query:
-            profile_sample_config = self._get_profile_sample(table_entity)
-            table_partition_config = self._get_partition_details(table_entity)
-
-        if not isinstance(service_connection_config, DatalakeConnection):
-            sqa_metadata_obj = MetaData()
-            return SQATestSuiteInterface(
-                service_connection_config=service_connection_config,
-                ometa_client=self.client,
-                sqa_metadata_obj=sqa_metadata_obj,
-                table_entity=table_entity,
-                profile_sample_config=profile_sample_config,
-                table_sample_query=table_sample_query,
-                table_partition_config=table_partition_config,
-            )
-        return PandasTestSuiteInterface(
-            service_connection_config=service_connection_config,
-            ometa_client=self.client,
-            profile_sample_config=profile_sample_config,
-            table_entity=table_entity,
-            table_partition_config=table_partition_config,
-        )
-
-    def _create_data_tests_runner(self, sqa_interface):
-        """create main object to run data test validation"""
-        return DataTestsRunner(sqa_interface)
-
-    def get_test_suite_entity_for_ui_workflow(self) -> Optional[List[TestSuite]]:
+    def create_or_return_test_suite_entity(self) -> Optional[TestSuite]:
         """
         try to get test suite name from source.servicName.
         In the UI workflow we'll write the entity name (i.e. the test suite)
         to source.serviceName.
         """
-        test_suite = self.metadata.get_by_name(
-            entity=TestSuite,
-            fqn=self.config.source.serviceName,
-        )
-
-        if test_suite:
-            return [test_suite]
-        return None
-
-    def get_or_create_test_suite_entity_for_cli_workflow(
-        self,
-    ) -> List[TestSuite]:
-        """
-        For the CLI workflow we'll have n testSuite in the processor.config.testSuites
-        """
-        test_suite_entities = []
-        test_suites = self.processor_config.testSuites or []
+        self.table_entity = cast(Table, self.table_entity)  # satisfy type checker
+        test_suite = self.table_entity.testSuite
+        if test_suite and not test_suite.executable:
+            logger.debug(
+                f"Test suite {test_suite.fullyQualifiedName.__root__} is not executable."
+            )
+            return None
 
-        for test_suite in test_suites:
-            test_suite_entity = self.metadata.get_by_name(
-                entity=TestSuite,
-                fqn=test_suite.name,
-            )
-            if not test_suite_entity:
-                test_suite_entity = self.metadata.create_or_update(
-                    CreateTestSuiteRequest(
-                        name=test_suite.name,
-                        description=test_suite.description,
-                    )
+        if self.processor_config.testCases and not test_suite:
+            # This should cover scenarios where we are running the tests from the CLI workflow
+            # and no corresponding tests suite exist in the platform. We, therefore, will need
+            # to create the test suite first.
+            logger.debug(
+                "Test suite name not found in the platform. Creating the test suite from processor config."
+            )
+            test_suite = self.metadata.create_or_update_executable_test_suite(
+                CreateTestSuiteRequest(
+                    name=f"{self.source_config.entityFullyQualifiedName.__root__}.TestSuite",
+                    displayName=f"{self.source_config.entityFullyQualifiedName.__root__} Test Suite",
+                    description="Test Suite created from YAML processor config file",
+                    owner=None,
+                    executableEntityReference=self.source_config.entityFullyQualifiedName.__root__,
                 )
-            test_suite_entities.append(test_suite_entity)
+            )
 
-        return test_suite_entities
+        return test_suite
 
     def get_test_cases_from_test_suite(
-        self, test_suites: List[TestSuite]
-    ) -> List[TestCase]:
+        self, test_suite: TestSuite
+    ) -> Optional[List[TestCase]]:
         """
         Get test cases from test suite name
 
         Args:
             test_suite_name: the name of the test suite
         """
-
-        test_cases_entity = []
-        for test_suite in test_suites:
-            test_case_entity_list = self.metadata.list_entities(
-                entity=TestCase,
-                fields=["testSuite", "entityLink", "testDefinition"],
-                params={"testSuiteId": test_suite.id.__root__},
+        test_cases = self.metadata.list_entities(
+            entity=TestCase,
+            fields=["testSuite", "entityLink", "testDefinition"],
+            params={"testSuiteId": test_suite.id.__root__},
+        ).entities
+        test_cases = cast(List[TestCase], test_cases)  # satisfy type checker
+        if self.processor_config.testCases is not None:
+            cli_test_cases = self.get_test_case_from_cli_config()  # type: ignore
+            cli_test_cases = cast(
+                List[TestCaseDefinition], cli_test_cases
+            )  # satisfy type checker
+            test_cases = self.compare_and_create_test_cases(
+                cli_test_cases, test_cases, test_suite
             )
-            test_cases_entity.extend(test_case_entity_list.entities)
 
-        return test_cases_entity
+        return test_cases
 
-    def get_test_case_from_cli_config(
-        self,
-    ) -> List[Tuple[TestCaseDefinition, TestSuiteDefinition]]:
-        """Get all the test cases names defined in the CLI config file"""
-        return [
-            (test_case, test_suite)
-            for test_suite in self.processor_config.testSuites
-            for test_case in test_suite.testCases
-        ]
-
-    def get_unique_test_case_name_in_config(self, test_cases_in_config: set) -> set:
-        """Get unique test case names in config. If a test case is created for the same entity
-        with the same name in different test suites, we only create one test case in the platform.
-        The other ones will be skipped.
+    def filter_for_om_test_cases(self, test_cases: List[TestCase]) -> List[TestCase]:
+        """
+        Filter test cases for OM test cases only. This will prevent us from running non OM test cases
 
         Args:
-            test_cases_in_config (set): set of test cases in config
-
-        Returns:
-            set: set of unique test case names in config
+            test_cases: list of test cases
         """
-        seen = []
-        unique_test_case = []
-        for test_case in test_cases_in_config:
-            unique_test_case_name_in_entity = (
-                f"{test_case.test_case_name}/{test_case.entity_link}"
-            )
-            if unique_test_case_name_in_entity not in seen:
-                seen.append(unique_test_case_name_in_entity)
-                unique_test_case.append(test_case)
-                continue
-            logger.info(
-                f"Test case {test_case.test_case_name} for entity {test_case.entity_link}"
-                " was already defined in your profiler config file. Skipping it."
+        om_test_cases: List[TestCase] = []
+        for test_case in test_cases:
+            test_definition: TestDefinition = self.metadata.get_by_id(
+                TestDefinition, test_case.testDefinition.id
             )
+            if TestPlatform.OpenMetadata not in test_definition.testPlatforms:
+                logger.debug(
+                    f"Test case {test_case.name.__root__} is not an OpenMetadata test case."
+                )
+                continue
+            om_test_cases.append(test_case)
 
-        return set(unique_test_case)
+        return om_test_cases
 
-    def test_case_name_exists(self, test_case: TestCaseToCreate) -> bool:
-        """Check if a test case name already exists in the platform
-        for the same entity.
+    def get_test_case_from_cli_config(
+        self,
+    ) -> Optional[List[TestCaseDefinition]]:
+        """Get all the test cases names defined in the CLI config file"""
+        if self.processor_config.testCases is not None:
+            return list(self.processor_config.testCases)
+        return None
+
+    def _update_test_cases(
+        self, test_cases_to_update: List[TestCaseDefinition], test_cases: List[TestCase]
+    ):
+        """Given a list of CLI test definition patch test cases in the platform
 
         Args:
-            other (set): a set of platform test cases
-        Returns:
-            Optional["TestCaseToCreate"]
+            test_cases_to_update (List[TestCaseDefinition]): list of test case definitions
         """
-        try:
-            entity_fqn = entity_link.get_table_or_column_fqn(test_case.entity_link)
-        except ValueError as exc:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Failed to get entity fqn: {exc}")
-            # we'll assume that the test case name is not unique
-            return True
-        except ParseCancellationException as err:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Failed to parse: {test_case.entity_link}, err: {err}")
-            # we'll assume that the test case name is not unique
-            return True
-        test_case_fqn = f"{entity_fqn}.{test_case.test_case_name}"
-
-        test_case = self.metadata.get_by_name(
-            entity=TestCase,
-            fqn=test_case_fqn,
-            fields=["testDefinition", "testSuite"],
-        )
+        test_cases_to_update_names = {
+            test_case_to_update.name for test_case_to_update in test_cases_to_update
+        }
+        for indx, test_case in enumerate(deepcopy(test_cases)):
+            if test_case.name.__root__ in test_cases_to_update_names:
+                test_case_definition = next(
+                    test_case_to_update
+                    for test_case_to_update in test_cases_to_update
+                    if test_case_to_update.name == test_case.name.__root__
+                )
+                updated_test_case = self.metadata.patch_test_case_definition(
+                    source=test_case,
+                    entity_link=entity_link.get_entity_link(
+                        self.source_config.entityFullyQualifiedName.__root__,
+                        test_case_definition.columnName,
+                    ),
+                    test_case_parameter_values=test_case_definition.parameterValues,
+                )
+                if updated_test_case:
+                    test_cases.pop(indx)
+                    test_cases.append(updated_test_case)
 
-        if not test_case:
-            return False
-        return True
+        return test_cases
 
     def compare_and_create_test_cases(
         self,
-        cli_config_test_cases_def: List[Tuple[TestCaseDefinition, TestSuiteDefinition]],
+        cli_test_cases_definitions: Optional[List[TestCaseDefinition]],
         test_cases: List[TestCase],
+        test_suite: TestSuite,
     ) -> Optional[List[TestCase]]:
         """
         compare test cases defined in CLI config workflow with test cases
         defined on the server
 
         Args:
-            cli_config_test_case_name: test cases defined in CLI workflow associated with its test suite
+            cli_test_cases_definitions: test cases defined in CLI workflow associated with its test suite
             test_cases: list of test cases entities fetch from the server using test suite names in the config file
         """
-        cli_test_cases = {
-            TestCaseToCreate(
-                test_suite_name=test_case_def[1].name,
-                test_case_name=test_case_def[0].name,
-                entity_link=test_case_def[0].entityLink.__root__,
-            )
-            for test_case_def in cli_config_test_cases_def
-        }
-        platform_test_cases = {
-            TestCaseToCreate(
-                test_suite_name=test_case.testSuite.name,
-                test_case_name=test_case.name.__root__,
-                entity_link=test_case.entityLink.__root__,
-            )
-            for test_case in test_cases
-        }
+        if not cli_test_cases_definitions:
+            return test_cases
+        test_cases = deepcopy(test_cases)
+        test_case_names = {test_case.name.__root__ for test_case in test_cases}
 
         # we'll check the test cases defined in the CLI config file and not present in the platform
-        unique_test_cases_across_test_suites = cli_test_cases - platform_test_cases
-        # we'll check if we are creating a test case for an entity that already has the same name
-        unique_test_case_across_entities = {
-            test_case
-            for test_case in unique_test_cases_across_test_suites
-            if not self.test_case_name_exists(test_case)
-        }
-        test_case_names_to_create = self.get_unique_test_case_name_in_config(
-            unique_test_case_across_entities
-        )
+        test_cases_to_create = [
+            cli_test_case_definition
+            for cli_test_case_definition in cli_test_cases_definitions
+            if cli_test_case_definition.name not in test_case_names
+        ]
 
-        if not test_case_names_to_create:
-            return None
+        if self.processor_config and self.processor_config.forceUpdate:
+            test_cases_to_update = [
+                cli_test_case_definition
+                for cli_test_case_definition in cli_test_cases_definitions
+                if cli_test_case_definition.name in test_case_names
+            ]
+            test_cases = self._update_test_cases(test_cases_to_update, test_cases)
 
-        created_test_case = []
-        for test_case_name_to_create in test_case_names_to_create:
-            logger.info(f"Creating test case with name {test_case_name_to_create}")
-            test_case_to_create, test_suite = next(
-                (
-                    cli_config_test_case_def
-                    for cli_config_test_case_def in cli_config_test_cases_def
-                    if (
-                        cli_config_test_case_def[0].name
-                        == test_case_name_to_create.test_case_name
-                        and cli_config_test_case_def[0].entityLink.__root__
-                        == test_case_name_to_create.entity_link
-                        and cli_config_test_case_def[1].name
-                        == test_case_name_to_create.test_suite_name
-                    )
-                ),
-                (None, None),
-            )
+        if not test_cases_to_create:
+            return test_cases
+
+        for test_case_to_create in test_cases_to_create:
+            logger.debug(f"Creating test case with name {test_case_to_create.name}")
             try:
-                created_test_case.append(
-                    self.metadata.create_or_update(
-                        CreateTestCaseRequest(
-                            name=test_case_to_create.name,
-                            entityLink=test_case_to_create.entityLink,
-                            testDefinition=FullyQualifiedEntityName(
-                                __root__=test_case_to_create.testDefinitionName
-                            ),
-                            testSuite=FullyQualifiedEntityName(
-                                __root__=test_suite.name
-                            ),
-                            parameterValues=list(test_case_to_create.parameterValues)
-                            if test_case_to_create.parameterValues
-                            else None,
-                        )
+                test_case = self.metadata.create_or_update(
+                    CreateTestCaseRequest(
+                        name=test_case_to_create.name,
+                        description=test_case_to_create.description,
+                        displayName=test_case_to_create.displayName,
+                        testDefinition=FullyQualifiedEntityName(
+                            __root__=test_case_to_create.testDefinitionName
+                        ),
+                        entityLink=EntityLink(
+                            __root__=entity_link.get_entity_link(
+                                self.source_config.entityFullyQualifiedName.__root__,
+                                test_case_to_create.columnName,
+                            )
+                        ),
+                        testSuite=test_suite.fullyQualifiedName.__root__,
+                        parameterValues=list(test_case_to_create.parameterValues)
+                        if test_case_to_create.parameterValues
+                        else None,
+                        owner=None,
                     )
                 )
+                test_cases.append(test_case)
             except Exception as exc:
                 error = (
-                    f"Couldn't create test case name {test_case_name_to_create}: {exc}"
+                    f"Couldn't create test case name {test_case_to_create.name}: {exc}"
                 )
-                logger.warning(error)
+                logger.error(error)
                 logger.debug(traceback.format_exc())
                 self.status.failed(
-                    test_case_to_create.entityLink.__root__.split("::")[2],
+                    self.source_config.entityFullyQualifiedName.__root__,
                     error,
                     traceback.format_exc(),
                 )
 
-        return created_test_case
-
-    def add_test_cases_from_cli_config(self, test_cases: list) -> list:
-        cli_config_test_cases_def = self.get_test_case_from_cli_config()
-        runtime_created_test_cases = self.compare_and_create_test_cases(
-            cli_config_test_cases_def, test_cases
-        )
-        if runtime_created_test_cases:
-            return runtime_created_test_cases
-        return []
+        return test_cases
 
     def run_test_suite(self):
-        """
-        Main running logic
-        """
-        test_suites = (
-            self.get_test_suite_entity_for_ui_workflow()
-            or self.get_or_create_test_suite_entity_for_cli_workflow()
-        )
-        if not test_suites:
-            logger.warning("No testSuite found in configuration file. Exiting.")
-            sys.exit(1)
+        """Main logic to run the tests"""
+        if not self.table_entity:
+            logger.debug(traceback.format_exc())
+            raise ValueError(
+                f"Could not retrieve table entity for {self.source_config.entityFullyQualifiedName.__root__}. "
+                "Make sure the table exists in OpenMetadata and/or the JWT Token provided is valid."
+            )
 
-        test_cases = self.get_test_cases_from_test_suite(test_suites)
-        if self.processor_config.testSuites:
-            test_cases.extend(self.add_test_cases_from_cli_config(test_cases))
+        test_suite = self.create_or_return_test_suite_entity()
+        if not test_suite:
+            logger.debug(
+                f"No test suite found for table {self.source_config.entityFullyQualifiedName.__root__} "
+                "or test suite is not executable."
+            )
+            return
 
-        unique_entity_fqns = self._get_unique_entities_from_test_cases(test_cases)
+        test_cases = self.get_test_cases_from_test_suite(test_suite)
+        if not test_cases:
+            logger.debug(
+                f"No test cases found for table {self.source_config.entityFullyQualifiedName.__root__}"
+                f"and test suite {test_suite.fullyQualifiedName.__root__}"
+            )
+            return
 
-        for entity_fqn in unique_entity_fqns:
-            try:
-                runner_interface = self._create_runner_interface(entity_fqn)
-                data_test_runner = self._create_data_tests_runner(runner_interface)
+        openmetadata_test_cases = self.filter_for_om_test_cases(test_cases)
 
-                for test_case in self._filter_test_cases_for_entity(
-                    entity_fqn, test_cases
-                ):
-                    try:
-                        test_result = data_test_runner.run_and_handle(test_case)
-                        if not test_result:
-                            continue
-                        if hasattr(self, "sink"):
-                            self.sink.write_record(test_result)
-                        logger.info(
-                            f"Successfully ran test case {test_case.name.__root__}"
-                        )
-                        self.status.processed(test_case.fullyQualifiedName.__root__)
-                    except Exception as exc:
-                        error = (
-                            f"Could not run test case {test_case.name.__root__}: {exc}"
-                        )
-                        logger.debug(traceback.format_exc())
-                        logger.warning(error)
-                        self.status.failed(
-                            test_case.name.__root__, error, traceback.format_exc()
-                        )
-            except TypeError as exc:
-                error = f"Could not run test case for table {entity_fqn}: {exc}"
+        test_suite_runner = test_suite_source_factory.create(
+            self.service.serviceType.value.lower(),
+            self.config,
+            self.metadata,
+            self.table_entity,
+        ).get_data_quality_runner()
+
+        for test_case in openmetadata_test_cases:
+            try:
+                test_result = test_suite_runner.run_and_handle(test_case)
+                if not test_result:
+                    continue
+                if hasattr(self, "sink"):
+                    self.sink.write_record(test_result)
+                logger.debug(f"Successfully ran test case {test_case.name.__root__}")
+                self.status.processed(test_case.fullyQualifiedName.__root__)
+            except Exception as exc:
+                error = f"Could not run test case {test_case.name.__root__}: {exc}"
                 logger.debug(traceback.format_exc())
-                logger.warning(error)
-                self.status.failed(entity_fqn, error, traceback.format_exc())
+                logger.error(error)
+                self.status.failed(
+                    test_case.name.__root__, error, traceback.format_exc()
+                )
+
+    def _retrieve_service_connection(self) -> None:
+        """
+        We override the current `serviceConnection` source config object if source workflow service already exists
+        in OM. When it is configured, we retrieve the service connection from the secrets' manager. Otherwise, we get it
+        from the service object itself through the default `SecretsManager`.
+        """
+        if (
+            not self.config.source.serviceConnection
+            and not self.metadata.config.forceEntityOverwriting
+        ):
+            self.config.source.serviceConnection = ServiceConnection(
+                __root__=self.service.connection
+            )
 
     def execute(self):
         """Execute test suite workflow"""
         try:
             self.run_test_suite()
             # At the end of the `execute`, update the associated Ingestion Pipeline status as success
             self.set_ingestion_pipeline_status(PipelineState.success)
 
         # Any unhandled exception breaking the workflow should update the status
         except Exception as err:
+            logger.debug(traceback.format_exc())
             self.set_ingestion_pipeline_status(PipelineState.failed)
             raise err
 
     def print_status(self) -> None:
         """
         Print the workflow results with click
         """
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py`

 * *Files 9% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 """
 Interfaces with database for all database engine
 supporting sqlalchemy abstraction layer
 """
 from datetime import datetime, timezone
 from typing import Optional
 
-from metadata.data_quality.interface.test_suite_protocol import TestSuiteProtocol
+from metadata.data_quality.interface.test_suite_interface import TestSuiteInterface
 from metadata.data_quality.validations.validator import Validator
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.generated.schema.tests.basic import TestCaseResult
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testDefinition import TestDefinition
@@ -29,41 +29,44 @@
 from metadata.mixins.pandas.pandas_mixin import PandasInterfaceMixin
 from metadata.utils.importer import import_test_case_class
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class PandasTestSuiteInterface(TestSuiteProtocol, PandasInterfaceMixin):
+class PandasTestSuiteInterface(TestSuiteInterface, PandasInterfaceMixin):
     """
     Sequential interface protocol for testSuite and Profiler. This class
     implements specific operations needed to run profiler and test suite workflow
     against a Datalake source.
     """
 
     def __init__(
         self,
-        ometa_client: OpenMetadata = None,
-        service_connection_config: DatalakeConnection = None,
-        table_entity=None,
-        table_partition_config=None,
-        profile_sample_config=None,
+        ometa_client: OpenMetadata,
+        service_connection_config: DatalakeConnection,
+        table_entity,
     ):
         self.table_entity = table_entity
-        self.profile_sample_config = profile_sample_config
 
         self.ometa_client = ometa_client
         self.service_connection_config = service_connection_config
+
+        (
+            self.table_sample_query,
+            self.table_sample_config,
+            self.table_partition_config,
+        ) = self._get_table_config()
+
         # add partition logic to test suite
-        self.table_partition_config = table_partition_config
         self.dfs = self.return_ometa_dataframes_sampled(
             service_connection_config=self.service_connection_config,
             client=get_connection(self.service_connection_config).client,
             table=self.table_entity,
-            profile_sample_config=self.profile_sample_config,
+            profile_sample_config=self.table_sample_config,
         )
         if self.dfs and self.table_partition_config:
             self.dfs = self.get_partitioned_df(self.dfs)
 
     def run_test_case(
         self,
         test_case: TestCase,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,74 +13,67 @@
 Interfaces with database for all database engine
 supporting sqlalchemy abstraction layer
 """
 
 from datetime import datetime, timezone
 from typing import Optional, Union
 
-from sqlalchemy import MetaData
 from sqlalchemy.orm import DeclarativeMeta
 from sqlalchemy.orm.util import AliasedClass
 
-from metadata.data_quality.interface.test_suite_protocol import TestSuiteProtocol
+from metadata.data_quality.interface.test_suite_interface import TestSuiteInterface
 from metadata.data_quality.validations.validator import Validator
-from metadata.generated.schema.entity.data.table import PartitionProfilerConfig, Table
+from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
 from metadata.generated.schema.tests.basic import TestCaseResult
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testDefinition import TestDefinition
 from metadata.ingestion.connections.session import create_and_bind_session
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.mixins.sqalchemy.sqa_mixin import SQAInterfaceMixin
-from metadata.profiler.api.models import ProfileSampleConfig
 from metadata.profiler.processor.runner import QueryRunner
-from metadata.profiler.processor.sampler import Sampler
+from metadata.profiler.processor.sqlalchemy.sampler import Sampler
 from metadata.utils.constants import TEN_MIN
 from metadata.utils.importer import import_test_case_class
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.timeout import cls_timeout
 
 logger = test_suite_logger()
 
 
-class SQATestSuiteInterface(SQAInterfaceMixin, TestSuiteProtocol):
+class SQATestSuiteInterface(SQAInterfaceMixin, TestSuiteInterface):
     """
     Sequential interface protocol for testSuite and Profiler. This class
     implements specific operations needed to run profiler and test suite workflow
     against a SQAlchemy source.
     """
 
-    # pylint: disable=too-many-arguments
     def __init__(
         self,
         service_connection_config: DatabaseConnection,
         ometa_client: OpenMetadata,
-        sqa_metadata_obj: Optional[MetaData] = None,
-        profile_sample_config: Optional[ProfileSampleConfig] = None,
-        table_sample_query: str = None,
-        table_partition_config: Optional[PartitionProfilerConfig] = None,
         table_entity: Table = None,
     ):
         self.ometa_client = ometa_client
         self.table_entity = table_entity
         self.service_connection_config = service_connection_config
         self.session = create_and_bind_session(
             get_connection(self.service_connection_config)
         )
         self.set_session_tag(self.session)
         self.set_catalog(self.session)
 
-        self._table = self._convert_table_to_orm_object(sqa_metadata_obj)
+        self._table = self._convert_table_to_orm_object()
 
-        self.profile_sample_config = profile_sample_config
-        self.table_sample_query = table_sample_query
-        self.table_partition_config = (
-            table_partition_config if not self.table_sample_query else None
-        )
+        (
+            self.table_sample_query,
+            self.table_sample_config,
+            self.table_partition_config,
+        ) = self._get_table_config()
 
         self._sampler = self._create_sampler()
         self._runner = self._create_runner()
 
     @property
     def sample(self) -> Union[DeclarativeMeta, AliasedClass]:
         """_summary_
@@ -124,15 +117,15 @@
 
     def _create_sampler(self) -> Sampler:
         """Create sampler instance"""
         return Sampler(
             session=self.session,
             table=self.table,
             sample_columns=self._get_sample_columns(),
-            profile_sample_config=self.profile_sample_config,
+            profile_sample_config=self.table_sample_config,
             partition_details=self.table_partition_config,
             profile_sample_query=self.table_sample_query,
         )
 
     def _create_runner(self) -> None:
         """Create a QueryRunner Instance"""
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/interface/test_suite_protocol.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/tests_data.py`

 * *Files 21% similar despite different names*

```diff
@@ -4,41 +4,37 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Interfaces with database for all database engine
-supporting sqlalchemy abstraction layer
+Custom pydantic models for tests suites and requests
 """
 
-from abc import ABC, abstractmethod
-from typing import Optional, Union
+from typing import List
+
+from pydantic import BaseModel
 
-from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
-    DatalakeConnection,
-)
-from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
+from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
+from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
 from metadata.generated.schema.tests.basic import TestCaseResult
 from metadata.generated.schema.tests.testCase import TestCase
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-class TestSuiteProtocol(ABC):
-    """Protocol interface for the processor"""
+class OMetaTestSuiteSample(BaseModel):
+    test_suite: CreateTestSuiteRequest
+
+
+class OMetaLogicalTestSuiteSample(BaseModel):
+    test_suite: CreateTestSuiteRequest
+    test_cases: List[TestCase]
+
+
+class OMetaTestCaseSample(BaseModel):
+    test_case: CreateTestCaseRequest
+
 
-    @abstractmethod
-    def __init__(
-        self,
-        ometa_client: OpenMetadata = None,
-        service_connection_config: Union[DatabaseConnection, DatalakeConnection] = None,
-    ):
-        """Required attribute for the interface"""
-        raise NotImplementedError
-
-    @abstractmethod
-    def run_test_case(self, test_case: TestCase) -> Optional[TestCaseResult]:
-        """run column data quality tests"""
-        raise NotImplementedError
+class OMetaTestCaseResultsSample(BaseModel):
+    test_case_results: TestCaseResult
+    test_case_name: str
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/runner/core.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/runner/core.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,26 +10,26 @@
 #  limitations under the License.
 
 """
 Main class to run data tests
 """
 
 
-from metadata.data_quality.interface.test_suite_protocol import TestSuiteProtocol
+from metadata.data_quality.interface.test_suite_interface import TestSuiteInterface
 from metadata.data_quality.runner.models import TestCaseResultResponse
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class DataTestsRunner:
     """class to execute the test validation"""
 
-    def __init__(self, test_runner_interface: TestSuiteProtocol):
+    def __init__(self, test_runner_interface: TestSuiteInterface):
         self.test_runner_interace = test_runner_interface
 
     def run_and_handle(self, test_case: TestCase):
         """run and handle test case validation"""
         logger.info(
             f"Executing test case {test_case.name.__root__} "
             f"for entity {self.test_runner_interace.table_entity.fullyQualifiedName.__root__}"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/runner/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/runner/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/sink/metadata_rest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/sink/metadata_rest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/base_test_handler.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/base_test_handler.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,19 +17,22 @@
 
 import reprlib
 from abc import ABC, abstractmethod
 from datetime import datetime
 from typing import Callable, List, Optional, TypeVar, Union
 
 from metadata.generated.schema.tests.basic import (
+    TestCaseFailureStatus,
+    TestCaseFailureStatusType,
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.generated.schema.tests.testCase import TestCase, TestCaseParameterValue
+from metadata.generated.schema.type.basic import Timestamp
 from metadata.profiler.processor.runner import QueryRunner
 
 T = TypeVar("T", bound=Callable)
 R = TypeVar("R")
 
 
 class BaseTestValidator(ABC):
@@ -98,20 +101,32 @@
             execution_date (Union[datetime, float]): test case execution datetime
             status (TestCaseStatus): failed, success, aborted
             result (str): test case result
             test_result_value (List[TestResultValue]): test result value to display in UI
         Returns:
             TestCaseResult:
         """
+        if status == TestCaseStatus.Failed:
+            test_case_failure_status = TestCaseFailureStatus(
+                testCaseFailureStatusType=TestCaseFailureStatusType.New,
+                testCaseFailureReason=None,
+                testCaseFailureComment=None,
+                updatedAt=Timestamp(__root__=int(datetime.utcnow().timestamp() * 1000)),
+                updatedBy=None,
+            )
+        else:
+            test_case_failure_status = None
+
         return TestCaseResult(
             timestamp=execution_date,  # type: ignore
             testCaseStatus=status,
             result=result,
             testResultValue=test_result_value,
             sampleData=None,
+            testCaseFailureStatus=test_case_failure_status,
         )
 
     def format_column_list(self, status: TestCaseStatus, cols: List):
         """Format column list based on the test status
 
         Args:
             cols: list of columns
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 
 """
 Validator for table custom SQL Query test case
 """
 
 import traceback
 from abc import abstractmethod
+from enum import Enum
 from typing import cast
 
 from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
@@ -26,53 +27,74 @@
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 RESULT_ROW_COUNT = "resultRowCount"
 
 
+class Strategy(Enum):
+    COUNT = "COUNT"
+    ROWS = "ROWS"
+
+
 class BaseTableCustomSQLQueryValidator(BaseTestValidator):
     """Validator table custom SQL Query test case"""
 
     def run_validation(self) -> TestCaseResult:
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
         sql_expression = self.get_test_case_param_value(
             self.test_case.parameterValues,  # type: ignore
             "sqlExpression",
             str,
         )
+
+        threshold = self.get_test_case_param_value(
+            self.test_case.parameterValues,  # type: ignore
+            "threshold",
+            int,
+            default=0,
+        )
+
+        strategy = self.get_test_case_param_value(
+            self.test_case.parameterValues,  # type: ignore
+            "strategy",
+            Strategy,
+        )
+
         sql_expression = cast(str, sql_expression)  # satisfy mypy
+        threshold = cast(int, threshold)  # satisfy mypy
+        strategy = cast(Strategy, strategy)  # satisfy mypy
 
         try:
-            rows = self._run_results(sql_expression)
+            rows = self._run_results(sql_expression, strategy)
         except Exception as exc:
             msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
                 [TestResultValue(name=RESULT_ROW_COUNT, value=None)],
             )
         len_rows = rows if isinstance(rows, int) else len(rows)
-        if len_rows == 0:
+        if len_rows <= threshold:
             status = TestCaseStatus.Success
-            result_value = 0
+            result_value = len_rows
         else:
             status = TestCaseStatus.Failed
             result_value = len_rows
 
         return self.get_test_case_result_object(
             self.execution_date,
             status,
             f"Found {result_value} row(s). Test query is expected to return 0 row.",
             [TestResultValue(name=RESULT_ROW_COUNT, value=str(result_value))],
         )
 
     @abstractmethod
-    def _run_results(self, sql_expression: str):
+    def _run_results(self, sql_expression: str, strategy: Strategy = Strategy.ROWS):
         raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,26 +14,27 @@
 """
 
 from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
 from metadata.data_quality.validations.table.base.tableCustomSQLQuery import (
     BaseTableCustomSQLQueryValidator,
+    Strategy,
 )
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class TableCustomSQLQueryValidator(
     BaseTableCustomSQLQueryValidator, PandasValidatorMixin
 ):
     """Validator for table custom SQL Query test case"""
 
-    def _run_results(self, sql_expression: str):
+    def _run_results(self, sql_expression: str, strategy: Strategy = Strategy.ROWS):
         """compute result of the test case"""
         return sum(  # pylint: disable=consider-using-generator
             [
                 len(runner.query(sql_expression))
                 for runner in self.runner
                 if len(runner.query(sql_expression))
             ]
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py`

 * *Files 21% similar despite different names*

```diff
@@ -16,22 +16,29 @@
 from sqlalchemy import text
 
 from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
     SQAValidatorMixin,
 )
 from metadata.data_quality.validations.table.base.tableCustomSQLQuery import (
     BaseTableCustomSQLQueryValidator,
+    Strategy,
 )
+from metadata.utils.helpers import is_safe_sql_query
 
 
 class TableCustomSQLQueryValidator(BaseTableCustomSQLQueryValidator, SQAValidatorMixin):
     """Validator for table custom SQL Query test case"""
 
-    def _run_results(self, sql_expression):
+    def _run_results(self, sql_expression: str, strategy: Strategy = Strategy.ROWS):
         """compute result of the test case"""
+        if not is_safe_sql_query(sql_expression):
+            raise RuntimeError(f"SQL expression is not safe\n\n{sql_expression}")
         try:
-            return self.runner._session.execute(  # pylint: disable=protected-access
+            cursor = self.runner._session.execute(  # pylint: disable=protected-access
                 text(sql_expression)
-            ).all()
+            )
+            if strategy == Strategy.COUNT:
+                return cursor.scalar()
+            return cursor.fetchall()
         except Exception as exc:
             self.runner._session.rollback()  # pylint: disable=protected-access
             raise exc
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/data_quality/validations/validator.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/data_quality/validations/validator.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/bigquery.yaml` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/bigquery.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -2,25 +2,26 @@
   type: bigquery
   serviceName: local_bigquery
   serviceConnection:
     config:
       type: BigQuery
       taxonomyProjectID: [ project-id-where-policy-tags-exist ]
       credentials:
-        gcsConfig:
+        gcpConfig:
           type: service_account
           projectId: project_id
           privateKeyId: private_key_id
           privateKey: private_key
           clientEmail: gcpuser@project_id.iam.gserviceaccount.com
           clientId: client_id
           authUri: https://accounts.google.com/o/oauth2/auth
           tokenUri: https://oauth2.googleapis.com/token
           authProviderX509CertUrl: https://www.googleapis.com/oauth2/v1/certs
-          clientX509CertUrl: clientX509CertUrl
+          clientX509CertUrl: https://www.googleapis.com/oauth2/v1/certs
+
   sourceConfig:
     config:
       type: DatabaseMetadata
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/datalake_profiler.yaml` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/datalake_profiler.yaml`

 * *Files 0% similar despite different names*

```diff
@@ -21,9 +21,9 @@
   type: metadata-rest
   config: {}
 workflowConfig:
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/kafka.yaml` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/kafka.yaml`

 * *Files 7% similar despite different names*

```diff
@@ -26,8 +26,8 @@
   config: {}
 workflowConfig:
   loggerLevel: DEBUG
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      "jwtToken": "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/migrate_source.yaml` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/looker.yaml`

 * *Files 25% similar despite different names*

```diff
@@ -1,28 +1,23 @@
 source:
-  type: migrate
-  serviceName: local_metadata
+  type: looker
+  serviceName: local_looker
   serviceConnection:
     config:
-      type: OpenMetadata
-      hostPort: http://<hostport of 0.9.0 Openmetadata Server>:8585/api
-      authProvider: no-auth
-      includeTables: true
-      includeUsers: true
-      includeTopics: true
-      enableVersionValidation: false
-      limitRecords: 10
+      type: Looker
+      clientId: clientId
+      clientSecret: clientSecret
+      hostPort: http://hostPort
   sourceConfig:
     config:
-      type: DatabaseMetadata
-stage:
-  type: migrate
-  config:
-    dirPath: <Directory Path to store data>
-bulkSink:
-  type: migrate
-  config:
-    dirPath: <Directory Path to store data>
+      type: DashboardMetadata
+      dashboardFilterPattern: {}
+      chartFilterPattern: {}
+sink:
+  type: metadata-rest
+  config: {}
 workflowConfig:
   openMetadataServerConfig:
-    hostPort: http://<hostport of 0.10.0 Openmetadata Server>/api
-    authProvider: no-auth
+    hostPort: http://localhost:8585/api
+    authProvider: openmetadata
+    securityConfig:
+      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/examples/workflows/mlflow.yaml` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/examples/workflows/mlflow.yaml`

 * *Files 1% similar despite different names*

```diff
@@ -14,8 +14,8 @@
   config: {}
 workflowConfig:
   openMetadataServerConfig:
     enableVersionValidation: false
     hostPort: http://localhost:8585/api
     authProvider: openmetadata
     securityConfig:
-      jwtToken: "eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
+      jwtToken: eyJraWQiOiJHYjM4OWEtOWY3Ni1nZGpzLWE5MmotMDI0MmJrOTQzNTYiLCJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlzQm90IjpmYWxzZSwiaXNzIjoib3Blbi1tZXRhZGF0YS5vcmciLCJpYXQiOjE2NjM5Mzg0NjIsImVtYWlsIjoiYWRtaW5Ab3Blbm1ldGFkYXRhLm9yZyJ9.tS8um_5DKu7HgzGBzS1VTA5uUjKWOCU0B_j08WXBiEC0mr0zNREkqVfwFDD-d24HlNEbrqioLsBuFRiwIWKc1m_ZlVQbG7P36RUxhuv2vbSp80FKyNM-Tj93FDzq91jsyNmsQhyNv_fNr3TXfzzSPjHt8Go0FMMP66weoKMgW2PbXlhVKwEuXUHyakLLzewm9UMeQaEiRzhiTMU3UkLXcKbYEJJvfNFcLwSl9W8JCO_l0Yj3ud-qt_nQYEZwqW6u5nfdQllN133iikV4fM5QZsMCnm8Rq1mvLR0y9bmJiD7fwM1tmJ791TUWqmKaTnP49U493VanKpUAfzIiOiIbhg"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/EntityLinkLexer.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/EntityLinkLexer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/EntityLinkListener.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/EntityLinkListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/EntityLinkParser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/EntityLinkParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/FqnLexer.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/FqnLexer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/FqnListener.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/FqnListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/antlr/FqnParser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/antlr/FqnParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/basic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/basic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/basic.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportData.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportData.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportData.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/entityReportData.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/webAnalyticEntityViewReportData.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/webAnalyticUserActivityReportData.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEvent.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEvent.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEvent.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventData.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventData.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventData.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventType/customEvent.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventType/pageViewEvent.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/analytics/createWebAnalyticEvent.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/automations/createWorkflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/automations/createWorkflow.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/automations/createWorkflow.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/createClassification.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/createClassification.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/createClassification.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/createTag.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/createTag.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/createTag.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/classification/loadTags.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/classification/loadTags.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/loadTags.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/createBot.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/createBot.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createBot.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/createEventPublisherJob.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/createEventPublisherJob.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createEventPublisherJob.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/createType.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/createType.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createType.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createChart.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createChart.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createChart.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -22,15 +22,15 @@
         description='Display Name that identifies this Chart. It could be title or label from the source services',
     )
     description: Optional[basic.Markdown] = Field(
         None,
         description='Description of the chart instance. What it has and how to use it.',
     )
     chartType: Optional[chart.ChartType] = None
-    chartUrl: Optional[str] = Field(
+    sourceUrl: Optional[basic.SourceUrl] = Field(
         None, description='Chart URL suffix from its service.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this chart'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this chart'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createContainer.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createContainer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createContainer.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDashboard.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDashboard.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDashboard.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from ...entity.data import dashboard
 from ...type import basic, entityReference, tagLabel
 
 
 class CreateDashboardRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
@@ -22,17 +23,22 @@
         None,
         description='Display Name that identifies this Dashboard. It could be title or label from the source services',
     )
     description: Optional[basic.Markdown] = Field(
         None,
         description='Description of the database instance. What it has and how to use it.',
     )
-    dashboardUrl: Optional[str] = Field(
+    dashboardType: Optional[dashboard.DashboardType] = dashboard.DashboardType.Dashboard
+    sourceUrl: Optional[basic.SourceUrl] = Field(
         None, description='Dashboard URL suffix from its service.'
     )
+    project: Optional[str] = Field(
+        None,
+        description='Name of the project / workspace / collection in which the dashboard is contained',
+    )
     charts: Optional[List[basic.FullyQualifiedEntityName]] = Field(
         None,
         description='List of fully qualified name of charts included in this Dashboard.',
     )
     dataModels: Optional[List[basic.FullyQualifiedEntityName]] = Field(
         None,
         description='List of fully qualified name of data models included in this Dashboard.',
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDashboardDataModel.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDashboardDataModel.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDashboardDataModel.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -42,7 +42,11 @@
         None, description='Service type where this data model is hosted in.'
     )
     dataModelType: dashboardDataModel.DataModelType
     sql: Optional[basic.SqlQuery] = Field(
         None, description='In case the Data Model is based on a SQL query.'
     )
     columns: List[table.Column] = Field(..., description='Columns from the data model.')
+    project: Optional[str] = Field(
+        None,
+        description='Name of the project / workspace / collection in which the dataModel is contained',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDatabase.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDatabase.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDatabase.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -36,7 +36,15 @@
         ...,
         description='Link to the database service fully qualified name where this database is hosted in',
     )
     default: Optional[bool] = Field(
         False,
         description="Some databases don't support a database/catalog in the hierarchy and use default database. For example, `MySql`. For such databases, set this flag to true to indicate that this is a default database.",
     )
+    retentionPeriod: Optional[basic.Duration] = Field(
+        None,
+        description='Retention period of the data in the database. Period is expressed as duration in ISO 8601 format in UTC. Example - `P23DT23H`.',
+    )
+    extension: Optional[basic.EntityExtension] = Field(
+        None,
+        description='Entity extension data with custom attributes added to the entity.',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createDatabaseSchema.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createDatabaseSchema.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDatabaseSchema.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -32,7 +32,15 @@
     database: basic.FullyQualifiedEntityName = Field(
         ...,
         description='Link to the database fully qualified name where this schema is hosted in',
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this table'
     )
+    retentionPeriod: Optional[basic.Duration] = Field(
+        None,
+        description='Retention period of the data in the database. Period is expressed as duration in ISO 8601 format in UTC. Example - `P23DT23H`.',
+    )
+    extension: Optional[basic.EntityExtension] = Field(
+        None,
+        description='Entity extension data with custom attributes added to the entity.',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createGlossary.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createGlossary.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createGlossary.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createGlossaryTerm.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createGlossaryTerm.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createGlossaryTerm.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -13,16 +13,16 @@
 from ...type import basic, entityReference, tagLabel
 
 
 class CreateGlossaryTermRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    glossary: basic.EntityName = Field(
-        ..., description='Name of the glossary that this term is part of.'
+    glossary: basic.FullyQualifiedEntityName = Field(
+        ..., description='FullyQualifiedName of the glossary that this term is part of.'
     )
     parent: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='Fully qualified name of  the parent glossary term.'
     )
     name: basic.EntityName = Field(
         ..., description='Preferred name for the glossary term.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createMlModel.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createMlModel.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createMlModel.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createPipeline.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -23,15 +23,15 @@
         None,
         description='Display Name that identifies this Pipeline. It could be title or label from the source services.',
     )
     description: Optional[basic.Markdown] = Field(
         None,
         description='Description of the pipeline instance. What it has and how to use it.',
     )
-    pipelineUrl: Optional[str] = Field(
+    sourceUrl: Optional[basic.SourceUrl] = Field(
         None,
         description='Pipeline URL suffix to visit/manage. This URL points to respective pipeline service UI',
     )
     concurrency: Optional[int] = Field(None, description='Concurrency of the Pipeline')
     pipelineLocation: Optional[str] = Field(None, description='Pipeline Code Location')
     startDate: Optional[basic.DateTime] = Field(
         None, description='Start date of the workflow'
@@ -49,7 +49,10 @@
         ...,
         description='Link to the pipeline service fqn where this pipeline is hosted in',
     )
     extension: Optional[basic.EntityExtension] = Field(
         None,
         description='Entity extension data with custom attributes added to the entity.',
     )
+    scheduleInterval: Optional[str] = Field(
+        None, description='Scheduler Interval for the pipeline in cron format.'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createQuery.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createQuery.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createQuery.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityReference, tagLabel
+from ...type import basic, entityReference, entityReferenceList, tagLabel
 
 
 class CreateQueryRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: Optional[basic.EntityName] = Field(
@@ -36,10 +36,10 @@
     )
     users: Optional[List[basic.FullyQualifiedEntityName]] = Field(
         None, description='UserName of the user running the query.'
     )
     queryDate: Optional[basic.Timestamp] = Field(
         None, description='Date on which the query ran.'
     )
-    queryUsedIn: Optional[entityReference.EntityReferenceList] = Field(
+    queryUsedIn: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='list of entities to which the query is joined.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createTable.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createTable.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTable.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -41,11 +41,18 @@
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this table'
     )
     viewDefinition: Optional[basic.SqlQuery] = Field(
         None, description='View Definition in SQL. Applies to TableType.View only'
     )
+    retentionPeriod: Optional[basic.Duration] = Field(
+        None,
+        description='Retention period of the data in the database. Period is expressed as duration in ISO 8601 format in UTC. Example - `P23DT23H`.',
+    )
     extension: Optional[basic.EntityExtension] = Field(
         None,
         description='Entity extension data with custom attributes added to the entity.',
     )
+    sourceUrl: Optional[basic.SourceUrl] = Field(
+        None, description='Source URL of table.'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createTableProfile.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createTableProfile.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTableProfile.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/data/createTopic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/data/createTopic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTopic.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/dataInsight/createDataInsightChart.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/dataInsight/kpi/createKpiRequest.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/createPost.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/createPost.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createPost.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
 class CreatePostRequest(BaseModel):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/createThread.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/createThread.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createThread.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ...entity.feed import thread
-from ...type import basic, entityReference
+from ...type import basic, entityReferenceList
 
 
 class CreateTaskDetails(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: thread.TaskType
-    assignees: entityReference.EntityReferenceList = Field(
+    assignees: entityReferenceList.EntityReferenceListModel = Field(
         ..., description='List of users or teams the task is assigned to'
     )
     oldValue: Optional[str] = Field(
         None, description='The value of old object for which the task is created.'
     )
     suggestion: Optional[str] = Field(
         None, description='The suggestion object for the task provided by the creator.'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/feed/threadCount.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/feed/threadCount.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/threadCount.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/lineage/addLineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,21 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  api/lineage/addLineage.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  dataInsight/type/totalEntitiesByTier.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field, confloat
 
-from ...type import basic, entityLineage
+from ...type import basic
 
 
-class AddLineageRequest(BaseModel):
+class TotalEntitiesByTier(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    description: Optional[basic.Markdown] = Field(
-        None, description='User provided description of the lineage details.'
+    timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
+    entityTier: Optional[str] = Field(
+        None, description='Tier of entity. Derived from tags.'
+    )
+    entityCountFraction: Optional[confloat(ge=0.0, le=1.0)] = Field(
+        None, description='Total count of entity for the given entity type'
+    )
+    entityCount: Optional[float] = Field(
+        None, description='Total count of entity for the given entity type'
     )
-    edge: entityLineage.EntitiesEdge = Field(..., description='Lineage edge details.')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/openMetadataServerVersion.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/openMetadataServerVersion.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/openMetadataServerVersion.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/policies/createPolicy.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/policies/createPolicy.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/policies/createPolicy.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createDashboardService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createDashboardService.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createDashboardService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createDatabaseService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createDatabaseService.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createDatabaseService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createMessagingService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createMessagingService.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMessagingService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createMetadataService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createMetadataService.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMetadataService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createMlModelService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createMlModelService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMlModelService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createPipelineService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createPipelineService.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createPipelineService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -29,7 +29,10 @@
     connection: pipelineService.PipelineConnection
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this Pipeline Service.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this pipeline service.'
     )
+    scheduleInterval: Optional[str] = Field(
+        None, description='Scheduler Interval for the pipeline in cron format.'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/createStorageService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/createStorageService.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createStorageService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/ingestionPipelines/createIngestionPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/setOwner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/setOwner.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/setOwner.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/createRole.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/createRole.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createRole.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/createTeam.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/createTeam.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createTeam.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/teams/createUser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/teams/createUser.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createUser.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createCustomMetric.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createCustomMetric.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createCustomMetric.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createTestCase.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createTestCase.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestCase.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createTestDefinition.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createTestDefinition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestDefinition.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/api/tests/createTestSuite.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/api/tests/createTestSuite.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,29 +1,40 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestSuite.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field, constr
 
 from ...type import basic, entityReference
 
 
+class TestSuiteEntityName(BaseModel):
+    __root__: constr(min_length=1, max_length=256) = Field(
+        ...,
+        description='Name of a test suite entity. For executable testSuite, this should match the entity FQN in the platform.',
+    )
+
+
 class CreateTestSuiteRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: basic.EntityName = Field(
+    name: TestSuiteEntityName = Field(
         ..., description='Name that identifies this test suite.'
     )
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this test suite.'
     )
-    description: basic.Markdown = Field(
-        ..., description='Description of the test suite.'
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the test suite.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this test suite'
     )
+    executableEntityReference: Optional[basic.FullyQualifiedEntityName] = Field(
+        None,
+        description='FQN of the entity the test suite is executed against.. Only applicable for executable test suites.',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/changePasswordRequest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/changePasswordRequest.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/changePasswordRequest.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/emailVerificationToken.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/emailVerificationToken.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/emailVerificationToken.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/jwtAuth.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/jwtAuth.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/jwtAuth.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/logoutRequest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # generated by datamodel-codegen:
-#   filename:  auth/logoutRequest.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  dataInsight/type/pageViewsByEntities.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..type import basic
+from ...type import basic
 
 
-class LogoutRequest(BaseModel):
+class PageViewsByEntities(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    username: Optional[str] = Field(None, description='Logout Username')
-    token: str = Field(..., description='Token To be Expired')
-    logoutTime: Optional[basic.DateTime] = Field(None, description='Logout Time')
-    refreshToken: Optional[str] = Field(None, description='Refresh Token')
+    timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
+    pageViews: Optional[float] = Field(None, description='Number of page views')
+    entityType: Optional[str] = Field(
+        None, description='Type of entity. Derived from the page URL.'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/passwordResetRequest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/passwordResetRequest.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/passwordResetRequest.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/passwordResetToken.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/passwordResetToken.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/passwordResetToken.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/personalAccessToken.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/personalAccessToken.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/personalAccessToken.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/refreshToken.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/refreshToken.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/refreshToken.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/registrationRequest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/registrationRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/registrationRequest.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field, constr
 
 from ..type import basic
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/auth/ssoAuth.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/auth/ssoAuth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/ssoAuth.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/applicationConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/applicationConfiguration.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/applicationConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/authConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/authConfig.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/authenticationConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/authenticationConfiguration.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authenticationConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/authorizerConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/authorizerConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authorizerConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/queryParserData.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,37 +1,41 @@
 # generated by datamodel-codegen:
-#   filename:  configuration/elasticSearchConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/queryParserData.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from enum import Enum
-from typing import Optional
+from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 
-class SearchIndexMappingLanguage(Enum):
-    EN = 'EN'
-    JP = 'JP'
-    ZH = 'ZH'
+class ParsedData(BaseModel):
+    tables: List[str] = Field(..., description='List of tables used in query')
+    databaseName: Optional[str] = Field(
+        None, description='Database associated with the table in the query'
+    )
+    joins: Optional[Dict[str, Any]] = Field(
+        None,
+        description='Maps each parsed table name of a query to the join information',
+    )
+    sql: str = Field(..., description='SQL query')
+    serviceName: str = Field(
+        ..., description='Name that identifies this database service.'
+    )
+    userName: Optional[str] = Field(
+        None, description='Name of the user that executed the SQL query'
+    )
+    date: Optional[str] = Field(None, description='Date of execution of SQL query')
+    databaseSchema: Optional[str] = Field(
+        None, description='Database schema of the associated with query'
+    )
+    duration: Optional[float] = Field(
+        None, description='How long did the query took to run in seconds.'
+    )
 
 
-class ElasticSearchConfiguration(BaseModel):
+class QueryParserData(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    host: str = Field(..., description='Elastic Search Host')
-    port: int = Field(..., description='Elastic Search port')
-    scheme: str = Field(..., description='Http/Https connection scheme')
-    username: Optional[str] = Field(
-        None, description='Elastic Search Username for Login'
-    )
-    password: Optional[str] = Field(
-        None, description='Elastic Search Password for Login'
-    )
-    truststorePath: Optional[str] = Field(None, description='Truststore Path')
-    truststorePassword: Optional[str] = Field(None, description='Truststore Password')
-    connectionTimeoutSecs: int = Field(..., description='Connection Timeout in Seconds')
-    socketTimeoutSecs: int = Field(..., description='Socket Timeout in Seconds')
-    batchSize: int = Field(..., description='Batch Size for Requests')
-    searchIndexMappingLanguage: SearchIndexMappingLanguage
+    parsedData: Optional[List[ParsedData]] = None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/jwtTokenConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/kafkaEventConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapConfiguration.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/customTrustManagerConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/hostNameConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/truststoreConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/pipelineServiceClientConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from ..security.secrets import secretsManagerClientLoader
 from ..security.ssl import verifySSLConfig
 from . import authConfig
 
 
 class PipelineServiceClientConfiguration(BaseModel):
     class Config:
         extra = Extra.forbid
@@ -23,14 +24,18 @@
         ...,
         description='External API root to interact with the Pipeline Service Client',
     )
     hostIp: Optional[str] = Field(
         None,
         description='Pipeline Service Client host IP that will be used to connect to the sources.',
     )
+    healthCheckInterval: Optional[int] = Field(
+        300,
+        description='Interval in seconds that the server will use to check the /status of the pipelineServiceClient and flag any errors in a Prometheus metric `pipelineServiceClientStatus.counter`.',
+    )
     ingestionIpInfoEnabled: Optional[bool] = Field(
         False,
         description='Enable or disable the API that fetches the public IP running the ingestion process.',
     )
     metadataApiEndpoint: str = Field(
         ..., description='Metadata api endpoint, e.g., `http://localhost:8585/api`'
     )
@@ -38,14 +43,17 @@
         verifySSLConfig.VerifySSL.no_ssl,
         description='Client SSL verification policy when connecting to the OpenMetadata server: no-ssl, ignore, validate.',
     )
     sslConfig: Optional[verifySSLConfig.SslConfig] = Field(
         None,
         description='OpenMetadata Client SSL configuration. This SSL information is about the OpenMetadata server. It will be picked up from the pipelineServiceClient to use/ignore SSL when connecting to the OpenMetadata server.',
     )
+    secretsManagerLoader: Optional[
+        secretsManagerClientLoader.SecretsManagerClientLoader
+    ] = secretsManagerClientLoader.SecretsManagerClientLoader.noop
     authProvider: Optional[str] = Field(
         None,
         description='Auth Provider like no-auth, azure , google, okta, auth0, customOidc, openmetadata',
     )
     authConfig: Optional[authConfig.AuthConfiguration] = Field(
         None, description='Auth Provider Configuration.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/testResultNotificationConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/dataInsightChart.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChart.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/dataInsightChart.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/dataInsightChartResult.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/kpi/basic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/basic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/kpi/basic.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/kpi/kpi.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/kpi.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/kpi/kpi.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/filterPattern.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,21 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  dataInsight/type/dailyActiveUsers.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/filterPattern.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic
 
+class FilterPatternModel(BaseModel):
+    pass
 
-class DailyActiveUsers(BaseModel):
+
+class FilterPattern(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
-    activeUsers: Optional[int] = Field(
-        None, description='Number of active users (user with at least 1 session).'
+    includes: Optional[List[str]] = Field(
+        None,
+        description='List of strings/regex patterns to match and include only database entities that match.',
+    )
+    excludes: Optional[List[str]] = Field(
+        None,
+        description='List of strings/regex patterns to match and exclude only database entities that match.',
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/mostActiveUsers.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/mostViewedEntities.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,22 +1,20 @@
 # generated by datamodel-codegen:
-#   filename:  dataInsight/type/pageViewsByEntities.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/database/common/iamAuthConfig.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic
+from ......security.credentials import awsCredentials
 
 
-class PageViewsByEntities(BaseModel):
+class IamAuthConfigurationSource(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
-    pageViews: Optional[float] = Field(None, description='Number of page views')
-    entityType: Optional[str] = Field(
-        None, description='Type of entity. Derived from the page URL.'
+    awsConfig: Optional[awsCredentials.AWSCredentials] = Field(
+        None, title='AWS Credentials Configuration'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfEntitiesWithDescriptionByType.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfEntitiesWithOwnerByType.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  dataInsight/type/totalEntitiesByTier.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  dataInsight/type/totalEntitiesByType.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
 
 from ...type import basic
 
 
-class TotalEntitiesByTier(BaseModel):
+class TotalEntitiesByType(BaseModel):
     class Config:
         extra = Extra.forbid
 
     timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
-    entityTier: Optional[str] = Field(
-        None, description='Tier of entity. Derived from tags.'
+    entityType: Optional[str] = Field(
+        None, description='Type of entity. Derived from the entity class.'
     )
-    entityCountFraction: Optional[confloat(ge=0.0, le=1.0)] = Field(
+    entityCount: Optional[float] = Field(
         None, description='Total count of entity for the given entity type'
     )
-    entityCount: Optional[float] = Field(
+    entityCountFraction: Optional[confloat(ge=0.0, le=1.0)] = Field(
         None, description='Total count of entity for the given entity type'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/email/emailRequest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/email/emailRequest.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  email/emailRequest.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/email/smtpSettings.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/email/smtpSettings.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  email/smtpSettings.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/automations/testServiceConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/automations/testServiceConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/automations/testServiceConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/automations/workflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/automations/workflow.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/automations/workflow.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/bot.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/bot.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/bot.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/classification/classification.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/classification/classification.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/classification/classification.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/classification/tag.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/classification/tag.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  entity/classification/tag.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference
+from ...type import basic, entityHistory, entityReference, entityReferenceList
 
 
 class TagName(BaseModel):
     __root__: constr(
         regex=r'^(?u)[\w\'\- .&()]+$', min_length=2, max_length=64
     ) = Field(..., description='Name of the tag.')
 
@@ -35,15 +35,15 @@
     classification: Optional[entityReference.EntityReference] = Field(
         None, description='Reference to the classification that this tag is part of.'
     )
     parent: Optional[entityReference.EntityReference] = Field(
         None,
         description='Reference to the parent tag. When null, the term is at the root of the Classification.',
     )
-    children: Optional[entityReference.EntityReferenceList] = Field(
+    children: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Children tags under this tag.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/chart.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/chart.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,26 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/chart.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel, usageDetails
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    tagLabel,
+    usageDetails,
+)
 from ..services import dashboardService
 
 
 class ChartType(Enum):
     Line = 'Line'
     Table = 'Table'
     Bar = 'Bar'
@@ -50,37 +57,37 @@
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     chartType: Optional[ChartType] = None
-    chartUrl: Optional[str] = Field(
+    sourceUrl: Optional[basic.SourceUrl] = Field(
         None, description='Chart URL suffix from its service.'
     )
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this dashboard.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this chart.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this chart.'
     )
     service: entityReference.EntityReference = Field(
         ..., description='Link to service where this dashboard is hosted in.'
     )
     serviceType: Optional[dashboardService.DashboardServiceType] = Field(
         None, description='Service type where this chart is hosted in.'
     )
     usageSummary: Optional[usageDetails.UsageDetails] = Field(
-        None, description='Latest usage information for this database.'
+        None, description='Latest usage information for this chart.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/container.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/container.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/container.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from ..services import storageService
 from . import table
 
 
 class EntityName(BaseModel):
     __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=128) = Field(
         ...,
@@ -81,15 +81,15 @@
         ...,
         description='Link to the storage service where this container is hosted in.',
     )
     parent: Optional[entityReference.EntityReference] = Field(
         None,
         description='Link to the parent container under which this entity sits, if not top level.',
     )
-    children: Optional[entityReference.EntityReferenceList] = Field(
+    children: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='References to child containers residing under this entity.'
     )
     dataModel: Optional[ContainerDataModel] = Field(
         None,
         description="References to the container's data model, if data is structured, or null otherwise",
     )
     prefix: Optional[str] = Field(
@@ -104,15 +104,15 @@
     fileFormats: Optional[List[FileFormat]] = Field(
         None,
         description='File & data formats identified for the container:  e.g. dataFormats=[csv, json]. These can be present both when the container has a dataModel or not',
     )
     serviceType: Optional[storageService.StorageServiceType] = Field(
         None, description='Service type this table is hosted in.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this container.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this container.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/dashboard.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/messagingService.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,84 +1,104 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/dashboard.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/messagingService.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from enum import Enum
+from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel, usageDetails
-from ..services import dashboardService
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
+from .connections import testConnectionResult
+from .connections.messaging import (
+    customMessagingConnection,
+    kafkaConnection,
+    kinesisConnection,
+    redpandaConnection,
+)
 
 
-class Dashboard(BaseModel):
+class MessagingServiceType(Enum):
+    Kafka = 'Kafka'
+    Redpanda = 'Redpanda'
+    Kinesis = 'Kinesis'
+    CustomMessaging = 'CustomMessaging'
+
+
+class Brokers(BaseModel):
+    __root__: List[str] = Field(
+        ...,
+        description='Multiple bootstrap addresses for Kafka. Single proxy address for Pulsar.',
+    )
+
+
+class MessagingConnection(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    config: Optional[
+        Union[
+            kafkaConnection.KafkaConnection,
+            redpandaConnection.RedpandaConnection,
+            kinesisConnection.KinesisConnection,
+            customMessagingConnection.CustomMessagingConnection,
+        ]
+    ] = None
+
+
+class MessagingService(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies a dashboard instance.'
+        ..., description='Unique identifier of this messaging service instance.'
     )
     name: basic.EntityName = Field(
-        ..., description='Name that identifies this dashboard.'
+        ..., description='Name that identifies this messaging service.'
+    )
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='FullyQualifiedName same as `name`.'
+    )
+    serviceType: MessagingServiceType = Field(
+        ..., description='Type of messaging service such as Kafka or Pulsar...'
+    )
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of a messaging service instance.'
     )
     displayName: Optional[str] = Field(
         None,
-        description='Display Name that identifies this Dashboard. It could be title or label from the source services.',
+        description='Display Name that identifies this messaging service. It could be title or label from the source services.',
     )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+    connection: Optional[MessagingConnection] = None
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
-        description="A unique name that identifies a dashboard in the format 'ServiceName.DashboardName'.",
+        description='References to pipelines deployed for this messaging service to extract topic configs and schemas.',
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the dashboard, what it is, and how to use it.'
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
+        None, description='Last test connection results for this service'
+    )
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this Message Service.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    dashboardUrl: Optional[str] = Field(
-        None, description='Dashboard URL suffix from its service.'
-    )
-    charts: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='All the charts included in this Dashboard.'
-    )
-    dataModels: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='List of data models used by this dashboard or the charts contained on it.',
-    )
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
-    )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this dashboard.'
-    )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Followers of this dashboard.'
+        None, description='Owner of this messaging service.'
     )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this dashboard.'
-    )
-    service: entityReference.EntityReference = Field(
-        ..., description='Link to service where this dashboard is hosted in.'
-    )
-    serviceType: Optional[dashboardService.DashboardServiceType] = Field(
-        None, description='Service type where this dashboard is hosted in.'
-    )
-    usageSummary: Optional[usageDetails.UsageDetails] = Field(
-        None, description='Latest usage information for this database.'
+    href: Optional[basic.Href] = Field(
+        None,
+        description='Link to the resource corresponding to this messaging service.',
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    extension: Optional[basic.EntityExtension] = Field(
-        None,
-        description='Entity extension data with custom attributes added to the entity.',
-    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/dashboardDataModel.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/dashboardDataModel.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/dashboardDataModel.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from ..services import dashboardService
 from . import table
 
 
 class DataModelType(Enum):
     TableauDataModel = 'TableauDataModel'
     SupersetDataModel = 'SupersetDataModel'
     MetabaseDataModel = 'MetabaseDataModel'
     LookMlView = 'LookMlView'
     LookMlExplore = 'LookMlExplore'
+    PowerBIDataModel = 'PowerBIDataModel'
 
 
 class DashboardDataModel(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
@@ -63,24 +64,28 @@
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this dashboard.'
     )
     service: Optional[entityReference.EntityReference] = Field(
         None, description='Link to service where this data model is hosted in.'
     )
     serviceType: Optional[dashboardService.DashboardServiceType] = Field(
         None, description='Service type where this data model is hosted in.'
     )
     dataModelType: DataModelType
     sql: Optional[basic.SqlQuery] = Field(
         None, description='In case the Data Model is based on a SQL query.'
     )
     columns: List[table.Column] = Field(..., description='Columns from the data model.')
-    dataModels: Optional[entityReference.EntityReferenceList] = Field(
+    dataModels: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='List of data models used by this data model.'
     )
+    project: Optional[str] = Field(
+        None,
+        description='Name of the project / workspace / collection in which the dataModel is contained',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/database.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/databaseSchema.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,81 +1,92 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/database.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/data/databaseSchema.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference, tagLabel, usageDetails
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    tagLabel,
+    usageDetails,
+)
 from ..services import databaseService
 
 
 class EntityName(BaseModel):
     __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=128) = Field(
         ..., description='Name of a table. Expected to be unique within a database.'
     )
 
 
-class Database(BaseModel):
+class DatabaseSchema(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier that identifies this database instance.'
+        None, description='Unique identifier that identifies this schema instance.'
     )
-    name: EntityName = Field(..., description='Name that identifies the database.')
+    name: EntityName = Field(..., description='Name that identifies the schema.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None,
-        description="Name that uniquely identifies a database in the format 'ServiceName.DatabaseName'.",
+        description="Name that uniquely identifies a schema in the format 'ServiceName.DatabaseName.SchemaName'.",
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this database.'
+        None, description='Display Name that identifies this schema.'
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of the database instance.'
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this Database.'
+        None, description='Description of the schema instance.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this database.'
+        None, description='Owner of this schema.'
     )
     service: entityReference.EntityReference = Field(
         ...,
-        description='Link to the database cluster/service where this database is hosted in.',
+        description='Link to the database cluster/service where this schema is hosted in.',
     )
     serviceType: Optional[databaseService.DatabaseServiceType] = Field(
-        None, description='Service type where this database is hosted in.'
+        None, description='Service type where this schema is hosted in.'
+    )
+    database: entityReference.EntityReference = Field(
+        ..., description='Reference to Database that contains this table.'
     )
-    location: Optional[entityReference.EntityReference] = Field(
-        None, description='Reference to the Location that contains this database.'
+    tables: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='References to tables in the schema.'
     )
     usageSummary: Optional[usageDetails.UsageDetails] = Field(
         None, description='Latest usage information for this database.'
     )
-    databaseSchemas: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='References to schemas in the database.'
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this Database Schema Service.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
-    default: Optional[bool] = Field(
-        False,
-        description="Some databases don't support a database/catalog in the hierarchy and use default database. For example, `MySql`. For such databases, set this flag to true to indicate that this is a default database.",
-    )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
+    retentionPeriod: Optional[basic.Duration] = Field(
+        None,
+        description='Retention period of the data in the database schema. Period is expressed as duration in ISO 8601 format in UTC. Example - `P23DT23H`. When not set, the retention period is inherited from the parent database, if it exists.',
+    )
+    extension: Optional[basic.EntityExtension] = Field(
+        None,
+        description='Entity extension data with custom attributes added to the entity.',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/databaseSchema.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/report.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,77 +1,60 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/databaseSchema.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/data/report.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Optional
 
-from pydantic import BaseModel, Extra, Field, constr
+from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel, usageDetails
-from ..services import databaseService
+from ...type import basic, entityHistory, entityReference, usageDetails
 
 
-class EntityName(BaseModel):
-    __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=128) = Field(
-        ..., description='Name of a table. Expected to be unique within a database.'
-    )
-
-
-class DatabaseSchema(BaseModel):
+class Report(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier that identifies this schema instance.'
+    id: basic.Uuid = Field(
+        ..., description='Unique identifier that identifies this report.'
+    )
+    name: basic.EntityName = Field(
+        ..., description='Name that identifies this report instance uniquely.'
     )
-    name: EntityName = Field(..., description='Name that identifies the schema.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None,
-        description="Name that uniquely identifies a schema in the format 'ServiceName.DatabaseName.SchemaName'.",
+        description="A unique name that identifies a report in the format 'ServiceName.ReportName'.",
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this schema.'
+        None,
+        description='Display Name that identifies this report. It could be title or label from the source services.',
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of the schema instance.'
+        None, description='Description of this report instance.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
+        None, description='Link to the resource corresponding to this report.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this schema.'
+        None, description='Owner of this pipeline.'
     )
     service: entityReference.EntityReference = Field(
-        ...,
-        description='Link to the database cluster/service where this schema is hosted in.',
-    )
-    serviceType: Optional[databaseService.DatabaseServiceType] = Field(
-        None, description='Service type where this schema is hosted in.'
-    )
-    database: entityReference.EntityReference = Field(
-        ..., description='Reference to Database that contains this table.'
-    )
-    tables: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='References to tables in the schema.'
+        ..., description='Link to service where this report is hosted in.'
     )
     usageSummary: Optional[usageDetails.UsageDetails] = Field(
         None, description='Latest usage information for this database.'
     )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this Database Schema Service.'
-    )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/glossary.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/glossary.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/glossary.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/glossaryTerm.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/glossaryTerm.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/glossaryTerm.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 
 
 class TermReference(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: Optional[str] = Field(
@@ -59,19 +59,19 @@
     glossary: entityReference.EntityReference = Field(
         ..., description='Glossary that this term belongs to.'
     )
     parent: Optional[entityReference.EntityReference] = Field(
         None,
         description='Parent glossary term that this term is child of. When `null` this term is the root term of the glossary.',
     )
-    children: Optional[entityReference.EntityReferenceList] = Field(
+    children: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
         description='Other glossary terms that are children of this glossary term.',
     )
-    relatedTerms: Optional[entityReference.EntityReferenceList] = Field(
+    relatedTerms: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Other glossary terms that are related to this glossary term.'
     )
     references: Optional[List[TermReference]] = Field(
         None, description='Link to a reference from an external glossary.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
@@ -80,15 +80,15 @@
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
-    reviewers: Optional[entityReference.EntityReferenceList] = Field(
+    reviewers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='User names of the reviewers for this glossary.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this glossary term.'
     )
     usageCount: Optional[int] = Field(
         None,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/metrics.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/metrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/metrics.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/mlmodel.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/mlmodel.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,26 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/mlmodel.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel, usageDetails
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    tagLabel,
+    usageDetails,
+)
 from ..services import mlmodelService
 
 
 class FeatureType(Enum):
     numerical = 'numerical'
     categorical = 'categorical'
 
@@ -138,15 +145,15 @@
     )
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this ML Model.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this ML Model.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this ML Model.'
     )
     usageSummary: Optional[usageDetails.UsageDetails] = Field(
         None, description='Latest usage information for this ML Model.'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/pipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/pipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from ..services import pipelineService
 
 
 class EntityName(BaseModel):
     __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=128) = Field(
         ..., description='Name of a table. Expected to be unique within a database.'
     )
@@ -67,15 +67,15 @@
     fullyQualifiedName: Optional[str] = Field(
         None,
         description="A unique name that identifies a pipeline in the format 'ServiceName.PipelineName.TaskName'.",
     )
     description: Optional[basic.Markdown] = Field(
         None, description='Description of this Task.'
     )
-    taskUrl: Optional[str] = Field(
+    sourceUrl: Optional[basic.SourceUrl] = Field(
         None,
         description='Task URL to visit/manage. This URL points to respective pipeline service UI.',
     )
     downstreamTasks: Optional[List[str]] = Field(
         None, description='All the tasks that are downstream of this task.'
     )
     taskType: Optional[str] = Field(
@@ -116,30 +116,30 @@
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    pipelineUrl: Optional[str] = Field(
+    sourceUrl: Optional[basic.SourceUrl] = Field(
         None,
         description='Pipeline  URL to visit/manage. This URL points to respective pipeline service UI.',
     )
     concurrency: Optional[int] = Field(None, description='Concurrency of the Pipeline.')
     pipelineLocation: Optional[str] = Field(None, description='Pipeline Code Location.')
     startDate: Optional[basic.DateTime] = Field(
         None, description='Start date of the workflow.'
     )
     tasks: Optional[List[Task]] = Field(
         None, description='All the tasks that are part of pipeline.'
     )
     pipelineStatus: Optional[PipelineStatus] = Field(
         None, description='Latest Pipeline Status.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this Pipeline.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this Pipeline.'
     )
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
@@ -159,7 +159,10 @@
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
     extension: Optional[basic.EntityExtension] = Field(
         None,
         description='Entity extension data with custom attributes added to the entity.',
     )
+    scheduleInterval: Optional[str] = Field(
+        None, description='Scheduler Interval for the pipeline in cron format.'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/query.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/query.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,25 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/query.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel, votes
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    tagLabel,
+    votes,
+)
 
 
 class Query(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
@@ -48,27 +55,27 @@
     )
     duration: Optional[float] = Field(
         None, description='How long did the query took to run in seconds.'
     )
     users: Optional[List[entityReference.EntityReference]] = Field(
         None, description='List of users who ran this query.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this Query.'
     )
     votes: Optional[votes.Votes] = None
     query: basic.SqlQuery = Field(..., description='SQL Query definition.')
     checksum: Optional[str] = Field(
         None, description='Checksum to avoid registering duplicate queries.'
     )
     queryDate: Optional[basic.Timestamp] = Field(
         None, description='Date on which the query ran.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this SQL query.'
     )
-    queryUsedIn: Optional[entityReference.EntityReferenceList] = Field(
+    queryUsedIn: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Entities that are using this query'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/report.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityReference.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,60 +1,39 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/report.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/entityReference.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, usageDetails
+from . import basic
 
 
-class Report(BaseModel):
+class EntityReference(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies this report.'
+        ..., description='Unique identifier that identifies an entity instance.'
     )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this report instance uniquely.'
+    type: str = Field(
+        ...,
+        description='Entity type/class name - Examples: `database`, `table`, `metrics`, `databaseService`, `dashboardService`...',
     )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+    name: Optional[str] = Field(None, description='Name of the entity instance.')
+    fullyQualifiedName: Optional[str] = Field(
         None,
-        description="A unique name that identifies a report in the format 'ServiceName.ReportName'.",
-    )
-    displayName: Optional[str] = Field(
-        None,
-        description='Display Name that identifies this report. It could be title or label from the source services.',
+        description="Fully qualified name of the entity instance. For entities such as tables, databases fullyQualifiedName is returned in this field. For entities that don't have name hierarchy such as `user` and `team` this will be same as the `name` field.",
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of this report instance.'
-    )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
-    )
-    updatedAt: Optional[basic.Timestamp] = Field(
-        None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+        None, description='Optional description of entity.'
     )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this report.'
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this pipeline.'
-    )
-    service: entityReference.EntityReference = Field(
-        ..., description='Link to service where this report is hosted in.'
-    )
-    usageSummary: Optional[usageDetails.UsageDetails] = Field(
-        None, description='Latest usage information for this database.'
-    )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this entity.'
     )
     deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
+        None, description='If true the entity referred to has been soft-deleted.'
     )
+    href: Optional[basic.Href] = Field(None, description='Link to the entity resource.')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/table.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/table.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,28 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/table.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
+from datetime import datetime
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field, constr
 
-from ...tests import customMetric
-from ...type import basic, entityHistory, entityReference, tagLabel, usageDetails
+from ...tests import customMetric, testSuite
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    tagLabel,
+    usageDetails,
+)
 from ..services import databaseService
 
 
 class EntityName(BaseModel):
     __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=128) = Field(
         ..., description='Name of a table. Expected to be unique within a database.'
     )
@@ -414,14 +422,16 @@
     columnCount: Optional[float] = Field(
         None, description='No.of columns in the table.'
     )
     rowCount: Optional[float] = Field(
         None,
         description='No.of rows in the table. This is always executed on the whole table.',
     )
+    sizeInByte: Optional[float] = Field(None, description='Table size in GB')
+    createDateTime: Optional[datetime] = Field(None, description='Table creation time.')
 
 
 class ModelType(Enum):
     DBT = 'DBT'
 
 
 class Column(BaseModel):
@@ -574,15 +584,15 @@
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this table.'
     )
     usageSummary: Optional[usageDetails.UsageDetails] = Field(
         None, description='Latest usage information for this table.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this table.'
     )
     joins: Optional[TableJoins] = Field(
         None,
         description='Details of other tables this table is frequently joined with.',
     )
     sampleData: Optional[TableData] = Field(
@@ -591,24 +601,34 @@
     tableProfilerConfig: Optional[TableProfilerConfig] = Field(
         None,
         description='Table Profiler Config to include or exclude columns from profiling.',
     )
     profile: Optional[TableProfile] = Field(
         None, description='Latest Data profile for a table.'
     )
+    testSuite: Optional[testSuite.TestSuite] = Field(
+        None, description='Executable test suite associated with this table'
+    )
     dataModel: Optional[DataModel] = Field(
         None,
         description='This captures information about how the table is modeled. Currently only DBT model is supported.',
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
+    retentionPeriod: Optional[basic.Duration] = Field(
+        None,
+        description='Retention period of the data in the table. Period is expressed as duration in ISO 8601 format in UTC. Example - `P23DT23H`. When not set, the retention period is inherited from the parent database schema, if it exists.',
+    )
     extension: Optional[basic.EntityExtension] = Field(
         None,
         description='Entity extension data with custom attributes added to the entity.',
     )
+    sourceUrl: Optional[basic.SourceUrl] = Field(
+        None, description='Source URL of table.'
+    )
 
 
 Column.update_forward_refs()
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/data/topic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/topic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,26 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/topic.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
 
-from ...type import basic, entityHistory, entityReference, schema, tagLabel
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    schema,
+    tagLabel,
+)
 from ..services import messagingService
 
 
 class CleanupPolicy(Enum):
     delete = 'delete'
     compact = 'compact'
 
@@ -97,15 +104,15 @@
     )
     sampleData: Optional[TopicSampleData] = Field(
         None, description='Sample data for a topic.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this topic.'
     )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Followers of this table.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this table.'
     )
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/events/webhook.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/events/webhook.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/events/webhook.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/feed/thread.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/feed/thread.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/feed/thread.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityReference, reaction
+from ...type import basic, entityReferenceList, reaction
 
 
 class TaskType(Enum):
     RequestDescription = 'RequestDescription'
     UpdateDescription = 'UpdateDescription'
     RequestTag = 'RequestTag'
     UpdateTag = 'UpdateTag'
@@ -50,15 +50,15 @@
 
 class TaskDetails(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: int = Field(..., description='Unique identifier that identifies the task.')
     type: TaskType
-    assignees: entityReference.EntityReferenceList = Field(
+    assignees: entityReferenceList.EntityReferenceListModel = Field(
         ..., description='List of users or teams the task is assigned to'
     )
     status: Optional[ThreadTaskStatus] = ThreadTaskStatus.Open
     closedBy: Optional[str] = Field(None, description='The user that closed the task.')
     closedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Timestamp when the task was closed in Unix epoch time milliseconds.',
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/resourceDescriptor.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -34,14 +34,15 @@
     EditRole = 'EditRole'
     EditSampleData = 'EditSampleData'
     EditStatus = 'EditStatus'
     EditTags = 'EditTags'
     EditTeams = 'EditTeams'
     EditTier = 'EditTier'
     EditTests = 'EditTests'
+    EditUsage = 'EditUsage'
     EditUsers = 'EditUsers'
 
 
 class ResourceDescriptor(BaseModel):
     class Config:
         extra = Extra.forbid
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/resourcePermission.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/accessControl/rule.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/rule.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/rule.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/filters.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/filters.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/filters.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Any, List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/policies/policy.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/policies/policy.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/policy.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference
+from ...type import basic, entityHistory, entityReference, entityReferenceList
 from .accessControl import rule
 
 
 class Rules(BaseModel):
     __root__: List[rule.Rule] = Field(
         ..., description='A set of rules associated with the Policy.'
     )
@@ -53,18 +53,18 @@
         description='Last update time corresponding to the new version of the Policy in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that led to this version of the Policy.'
     )
     rules: Rules = Field(..., description='Set of rules that the policy contains.')
-    teams: Optional[entityReference.EntityReferenceList] = Field(
+    teams: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Teams that use this policy directly and not through roles.'
     )
-    roles: Optional[entityReference.EntityReferenceList] = Field(
+    roles: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Roles that use this policy.'
     )
     location: Optional[entityReference.EntityReference] = Field(
         None,
         description='Location to which a policy is applied. This field is relevant only for `lifeCycle` policies.',
     )
     allowDelete: Optional[bool] = Field(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/connectionBasicType.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/customDashboardConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/domoDashboardConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,56 +1,37 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/dashboard/lookerConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/pipeline/airbyteConnection.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional, Union
+from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
-from .....security.credentials import githubCredentials
 from .. import connectionBasicType
 
 
-class LookerType(Enum):
-    Looker = 'Looker'
+class AirbyteType(Enum):
+    Airbyte = 'Airbyte'
 
 
-class NoGitHubCredentials(BaseModel):
-    pass
-
+class AirbyteConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-
-class LookerConnection(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    type: Optional[LookerType] = Field(
-        LookerType.Looker, description='Service Type', title='Service Type'
-    )
-    clientId: str = Field(
-        ...,
-        description="User's Client ID. This user should have privileges to read all the metadata in Looker.",
-        title='Client ID',
-    )
-    clientSecret: CustomSecretStr = Field(
-        ..., description="User's Client Secret.", title='Client Secret'
+    type: Optional[AirbyteType] = Field(
+        AirbyteType.Airbyte, description='Service Type', title='Service Type'
     )
-    hostPort: AnyUrl = Field(
-        ..., description='URL to the Looker instance.', title='Host and Port'
+    hostPort: AnyUrl = Field(..., description='Pipeline Service Management/UI URL.')
+    username: Optional[str] = Field(
+        None, description='Username to connect to Airbyte.', title='Username'
     )
-    githubCredentials: Optional[
-        Union[NoGitHubCredentials, githubCredentials.GitHubCredentials]
-    ] = Field(
-        None,
-        description='Credentials to extract the .lkml files from a repository. This is required to get all the lineage and definitions.',
-        title='GitHub Credentials',
+    password: Optional[CustomSecretStr] = Field(
+        None, description='Password to connect to Airbyte.', title='Password'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/metabaseConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/modeConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/powerBIConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/quickSightConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/redashConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/supersetConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
@@ -25,15 +25,15 @@
     type: Optional[SupersetType] = Field(
         SupersetType.Superset, description='Service Type', title='Service Type'
     )
     hostPort: AnyUrl = Field(
         ..., description='URL for the superset instance.', title='Host and Port'
     )
     connection: Union[
-        supersetApiConnection.SupersetAPIConnection,
+        supersetApiConnection.SupersetApiConnection,
         postgresConnection.PostgresConnection,
         mysqlConnection.MysqlConnection,
     ] = Field(
         ...,
         description='Choose between API or database connection fetch metadata from superset.',
         title='Superset Connection',
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/tableauConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/athenaConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/azureSQLConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/bigQueryConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import gcsCredentials
+from .....security.credentials import gcpCredentials
 from .. import connectionBasicType
 
 
 class BigqueryType(Enum):
     BigQuery = 'BigQuery'
 
 
@@ -34,16 +34,16 @@
         title='Connection Scheme',
     )
     hostPort: Optional[str] = Field(
         'bigquery.googleapis.com',
         description='BigQuery APIs URL.',
         title='Host and Port',
     )
-    credentials: gcsCredentials.GCSCredentials = Field(
-        ..., description='GCS Credentials', title='GCS Credentials'
+    credentials: gcpCredentials.GCPCredentials = Field(
+        ..., description='GCP Credentials', title='GCP Credentials'
     )
     taxonomyProjectID: Optional[List[str]] = Field(
         None,
         description='Project IDs used to fetch policy tags',
         title='Taxonomy Project IDs',
     )
     taxonomyLocation: Optional[str] = Field(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/clickhouseConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/customDatabaseConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/databricksConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -56,14 +56,19 @@
         title='Database Schema',
     )
     connectionTimeout: Optional[int] = Field(
         120,
         description='The maximum amount of time (in seconds) to wait for a successful connection to the data source. If the connection attempt takes longer than this timeout period, an error will be returned.',
         title='Connection Timeout',
     )
+    useUnityCatalog: Optional[bool] = Field(
+        False,
+        description='Use unity catalog for fetching the metadata instead of using the hive metastore',
+        title='Use Unity Catalog',
+    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsUsageExtraction: Optional[
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalake/azureConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalake/gcsConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ......security.credentials import gcsCredentials
+from ......security.credentials import gcpCredentials
 
 
 class GCSConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    securityConfig: Optional[gcsCredentials.GCSCredentials] = Field(
+    securityConfig: Optional[gcpCredentials.GCPCredentials] = Field(
         None, title='DataLake GCS Security Config'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalake/s3Config.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalakeConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/db2Connection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/deltaLakeConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/domoDatabaseConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/druidConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/dynamoDBConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/glueConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/hiveConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/impalaConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/mariaDBConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,66 +1,60 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/mssqlConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/database/prestoConnection.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class MssqlType(Enum):
-    Mssql = 'Mssql'
+class PrestoType(Enum):
+    Presto = 'Presto'
 
 
-class MssqlScheme(Enum):
-    mssql_pyodbc = 'mssql+pyodbc'
-    mssql_pytds = 'mssql+pytds'
-    mssql_pymssql = 'mssql+pymssql'
+class PrestoScheme(Enum):
+    presto = 'presto'
 
 
-class MssqlConnection(BaseModel):
+class PrestoConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[MssqlType] = Field(
-        MssqlType.Mssql, description='Service Type', title='Service Type'
+    type: Optional[PrestoType] = Field(
+        PrestoType.Presto, description='Service Type', title='Service Type'
     )
-    scheme: Optional[MssqlScheme] = Field(
-        MssqlScheme.mssql_pytds,
+    scheme: Optional[PrestoScheme] = Field(
+        PrestoScheme.presto,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
-    username: Optional[str] = Field(
-        None,
-        description='Username to connect to MSSQL. This user should have privileges to read all the metadata in MsSQL.',
+    username: str = Field(
+        ...,
+        description='Username to connect to Presto. This user should have privileges to read all the metadata in Postgres.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to MSSQL.', title='Password'
+        None, description='Password to connect to Presto.', title='Password'
     )
-    hostPort: Optional[str] = Field(
-        None, description='Host and port of the MSSQL service.', title='Host and Port'
+    hostPort: str = Field(
+        ..., description='Host and port of the Presto service.', title='Host and Port'
     )
-    database: Optional[str] = Field(
+    databaseSchema: Optional[str] = Field(
         None,
-        description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
-        title='Database',
-    )
-    driver: Optional[str] = Field(
-        'ODBC Driver 18 for SQL Server',
-        description='ODBC driver version in case of pyodbc connection.',
-        title='Driver',
+        description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
+        title='Database Schema',
     )
+    catalog: Optional[str] = Field(None, description='Presto catalog', title='Catalog')
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
@@ -69,10 +63,10 @@
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
     supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
         None, title='Supports Database'
     )
-    supportsUsageExtraction: Optional[
-        connectionBasicType.SupportsUsageExtraction
-    ] = None
+    supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
+        None, title='Supports Query Comment'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/mysqlConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
 from .. import connectionBasicType
+from .common import basicAuth, iamAuthConfig
 
 
 class MySQLType(Enum):
     Mysql = 'Mysql'
 
 
 class MySQLScheme(Enum):
@@ -35,16 +34,18 @@
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
         description='Username to connect to MySQL. This user should have privileges to read all the metadata in Mysql.',
         title='Username',
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to MySQL.', title='Password'
+    authType: Optional[
+        Union[basicAuth.BasicAuth, iamAuthConfig.IamAuthConfigurationSource]
+    ] = Field(
+        None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
     hostPort: str = Field(
         ..., description='Host and port of the MySQL service.', title='Host and Port'
     )
     databaseName: Optional[str] = Field(
         None,
         description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/oracleConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -67,23 +67,34 @@
         title='Oracle Connection Type',
     )
     instantClientDirectory: Optional[str] = Field(
         '/instantclient',
         description='This directory will be used to set the LD_LIBRARY_PATH env variable. It is required if you need to enable thick connection mode. By default, we bring instant client 19 and point to /instantclient.',
         title='Oracle instant client directory',
     )
+    databaseName: Optional[str] = Field(
+        None,
+        description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
+        title='Database Name',
+    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsUsageExtraction: Optional[
+        connectionBasicType.SupportsUsageExtraction
+    ] = None
+    supportsLineageExtraction: Optional[
+        connectionBasicType.SupportsLineageExtraction
+    ] = None
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/pinotDBConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/postgresConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
 from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
+from .common import basicAuth, iamAuthConfig
 
 
 class SslMode(Enum):
     disable = 'disable'
     allow = 'allow'
     prefer = 'prefer'
     require = 'require'
@@ -45,19 +44,21 @@
         title='Connection Scheme',
     )
     username: str = Field(
         ...,
         description='Username to connect to Postgres. This user should have privileges to read all the metadata in Postgres.',
         title='Username',
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Postgres.', title='Password'
+    authType: Optional[
+        Union[basicAuth.BasicAuth, iamAuthConfig.IamAuthConfigurationSource]
+    ] = Field(
+        None, description='Choose Auth Config Type.', title='Auth Configuration Type'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the Postgres service.', title='Host and Port'
+        ..., description='Host and port of the source service.', title='Host and Port'
     )
     database: str = Field(
         ...,
         description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
         title='Database',
     )
     sslMode: Optional[SslMode] = Field(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,72 +1,93 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/prestoConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/database/sapHanaConnection.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class PrestoType(Enum):
-    Presto = 'Presto'
+class SapHanaType(Enum):
+    SapHana = 'SapHana'
 
 
-class PrestoScheme(Enum):
-    presto = 'presto'
+class SapHanaScheme(Enum):
+    hana = 'hana'
 
 
-class PrestoConnection(BaseModel):
+class SqlConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[PrestoType] = Field(
-        PrestoType.Presto, description='Service Type', title='Service Type'
-    )
-    scheme: Optional[PrestoScheme] = Field(
-        PrestoScheme.presto,
-        description='SQLAlchemy driver scheme options.',
-        title='Connection Scheme',
+    hostPort: str = Field(
+        ..., description='Host and port of the Hana service.', title='Host and Port'
     )
     username: str = Field(
         ...,
-        description='Username to connect to Presto. This user should have privileges to read all the metadata in Postgres.',
+        description='Username to connect to Hana. This user should have privileges to read all the metadata.',
         title='Username',
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Presto.', title='Password'
-    )
-    hostPort: str = Field(
-        ..., description='Host and port of the Presto service.', title='Host and Port'
+    password: CustomSecretStr = Field(
+        ..., description='Password to connect to Hana.', title='Password'
     )
     databaseSchema: Optional[str] = Field(
         None,
-        description='Database Schema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
+        description='Database Schema of the data source. This is an optional parameter, if you would like to restrict the metadata reading to a single schema. When left blank, OpenMetadata Ingestion attempts to scan all the schemas.',
         title='Database Schema',
     )
-    catalog: Optional[str] = Field(None, description='Presto catalog', title='Catalog')
+    database: Optional[str] = Field(
+        None, description='Database of the data source.', title='Database'
+    )
+
+
+class HdbUserStoreConnection(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    userKey: Optional[str] = Field(
+        None,
+        description='HDB Store User Key generated from the command `hdbuserstore SET <KEY> <host:port> <USERNAME> <PASSWORD>`',
+        title='User Key',
+    )
+
+
+class SapHanaConnection(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    type: Optional[SapHanaType] = Field(
+        SapHanaType.SapHana, description='Service Type', title='Service Type'
+    )
+    scheme: Optional[SapHanaScheme] = Field(
+        SapHanaScheme.hana,
+        description='SQLAlchemy driver scheme options.',
+        title='Connection Scheme',
+    )
+    connection: Union[SqlConnection, HdbUserStoreConnection] = Field(
+        ...,
+        description='Choose between Database connection or HDB User Store connection.',
+        title='SAP Hana Connection',
+    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
-    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
-        None, title='Supports Database'
-    )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/redshiftConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/salesforceConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/singleStoreConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/snowflakeConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -70,14 +70,19 @@
         title='Snowflake Passphrase Key',
     )
     includeTempTables: Optional[bool] = Field(
         False,
         description='Optional configuration for ingestion of TRANSIENT and TEMPORARY tables, By default, it will skip the TRANSIENT and TEMPORARY tables.',
         title='Include Temporary and Transient Tables',
     )
+    clientSessionKeepAlive: Optional[bool] = Field(
+        False,
+        description='Optional configuration for ingestion to keep the client session active in case the ingestion process runs for longer durations.',
+        title='Client Session Keep Alive',
+    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/sqliteConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/trinoConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/verticaConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/customMessagingConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/kafkaConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/kinesisConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/pulsarConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/redpandaConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/amundsenConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/atlasConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/openMetadataConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -13,16 +13,15 @@
     auth0SSOClientConfig,
     azureSSOClientConfig,
     customOidcSSOClientConfig,
     googleSSOClientConfig,
     oktaSSOClientConfig,
     openMetadataJWTClientConfig,
 )
-from .....security.credentials import awsCredentials
-from .....security.secrets import secretsManagerProvider
+from .....security.secrets import secretsManagerClientLoader, secretsManagerProvider
 from .....security.ssl import verifySSLConfig
 from .....type import basic
 from .. import connectionBasicType
 
 
 class AuthProvider(Enum):
     no_auth = 'no-auth'
@@ -80,17 +79,17 @@
             customOidcSSOClientConfig.CustomOIDCSSOClientConfig,
             openMetadataJWTClientConfig.OpenMetadataJWTClientConfig,
         ]
     ] = Field(None, description='OpenMetadata Client security configuration.')
     secretsManagerProvider: Optional[
         secretsManagerProvider.SecretsManagerProvider
     ] = secretsManagerProvider.SecretsManagerProvider.noop
-    secretsManagerCredentials: Optional[awsCredentials.AWSCredentials] = Field(
-        None, description='OpenMetadata Secrets Manager Client credentials'
-    )
+    secretsManagerLoader: Optional[
+        secretsManagerClientLoader.SecretsManagerClientLoader
+    ] = secretsManagerClientLoader.SecretsManagerClientLoader.noop
     apiVersion: Optional[str] = Field(
         'v1', description='OpenMetadata server API version to use.'
     )
     includeTopics: Optional[bool] = Field(
         True, description='Include Topics for Indexing'
     )
     includeTables: Optional[bool] = Field(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/customMlModelConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/mlflowConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/sageMakerConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/sklearnConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,39 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/airbyteConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/pipeline/dagsterConnection.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class AirbyteType(Enum):
-    Airbyte = 'Airbyte'
+class DagsterType(Enum):
+    Dagster = 'Dagster'
 
 
-class AirbyteConnection(BaseModel):
+class DagsterConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[AirbyteType] = Field(
-        AirbyteType.Airbyte, description='Service Type', title='Service Type'
+    type: Optional[DagsterType] = Field(
+        DagsterType.Dagster, description='Service Type', title='Service Type'
     )
-    hostPort: AnyUrl = Field(..., description='Pipeline Service Management/UI URL.')
-    username: Optional[str] = Field(
-        None, description='Username to connect to Airbyte.', title='Username'
+    host: AnyUrl = Field(..., description='URL to the Dagster instance', title='Host')
+    token: Optional[CustomSecretStr] = Field(
+        None, description='To Connect to Dagster Cloud', title='Token'
     )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Airbyte.', title='Password'
+    timeout: Optional[int] = Field(
+        '1000',
+        description='Connection Time Limit Between OM and Dagster Graphql API in second',
+        title='Time Out',
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/airflowConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,23 +1,20 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/backendConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/database/common/basicAuth.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
-class BackendType(Enum):
-    Backend = 'Backend'
 
-
-class BackendConnection(BaseModel):
+class BasicAuth(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[BackendType] = Field(
-        BackendType.Backend, description='Service Type', title='Service Type'
+    password: Optional[CustomSecretStr] = Field(
+        None, description='Password to connect to source.', title='Password'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/customPipelineConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,39 +1,36 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/pipeline/dagsterConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/pipeline/splineConnection.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
 from .. import connectionBasicType
 
 
-class DagsterType(Enum):
-    Dagster = 'Dagster'
+class SplineType(Enum):
+    Spline = 'Spline'
 
 
-class DagsterConnection(BaseModel):
+class SplineConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[DagsterType] = Field(
-        DagsterType.Dagster, description='Service Type', title='Service Type'
+    type: Optional[SplineType] = Field(
+        SplineType.Spline, description='Service Type', title='Service Type'
     )
-    host: AnyUrl = Field(..., description='URL to the Dagster instance', title='Host')
-    token: Optional[CustomSecretStr] = Field(
-        None, description='To Connect to Dagster Cloud', title='Token'
+    hostPort: AnyUrl = Field(
+        ...,
+        description='Spline REST Server Host & Port.',
+        title='Spline REST Server Host & Port',
     )
-    timeout: Optional[int] = Field(
-        '1000',
-        description='Connection Time Limit Between OM and Dagster Graphql API in second',
-        title='Time Out',
+    uiHostPort: Optional[AnyUrl] = Field(
+        None, description='Spline UI Host & Port.', title='Spline UI Host & Port'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/databricksPipelineConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/domoPipelineConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/fivetranConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/gluePipelineConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/nifiConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/serviceConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/storage/adlsConection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/storage/customStorageConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/storage/gcsConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import gcsCredentials
+from .....security.credentials import gcpCredentials
 from .. import connectionBasicType
 
 
 class GcsType(Enum):
     Gcs = 'Gcs'
 
 
 class GcsConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[GcsType] = Field(
         GcsType.Gcs, description='Service Type', title='Service Type'
     )
-    credentials: gcsCredentials.GCSCredentials = Field(
-        ..., description='GCS Credentials', title='GCS Credentials'
+    credentials: gcpCredentials.GCPCredentials = Field(
+        ..., description='GCP Credentials', title='GCP Credentials'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/storage/s3Connection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/testConnectionDefinition.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/testConnectionResult.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/dashboardService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/dashboardService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/dashboardService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
 from .connections.dashboard import (
     customDashboardConnection,
     domoDashboardConnection,
     lookerConnection,
     metabaseConnection,
     modeConnection,
@@ -77,15 +77,15 @@
     serviceType: DashboardServiceType = Field(
         ..., description='Type of dashboard service such as Looker or Superset...'
     )
     description: Optional[basic.Markdown] = Field(
         None, description='Description of a dashboard service instance.'
     )
     connection: Optional[DashboardConnection] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='References to pipelines deployed for this dashboard service.'
     )
     testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
         None, description='Last test connection results for this service'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this Dashboard Service.'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/databaseService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/databaseService.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/databaseService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
 from .connections.database import (
     athenaConnection,
     azureSQLConnection,
     bigQueryConnection,
     clickhouseConnection,
     customDatabaseConnection,
@@ -24,22 +24,24 @@
     domoDatabaseConnection,
     druidConnection,
     dynamoDBConnection,
     glueConnection,
     hiveConnection,
     impalaConnection,
     mariaDBConnection,
+    mongoDBConnection,
     mssqlConnection,
     mysqlConnection,
     oracleConnection,
     pinotDBConnection,
     postgresConnection,
     prestoConnection,
     redshiftConnection,
     salesforceConnection,
+    sapHanaConnection,
     singleStoreConnection,
     snowflakeConnection,
     sqliteConnection,
     trinoConnection,
     verticaConnection,
 )
 
@@ -72,14 +74,16 @@
     Salesforce = 'Salesforce'
     PinotDB = 'PinotDB'
     Datalake = 'Datalake'
     DomoDatabase = 'DomoDatabase'
     QueryLog = 'QueryLog'
     CustomDatabase = 'CustomDatabase'
     Dbt = 'Dbt'
+    SapHana = 'SapHana'
+    MongoDB = 'MongoDB'
 
 
 class DatabaseConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
@@ -109,14 +113,16 @@
             snowflakeConnection.SnowflakeConnection,
             trinoConnection.TrinoConnection,
             verticaConnection.VerticaConnection,
             pinotDBConnection.PinotDBConnection,
             datalakeConnection.DatalakeConnection,
             domoDatabaseConnection.DomoDatabaseConnection,
             customDatabaseConnection.CustomDatabaseConnection,
+            sapHanaConnection.SapHanaConnection,
+            mongoDBConnection.MongoDBConnection,
         ]
     ] = None
 
 
 class DatabaseService(BaseModel):
     class Config:
         extra = Extra.forbid
@@ -137,15 +143,15 @@
         ...,
         description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
     )
     description: Optional[basic.Markdown] = Field(
         None, description='Description of a database service instance.'
     )
     connection: Optional[DatabaseConnection] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
         description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
     )
     testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
         None, description='Last test connection results for this service'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/ingestionPipelines/ingestionPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/messagingService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/mlmodelService.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,104 +1,96 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/messagingService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/mlmodelService.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
-from .connections.messaging import (
-    customMessagingConnection,
-    kafkaConnection,
-    kinesisConnection,
-    redpandaConnection,
+from .connections.mlmodel import (
+    customMlModelConnection,
+    mlflowConnection,
+    sageMakerConnection,
+    sklearnConnection,
 )
 
 
-class MessagingServiceType(Enum):
-    Kafka = 'Kafka'
-    Redpanda = 'Redpanda'
-    Kinesis = 'Kinesis'
-    CustomMessaging = 'CustomMessaging'
+class MlModelServiceType(Enum):
+    Mlflow = 'Mlflow'
+    Sklearn = 'Sklearn'
+    CustomMlModel = 'CustomMlModel'
+    SageMaker = 'SageMaker'
 
 
-class Brokers(BaseModel):
-    __root__: List[str] = Field(
-        ...,
-        description='Multiple bootstrap addresses for Kafka. Single proxy address for Pulsar.',
-    )
-
-
-class MessagingConnection(BaseModel):
+class MlModelConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
         Union[
-            kafkaConnection.KafkaConnection,
-            redpandaConnection.RedpandaConnection,
-            kinesisConnection.KinesisConnection,
-            customMessagingConnection.CustomMessagingConnection,
+            mlflowConnection.MlflowConnection,
+            sklearnConnection.SklearnConnection,
+            customMlModelConnection.CustomMlModelConnection,
+            sageMakerConnection.SageMakerConnection,
         ]
     ] = None
 
 
-class MessagingService(BaseModel):
+class MlModelService(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier of this messaging service instance.'
+        ..., description='Unique identifier of this pipeline service instance.'
     )
     name: basic.EntityName = Field(
-        ..., description='Name that identifies this messaging service.'
+        ..., description='Name that identifies this pipeline service.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    serviceType: MessagingServiceType = Field(
-        ..., description='Type of messaging service such as Kafka or Pulsar...'
+    serviceType: MlModelServiceType = Field(
+        ..., description='Type of pipeline service such as Airflow or Prefect...'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of a messaging service instance.'
+    description: Optional[str] = Field(
+        None, description='Description of a pipeline service instance.'
     )
     displayName: Optional[str] = Field(
         None,
-        description='Display Name that identifies this messaging service. It could be title or label from the source services.',
-    )
-    connection: Optional[MessagingConnection] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='References to pipelines deployed for this messaging service to extract topic configs and schemas.',
-    )
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
-        None, description='Last test connection results for this service'
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this Message Service.'
+        description='Display Name that identifies this pipeline service. It could be title or label from the source services.',
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None,
+        description='References to pipelines deployed for this pipeline service to extract metadata',
+    )
+    connection: Optional[MlModelConnection] = None
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
+        None, description='Last test connection results for this service'
+    )
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this MlModel Service.'
+    )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this messaging service.'
+        None, description='Owner of this pipeline service.'
     )
     href: Optional[basic.Href] = Field(
-        None,
-        description='Link to the resource corresponding to this messaging service.',
+        None, description='Link to the resource corresponding to this pipeline service.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/metadataService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/metadataService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/metadataService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
 from .connections.metadata import (
     amundsenConnection,
     atlasConnection,
     metadataESConnection,
     openMetadataConnection,
 )
@@ -60,15 +60,15 @@
         ...,
         description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
     )
     description: Optional[basic.Markdown] = Field(
         None, description='Description of a database service instance.'
     )
     connection: Optional[MetadataConnection] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
         description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/mlmodelService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/storageService.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,96 +1,81 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/mlmodelService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/storageService.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional, Union
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
-from .connections.mlmodel import (
-    customMlModelConnection,
-    mlflowConnection,
-    sageMakerConnection,
-    sklearnConnection,
-)
-
-
-class MlModelServiceType(Enum):
-    Mlflow = 'Mlflow'
-    Sklearn = 'Sklearn'
-    CustomMlModel = 'CustomMlModel'
-    SageMaker = 'SageMaker'
+from .connections.storage import s3Connection
 
 
-class MlModelConnection(BaseModel):
+class StorageServiceType(Enum):
+    S3 = 'S3'
+    CustomStorage = 'CustomStorage'
+
+
+class StorageConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    config: Optional[
-        Union[
-            mlflowConnection.MlflowConnection,
-            sklearnConnection.SklearnConnection,
-            customMlModelConnection.CustomMlModelConnection,
-            sageMakerConnection.SageMakerConnection,
-        ]
-    ] = None
+    config: Optional[s3Connection.S3Connection] = None
 
 
-class MlModelService(BaseModel):
+class StorageService(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier of this pipeline service instance.'
+        ..., description='Unique identifier of this storage service instance.'
     )
     name: basic.EntityName = Field(
-        ..., description='Name that identifies this pipeline service.'
+        ..., description='Name that identifies this storage service.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    serviceType: MlModelServiceType = Field(
-        ..., description='Type of pipeline service such as Airflow or Prefect...'
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this storage service.'
     )
-    description: Optional[str] = Field(
-        None, description='Description of a pipeline service instance.'
+    serviceType: StorageServiceType = Field(
+        ..., description='Type of storage service such as S3, GCS, AZURE...'
     )
-    displayName: Optional[str] = Field(
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of a storage service instance.'
+    )
+    connection: Optional[StorageConnection] = None
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
-        description='Display Name that identifies this pipeline service. It could be title or label from the source services.',
+        description='References to pipelines deployed for this storage service to extract metadata, usage, lineage etc..',
+    )
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
+        None, description='Last test connection results for this service'
+    )
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this storage Service.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='References to pipelines deployed for this pipeline service to extract metadata',
-    )
-    connection: Optional[MlModelConnection] = None
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
-        None, description='Last test connection results for this service'
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this MlModel Service.'
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this storage service.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this pipeline service.'
-    )
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this pipeline service.'
+        None, description='Owner of this storage service.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/pipelineService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/pipelineService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,43 +1,45 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/pipelineService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, tagLabel
+from ...type import basic, entityHistory, entityReference, entityReferenceList, tagLabel
 from .connections import testConnectionResult
 from .connections.pipeline import (
     airbyteConnection,
     airflowConnection,
     customPipelineConnection,
     dagsterConnection,
     databricksPipelineConnection,
     domoPipelineConnection,
     fivetranConnection,
     gluePipelineConnection,
     nifiConnection,
+    splineConnection,
 )
 
 
 class PipelineServiceType(Enum):
     Airflow = 'Airflow'
     GluePipeline = 'GluePipeline'
     Airbyte = 'Airbyte'
     Fivetran = 'Fivetran'
     Dagster = 'Dagster'
     Nifi = 'Nifi'
     DomoPipeline = 'DomoPipeline'
     CustomPipeline = 'CustomPipeline'
     DatabricksPipeline = 'DatabricksPipeline'
+    Spline = 'Spline'
 
 
 class PipelineConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
@@ -47,14 +49,15 @@
             airbyteConnection.AirbyteConnection,
             fivetranConnection.FivetranConnection,
             dagsterConnection.DagsterConnection,
             nifiConnection.NifiConnection,
             domoPipelineConnection.DomoPipelineConnection,
             customPipelineConnection.CustomPipelineConnection,
             databricksPipelineConnection.DatabricksPipelineConnection,
+            splineConnection.SplineConnection,
         ]
     ] = None
 
 
 class PipelineService(BaseModel):
     class Config:
         extra = Extra.forbid
@@ -88,15 +91,15 @@
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
         None, description='Last test connection results for this service'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this Pipeline Service.'
     )
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
         description='References to pipelines deployed for this pipeline service to extract metadata',
     )
     connection: Optional[PipelineConnection] = None
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this pipeline service.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/services/storageService.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/type.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,81 +1,84 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/storageService.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/type.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference, tagLabel
-from .connections import testConnectionResult
-from .connections.storage import s3Connection
+from ..type import basic, entityHistory, entityReference
 
 
-class StorageServiceType(Enum):
-    S3 = 'S3'
-    CustomStorage = 'CustomStorage'
+class EntityName(BaseModel):
+    __root__: constr(regex=r'^(?u)[\w]+$') = Field(
+        ...,
+        description='Name of the property or entity types. Note a property name must be unique for an entity. Property name must follow camelCase naming adopted by openMetadata - must start with lower case with no space, underscore, or dots.',
+    )
+
+
+class Category(Enum):
+    field = 'field'
+    entity = 'entity'
 
 
-class StorageConnection(BaseModel):
+class CustomProperty(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    config: Optional[s3Connection.S3Connection] = None
+    name: EntityName = Field(
+        ...,
+        description='Name of the entity property. Note a property name must be unique for an entity. Property name must follow camelCase naming adopted by openMetadata - must start with lower case with no space, underscore, or dots.',
+    )
+    description: basic.Markdown
+    propertyType: entityReference.EntityReference = Field(
+        ...,
+        description='Reference to a property type. Only property types are allowed and entity types are not allowed as custom properties to extend an existing entity',
+    )
 
 
-class StorageService(BaseModel):
+class Type(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier of this storage service instance.'
-    )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this storage service.'
+    id: Optional[basic.Uuid] = Field(
+        None, description='Unique identifier of the type instance.'
     )
+    name: EntityName = Field(..., description='Unique name that identifies the type.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this storage service.'
+        None, description='Display Name that identifies this type.'
     )
-    serviceType: StorageServiceType = Field(
-        ..., description='Type of storage service such as S3, GCS, AZURE...'
+    description: basic.Markdown = Field(
+        ..., description='Optional description of entity.'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of a storage service instance.'
+    category: Optional[Category] = None
+    nameSpace: Optional[str] = Field(
+        'custom',
+        description='Namespace or group to which this type belongs to. For example, some of the property types commonly used can come from `basic` namespace. Some of the entities such as `table`, `database`, etc. come from `data` namespace.',
     )
-    connection: Optional[StorageConnection] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+    schema_: Optional[basic.JsonSchema] = Field(
         None,
-        description='References to pipelines deployed for this storage service to extract metadata, usage, lineage etc..',
+        alias='schema',
+        description='JSON schema encoded as string that defines the type. This will be used to validate the type values.',
     )
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
-        None, description='Last test connection results for this service'
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this storage Service.'
+    customProperties: Optional[List[CustomProperty]] = Field(
+        None,
+        description='Custom properties added to extend the entity. Only available for entity type',
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this storage service.'
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this storage service.'
-    )
+    href: Optional[basic.Href] = Field(None, description='Link to this table resource.')
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
-    deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
-    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/role.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/role.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/role.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference
+from ...type import basic, entityHistory, entityReferenceList
 
 
 class RoleName(BaseModel):
     __root__: basic.EntityName = Field(..., description='A unique name for the role.')
 
 
 class Role(BaseModel):
@@ -49,21 +49,21 @@
     )
     allowEdit: Optional[bool] = Field(
         None, description="Some system roles can't be edited"
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    policies: Optional[entityReference.EntityReferenceList] = Field(
+    policies: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Policies that is attached to this role.'
     )
-    users: Optional[entityReference.EntityReferenceList] = Field(
+    users: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Users that have this role assigned to them.'
     )
-    teams: Optional[entityReference.EntityReferenceList] = Field(
+    teams: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='Teams that have this role assigned to them.'
     )
     provider: Optional[basic.ProviderType] = basic.ProviderType.user
     disabled: Optional[bool] = Field(
         None,
         description="System policy can't be deleted. Use this flag to disable them.",
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/team.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/user.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,100 +1,110 @@
 # generated by datamodel-codegen:
-#   filename:  entity/teams/team.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/teams/user.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference, profile
+from ...auth import basicAuth, jwtAuth, ssoAuth
+from ...type import basic, entityHistory, entityReferenceList, profile
 
 
-class TeamType(Enum):
-    Group = 'Group'
-    Department = 'Department'
-    Division = 'Division'
-    BusinessUnit = 'BusinessUnit'
-    Organization = 'Organization'
+class EntityName(BaseModel):
+    __root__: constr(regex=r'^(?u)[\w\-.]+$', min_length=1, max_length=64) = Field(
+        ...,
+        description='Login name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
+    )
+
 
+class AuthType(Enum):
+    JWT = 'JWT'
+    SSO = 'SSO'
+    BASIC = 'BASIC'
 
-class Team(BaseModel):
+
+class AuthenticationMechanism(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid
-    teamType: Optional[TeamType] = Field(TeamType.Group, description='Team type')
-    name: basic.EntityName = Field(
+    config: Optional[
+        Union[
+            ssoAuth.SSOAuthMechanism,
+            jwtAuth.JWTAuthMechanism,
+            basicAuth.BasicAuthMechanism,
+        ]
+    ] = None
+    authType: Optional[AuthType] = None
+
+
+class User(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    id: basic.Uuid = Field(
+        ..., description='Unique identifier that identifies a user entity instance.'
+    )
+    name: EntityName = Field(
         ...,
-        description='A unique name of the team typically the team ID from an identity provider. Example - group Id from LDAP.',
+        description='A unique name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
     )
-    email: Optional[basic.Email] = Field(None, description='Email address of the team.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    displayName: Optional[str] = Field(
-        None, description="Name used for display purposes. Example 'Data Science team'."
-    )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of the team.'
+        None, description='Used for user biography.'
+    )
+    displayName: Optional[str] = Field(
+        None,
+        description="Name used for display purposes. Example 'FirstName LastName'.",
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
+    email: basic.Email = Field(..., description='Email address of the user.')
     href: basic.Href = Field(
         ..., description='Link to the resource corresponding to this entity.'
     )
-    profile: Optional[profile.Profile] = Field(
-        None, description='Team profile information.'
-    )
-    parents: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='Parent teams. For an `Organization` the `parent` is always null. A `BusinessUnit` always has only one parent of type `BusinessUnit` or an `Organization`. A `Division` can have multiple parents of type `BusinessUnit` or `Division`. A `Department` can have multiple parents of type `Division` or `Department`.',
+    timezone: Optional[str] = Field(None, description='Timezone of the user.')
+    isBot: Optional[bool] = Field(
+        None, description='When true indicates a special type of user called Bot.'
     )
-    children: Optional[entityReference.EntityReferenceList] = Field(
+    isAdmin: Optional[bool] = Field(
         None,
-        description='Children teams. An `Organization` can have `BusinessUnit`, `Division` or `Department` as children. A `BusinessUnit` can have `BusinessUnit`, `Division`, or `Department` as children. A `Division` can have `Division` or `Department` as children. A `Department` can have can have `Department` as children.',
-    )
-    users: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Users that are part of the team.'
-    )
-    childrenCount: Optional[int] = Field(
-        None, description='Total count of Children teams.'
+        description='When true indicates user is an administrator for the system with superuser privileges.',
     )
-    userCount: Optional[int] = Field(
-        None, description='Total count of users that are part of the team.'
+    authenticationMechanism: Optional[AuthenticationMechanism] = None
+    profile: Optional[profile.Profile] = Field(None, description='Profile of the user.')
+    teams: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='Teams that the user belongs to.'
     )
-    owns: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='List of entities owned by the team.'
+    owns: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='List of entities owned by the user.'
     )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this team. '
-    )
-    isJoinable: Optional[bool] = Field(
-        True,
-        description='Can any user join this team during sign up? Value of true indicates yes, and false no.',
+    follows: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='List of entities followed by the user.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    defaultRoles: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='Default roles of a team. These roles will be inherited by all the users that are part of this team.',
+    roles: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='Roles that the user has been assigned.'
     )
-    inheritedRoles: Optional[entityReference.EntityReferenceList] = Field(
+    inheritedRoles: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None,
-        description='Roles that a team is inheriting through membership in teams that have set team default roles.',
+        description='Roles that a user is inheriting through membership in teams that have set team default roles.',
     )
-    policies: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Policies that is attached to this team.'
+    isEmailVerified: Optional[bool] = Field(
+        None, description='If the User has verified the mail'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/teamHierarchy.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/teams/teamHierarchy.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/teamHierarchy.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/teams/user.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/testDefinition.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,110 +1,104 @@
 # generated by datamodel-codegen:
-#   filename:  entity/teams/user.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  tests/testDefinition.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional, Union
+from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field, constr
+from pydantic import BaseModel, Extra, Field
 
-from ...auth import basicAuth, jwtAuth, ssoAuth
-from ...type import basic, entityHistory, entityReference, profile
+from ..entity.data import table
+from ..type import basic, entityHistory, entityReference
 
 
-class EntityName(BaseModel):
-    __root__: constr(regex=r'^(?u)[\w\-.]+$', min_length=1, max_length=64) = Field(
-        ...,
-        description='Login name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
-    )
-
+class TestPlatform(Enum):
+    OpenMetadata = 'OpenMetadata'
+    GreatExpectations = 'GreatExpectations'
+    DBT = 'DBT'
+    Deequ = 'Deequ'
+    Soda = 'Soda'
+    Other = 'Other'
+
+
+class TestDataType(Enum):
+    NUMBER = 'NUMBER'
+    INT = 'INT'
+    FLOAT = 'FLOAT'
+    DOUBLE = 'DOUBLE'
+    DECIMAL = 'DECIMAL'
+    TIMESTAMP = 'TIMESTAMP'
+    TIME = 'TIME'
+    DATE = 'DATE'
+    DATETIME = 'DATETIME'
+    ARRAY = 'ARRAY'
+    MAP = 'MAP'
+    SET = 'SET'
+    STRING = 'STRING'
+    BOOLEAN = 'BOOLEAN'
+
+
+class EntityType(Enum):
+    TABLE = 'TABLE'
+    COLUMN = 'COLUMN'
 
-class AuthType(Enum):
-    JWT = 'JWT'
-    SSO = 'SSO'
-    BASIC = 'BASIC'
 
-
-class AuthenticationMechanism(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    config: Optional[
-        Union[
-            ssoAuth.SSOAuthMechanism,
-            jwtAuth.JWTAuthMechanism,
-            basicAuth.BasicAuthMechanism,
-        ]
-    ] = None
-    authType: Optional[AuthType] = None
+class TestCaseParameterDefinition(BaseModel):
+    name: Optional[str] = Field(None, description='name of the parameter.')
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this parameter name.'
+    )
+    dataType: Optional[TestDataType] = Field(
+        None, description='Data type of the parameter (int, date etc.).'
+    )
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the parameter.'
+    )
+    required: Optional[bool] = Field(False, description='Is this parameter required.')
+    optionValues: Optional[List] = Field(
+        [], description='List of values that can be passed for this parameter.'
+    )
 
 
-class User(BaseModel):
+class TestDefinition(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies a user entity instance.'
+    id: Optional[basic.Uuid] = Field(
+        None, description='Unique identifier of this test case definition instance.'
+    )
+    name: basic.EntityName = Field(
+        ..., description='Name that identifies this test case.'
     )
-    name: EntityName = Field(
-        ...,
-        description='A unique name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this test case.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Used for user biography.'
-    )
-    displayName: Optional[str] = Field(
-        None,
-        description="Name used for display purposes. Example 'FirstName LastName'.",
+    description: basic.Markdown = Field(..., description='Description of the testcase.')
+    entityType: Optional[EntityType] = None
+    testPlatforms: List[TestPlatform]
+    supportedDataTypes: Optional[List[table.DataType]] = None
+    parameterDefinition: Optional[List[TestCaseParameterDefinition]] = None
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this TestCase definition.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    email: basic.Email = Field(..., description='Email address of the user.')
-    href: basic.Href = Field(
-        ..., description='Link to the resource corresponding to this entity.'
-    )
-    timezone: Optional[str] = Field(None, description='Timezone of the user.')
-    isBot: Optional[bool] = Field(
-        None, description='When true indicates a special type of user called Bot.'
-    )
-    isAdmin: Optional[bool] = Field(
-        None,
-        description='When true indicates user is an administrator for the system with superuser privileges.',
-    )
-    authenticationMechanism: Optional[AuthenticationMechanism] = None
-    profile: Optional[profile.Profile] = Field(None, description='Profile of the user.')
-    teams: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Teams that the user belongs to.'
-    )
-    owns: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='List of entities owned by the user.'
-    )
-    follows: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='List of entities followed by the user.'
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    roles: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Roles that the user has been assigned.'
-    )
-    inheritedRoles: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='Roles that a user is inheriting through membership in teams that have set team default roles.',
-    )
-    isEmailVerified: Optional[bool] = Field(
-        None, description='If the User has verified the mail'
-    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/type.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/system/eventPublisherJob.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,84 +1,118 @@
 # generated by datamodel-codegen:
-#   filename:  entity/type.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  system/eventPublisherJob.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field, constr
+from pydantic import BaseModel, Extra, Field
 
-from ..type import basic, entityHistory, entityReference
+from ..configuration import elasticSearchConfiguration
+from ..type import basic
 
 
-class EntityName(BaseModel):
-    __root__: constr(regex=r'^(?u)[\w]+$') = Field(
-        ...,
-        description='Name of the property or entity types. Note a property name must be unique for an entity. Property name must follow camelCase naming adopted by openMetadata - must start with lower case with no space, underscore, or dots.',
-    )
-
+class Status(Enum):
+    STARTED = 'STARTED'
+    RUNNING = 'RUNNING'
+    COMPLETED = 'COMPLETED'
+    FAILED = 'FAILED'
+    ACTIVE = 'ACTIVE'
+    ACTIVE_WITH_ERROR = 'ACTIVE_WITH_ERROR'
+    STOPPED = 'STOPPED'
 
-class Category(Enum):
-    field = 'field'
-    entity = 'entity'
 
-
-class CustomProperty(BaseModel):
+class FailureDetails(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: EntityName = Field(
-        ...,
-        description='Name of the entity property. Note a property name must be unique for an entity. Property name must follow camelCase naming adopted by openMetadata - must start with lower case with no space, underscore, or dots.',
+    context: Optional[str] = Field(None, description='Additional Context for Failure.')
+    lastFailedAt: Optional[basic.Timestamp] = Field(
+        None,
+        description='Last non-successful callback time in UNIX UTC epoch time in milliseconds.',
     )
-    description: basic.Markdown
-    propertyType: entityReference.EntityReference = Field(
-        ...,
-        description='Reference to a property type. Only property types are allowed and entity types are not allowed as custom properties to extend an existing entity',
+    lastFailedReason: Optional[str] = Field(
+        None,
+        description='Last non-successful activity response reason received during callback.',
     )
 
 
-class Type(BaseModel):
+class StepStats(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of the type instance.'
-    )
-    name: EntityName = Field(..., description='Unique name that identifies the type.')
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
-    )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this type.'
-    )
-    description: basic.Markdown = Field(
-        ..., description='Optional description of entity.'
+    totalRecords: Optional[int] = Field(0, description='Count of Total Failed Records')
+    processedRecords: Optional[int] = Field(
+        0, description='Records that are processed in'
     )
-    category: Optional[Category] = None
-    nameSpace: Optional[str] = Field(
-        'custom',
-        description='Namespace or group to which this type belongs to. For example, some of the property types commonly used can come from `basic` namespace. Some of the entities such as `table`, `database`, etc. come from `data` namespace.',
+    successRecords: Optional[int] = Field(
+        0, description='Count of Total Successfully Records'
     )
-    schema_: Optional[basic.JsonSchema] = Field(
-        None,
-        alias='schema',
-        description='JSON schema encoded as string that defines the type. This will be used to validate the type values.',
-    )
-    customProperties: Optional[List[CustomProperty]] = Field(
-        None,
-        description='Custom properties added to extend the entity. Only available for entity type',
-    )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
+    failedRecords: Optional[int] = Field(0, description='Count of Total Failed Records')
+
+
+class Stats(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    sourceStats: Optional[StepStats] = None
+    processorStats: Optional[StepStats] = None
+    sinkStats: Optional[StepStats] = None
+    jobStats: Optional[StepStats] = None
+
+
+class RunMode(Enum):
+    stream = 'stream'
+    batch = 'batch'
+
+
+class PublisherType(Enum):
+    elasticSearch = 'elasticSearch'
+    kafka = 'kafka'
+
+
+class Failure(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    sourceError: Optional[FailureDetails] = None
+    processorError: Optional[FailureDetails] = None
+    sinkError: Optional[FailureDetails] = None
+    jobError: Optional[FailureDetails] = None
+
+
+class EventPublisherResult(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    id: basic.Uuid = Field(..., description='Unique identifier of the Job.')
+    name: Optional[str] = Field(None, description='Name of the result')
+    startedBy: Optional[str] = Field(None, description='Job started by')
+    publisherType: Optional[PublisherType] = None
+    runMode: RunMode
+    timestamp: basic.Timestamp
+    startTime: Optional[basic.Timestamp] = None
+    endTime: Optional[basic.Timestamp] = None
+    status: Status = Field(..., description='This schema publisher run job status.')
+    failure: Optional[Failure] = Field(None, description='List of Failures in the Job')
+    stats: Optional[Stats] = None
+    entities: Optional[List[str]] = Field(
+        None, description='List of Entities to Reindex', unique_items=True
+    )
+    recreateIndex: Optional[bool] = Field(
+        None, description='This schema publisher run modes.'
+    )
+    batchSize: Optional[int] = Field(
+        None, description='Maximum number of events sent in a batch (Default 10).'
+    )
+    searchIndexMappingLanguage: Optional[
+        elasticSearchConfiguration.SearchIndexMappingLanguage
+    ] = Field(
+        elasticSearchConfiguration.SearchIndexMappingLanguage.EN,
+        description='Recreate Indexes with updated Language',
     )
-    updatedAt: Optional[basic.Timestamp] = Field(
+    afterCursor: Optional[str] = Field(
         None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
-    )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(None, description='Link to this table resource.')
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
+        description='Provide After in case of failure to start reindexing after the issue is solved',
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/entitiesCount.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/entitiesCount.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/entitiesCount.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/servicesCount.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/servicesCount.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/servicesCount.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/supersetApiConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 
 from pydantic import BaseModel, Extra, Field
 
@@ -12,15 +12,15 @@
 
 
 class ApiProvider(Enum):
     db = 'db'
     ldap = 'ldap'
 
 
-class SupersetAPIConnection(BaseModel):
+class SupersetApiConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     provider: ApiProvider = Field(
         ...,
         description="Authentication provider for the Superset service. For basic user/password authentication, the default value `db` can be used. This parameter is used internally to connect to Superset's REST API.",
         title='Provider',
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/api/createEventSubscription.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/api/createEventSubscription.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/api/createEventSubscription.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/emailAlertConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/emailAlertConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/emailAlertConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/eventFilterRule.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/eventFilterRule.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/eventFilterRule.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/eventSubscription.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/eventSubscription.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/eventSubscription.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -39,42 +39,14 @@
     disabled = 'disabled'
     failed = 'failed'
     retryLimitReached = 'retryLimitReached'
     awaitingRetry = 'awaitingRetry'
     active = 'active'
 
 
-class ScheduleInfo(Enum):
-    Daily = 'Daily'
-    Weekly = 'Weekly'
-    Monthly = 'Monthly'
-    Custom = 'Custom'
-
-
-class Trigger(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    triggerType: TriggerType
-    scheduleInfo: Optional[ScheduleInfo] = Field(
-        ScheduleInfo.Weekly, description='Schedule Info'
-    )
-    cronExpression: Optional[str] = Field(
-        None, description='Cron Expression in case of Custom scheduled Trigger'
-    )
-
-
-class SubscriptionConfig(BaseModel):
-    __root__: Union[
-        webhook.Webhook,
-        emailAlertConfig.EmailAlertConfig,
-        dataInsightAlertConfig.DataInsightAlertConfig,
-    ]
-
-
 class SubscriptionStatus(BaseModel):
     class Config:
         extra = Extra.forbid
 
     status: Optional[Status] = None
     lastSuccessfulAt: Optional[basic.Timestamp] = Field(
         None,
@@ -95,23 +67,40 @@
     nextAttempt: Optional[basic.Timestamp] = Field(
         None,
         description='Next retry will be done at this time in Unix epoch time milliseconds. Only valid is `status` is `awaitingRetry`.',
     )
     timestamp: Optional[basic.Timestamp] = None
 
 
-class Subscription(BaseModel):
+class ScheduleInfo(Enum):
+    Daily = 'Daily'
+    Weekly = 'Weekly'
+    Monthly = 'Monthly'
+    Custom = 'Custom'
+
+
+class Trigger(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    subscriptionType: Optional[SubscriptionType] = None
-    subscriptionName: Optional[str] = Field(
-        None, description='Name for the subscription'
+    triggerType: TriggerType
+    scheduleInfo: Optional[ScheduleInfo] = Field(
+        ScheduleInfo.Weekly, description='Schedule Info'
+    )
+    cronExpression: Optional[str] = Field(
+        None, description='Cron Expression in case of Custom scheduled Trigger'
     )
-    subscriptionConfig: Optional[SubscriptionConfig] = None
+
+
+class SubscriptionConfig(BaseModel):
+    __root__: Union[
+        webhook.Webhook,
+        emailAlertConfig.EmailAlertConfig,
+        dataInsightAlertConfig.DataInsightAlertConfig,
+    ]
 
 
 class FilteringRules(BaseModel):
     class Config:
         extra = Extra.forbid
 
     resources: List[str] = Field(
@@ -129,16 +118,17 @@
 
     id: basic.Uuid = Field(
         ..., description='Unique identifier that identifies this Event Subscription.'
     )
     name: basic.EntityName = Field(
         ..., description='Name that uniquely identifies this Event Subscription.'
     )
-    fullyQualifiedName: Optional[basic.EntityName] = Field(
-        None, description='Name that uniquely identifies a Event Subscription.'
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None,
+        description='FullyQualifiedName that uniquely identifies a Event Subscription.',
     )
     displayName: Optional[str] = Field(
         None, description='Display name for this Event Subscription.'
     )
     description: Optional[basic.Markdown] = Field(
         None,
         description='A short description of the Event Subscription, comprehensible to regular users.',
@@ -183,7 +173,18 @@
         12, description='Read timeout in seconds. (Default 12s).'
     )
     statusDetails: Optional[SubscriptionStatus] = None
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
     provider: Optional[basic.ProviderType] = basic.ProviderType.user
+
+
+class Subscription(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    subscriptionType: Optional[SubscriptionType] = None
+    subscriptionName: Optional[str] = Field(
+        None, description='Name for the subscription'
+    )
+    subscriptionConfig: Optional[SubscriptionConfig] = None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/subscriptionResourceDescriptor.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dashboardServiceMetadataPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceMetadataPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceProfilerPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceQueryLineagePipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceQueryUsagePipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
+from ..type import filterPattern
 from .dbtconfig import (
+    dbtAzureConfig,
     dbtCloudConfig,
     dbtGCSConfig,
     dbtHttpConfig,
     dbtLocalConfig,
     dbtS3Config,
 )
 
@@ -32,14 +34,15 @@
     dbtConfigSource: Optional[
         Union[
             dbtCloudConfig.DbtCloudConfig,
             dbtLocalConfig.DbtLocalConfig,
             dbtHttpConfig.DbtHttpConfig,
             dbtS3Config.DbtS3Config,
             dbtGCSConfig.DbtGcsConfig,
+            dbtAzureConfig.DbtAzureConfig,
         ]
     ] = Field(
         None,
         description='Available sources to fetch DBT catalog and manifest files.',
         title='DBT Configuration Source',
     )
     dbtUpdateDescriptions: Optional[bool] = Field(
@@ -50,7 +53,17 @@
         True, description='Optional configuration to toggle the tags ingestion.'
     )
     dbtClassificationName: Optional[str] = Field(
         'dbtTags',
         description='Custom OpenMetadata Classification name for dbt tags.',
         title='dbt Classification Name',
     )
+    schemaFilterPattern: Optional[filterPattern.FilterPattern] = Field(
+        None,
+        description='Regex to only fetch tables or databases that matches the pattern.',
+    )
+    tableFilterPattern: Optional[filterPattern.FilterPattern] = Field(
+        None, description='Regex exclude tables or databases that matches the pattern.'
+    )
+    databaseFilterPattern: Optional[filterPattern.FilterPattern] = Field(
+        None, description='Regex to only fetch databases that matches the pattern.'
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtBucketDetails.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtCloudConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/dbtconfig/dbtGCSConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  metadataIngestion/dbtconfig/dbtAzureConfig.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Field
 
-from ...security.credentials import gcsCredentials
+from ...security.credentials import azureCredentials
 from . import dbtBucketDetails
 
 
-class DbtGcsConfig(BaseModel):
-    dbtSecurityConfig: Optional[gcsCredentials.GCSCredentials] = Field(
-        None, title='DBT GCS Security Config'
+class DbtAzureConfig(BaseModel):
+    dbtSecurityConfig: Optional[azureCredentials.AzureCredentials] = Field(
+        None, title='DBT Azure Security Config'
     )
     dbtPrefixConfig: Optional[dbtBucketDetails.DbtBucketDetails] = Field(
         None, title='DBT Prefix Config'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtHttpConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtLocalConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtS3Config.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/messagingServiceMetadataPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,47 +1,60 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/metadataToElasticSearchPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/services/connections/dashboard/lookerConnection.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import AnyUrl, BaseModel, Extra, Field
 
+from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
-class MetadataToESConfigType(Enum):
-    MetadataToElasticSearch = 'MetadataToElasticSearch'
+from .....security.credentials import bitbucketCredentials, githubCredentials
+from .. import connectionBasicType
 
 
-class MetadataToElasticSearchPipeline(BaseModel):
+class LookerType(Enum):
+    Looker = 'Looker'
+
+
+class NoGitCredentials(BaseModel):
+    pass
+
+    class Config:
+        extra = Extra.forbid
+
+
+class LookerConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: MetadataToESConfigType = Field(..., description='Pipeline type')
-    useSSL: Optional[bool] = Field(
-        False,
-        description='Indicates whether to use SSL when connecting to ElasticSearch. By default, we will ignore SSL settings.',
-        title='Use SSL',
+    type: Optional[LookerType] = Field(
+        LookerType.Looker, description='Service Type', title='Service Type'
     )
-    verifyCerts: Optional[bool] = Field(
-        False,
-        description='Indicates whether to verify certificates when using SSL connection to ElasticSearch. Ignored by default. Is set to true, make sure to send the certificates in the property `CA Certificates`.',
-        title='Validate Certificates',
+    clientId: str = Field(
+        ...,
+        description="User's Client ID. This user should have privileges to read all the metadata in Looker.",
+        title='Client ID',
     )
-    timeout: Optional[int] = Field(30, description='Connection Timeout')
-    caCerts: Optional[str] = Field(
-        None,
-        description='Certificate path to be added in configuration. The path should be local in the Ingestion Container.',
-        title='CA Certificates',
+    clientSecret: CustomSecretStr = Field(
+        ..., description="User's Client Secret.", title='Client Secret'
     )
-    useAwsCredentials: Optional[bool] = Field(
-        False,
-        description='Indicates whether to use aws credentials when connecting to OpenSearch in AWS.',
-        title='Use AWS Credentials',
+    hostPort: AnyUrl = Field(
+        ..., description='URL to the Looker instance.', title='Host and Port'
     )
-    regionName: Optional[str] = Field(
+    gitCredentials: Optional[
+        Union[
+            NoGitCredentials,
+            githubCredentials.GitHubCredentials,
+            bitbucketCredentials.BitBucketCredentials,
+        ]
+    ] = Field(
         None,
-        description='Region name. Required when using AWS Credentials.',
-        title='AWS Region Name',
+        description='Credentials to extract the .lkml files from a repository. This is required to get all the lineage and definitions.',
+        title='GitHub Credentials',
     )
+    supportsMetadataExtraction: Optional[
+        connectionBasicType.SupportsMetadataExtraction
+    ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/mlmodelServiceMetadataPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storage/containerMetadataConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/storageServiceMetadataPipeline.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/metadataIngestion/workflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/metadataIngestion/workflow.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/workflow.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -25,14 +25,21 @@
     mlmodelServiceMetadataPipeline,
     pipelineServiceMetadataPipeline,
     storageServiceMetadataPipeline,
     testSuitePipeline,
 )
 
 
+class LogLevels(Enum):
+    DEBUG = 'DEBUG'
+    INFO = 'INFO'
+    WARN = 'WARN'
+    ERROR = 'ERROR'
+
+
 class Processor(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: str = Field(..., description='Type of processor component ex: pii-processor')
     config: Optional[basic.ComponentConfig] = None
 
@@ -57,19 +64,21 @@
     class Config:
         extra = Extra.forbid
 
     type: str = Field(..., description='Type of BulkSink component ex: metadata-usage')
     config: Optional[basic.ComponentConfig] = None
 
 
-class LogLevels(Enum):
-    DEBUG = 'DEBUG'
-    INFO = 'INFO'
-    WARN = 'WARN'
-    ERROR = 'ERROR'
+class WorkflowConfig(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    loggerLevel: Optional[LogLevels] = LogLevels.INFO
+    openMetadataServerConfig: openMetadataConnection.OpenMetadataConnection
+    config: Optional[basic.ComponentConfig] = None
 
 
 class SourceConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config: Optional[
@@ -87,23 +96,14 @@
             metadataToElasticSearchPipeline.MetadataToElasticSearchPipeline,
             dataInsightPipeline.DataInsightPipeline,
             dbtPipeline.DbtPipeline,
         ]
     ] = None
 
 
-class WorkflowConfig(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    loggerLevel: Optional[LogLevels] = LogLevels.INFO
-    openMetadataServerConfig: openMetadataConnection.OpenMetadataConnection
-    config: Optional[basic.ComponentConfig] = None
-
-
 class Source(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: str = Field(
         ...,
         description='Type of the source connector ex: mysql, snowflake, tableau etc..',
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/auth0SSOClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/azureSSOClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/customOidcSSOClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/googleSSOClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/oktaSSOClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/samlSSOClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/accessTokenAuth.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/accessTokenAuth.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/accessTokenAuth.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/awsCredentials.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/awsCredentials.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/awsCredentials.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/azureCredentials.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/azureCredentials.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/azureCredentials.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/basicAuth.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/basicAuth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/basicAuth.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/gcsValues.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/gcpValues.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
-#   filename:  security/credentials/gcsValues.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  security/credentials/gcpValues.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
@@ -15,20 +15,22 @@
     __root__: str = Field(..., title='Single Project ID')
 
 
 class MultipleProjectId(BaseModel):
     __root__: List[str] = Field(..., title='Multiple Project ID')
 
 
-class GcsCredentialsValues(BaseModel):
+class GcpCredentialsValues(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[str] = Field(
-        None, description='Google Cloud service account type.', title='Credentials Type'
+        None,
+        description='Google Cloud Platform account type.',
+        title='Credentials Type',
     )
     projectId: Optional[Union[SingleProjectId, MultipleProjectId]] = Field(
         None, description='Project ID', title='Project ID'
     )
     privateKeyId: Optional[str] = Field(
         None, description='Google Cloud private key id.', title='Private Key ID'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/credentials/githubCredentials.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/credentials/gitCredentials.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,32 +1,37 @@
 # generated by datamodel-codegen:
-#   filename:  security/credentials/githubCredentials.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  security/credentials/gitCredentials.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from typing import Optional
-
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 
-class GitHubCredentials(BaseModel):
-    class Config:
-        extra = Extra.forbid
+class GitCredentials(BaseModel):
+    pass
+
 
-    repositoryOwner: str = Field(
+class RepositoryOwner(BaseModel):
+    __root__: str = Field(
         ...,
-        description='The owner (user or organization) of a GitHub repository. For example, in https://github.com/open-metadata/OpenMetadata, the owner is `open-metadata`.',
+        description='The owner (user or organization) of a Git repository. For example, in https://github.com/open-metadata/OpenMetadata, the owner is `open-metadata`.',
         title='Repository Owner',
     )
-    repositoryName: str = Field(
+
+
+class RepositoryName(BaseModel):
+    __root__: str = Field(
         ...,
-        description='The name of a GitHub repository. For example, in https://github.com/open-metadata/OpenMetadata, the name is `OpenMetadata`.',
+        description='The name of a Git repository. For example, in https://github.com/open-metadata/OpenMetadata, the name is `OpenMetadata`.',
         title='Repository Name',
     )
-    token: Optional[CustomSecretStr] = Field(
-        None,
+
+
+class Token(BaseModel):
+    __root__: CustomSecretStr = Field(
+        ...,
         description="Token to use the API. This is required for private repositories and to ensure we don't hit API limits.",
         title='API Token',
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/secrets/secretsManagerConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/securityConfiguration.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/paging.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,26 @@
 # generated by datamodel-codegen:
-#   filename:  security/securityConfiguration.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/paging.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 
-class ValidateSSLClientConfig(BaseModel):
+class Paging(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    maskPasswordsAPI: Optional[bool] = Field(
-        'false',
-        description='If enabled, it will mask all the password fields in the responses sent from the API except for the bots',
-        title='Mask Password API',
+    before: Optional[str] = Field(
+        None,
+        description='Before cursor used for getting the previous page (see API pagination for details).',
+    )
+    after: Optional[str] = Field(
+        None,
+        description='After cursor used for getting the next page (see API pagination for details).',
+    )
+    total: int = Field(
+        ..., description='Total number of entries available to page through.'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/ssl/validateSSLClientConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/ssl/verifySSLConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/settings/settings.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/settings/settings.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 # generated by datamodel-codegen:
 #   filename:  settings/settings.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Field
 
 from ..configuration import (
     authenticationConfiguration,
     authorizerConfiguration,
     elasticSearchConfiguration,
     eventHandlerConfiguration,
     fernetConfiguration,
     jwtTokenConfiguration,
     pipelineServiceClientConfiguration,
+    slackAppConfiguration,
     taskNotificationConfiguration,
 )
 from ..email import smtpSettings
 
 
 class SettingType(Enum):
     authorizerConfiguration = 'authorizerConfiguration'
@@ -32,29 +33,30 @@
     fernetConfiguration = 'fernetConfiguration'
     slackEventPublishers = 'slackEventPublishers'
     secretsManagerConfiguration = 'secretsManagerConfiguration'
     sandboxModeEnabled = 'sandboxModeEnabled'
     slackChat = 'slackChat'
     emailConfiguration = 'emailConfiguration'
     customLogoConfiguration = 'customLogoConfiguration'
+    slackAppConfiguration = 'slackAppConfiguration'
+    slackBot = 'slackBot'
+    slackInstaller = 'slackInstaller'
 
 
 class Settings(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
     config_type: SettingType = Field(
         ..., description='Unique identifier that identifies an entity instance.'
     )
     config_value: Optional[
         Union[
             pipelineServiceClientConfiguration.PipelineServiceClientConfiguration,
             authenticationConfiguration.AuthenticationConfiguration,
             authorizerConfiguration.AuthorizerConfiguration,
             elasticSearchConfiguration.ElasticSearchConfiguration,
             eventHandlerConfiguration.EventHandlerConfiguration,
             fernetConfiguration.FernetConfiguration,
             jwtTokenConfiguration.JWTTokenConfiguration,
             taskNotificationConfiguration.TaskNotificationConfiguration,
             smtpSettings.SmtpSettings,
+            slackAppConfiguration.SlackAppConfiguration,
         ]
     ] = None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/system/eventPublisherJob.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/entity/data/dashboard.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,118 +1,102 @@
 # generated by datamodel-codegen:
-#   filename:  system/eventPublisherJob.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  entity/data/dashboard.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..configuration import elasticSearchConfiguration
-from ..type import basic
+from ...type import (
+    basic,
+    entityHistory,
+    entityReference,
+    entityReferenceList,
+    tagLabel,
+    usageDetails,
+)
+from ..services import dashboardService
+
+
+class DashboardType(Enum):
+    Dashboard = 'Dashboard'
+    Report = 'Report'
 
 
-class Status(Enum):
-    STARTED = 'STARTED'
-    RUNNING = 'RUNNING'
-    COMPLETED = 'COMPLETED'
-    FAILED = 'FAILED'
-    ACTIVE = 'ACTIVE'
-    ACTIVE_WITH_ERROR = 'ACTIVE_WITH_ERROR'
-    STOPPED = 'STOPPED'
-
-
-class FailureDetails(BaseModel):
+class Dashboard(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    context: Optional[str] = Field(None, description='Additional Context for Failure.')
-    lastFailedAt: Optional[basic.Timestamp] = Field(
+    id: basic.Uuid = Field(
+        ..., description='Unique identifier that identifies a dashboard instance.'
+    )
+    name: basic.EntityName = Field(
+        ..., description='Name that identifies this dashboard.'
+    )
+    displayName: Optional[str] = Field(
         None,
-        description='Last non-successful callback time in UNIX UTC epoch time in milliseconds.',
+        description='Display Name that identifies this Dashboard. It could be title or label from the source services.',
     )
-    lastFailedReason: Optional[str] = Field(
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None,
-        description='Last non-successful activity response reason received during callback.',
+        description="A unique name that identifies a dashboard in the format 'ServiceName.DashboardName'.",
     )
-
-
-class StepStats(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    totalRecords: Optional[int] = Field(0, description='Count of Total Failed Records')
-    processedRecords: Optional[int] = Field(
-        0, description='Records that are processed in'
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the dashboard, what it is, and how to use it.'
     )
-    successRecords: Optional[int] = Field(
-        0, description='Count of Total Successfully Records'
+    project: Optional[str] = Field(
+        None,
+        description='Name of the project / workspace / collection in which the dashboard is contained',
     )
-    failedRecords: Optional[int] = Field(0, description='Count of Total Failed Records')
-
-
-class Stats(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    sourceStats: Optional[StepStats] = None
-    processorStats: Optional[StepStats] = None
-    sinkStats: Optional[StepStats] = None
-    jobStats: Optional[StepStats] = None
-
-
-class RunMode(Enum):
-    stream = 'stream'
-    batch = 'batch'
-
-
-class PublisherType(Enum):
-    elasticSearch = 'elasticSearch'
-    kafka = 'kafka'
-
-
-class Failure(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    sourceError: Optional[FailureDetails] = None
-    processorError: Optional[FailureDetails] = None
-    sinkError: Optional[FailureDetails] = None
-    jobError: Optional[FailureDetails] = None
-
-
-class EventPublisherResult(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    id: basic.Uuid = Field(..., description='Unique identifier of the Job.')
-    name: Optional[str] = Field(None, description='Name of the result')
-    startedBy: Optional[str] = Field(None, description='Job started by')
-    publisherType: Optional[PublisherType] = None
-    runMode: RunMode
-    timestamp: basic.Timestamp
-    startTime: Optional[basic.Timestamp] = None
-    endTime: Optional[basic.Timestamp] = None
-    status: Status = Field(..., description='This schema publisher run job status.')
-    failure: Optional[Failure] = Field(None, description='List of Failures in the Job')
-    stats: Optional[Stats] = None
-    entities: Optional[List[str]] = Field(
-        None, description='List of Entities to Reindex', unique_items=True
-    )
-    recreateIndex: Optional[bool] = Field(
-        None, description='This schema publisher run modes.'
-    )
-    batchSize: Optional[int] = Field(
-        None, description='Maximum number of events sent in a batch (Default 10).'
-    )
-    searchIndexMappingLanguage: Optional[
-        elasticSearchConfiguration.SearchIndexMappingLanguage
-    ] = Field(
-        elasticSearchConfiguration.SearchIndexMappingLanguage.EN,
-        description='Recreate Indexes with updated Language',
+    version: Optional[entityHistory.EntityVersion] = Field(
+        None, description='Metadata version of the entity.'
+    )
+    updatedAt: Optional[basic.Timestamp] = Field(
+        None,
+        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+    )
+    updatedBy: Optional[str] = Field(None, description='User who made the update.')
+    dashboardType: Optional[DashboardType] = DashboardType.Dashboard
+    sourceUrl: Optional[basic.SourceUrl] = Field(
+        None, description='Dashboard URL suffix from its service.'
+    )
+    charts: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='All the charts included in this Dashboard.'
+    )
+    dataModels: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None,
+        description='List of data models used by this dashboard or the charts contained on it.',
+    )
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this entity.'
+    )
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this dashboard.'
+    )
+    followers: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None, description='Followers of this dashboard.'
+    )
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this dashboard.'
+    )
+    service: entityReference.EntityReference = Field(
+        ..., description='Link to service where this dashboard is hosted in.'
+    )
+    serviceType: Optional[dashboardService.DashboardServiceType] = Field(
+        None, description='Service type where this dashboard is hosted in.'
+    )
+    usageSummary: Optional[usageDetails.UsageDetails] = Field(
+        None, description='Latest usage information for this dashboard.'
+    )
+    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
+        None, description='Change that lead to this version of the entity.'
+    )
+    deleted: Optional[bool] = Field(
+        False, description='When `true` indicates the entity has been soft deleted.'
     )
-    afterCursor: Optional[str] = Field(
+    extension: Optional[basic.EntityExtension] = Field(
         None,
-        description='Provide After in case of failure to start reindexing after the issue is solved',
+        description='Entity extension data with custom attributes added to the entity.',
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/basic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/databaseConnectionConfig.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,55 +1,53 @@
 # generated by datamodel-codegen:
-#   filename:  tests/basic.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/databaseConnectionConfig.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from enum import Enum
-from typing import Any, List, Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..type import basic
 
-
-class Basic(BaseModel):
-    __root__: Any = Field(
-        ...,
-        description='This schema defines basic types that are used by other test schemas.',
-        title='Basic',
-    )
-
-
-class TestResultValue(BaseModel):
-    name: Optional[str] = Field(None, description='name of the value')
-    value: Optional[str] = Field(None, description='test result value')
-
-
-class TestCaseStatus(Enum):
-    Success = 'Success'
-    Failed = 'Failed'
-    Aborted = 'Aborted'
-
-
-class TestCaseResult(BaseModel):
+class DatabaseConnectionConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    timestamp: Optional[basic.Timestamp] = Field(
-        None, description='Data one which test case result is taken.'
+    username: Optional[str] = Field(
+        None, description='username to connect  to the data source.'
+    )
+    password: Optional[str] = Field(
+        None, description='password to connect  to the data source.'
     )
-    testCaseStatus: Optional[TestCaseStatus] = Field(
-        None, description='Status of Test Case run.'
+    hostPort: Optional[str] = Field(
+        None, description='Host and port of the data source.'
     )
-    result: Optional[str] = Field(None, description='Details of test case results.')
-    sampleData: Optional[str] = Field(
+    database: Optional[str] = Field(None, description='Database of the data source.')
+    schema_: Optional[str] = Field(
+        None, alias='schema', description='schema of the data source.'
+    )
+    includeViews: Optional[bool] = Field(
+        True,
+        description='optional configuration to turn off fetching metadata for views.',
+    )
+    includeTables: Optional[bool] = Field(
+        True,
+        description='Optional configuration to turn off fetching metadata for tables.',
+    )
+    generateSampleData: Optional[bool] = Field(
+        True, description='Turn on/off collecting sample data.'
+    )
+    sampleDataQuery: Optional[str] = Field(
+        'select * from {}.{} limit 50', description='query to generate sample data.'
+    )
+    enableDataProfiler: Optional[bool] = Field(
+        False,
+        description='Run data profiler as part of ingestion to get table profile data.',
+    )
+    includeFilterPattern: Optional[List[str]] = Field(
         None,
-        description="sample data to capture rows/columns that didn't match the expressed testcase.",
+        description='Regex to only fetch tables or databases that matches the pattern.',
+    )
+    excludeFilterPattern: Optional[List[str]] = Field(
+        None, description='Regex exclude tables or databases that matches the pattern.'
     )
-    testResultValue: Optional[List[TestResultValue]] = None
-
-
-class TestSuiteExecutionFrequency(Enum):
-    Hourly = 'Hourly'
-    Daily = 'Daily'
-    Weekly = 'Weekly'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/customMetric.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/customMetric.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/customMetric.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/testCase.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/testCase.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # generated by datamodel-codegen:
 #   filename:  tests/testCase.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ..type import basic, entityHistory, entityReference
 from . import basic as basic_1
+from . import testSuite
 
 
 class TestCaseParameterValue(BaseModel):
     name: Optional[str] = Field(
         None,
         description='name of the parameter. Must match the parameter names in testCaseParameterDefinition',
     )
@@ -42,14 +43,15 @@
     description: Optional[basic.Markdown] = Field(
         None, description='Description of the testcase.'
     )
     testDefinition: entityReference.EntityReference
     entityLink: basic.EntityLink
     entityFQN: Optional[str] = None
     testSuite: entityReference.EntityReference
+    testSuites: Optional[List[testSuite.TestSuite]] = None
     parameterValues: Optional[List[TestCaseParameterValue]] = None
     testCaseResult: Optional[basic_1.TestCaseResult] = None
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this Pipeline.'
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/testDefinition.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/tests/testSuite.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,88 +1,62 @@
 # generated by datamodel-codegen:
-#   filename:  tests/testDefinition.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  tests/testSuite.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional
+from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..entity.data import table
-from ..type import basic, entityHistory, entityReference
+from ..api.tests import createTestSuite
+from ..entity.services.connections import testConnectionResult
+from ..type import basic, entityHistory, entityReference, entityReferenceList
+from . import basic as basic_1
 
 
-class TestPlatform(Enum):
-    OpenMetadata = 'OpenMetadata'
-    GreatExpectations = 'GreatExpectations'
-    DBT = 'DBT'
-    Deequ = 'Deequ'
-    Soda = 'Soda'
-    Other = 'Other'
-
-
-class TestDataType(Enum):
-    NUMBER = 'NUMBER'
-    INT = 'INT'
-    FLOAT = 'FLOAT'
-    DOUBLE = 'DOUBLE'
-    DECIMAL = 'DECIMAL'
-    TIMESTAMP = 'TIMESTAMP'
-    TIME = 'TIME'
-    DATE = 'DATE'
-    DATETIME = 'DATETIME'
-    ARRAY = 'ARRAY'
-    MAP = 'MAP'
-    SET = 'SET'
-    STRING = 'STRING'
-    BOOLEAN = 'BOOLEAN'
-
-
-class EntityType(Enum):
-    TABLE = 'TABLE'
-    COLUMN = 'COLUMN'
+class ServiceType(Enum):
+    TestSuite = 'TestSuite'
 
 
-class TestCaseParameterDefinition(BaseModel):
-    name: Optional[str] = Field(None, description='name of the parameter.')
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this parameter name.'
-    )
-    dataType: Optional[TestDataType] = Field(
-        None, description='Data type of the parameter (int, date etc.).'
-    )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the parameter.'
-    )
-    required: Optional[bool] = Field(False, description='Is this parameter required.')
+class TestSuiteConnection(BaseModel):
+    config: Optional[Any] = None
 
 
-class TestDefinition(BaseModel):
+class TestSuite(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this test case definition instance.'
+        None, description='Unique identifier of this test suite instance.'
     )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this test case.'
+    name: createTestSuite.TestSuiteEntityName = Field(
+        ..., description='Name that identifies this test suite.'
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this test case.'
+        None, description='Display Name that identifies this test suite.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    description: basic.Markdown = Field(..., description='Description of the testcase.')
-    entityType: Optional[EntityType] = None
-    testPlatforms: List[TestPlatform]
-    supportedDataTypes: Optional[List[table.DataType]] = None
-    parameterDefinition: Optional[List[TestCaseParameterDefinition]] = None
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the test suite.'
+    )
+    tests: Optional[List[entityReference.EntityReference]] = None
+    connection: Optional[TestSuiteConnection] = None
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = None
+    pipelines: Optional[entityReferenceList.EntityReferenceListModel] = Field(
+        None,
+        description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
+    )
+    serviceType: Optional[ServiceType] = Field(
+        ServiceType.TestSuite,
+        description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
+    )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this TestCase definition.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
@@ -95,7 +69,19 @@
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
+    executable: Optional[bool] = Field(
+        False,
+        description='Indicates if the test suite is executable. Set on the backend.',
+    )
+    executableEntityReference: Optional[entityReference.EntityReference] = Field(
+        None,
+        description='Entity reference the test suite is executed against. Only applicable if the test suite is executable.',
+    )
+    summary: Optional[basic_1.TestSummary] = Field(
+        None,
+        description='Summary of the previous day test cases execution for this test suite.',
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/tests/testSuite.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityLineage.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,73 +1,86 @@
 # generated by datamodel-codegen:
-#   filename:  tests/testSuite.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/entityLineage.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from enum import Enum
-from typing import Any, List, Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..entity.services.connections import testConnectionResult
-from ..type import basic, entityHistory, entityReference
+from . import basic, entityReference
 
 
-class ServiceType(Enum):
-    TestSuite = 'TestSuite'
+class ColumnLineage(BaseModel):
+    fromColumns: Optional[List[basic.FullyQualifiedEntityName]] = Field(
+        None,
+        description='One or more source columns identified by fully qualified column name used by transformation function to create destination column.',
+    )
+    toColumn: Optional[basic.FullyQualifiedEntityName] = Field(
+        None,
+        description='Destination column identified by fully qualified column name created by the transformation of source columns.',
+    )
+    function: Optional[basic.SqlFunction] = Field(
+        None,
+        description='Transformation function applied to source columns to create destination column. That is `function(fromColumns) -> toColumn`.',
+    )
 
 
-class TestSuiteConnection(BaseModel):
-    config: Optional[Any] = None
+class LineageDetails(BaseModel):
+    sqlQuery: Optional[basic.SqlQuery] = Field(
+        None, description='SQL used for transformation.'
+    )
+    columnsLineage: Optional[List[ColumnLineage]] = Field(
+        None,
+        description='Lineage information of how upstream columns were combined to get downstream column.',
+    )
+    pipeline: Optional[entityReference.EntityReference] = Field(
+        None, description='Pipeline where the sqlQuery is periodically run.'
+    )
+    description: Optional[str] = Field(None, description='description of lineage')
 
 
-class TestSuite(BaseModel):
+class Edge(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this test suite instance.'
-    )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this test suite.'
-    )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this test suite.'
-    )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
+    fromEntity: basic.Uuid = Field(
+        ..., description='From entity that is upstream of lineage edge.'
     )
-    description: basic.Markdown = Field(
-        ..., description='Description of the test suite.'
+    toEntity: basic.Uuid = Field(
+        ..., description='To entity that is downstream of lineage edge.'
     )
-    tests: Optional[List[entityReference.EntityReference]] = None
-    connection: Optional[TestSuiteConnection] = None
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+    description: Optional[basic.Markdown] = None
+    lineageDetails: Optional[LineageDetails] = Field(
         None,
-        description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
+        description='Optional lineageDetails provided only for table to table lineage edge.',
     )
-    serviceType: Optional[ServiceType] = Field(
-        ServiceType.TestSuite,
-        description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this TestCase definition.'
+
+
+class EntitiesEdge(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    fromEntity: entityReference.EntityReference = Field(
+        ..., description='From entity that is upstream of lineage edge.'
     )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
+    toEntity: entityReference.EntityReference = Field(
+        ..., description='To entity that is downstream of lineage edge.'
     )
-    updatedAt: Optional[basic.Timestamp] = Field(
+    description: Optional[basic.Markdown] = None
+    lineageDetails: Optional[LineageDetails] = Field(
         None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
-    )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
+        description='Optional lineageDetails provided only for table to table lineage edge.',
     )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
-    )
-    deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
+
+
+class EntityLineage(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    entity: entityReference.EntityReference = Field(
+        ..., description='Primary entity for which this lineage graph is created.'
     )
+    nodes: Optional[List[entityReference.EntityReference]] = None
+    upstreamEdges: Optional[List[Edge]] = None
+    downstreamEdges: Optional[List[Edge]] = None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/auditLog.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/auditLog.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/auditLog.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/basic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/basic.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/basic.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from datetime import date, datetime, time
 from enum import Enum
 from typing import Any, Dict, Optional
 from uuid import UUID
@@ -157,7 +157,11 @@
 
 
 class Status(Enum):
     success = 'success'
     failure = 'failure'
     aborted = 'aborted'
     partialSuccess = 'partialSuccess'
+
+
+class SourceUrl(BaseModel):
+    __root__: str = Field(..., description='Source Url of the respective entity.')
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/changeEvent.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/changeEvent.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/changeEvent.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/collectionDescriptor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/collectionDescriptor.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/collectionDescriptor.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvDocumentation.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvDocumentation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvDocumentation.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvFile.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvFile.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvFile.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/csvImportResult.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/csvImportResult.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvImportResult.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/databaseConnectionConfig.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/tableQuery.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,53 +1,51 @@
 # generated by datamodel-codegen:
-#   filename:  type/databaseConnectionConfig.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/tableQuery.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from . import basic
 
-class DatabaseConnectionConfig(BaseModel):
-    class Config:
-        extra = Extra.forbid
 
-    username: Optional[str] = Field(
-        None, description='username to connect  to the data source.'
+class TableQuery(BaseModel):
+    query: str = Field(..., description='SQL query')
+    userName: Optional[str] = Field(
+        None, description='Name of the user that executed the SQL query'
     )
-    password: Optional[str] = Field(
-        None, description='password to connect  to the data source.'
+    startTime: Optional[str] = Field(
+        None, description='Start time of execution of SQL query'
     )
-    hostPort: Optional[str] = Field(
-        None, description='Host and port of the data source.'
+    endTime: Optional[str] = Field(
+        None, description='End time of execution of SQL query'
     )
-    database: Optional[str] = Field(None, description='Database of the data source.')
-    schema_: Optional[str] = Field(
-        None, alias='schema', description='schema of the data source.'
+    analysisDate: Optional[basic.DateTime] = Field(
+        None, description='Date of execution of SQL query'
     )
-    includeViews: Optional[bool] = Field(
-        True,
-        description='optional configuration to turn off fetching metadata for views.',
+    aborted: Optional[bool] = Field(
+        None, description='Flag to check if query was aborted during execution'
     )
-    includeTables: Optional[bool] = Field(
-        True,
-        description='Optional configuration to turn off fetching metadata for tables.',
+    serviceName: str = Field(
+        ..., description='Name that identifies this database service.'
     )
-    generateSampleData: Optional[bool] = Field(
-        True, description='Turn on/off collecting sample data.'
+    databaseName: Optional[str] = Field(
+        None, description='Database associated with the table in the query'
     )
-    sampleDataQuery: Optional[str] = Field(
-        'select * from {}.{} limit 50', description='query to generate sample data.'
+    databaseSchema: Optional[str] = Field(
+        None, description='Database schema of the associated with query'
     )
-    enableDataProfiler: Optional[bool] = Field(
-        False,
-        description='Run data profiler as part of ingestion to get table profile data.',
+    duration: Optional[float] = Field(
+        None, description='How long did the query took to run in seconds.'
     )
-    includeFilterPattern: Optional[List[str]] = Field(
-        None,
-        description='Regex to only fetch tables or databases that matches the pattern.',
-    )
-    excludeFilterPattern: Optional[List[str]] = Field(
-        None, description='Regex exclude tables or databases that matches the pattern.'
+
+
+class TableQueries(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    queries: Optional[List[TableQuery]] = Field(
+        None, description='Date of execution of SQL query'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityHistory.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityHistory.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityHistory.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityReference.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/tableUsageCount.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,43 +1,53 @@
 # generated by datamodel-codegen:
-#   filename:  type/entityReference.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  type/tableUsageCount.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from . import basic
+from ..api.data import createQuery
 
 
-class EntityReference(BaseModel):
+class TableColumn(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies an entity instance.'
-    )
-    type: str = Field(
-        ...,
-        description='Entity type/class name - Examples: `database`, `table`, `metrics`, `databaseService`, `dashboardService`...',
+    table: Optional[str] = Field(None, description='Name of the table')
+    column: Optional[str] = Field(None, description='Name of the column')
+
+
+class TableColumnJoin(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    tableColumn: Optional[TableColumn] = Field(None, description='Source table column')
+    joinedWith: Optional[List[TableColumn]] = Field(
+        None, description='List of table columns with which the table is joined with'
     )
-    name: Optional[str] = Field(None, description='Name of the entity instance.')
-    fullyQualifiedName: Optional[str] = Field(
-        None,
-        description="Fully qualified name of the entity instance. For entities such as tables, databases fullyQualifiedName is returned in this field. For entities that don't have name hierarchy such as `user` and `team` this will be same as the `name` field.",
+
+
+class TableUsageCount(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    table: str = Field(..., description='Name of the table')
+    date: str = Field(..., description='Date of execution of SQL query')
+    databaseName: Optional[str] = Field(
+        None, description='Database associated with the table in the query'
+    )
+    count: Optional[int] = Field(1, description='Usage count of table')
+    databaseSchema: Optional[str] = Field(
+        None, description='Database schema of the associated with table'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Optional description of entity.'
+    sqlQueries: Optional[List[createQuery.CreateQueryRequest]] = Field(
+        None, description='List of SQL Queries associated with table'
     )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this entity.'
+    joins: Optional[List[TableColumnJoin]] = Field(
+        None, description='List of joins associated with table'
     )
-    deleted: Optional[bool] = Field(
-        None, description='If true the entity referred to has been soft-deleted.'
+    serviceName: str = Field(
+        ..., description='Name that identifies this database service.'
     )
-    href: Optional[basic.Href] = Field(None, description='Link to the entity resource.')
-
-
-class EntityReferenceList(BaseModel):
-    __root__: List[EntityReference]
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityRelationship.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityRelationship.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityRelationship.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/entityUsage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/entityUsage.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityUsage.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/filterPattern.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/configuration/extensionConfiguration.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 # generated by datamodel-codegen:
-#   filename:  type/filterPattern.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   filename:  configuration/extensionConfiguration.json
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 
-class FilterPatternModel(BaseModel):
-    pass
-
-
-class FilterPattern(BaseModel):
+class Extension(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    includes: Optional[List[str]] = Field(
-        None,
-        description='List of strings/regex patterns to match and include only database entities that match.',
+    className: str = Field(..., description='Class Name for the Extension Service.')
+    parameters: Optional[Dict[str, Any]] = Field(
+        None, description='Additional parameters for extension initialization.'
+    )
+
+
+class ExtensionConfiguration(BaseModel):
+    resourcePackage: Optional[List[str]] = Field(
+        None, description='Resources Package name for Extension.'
     )
-    excludes: Optional[List[str]] = Field(
-        None,
-        description='List of strings/regex patterns to match and exclude only database entities that match.',
+    extensions: Optional[List[Extension]] = Field(
+        None, description='Extension Class to Register in OM'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/function.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/function.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/function.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/jdbcConnection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/jdbcConnection.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/jdbcConnection.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
 class DriverClass(BaseModel):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/profile.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/profile.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/profile.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/reaction.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/reaction.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/reaction.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/schema.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/schema.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/schema.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/tagLabel.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/tagLabel.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/tagLabel.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/usageDetails.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/usageDetails.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/usageDetails.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat, conint
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/generated/schema/type/votes.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/generated/schema/type/votes.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 # generated by datamodel-codegen:
 #   filename:  type/votes.json
-#   timestamp: 2023-06-19T13:22:48+00:00
+#   timestamp: 2023-06-26T12:17:17+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from . import entityReference
+from . import entityReferenceList
 
 
 class VoteType(Enum):
     votedUp = 'votedUp'
     votedDown = 'votedDown'
     unVoted = 'unVoted'
 
 
 class Votes(BaseModel):
     class Config:
         extra = Extra.forbid
 
     upVotes: Optional[int] = Field(0, description='Total up-votes the entity has')
     downVotes: Optional[int] = Field(0, description='Total down-votes the entity has')
-    upVoters: Optional[entityReference.EntityReferenceList] = Field(
+    upVoters: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='List of all the Users who upVoted'
     )
-    downVoters: Optional[entityReference.EntityReferenceList] = Field(
+    downVoters: Optional[entityReferenceList.EntityReferenceListModel] = Field(
         None, description='List of all the Users who downVoted'
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/action.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/action.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,110 +12,99 @@
 Great Expectations subpackage to send expectation results to
 Open Metadata table quality.
 
 This subpackage needs to be used in Great Expectations
 checkpoints actions.
 """
 import traceback
-import warnings
 from datetime import datetime, timezone
-from typing import Dict, List, Optional, Union
+from typing import Dict, List, Optional, Union, cast
 
 from great_expectations.checkpoint.actions import ValidationAction
 from great_expectations.core.batch import Batch
 from great_expectations.core.batch_spec import SqlAlchemyDatasourceBatchSpec
 from great_expectations.core.expectation_validation_result import (
     ExpectationSuiteValidationResult,
 )
 from great_expectations.data_asset.data_asset import DataAsset
 from great_expectations.data_context.data_context import DataContext
 
 try:
     from great_expectations.data_context.types.resource_identifiers import (
+        GeCloudIdentifier,  # type: ignore
+    )
+    from great_expectations.data_context.types.resource_identifiers import (
         ExpectationSuiteIdentifier,
-        GeCloudIdentifier,
         ValidationResultIdentifier,
     )
 except ImportError:
     from great_expectations.data_context.types.resource_identifiers import (
         ExpectationSuiteIdentifier,
         GXCloudIdentifier as GeCloudIdentifier,
         ValidationResultIdentifier,
     )
 
 from great_expectations.validator.validator import Validator
 from sqlalchemy.engine.base import Connection, Engine
 from sqlalchemy.engine.url import URL
 
+from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.generated.schema.tests.testCase import TestCase, TestCaseParameterValue
 from metadata.generated.schema.tests.testDefinition import (
     EntityType,
     TestCaseParameterDefinition,
     TestPlatform,
 )
+from metadata.generated.schema.tests.testSuite import TestSuite
 from metadata.great_expectations.utils.ometa_config_handler import (
     create_jinja_environment,
     create_ometa_connection_obj,
     render_template,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils import fqn
+from metadata.utils.entity_link import get_entity_link
 from metadata.utils.logger import great_expectations_logger
 
 logger = great_expectations_logger()
 
 
 class OpenMetadataValidationAction(ValidationAction):
     """Open Metdata validation action. It inherits from
     great expection validation action class and implements the
     `_run` method.
 
     Attributes:
         data_context: great expectation data context
-        ometa_service_name: name of the service for the table
+        database_service_name: name of the service for the table
         api_version: default to v1
         config_file_path: path to the open metdata config path
     """
 
     def __init__(
         self,
-        data_context: DataContext,
+        data_context: DataContext,  # type: ignore
         *,
-        config_file_path: str = None,
+        config_file_path: Optional[str] = None,
         database_service_name: Optional[str] = None,
-        ometa_service_name: Optional[str] = None,
-        test_suite_name: str = "great_expectation_default",
+        schema_name: Optional[str] = "default",
+        database_name: Optional[str] = None,
     ):
         super().__init__(data_context)
-        self._ometa_service_name = (
-            ometa_service_name  # will be deprecated in future release
-        )
-        self._database_service_name = database_service_name
+        self.database_service_name = database_service_name
+        self.database_name = database_name
+        self.schema_name = schema_name  # for database without schema concept
         self.config_file_path = config_file_path
         self.ometa_conn = self._create_ometa_connection()
-        self.test_suite_name = test_suite_name
-
-    @property
-    def database_service_name(self):
-        """Handle depracation warning"""
-        if self._ometa_service_name:
-            warnings.warn(
-                "`ometa_service_name` will be deperacted in  openmetadata-ingestion==0.13."
-                " Use `database_service_name` instead",
-                DeprecationWarning,
-            )
-
-            return self._ometa_service_name
-        return self._database_service_name
 
     def _run(  # pylint: disable=arguments-renamed,unused-argument
         self,
         validation_result_suite: ExpectationSuiteValidationResult,
         validation_result_suite_identifier: Union[
             ValidationResultIdentifier, GeCloudIdentifier
         ],
@@ -133,27 +122,30 @@
             payload:
             expectation_suite_identifier: type of expectation suite
             checkpoint_identifier: identifier for the checkpoint
         """
         check_point_spec = self._get_checkpoint_batch_spec(data_asset)
         execution_engine_url = self._get_execution_engine_url(data_asset)
         table_entity = self._get_table_entity(
-            execution_engine_url.database,
-            check_point_spec.get("schema_name"),
+            execution_engine_url.database
+            if not self.database_name
+            else self.database_name,
+            check_point_spec.get("schema_name", self.schema_name),
             check_point_spec.get("table_name"),
         )
 
         if table_entity:
+            test_suite = self._check_or_create_test_suite(table_entity)
             for result in validation_result_suite.results:
-                self._handle_test_case(result, table_entity)
+                self._handle_test_case(result, table_entity, test_suite)
 
     @staticmethod
     def _get_checkpoint_batch_spec(
         data_asset: Union[Validator, DataAsset, Batch]
-    ) -> Optional[SqlAlchemyDatasourceBatchSpec]:
+    ) -> SqlAlchemyDatasourceBatchSpec:
         """Return run meta and check instance of data_asset
 
         Args:
             data_asset: data assets of the checkpoint run
         Returns:
             SqlAlchemyDatasourceBatchSpec
         Raises:
@@ -164,15 +156,18 @@
             return batch_spec
         raise ValueError(
             f"Type `{type(batch_spec).__name__,}` is not supported."
             " Make sur you ran your expectations against a relational database",
         )
 
     def _get_table_entity(
-        self, database: str, schema_name: str, table_name: str
+        self,
+        database: Optional[str],
+        schema_name: Optional[str],
+        table_name: Optional[str],
     ) -> Optional[Table]:
         """Return the table entity for the test. If service name is defined
         in GE checkpoint entity will be fetch using the FQN. If not provided
         iterative search will be perform among all the entities. If 2 entities
         are found with the same `database`.`schema`.`table` the method will
         raise an error.
 
@@ -184,41 +179,75 @@
         Return:
            Optional[Table]
 
         Raises:
              ValueError: if 2 entities with the same
                          `database`.`schema`.`table` are found
         """
+        if not all([schema_name, table_name]):
+            raise ValueError(
+                "No Schema or Table name provided. Can't fetch table entity from OpenMetadata."
+            )
+
         if self.database_service_name:
             return self.ometa_conn.get_by_name(
                 entity=Table,
                 fqn=f"{self.database_service_name}.{database}.{schema_name}.{table_name}",
+                fields=["testSuite"],
             )
 
         table_entity = [
             entity
-            for entity in self.ometa_conn.list_entities(entity=Table).entities
+            for entity in self.ometa_conn.list_entities(
+                entity=Table, fields=["testSuite"]
+            ).entities
             if f"{database}.{schema_name}.{table_name}"
             in entity.fullyQualifiedName.__root__
         ]
 
         if len(table_entity) > 1:
             raise ValueError(
                 f"Non unique `database`.`schema`.`table` found: {table_entity}."
-                "Please specify an `ometa_service_name` in you checkpoint.yml file.",
+                "Please specify an `database_service_name` in you checkpoint.yml file.",
             )
 
         if table_entity:
             return table_entity[0]
 
         logger.warning(
             "No entity found for %s.%s.%s", database, schema_name, table_name
         )
         return None
 
+    def _check_or_create_test_suite(self, table_entity: Table) -> TestSuite:
+        """Check if test suite already exists for a given table entity. If not
+        create a new one.
+
+        Args:
+            table_entity: table entity object
+        Returns:
+            TestSuite
+        """
+
+        if table_entity.testSuite:
+            test_suite = self.ometa_conn.get_by_name(
+                TestSuite, table_entity.testSuite.fullyQualifiedName.__root__
+            )
+            test_suite = cast(TestSuite, test_suite)
+            return test_suite
+
+        create_test_suite = CreateTestSuiteRequest(
+            name=f"{table_entity.fullyQualifiedName.__root__}.TestSuite",
+            executableEntityReference=table_entity.fullyQualifiedName.__root__,
+        )  # type: ignore
+        test_suite = self.ometa_conn.create_or_update_executable_test_suite(
+            create_test_suite
+        )
+        return test_suite
+
     @staticmethod
     def _get_execution_engine_url(
         data_asset: Union[Validator, DataAsset, Batch]
     ) -> URL:
         """Get execution engine used to run the expectation
 
         Args:
@@ -248,42 +277,26 @@
         """build test case fqn from table entity and GE test results
 
         Args:
             table_fqn (str): table fully qualified name
             result (Dict): result from great expectation tests
         """
         split_table_fqn = table_fqn.split(".")
-        return fqn.build(
+        fqn_ = fqn.build(
             self.ometa_conn,
             entity_type=TestCase,
             service_name=split_table_fqn[0],
             database_name=split_table_fqn[1],
             schema_name=split_table_fqn[2],
             table_name=split_table_fqn[3],
             column_name=result["expectation_config"]["kwargs"].get("column"),
-            test_case_name=f"{self.test_suite_name}-{result['expectation_config']['expectation_type']}",
-        )
-
-    def _build_entity_link_from_fqn(
-        self, table_fqn: str, column_name: Optional[str]
-    ) -> str:
-        """build entity link
-
-        Args:
-            table_fqn (str): table fqn
-            column_name (Optionla[str]): column name
-
-        Returns:
-            str: _description_
-        """
-        return (
-            f"<#E::table::{table_fqn}::columns::{column_name}>"
-            if column_name
-            else f"<#E::table::{table_fqn}>"
+            test_case_name=result["expectation_config"]["expectation_type"],
         )
+        fqn_ = cast(str, fqn_)
+        return fqn_
 
     def _get_test_case_params_value(self, result: dict) -> List[TestCaseParameterValue]:
         """Build test case parameter value from GE test result"""
         if "observed_value" not in result["result"]:
             return [
                 TestCaseParameterValue(
                     name="unexpected_percentage_total",
@@ -338,30 +351,29 @@
             test_result_value = TestResultValue(
                 name="unexpected_percentage_total",
                 value=str(unexpected_percent_total),
             )
 
         return [test_result_value]
 
-    def _handle_test_case(self, result: Dict, table_entity: Table):
+    def _handle_test_case(
+        self, result: Dict, table_entity: Table, test_suite: TestSuite
+    ):
         """Handle adding test to table entity based on the test case.
         Test Definitions will be created on the fly from the results of the
         great expectations run. We will then write the test case results to the
         specific test case.
 
         Args:
             result: GE test result
             table_entity: table entity object
+            test_suite: test suite object
         """
 
         try:
-            test_suite = self.ometa_conn.get_or_create_test_suite(
-                test_suite_name=self.test_suite_name,
-                test_suite_description="Test Suite Created from Great Expectation checkpoint run",
-            )
             test_definition = self.ometa_conn.get_or_create_test_definition(
                 test_definition_fqn=result["expectation_config"]["expectation_type"],
                 test_definition_description=result["expectation_config"][
                     "expectation_type"
                 ].replace("_", " "),
                 entity_type=EntityType.COLUMN
                 if "column" in result["expectation_config"]["kwargs"]
@@ -369,20 +381,21 @@
                 test_platforms=[TestPlatform.GreatExpectations],
                 test_case_parameter_definition=self._get_test_case_params_definition(
                     result
                 ),
             )
 
             test_case_fqn = self._build_test_case_fqn(
-                table_entity.fullyQualifiedName.__root__, result
+                table_entity.fullyQualifiedName.__root__,
+                result,
             )
 
             test_case = self.ometa_conn.get_or_create_test_case(
                 test_case_fqn,
-                entity_link=self._build_entity_link_from_fqn(
+                entity_link=get_entity_link(
                     table_entity.fullyQualifiedName.__root__,
                     fqn.split_test_case_fqn(test_case_fqn).column,
                 ),
                 test_suite_fqn=test_suite.fullyQualifiedName.__root__,
                 test_definition_fqn=test_definition.fullyQualifiedName.__root__,
                 test_case_parameter_values=self._get_test_case_params_value(result),
             )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/great_expectations/utils/ometa_config_handler.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/great_expectations/utils/ometa_config_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/bulk_sink.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/bulk_sink.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/closeable.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/closeable.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/common.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/models.py`

 * *Files 22% similar despite different names*

```diff
@@ -5,35 +5,25 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Common definitions for configuration management
+Auxiliary pydantic models used during metadata ingestion
 """
-from typing import Any, Optional, TypeVar
+from typing import Optional
 
-from pydantic import BaseModel
+from pydantic import BaseModel, Field
 
-from metadata.utils.logger import ingestion_logger
 
-T = TypeVar("T")
-
-logger = ingestion_logger()
-
-# Allow types from the generated pydantic models
-Entity = TypeVar("Entity", bound=BaseModel)
-
-
-class ConfigModel(BaseModel):
-    class Config:
-        extra = "forbid"
-
-
-class DynamicTypedConfig(ConfigModel):
-    type: str
-    config: Optional[Any]
-
-
-class WorkflowExecutionError(Exception):
-    """An error occurred when executing the workflow"""
+class TableView(BaseModel):
+    """
+    Pydantic model to define a view of a table
+    """
+
+    table_name: str = Field(..., description="Name of the table")
+    schema_name: str = Field(..., description="Name of the schema")
+    db_name: str = Field(..., description="Name of the Database")
+    view_definition: Optional[str] = Field(
+        None, description="Definition of the view in a specific SQL dialect"
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/parser.py`

 * *Files 5% similar despite different names*

```diff
@@ -38,14 +38,18 @@
     MlModelConnection,
     MlModelServiceType,
 )
 from metadata.generated.schema.entity.services.pipelineService import (
     PipelineConnection,
     PipelineServiceType,
 )
+from metadata.generated.schema.entity.services.storageService import (
+    StorageConnection,
+    StorageServiceType,
+)
 from metadata.generated.schema.metadataIngestion.dashboardServiceMetadataPipeline import (
     DashboardMetadataConfigType,
     DashboardServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseMetadataConfigType,
     DatabaseServiceMetadataPipeline,
@@ -66,14 +70,18 @@
     MlModelMetadataConfigType,
     MlModelServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.pipelineServiceMetadataPipeline import (
     PipelineMetadataConfigType,
     PipelineServiceMetadataPipeline,
 )
+from metadata.generated.schema.metadataIngestion.storageServiceMetadataPipeline import (
+    StorageMetadataConfigType,
+    StorageServiceMetadataPipeline,
+)
 from metadata.generated.schema.metadataIngestion.workflow import (
     OpenMetadataWorkflowConfig,
     WorkflowConfig,
 )
 from metadata.ingestion.ometa.provider_registry import PROVIDER_CLASS_MAP
 from metadata.utils.logger import ingestion_logger
 
@@ -89,24 +97,26 @@
     "Backend": PipelineConnection,  # For Airflow backend
     **{service: DatabaseConnection for service in DatabaseServiceType.__members__},
     **{service: DashboardConnection for service in DashboardServiceType.__members__},
     **{service: MessagingConnection for service in MessagingServiceType.__members__},
     **{service: MetadataConnection for service in MetadataServiceType.__members__},
     **{service: PipelineConnection for service in PipelineServiceType.__members__},
     **{service: MlModelConnection for service in MlModelServiceType.__members__},
+    **{service: StorageConnection for service in StorageServiceType.__members__},
 }
 
 SOURCE_CONFIG_CLASS_MAP = {
     DashboardMetadataConfigType.DashboardMetadata.value: DashboardServiceMetadataPipeline,
     ProfilerConfigType.Profiler.value: DatabaseServiceProfilerPipeline,
     DatabaseUsageConfigType.DatabaseUsage.value: DatabaseServiceQueryUsagePipeline,
     MessagingMetadataConfigType.MessagingMetadata.value: MessagingServiceMetadataPipeline,
     PipelineMetadataConfigType.PipelineMetadata.value: PipelineServiceMetadataPipeline,
     MlModelMetadataConfigType.MlModelMetadata.value: MlModelServiceMetadataPipeline,
     DatabaseMetadataConfigType.DatabaseMetadata.value: DatabaseServiceMetadataPipeline,
+    StorageMetadataConfigType.StorageMetadata.value: StorageServiceMetadataPipeline,
 }
 
 
 class ParsingConfigurationError(Exception):
     """A parsing configuration error has happened"""
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/processor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/sink.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/sink.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/stage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/stage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/status.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/topology_runner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/topology_runner.py`

 * *Files 1% similar despite different names*

```diff
@@ -227,11 +227,10 @@
             else:
                 yield entity
 
             if stage.context and not stage.cache_all:
                 self.update_context(key=stage.context, value=entity)
             if stage.context and stage.cache_all:
                 self.append_context(key=stage.context, value=entity)
-            logger.debug(self.context)
 
     def _is_force_overwrite_enabled(self) -> bool:
         return self.metadata.config and self.metadata.config.forceEntityOverwriting
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/api/workflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/api/workflow.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/bulksink/metadata_usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/bulksink/metadata_usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/builders.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/saphana/connection.py`

 * *Files 23% similar despite different names*

```diff
@@ -4,148 +4,163 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Get and test connection utilities
+Source connection handler
 """
 from functools import partial
-from typing import Any, Callable, Dict, Optional
+from typing import Callable, Dict, Optional
 from urllib.parse import quote_plus
 
-from pydantic import SecretStr
-from sqlalchemy import create_engine
+from sqlalchemy import inspect
 from sqlalchemy.engine import Engine
-from sqlalchemy.event import listen
-from sqlalchemy.pool import QueuePool
 
-from metadata.generated.schema.entity.services.connections.connectionBasicType import (
-    ConnectionArguments,
-    ConnectionOptions,
+from metadata.generated.schema.entity.automations.workflow import (
+    Workflow as AutomationWorkflow,
+)
+from metadata.generated.schema.entity.services.connections.database.sapHanaConnection import (
+    HdbUserStoreConnection,
+    SapHanaConnection,
+    SqlConnection,
+)
+from metadata.ingestion.connections.builders import (
+    create_generic_db_connection,
+    get_connection_args_common,
+    get_connection_options_dict,
 )
-from metadata.ingestion.connections.headers import inject_query_header_by_conn
-from metadata.ingestion.connections.secrets import connection_with_options_secrets
+from metadata.ingestion.connections.test_connections import (
+    execute_inspector_func,
+    test_connection_engine_step,
+    test_connection_steps,
+)
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-@connection_with_options_secrets
-def get_connection_args_common(connection) -> Dict[str, Any]:
+def get_database_connection_url(connection: SapHanaConnection) -> str:
     """
-    Read the connection arguments of a connection.
-
-    Any function operating on top of the connection
-    arguments should be decorated with `connection_with_options_secrets`
+    Build the SQLConnection URL for the database connection
     """
 
-    return (
-        connection.connectionArguments.__root__
-        if connection.connectionArguments and connection.connectionArguments.__root__
-        else {}
-    )
+    conn = connection.connection
 
+    if not isinstance(conn, SqlConnection):
+        raise ValueError("Database Connection requires the SQL connection details")
 
-def create_generic_db_connection(
-    connection, get_connection_url_fn: Callable, get_connection_args_fn: Callable
-) -> Engine:
-    """
-    Generic Engine creation from connection object
-
-    Args:
-        connection: JSON Schema connection model
-        get_connection_url_fn: url build callable
-        get_connection_args_fn: args build callable
-    Returns:
-        SQLAlchemy Engine
-    """
-    engine = create_engine(
-        get_connection_url_fn(connection),
-        connect_args=get_connection_args_fn(connection),
-        poolclass=QueuePool,
-        pool_reset_on_return=None,  # https://docs.sqlalchemy.org/en/14/core/pooling.html#reset-on-return
-        echo=False,
-        max_overflow=-1,
-    )
-
-    if hasattr(connection, "supportsQueryComment"):
-        listen(
-            engine,
-            "before_cursor_execute",
-            partial(inject_query_header_by_conn, connection),
-            retval=True,
-        )
-
-    return engine
-
-
-def get_connection_options_dict(connection) -> Optional[Dict[str, Any]]:
-    """
-    Given a connection object, returns the connection options
-    dictionary if exists
-    """
-    return (
-        connection.connectionOptions.__root__
-        if connection.connectionOptions and connection.connectionOptions.__root__
-        else None
+    url = (
+        f"{connection.scheme.value}://"
+        f"{quote_plus(conn.username)}:"
+        f"{quote_plus(conn.password.get_secret_value())}@"
+        f"{conn.hostPort}"
     )
 
+    if hasattr(connection, "database"):
+        url += f"/{connection.database}" if connection.database else ""
 
-def init_empty_connection_arguments() -> ConnectionArguments:
-    """
-    Initialize a ConnectionArguments model with an empty dictionary.
-    This helps set keys without further validations.
+    options = get_connection_options_dict(connection)
+    if options:
+        if hasattr(conn, "database") and not conn.database:
+            url += "/"
+        params = "&".join(
+            f"{key}={quote_plus(value)}" for (key, value) in options.items() if value
+        )
+        url = f"{url}?{params}"
+    return url
 
-    Running `ConnectionArguments()` returns `ConnectionArguments(__root__=None)`.
 
-    Instead, we want `ConnectionArguments(__root__={}})` so that
-    we can pass new keys easily as `connectionArguments.__root__["key"] = "value"`
+def get_hdb_connection_url(connection: SapHanaConnection) -> str:
     """
-    return ConnectionArguments(__root__={})
-
-
-def init_empty_connection_options() -> ConnectionOptions:
+    Build the SQLConnection URL for the database connection
     """
-    Initialize a ConnectionOptions model with an empty dictionary.
-    This helps set keys without further validations.
 
-    Running `ConnectionOptions()` returns `ConnectionOptions(__root__=None)`.
+    if not isinstance(connection.connection, HdbUserStoreConnection):
+        raise ValueError("Database Connection requires the SQL connection details")
 
-    Instead, we want `ConnectionOptions(__root__={}})` so that
-    we can pass new keys easily as `ConnectionOptions.__root__["key"] = "value"`
-    """
-    return ConnectionOptions(__root__={})
+    return f"{connection.scheme.value}://userkey={connection.connection.userKey}"
 
 
-def get_connection_url_common(connection):
+def get_connection(connection: SapHanaConnection) -> Engine:
     """
-    Common method for building the source connection urls
+    Create connection
     """
 
-    url = f"{connection.scheme.value}://"
+    if isinstance(connection.connection, SqlConnection):
+        return create_generic_db_connection(
+            connection=connection,
+            get_connection_url_fn=get_database_connection_url,
+            get_connection_args_fn=get_connection_args_common,
+        )
 
-    if connection.username:
-        url += f"{quote_plus(connection.username)}"
-        if not connection.password:
-            connection.password = SecretStr("")
-        url += f":{quote_plus(connection.password.get_secret_value())}"
-        url += "@"
+    if isinstance(connection.connection, HdbUserStoreConnection):
+        return create_generic_db_connection(
+            connection=connection,
+            get_connection_url_fn=get_hdb_connection_url,
+            get_connection_args_fn=get_connection_args_common,
+        )
 
-    url += connection.hostPort
-    if hasattr(connection, "database"):
-        url += f"/{connection.database}" if connection.database else ""
+    raise ValueError("Unrecognized SAP Hana connection type!")
 
-    elif hasattr(connection, "databaseSchema"):
-        url += f"/{connection.databaseSchema}" if connection.databaseSchema else ""
 
-    options = get_connection_options_dict(connection)
-    if options:
-        if (hasattr(connection, "database") and not connection.database) or (
-            hasattr(connection, "databaseSchema") and not connection.databaseSchema
-        ):
-            url += "/"
-        params = "&".join(
-            f"{key}={quote_plus(value)}" for (key, value) in options.items() if value
-        )
-        url = f"{url}?{params}"
-    return url
+def _build_test_fn_dict(
+    engine: Engine, service_connection: SapHanaConnection
+) -> Dict[str, Callable]:
+    """
+    Build the test connection steps dict
+    """
+
+    def custom_executor(engine_: Engine, inspector_fn_str: str):
+        """
+        Check if we can list tables or views from a given schema
+        or a random one
+        """
+
+        inspector = inspect(engine_)
+        inspector_fn = getattr(inspector, inspector_fn_str)
+
+        # HDB connection won't have a databaseSchema
+        if getattr(service_connection.connection, "databaseSchema"):
+            inspector_fn(service_connection.connection.databaseSchema)
+        else:
+            schema_name = inspector.get_schema_names() or []
+            for schema in schema_name:
+                inspector_fn(schema)
+                break
+
+    if isinstance(service_connection.connection, SqlConnection):
+        return {
+            "CheckAccess": partial(test_connection_engine_step, engine),
+            "GetSchemas": partial(execute_inspector_func, engine, "get_schema_names"),
+            "GetTables": partial(custom_executor, engine, "get_table_names"),
+            "GetViews": partial(custom_executor, engine, "get_view_names"),
+        }
+
+    if isinstance(service_connection.connection, HdbUserStoreConnection):
+        return {
+            "CheckAccess": partial(test_connection_engine_step, engine),
+            "GetSchemas": partial(execute_inspector_func, engine, "get_schema_names"),
+            "GetTables": partial(custom_executor, engine, "get_table_names"),
+            "GetViews": partial(custom_executor, engine, "get_view_names"),
+        }
+
+    raise ValueError(f"Unknown connection type for {service_connection.connection}")
+
+
+def test_connection(
+    metadata: OpenMetadata,
+    engine: Engine,
+    service_connection: SapHanaConnection,
+    automation_workflow: Optional[AutomationWorkflow] = None,
+) -> None:
+    """
+    Test connection. This can be executed either as part
+    of a metadata workflow or during an Automation Workflow
+    """
+
+    test_connection_steps(
+        metadata=metadata,
+        test_fn=_build_test_fn_dict(engine, service_connection),
+        service_type=service_connection.type.value,
+        automation_workflow=automation_workflow,
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/headers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/headers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/secrets.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/secrets.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/session.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/session.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/connections/test_connections.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/connections/test_connections.py`

 * *Files 1% similar despite different names*

```diff
@@ -109,21 +109,23 @@
     steps: List[TestConnectionStep],
     automation_workflow: AutomationWorkflow,
 ) -> None:
     """
     Run the test connection as part of the automation workflow
     We need to update the automation workflow in each step
     """
+    logger.info("Starting Test Connection Workflow Steps")
     test_connection_result = TestConnectionResult(
         status=StatusType.Running,
         steps=[],
     )
     try:
         for step in steps:
             try:
+                logger.info(f"Running {step.name}...")
                 step.function()
                 test_connection_result.steps.append(
                     TestConnectionStepResult(
                         name=step.name,
                         mandatory=step.mandatory,
                         passed=True,
                     )
@@ -153,14 +155,15 @@
 
         test_connection_result.status = (
             StatusType.Failed
             if any(step for step in test_connection_result.steps if not step.passed)
             else StatusType.Successful
         )
 
+        logger.info("Updating Workflow Response")
         metadata.patch_automation_workflow_response(
             automation_workflow, test_connection_result, WorkflowStatus.Successful
         )
 
     except Exception as err:
         logger.error(
             f"Wild error happened while testing the connection in the workflow - {err}"
@@ -342,21 +345,21 @@
     expected format for queries would be <TestConnectionStep>:<Query>
     queries = {
         "GetQueries": "select * from query_log",
     }
     """
     queries = queries or {}
 
-    def custom_executor(engine, inspector_fn_str: str):
+    def custom_executor(engine_: Engine, inspector_fn_str: str):
         """
         Check if we can list tables or views from a given schema
         or a random one
         """
 
-        inspector = inspect(engine)
+        inspector = inspect(engine_)
         inspector_fn = getattr(inspector, inspector_fn_str)
 
         if service_connection.databaseSchema:
             inspector_fn(service_connection.databaseSchema)
         else:
             schema_name = inspector.get_schema_names() or []
             for schema in schema_name:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/parser.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 """
 Lineage Parser configuration
 """
 import traceback
 from collections import defaultdict
 from copy import deepcopy
 from logging.config import DictConfigurator
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple
 
 import sqlparse
 from cached_property import cached_property
 from sqllineage import SQLPARSE_DIALECT
 from sqllineage.core.models import Column, Table
 from sqllineage.exceptions import SQLLineageException
 from sqllineage.runner import LineageRunner
@@ -114,22 +114,28 @@
         """
         Get a list of target tables
         """
         # These are @lazy_property, not properly being picked up by IDEs. Ignore the warning
         return self.retrieve_tables(self.parser.target_tables)
 
     @cached_property
-    def column_lineage(self) -> List[Union[Tuple[Column, Column]]]:
+    def column_lineage(self) -> List[Tuple[Column, Column]]:
         """
         Get a list of tuples of column lineage
         """
         if self.parser._dialect == SQLPARSE_DIALECT:  # pylint: disable=protected-access
             return self.parser.get_column_lineage()
         column_lineage = []
-        for src_column, tgt_column in self.parser.get_column_lineage():
+        for col_lineage in self.parser.get_column_lineage():
+            # In case of column level lineage it is possible that we get
+            # two or more columns as there might be some intermediate columns
+            # but the source columns will be the first value and
+            # the target column always will be the last columns
+            src_column = col_lineage[0]
+            tgt_column = col_lineage[-1]
             src_col = Column(src_column.raw_name)
             src_col._parent = src_column._parent  # pylint: disable=protected-access
             tgt_col = Column(tgt_column.raw_name)
             tgt_col._parent = tgt_column._parent  # pylint: disable=protected-access
             column_lineage.append((src_col, tgt_col))
         return column_lineage
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/lineage/sql_lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/lineage/sql_lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/custom_pydantic.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/custom_pydantic.py`

 * *Files 4% similar despite different names*

```diff
@@ -71,14 +71,21 @@
         warnings.warn(
             "`secret_str.display()` is deprecated, use `str(secret_str)` instead",
             DeprecationWarning,
         )
         return str(self)
 
     def get_secret_value(self, skip_secret_manager: bool = False) -> str:
+        """
+        This function should only be called after the SecretsManager has properly
+        been initialized (e.g., after instantiating the ometa client).
+
+        Since the SecretsManagerFactory is a singleton, getting it here
+        will pick up the object with all the necessary info already in it.
+        """
         # Importing inside function to avoid circular import error
         from metadata.utils.secrets.secrets_manager_factory import (  # pylint: disable=import-outside-toplevel,cyclic-import
             SecretsManagerFactory,
         )
 
         if (
             not skip_secret_manager
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/custom_types.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/custom_types.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/delete_entity.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/delete_entity.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/encoders.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/encoders.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/es_documents.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/es_documents.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,18 +19,16 @@
     MlFeature,
     MlHyperParameter,
     MlStore,
 )
 from metadata.generated.schema.entity.data.pipeline import Task
 from metadata.generated.schema.entity.data.table import Column
 from metadata.generated.schema.type import schema
-from metadata.generated.schema.type.entityReference import (
-    EntityReference,
-    EntityReferenceList,
-)
+from metadata.generated.schema.type.entityReference import EntityReference
+from metadata.generated.schema.type.entityReferenceList import EntityReferenceList
 from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.generated.schema.type.usageDetails import UsageDetails
 
 
 class ESSuggest(BaseModel):
     input: str
     weight: int
@@ -108,15 +106,15 @@
     name: str
     displayName: str
     fullyQualifiedName: str
     description: Optional[str] = None
     version: float
     updatedAt: Optional[int]
     updatedBy: Optional[str]
-    dashboardUrl: Optional[str]
+    sourceUrl: Optional[str]
     charts: List[EntityReference]
     href: Optional[str]
     owner: EntityReference = None
     followers: List[str]
     service: EntityReference
     serviceType: str
     usageSummary: UsageDetails = None
@@ -138,15 +136,15 @@
     name: str
     displayName: str
     fullyQualifiedName: str
     description: Optional[str] = None
     version: float
     updatedAt: Optional[int]
     updatedBy: Optional[str]
-    pipelineUrl: Optional[str]
+    sourceUrl: Optional[str]
     tasks: List[Task]
     deleted: bool
     href: Optional[str]
     owner: EntityReference = None
     followers: List[str]
     tags: List[TagLabel]
     tier: Optional[TagLabel] = None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/ometa_classification.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/ometa_classification.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/ometa_topic_data.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/ometa_topic_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/pipeline_status.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/pipeline_status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/profile_data.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/profile_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/table_metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/table_metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/topology.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/topology.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/models/user.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/models/user.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/auth_provider.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/auth_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Interface definition for an Auth provider
 """
-import http.client
 import json
 import os.path
 import sys
 import traceback
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
 from datetime import datetime
@@ -294,25 +293,28 @@
         self.expiry = None
 
     @classmethod
     def create(cls, config: OpenMetadataConnection):
         return cls(config)
 
     def auth_token(self) -> None:
-        conn = http.client.HTTPSConnection(self.security_config.domain)
+
         payload = (
             f"grant_type=client_credentials&client_id={self.security_config.clientId}"
             f"&client_secret={self.security_config.secretKey.get_secret_value()}"
             f"&audience=https://{self.security_config.domain}/api/v2/"
         )
         headers = {"content-type": "application/x-www-form-urlencoded"}
-        conn.request(
-            "POST", f"/{self.security_config.domain}/oauth/token", payload, headers
+        res = requests.post(
+            url=f"https://{self.security_config.domain}/oauth/token",
+            data=payload,
+            headers=headers,
+            timeout=60 * 5,
         )
-        res = conn.getresponse()
+
         data = json.loads(res.read().decode("utf-8"))
 
         token = data.get(ACCESS_TOKEN)
         if not token:
             raise AuthenticationException(f"Error getting access token: {data}")
 
         self.generated_auth_token = token
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/client.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,16 +10,15 @@
 #  limitations under the License.
 """
 Python API REST wrapper and helpers
 """
 import datetime
 import time
 import traceback
-from copy import deepcopy
-from typing import Any, Callable, Dict, List, Optional, Union
+from typing import Callable, Dict, List, Optional, Union
 
 import requests
 from requests.exceptions import HTTPError
 
 from metadata.config.common import ConfigModel
 from metadata.ingestion.ometa.credentials import URL, get_api_version
 from metadata.utils.logger import ometa_logger
@@ -164,41 +163,33 @@
         # If a header value is provided in modulo string format and matches an existing header,
         # the value will be set to that value.
         # Example: "Proxy-Authorization": "%(Authorization)s"
         # This will result in the Authorization value being set for the Proxy-Authorization Extra Header
         if self.config.extra_headers:
             extra_headers: Dict[str, str] = self.config.extra_headers
             extra_headers = {k: (v % headers) for k, v in extra_headers.items()}
-            logger.debug(
-                "Extra headers provided '%s'",
-                self._mask_authorization_headers(extra_headers),
-            )
             headers = {**headers, **extra_headers}
 
         opts = {
             "headers": headers,
             # Since we allow users to set endpoint URL via env var,
             # human error to put non-SSL endpoint could exploit
             # uncanny issues in non-GET request redirecting http->https.
             # It's better to fail early if the URL isn't right.
             "allow_redirects": self.config.allow_redirects,
             "verify": self._verify,
         }
 
-        masked_opts = self._mask_authorization_headers(opts)
-
         method_key = "params" if method.upper() == "GET" else "data"
         opts[method_key] = data
 
         total_retries = self._retry if self._retry > 0 else 0
         retry = total_retries
         while retry >= 0:
             try:
-                logger.debug("URL %s, method %s", url, method)
-                logger.debug("Data %s", masked_opts)
                 return self._one_request(method, url, opts, retry)
             except RetryException:
                 retry_wait = self._retry_wait * (total_retries - retry + 1)
                 logger.warning(
                     "sleep %s seconds and retrying %s %s more time(s)...",
                     retry_wait,
                     url,
@@ -334,16 +325,7 @@
         """
         Close requests session
         """
         self._session.close()
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.close()
-
-    def _mask_authorization_headers(self, opts: Dict[str, Any]) -> Dict[str, Any]:
-        if opts and opts.get("headers"):
-            if self.config.auth_header and opts["headers"][self.config.auth_header]:
-                masked_opts = deepcopy(opts)
-                if self.config.auth_header and opts["headers"][self.config.auth_header]:
-                    masked_opts["headers"][self.config.auth_header] = "********"
-                return masked_opts
-        return opts
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/client_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/client_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/credentials.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/credentials.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/es_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/es_mixin.py`

 * *Files 23% similar despite different names*

```diff
@@ -15,15 +15,19 @@
 """
 import functools
 import traceback
 from typing import Generic, List, Optional, Type, TypeVar
 
 from pydantic import BaseModel
 
-from metadata.ingestion.ometa.client import REST
+from metadata.generated.schema.api.createEventPublisherJob import (
+    CreateEventPublisherJob,
+)
+from metadata.generated.schema.system.eventPublisherJob import EventPublisherResult
+from metadata.ingestion.ometa.client import REST, APIError
 from metadata.utils.elasticsearch import ES_INDEX_MAP
 from metadata.utils.logger import ometa_logger
 
 logger = ometa_logger()
 
 T = TypeVar("T", bound=BaseModel)
 
@@ -104,7 +108,34 @@
             )
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Elasticsearch search failed for query [{query_string}]: {exc}"
             )
         return None
+
+    def reindex_es(
+        self,
+        config: CreateEventPublisherJob,
+    ) -> Optional[EventPublisherResult]:
+        """
+        Method to trigger elasticsearch reindex
+        """
+        try:
+            resp = self.client.post(path="/search/reindex", data=config.json())
+            return EventPublisherResult(**resp)
+        except APIError as err:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"Failed to trigger es reindex job due to {err}")
+            return None
+
+    def get_reindex_job_status(self, job_id: str) -> Optional[EventPublisherResult]:
+        """
+        Method to fetch the elasticsearch reindex job status
+        """
+        try:
+            resp = self.client.get(path=f"/search/reindex/{job_id}")
+            return EventPublisherResult(**resp)
+        except APIError as err:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"Failed to fetch reindex job status due to {err}")
+            return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/glossary_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/glossary_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/patch_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/patch_mixin.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,14 +24,16 @@
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.automations.workflow import WorkflowStatus
 from metadata.generated.schema.entity.data.table import Column, Table, TableConstraint
 from metadata.generated.schema.entity.services.connections.testConnectionResult import (
     TestConnectionResult,
 )
+from metadata.generated.schema.tests.testCase import TestCase, TestCaseParameterValue
+from metadata.generated.schema.type.basic import EntityLink
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.ingestion.ometa.client import REST
 from metadata.ingestion.ometa.mixins.patch_mixin_utils import (
     OMetaPatchMixinBase,
     PatchField,
     PatchOperation,
@@ -204,14 +206,41 @@
         table.tableConstraints = instance.tableConstraints
 
         destination = table.copy(deep=True)
         destination.tableConstraints = constraints
 
         return self.patch(entity=Table, source=table, destination=destination)
 
+    def patch_test_case_definition(
+        self,
+        source: TestCase,
+        entity_link: str,
+        test_case_parameter_values: Optional[List[TestCaseParameterValue]] = None,
+    ) -> Optional[TestCase]:
+        """Given a test case and a test case definition JSON PATCH the test case
+
+        Args
+            test_case: test case object
+            test_case_definition: test case definition to add
+        """
+        source: TestCase = self._fetch_entity_if_exists(
+            entity=TestCase, entity_id=source.id, fields=["testDefinition", "testSuite"]
+        )  # type: ignore
+
+        if not source:
+            return None
+
+        destination = source.copy(deep=True)
+
+        destination.entityLink = EntityLink(__root__=entity_link)
+        if test_case_parameter_values:
+            destination.parameterValues = test_case_parameter_values
+
+        return self.patch(entity=TestCase, source=source, destination=destination)
+
     def patch_tag(
         self,
         entity: Type[T],
         source: T,
         tag_label: TagLabel,
         operation: Union[
             PatchOperation.ADD, PatchOperation.REMOVE
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -76,15 +76,15 @@
         # All tasks are the union of the incoming tasks & the not updated tasks
         all_tasks = [*tasks, *not_updated_tasks]
 
         updated_pipeline = CreatePipelineRequest(
             name=pipeline.name,
             displayName=pipeline.displayName,
             description=pipeline.description,
-            pipelineUrl=pipeline.pipelineUrl,
+            sourceUrl=pipeline.sourceUrl,
             concurrency=pipeline.concurrency,
             pipelineLocation=pipeline.pipelineLocation,
             startDate=pipeline.startDate,
             service=pipeline.service.fullyQualifiedName,
             tasks=all_tasks,
             owner=pipeline.owner,
             tags=pipeline.tags,
@@ -103,15 +103,15 @@
         remove the task B from the entity
         """
 
         updated_pipeline = CreatePipelineRequest(
             name=pipeline.name,
             displayName=pipeline.displayName,
             description=pipeline.description,
-            pipelineUrl=pipeline.pipelineUrl,
+            sourceUrl=pipeline.sourceUrl,
             concurrency=pipeline.concurrency,
             pipelineLocation=pipeline.pipelineLocation,
             startDate=pipeline.startDate,
             service=pipeline.service.fullyQualifiedName,
             tasks=[task for task in pipeline.tasks if task.name in task_ids],
             owner=pipeline.owner,
             tags=pipeline.tags,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/query_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/query_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/server_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/server_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/service_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/service_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/table_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/table_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/topic_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/topic_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/user_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/user_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/mixins/version_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/mixins/version_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/ometa_api.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/ometa_api.py`

 * *Files 0% similar despite different names*

```diff
@@ -190,15 +190,15 @@
 
     def __init__(self, config: OpenMetadataConnection, raw_data: bool = False):
         self.config = config
 
         # Load the secrets' manager client
         self.secrets_manager_client = SecretsManagerFactory(
             config.secretsManagerProvider,
-            config.secretsManagerCredentials,
+            config.secretsManagerLoader,
         ).get_secrets_manager()
 
         # Load the auth provider init from the registry
         auth_provider_fn = auth_provider_registry.registry.get(
             self.config.authProvider.value
         )
         if not auth_provider_fn:
@@ -562,15 +562,15 @@
         is_create = "create" in data.__class__.__name__.lower()
 
         # Prepare the return Entity Type
         if is_create:
             entity_class = self.get_entity_from_create(entity)
         else:
             raise InvalidEntityException(
-                f"PUT operations need a CrateEntity, not {entity}"
+                f"PUT operations need a CreateEntity, not {entity}"
             )
         resp = self.client.put(
             self.get_suffix(entity), data=data.json(encoder=show_secrets_encoder)
         )
         if not resp:
             raise EmptyPayloadException(
                 f"Got an empty response when trying to PUT to {self.get_suffix(entity)}, {data.json()}"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/provider_registry.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/provider_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/ometa/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/ometa/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/processor/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/processor/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch.py`

 * *Files 1% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 from metadata.generated.schema.entity.data.topic import Topic
 from metadata.generated.schema.entity.policies.policy import Policy
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
-from metadata.generated.schema.type.entityReference import EntityReferenceList
+from metadata.generated.schema.type.entityReferenceList import EntityReferenceList
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.sink import Sink
 from metadata.ingestion.models.es_documents import (
     ContainerESDocument,
     DashboardESDocument,
     ESSuggest,
     GlossaryTermESDocument,
@@ -61,14 +61,15 @@
     TableESDocument,
     TagESDocument,
     TeamESDocument,
     TopicESDocument,
     UserESDocument,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.ometa.utils import model_str
 from metadata.ingestion.sink.elasticsearch_mapping.container_search_index_mapping import (
     CONTAINER_ELASTICSEARCH_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.dashboard_search_index_mapping import (
     DASHBOARD_ELASTICSEARCH_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.entity_report_data_index_mapping import (
@@ -644,15 +645,15 @@
         name=display_name,
         displayName=display_name,
         description=record.description.__root__ if record.description else "",
         fullyQualifiedName=record.fullyQualifiedName.__root__,
         version=record.version.__root__,
         updatedAt=record.updatedAt.__root__,
         updatedBy=record.updatedBy,
-        dashboardUrl=record.dashboardUrl,
+        sourceUrl=record.sourceUrl,
         charts=record.charts.__root__,
         href=record.href.__root__,
         deleted=record.deleted,
         service=record.service,
         serviceType=str(record.serviceType.name),
         usageSummary=record.usageSummary,
         tier=tier,
@@ -686,15 +687,15 @@
         name=record.name.__root__,
         displayName=display_name,
         description=record.description.__root__ if record.description else "",
         fullyQualifiedName=record.fullyQualifiedName.__root__,
         version=record.version.__root__,
         updatedAt=record.updatedAt.__root__,
         updatedBy=record.updatedBy,
-        pipelineUrl=record.pipelineUrl,
+        sourceUrl=record.sourceUrl,
         tasks=record.tasks,
         href=record.href.__root__,
         deleted=record.deleted,
         service=record.service,
         serviceType=str(record.serviceType.name),
         suggest=suggest,
         task_suggest=task_suggest,
@@ -902,15 +903,15 @@
     tag_docs = []
     tag_list = metadata.list_entities(
         entity=Tag, params={"parent": record.name.__root__}
     )
 
     for tag in tag_list.entities or []:
         suggest = [
-            ESSuggest(input=tag.fullyQualifiedName.__root__, weight=5),
+            ESSuggest(input=model_str(tag.fullyQualifiedName), weight=5),
             ESSuggest(input=tag.name.__root__, weight=10),
         ]
 
         tag_doc = TagESDocument(
             id=str(tag.id.__root__),
             name=str(tag.name.__root__),
             description=tag.description.__root__ if tag.description else "",
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/container_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/container_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py`

 * *Files 0% similar despite different names*

```diff
@@ -90,15 +90,15 @@
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "dashboardUrl": {
+      "sourceUrl": {
         "type": "text"
       },
       "charts": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,15 +90,15 @@
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "pipelineUrl": {
+      "sourceUrl": {
         "type": "text"
       },
       "mlFeatures": {
         "properties": {
           "name": {
             "type": "keyword",
             "fields": {
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py`

 * *Files 0% similar despite different names*

```diff
@@ -79,15 +79,15 @@
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "pipelineUrl": {
+      "sourceUrl": {
         "type": "text"
       },
       "tasks": {
         "properties": {
           "name": {
             "type": "keyword",
             "fields": {
@@ -106,15 +106,15 @@
               }
             }
           },
           "description": {
             "type": "text",
             "analyzer": "om_analyzer"
           },
-          "taskUrl": {
+          "sourceUrl": {
             "type": "text"
           },
           "taskType": {
             "type": "text"
           }
         }
       },
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/query_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/query_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/file.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/file.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/sink/metadata_rest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/sink/metadata_rest.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,14 +21,17 @@
 from requests.exceptions import HTTPError
 
 from metadata.config.common import ConfigModel
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.api.teams.createRole import CreateRoleRequest
 from metadata.generated.schema.api.teams.createTeam import CreateTeamRequest
 from metadata.generated.schema.api.teams.createUser import CreateUserRequest
+from metadata.generated.schema.api.tests.createLogicalTestCases import (
+    CreateLogicalTestCases,
+)
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.teams.role import Role
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.ingestion.api.common import Entity
@@ -36,14 +39,15 @@
 from metadata.ingestion.models.delete_entity import DeleteEntity
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.ometa_topic_data import OMetaTopicSampleData
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.models.profile_data import OMetaTableProfileSampleData
 from metadata.ingestion.models.table_metadata import OMetaTableConstraints
 from metadata.ingestion.models.tests_data import (
+    OMetaLogicalTestSuiteSample,
     OMetaTestCaseResultsSample,
     OMetaTestCaseSample,
     OMetaTestSuiteSample,
 )
 from metadata.ingestion.models.user import OMetaUserProfile
 from metadata.ingestion.ometa.client import APIError
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
@@ -97,14 +101,17 @@
         self.write_record.register(OMetaTableConstraints, self.write_table_constraints)
         self.write_record.register(
             OMetaTableProfileSampleData, self.write_profile_sample_data
         )
         self.write_record.register(OMetaTestSuiteSample, self.write_test_suite_sample)
         self.write_record.register(OMetaTestCaseSample, self.write_test_case_sample)
         self.write_record.register(
+            OMetaLogicalTestSuiteSample, self.write_logical_test_suite_sample
+        )
+        self.write_record.register(
             OMetaTestCaseResultsSample, self.write_test_case_results_sample
         )
         self.write_record.register(OMetaTopicSampleData, self.write_topic_sample_data)
 
     @classmethod
     def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
         config = MetadataRestSinkConfig.parse_obj(config_dict)
@@ -370,25 +377,45 @@
             )
 
     def write_test_suite_sample(self, record: OMetaTestSuiteSample):
         """
         Use the /testSuites endpoint to ingest sample test suite
         """
         try:
-            self.metadata.create_or_update(record.test_suite)
+            self.metadata.create_or_update_executable_test_suite(record.test_suite)
             logger.debug(
                 f"Successfully created test Suite {record.test_suite.name.__root__}"
             )
             self.status.records_written(f"testSuite: {record.test_suite.name.__root__}")
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Unexpected error writing test suite sample [{record}]: {exc}"
             )
 
+    def write_logical_test_suite_sample(self, record: OMetaLogicalTestSuiteSample):
+        """Create logical test suite and add tests cases to it"""
+        try:
+            test_suite = self.metadata.create_or_update(record.test_suite)
+            logger.debug(
+                f"Successfully created logical test Suite {record.test_suite.name.__root__}"
+            )
+            self.status.records_written(f"testSuite: {record.test_suite.name.__root__}")
+            self.metadata.add_logical_test_cases(
+                CreateLogicalTestCases(
+                    testSuiteId=test_suite.id,
+                    testCaseIds=[test_case.id for test_case in record.test_cases],  # type: ignore
+                )
+            )
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(
+                f"Unexpected error writing test suite sample [{record}]: {exc}"
+            )
+
     def write_test_case_sample(self, record: OMetaTestCaseSample):
         """
         Use the /dataQuality/testCases endpoint to ingest sample test suite
         """
         try:
             self.metadata.create_or_update(record.test_case)
             logger.debug(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/connections.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/connections.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/dashboard_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/dashboard_service.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,14 +61,20 @@
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_dashboard
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
+LINEAGE_MAP = {
+    Dashboard: "dashboard",
+    Table: "table",
+    DashboardDataModel: "dashboardDataModel",
+}
+
 
 class DashboardUsage(BaseModel):
     """
     Wrapper to handle type at the sink
     """
 
     dashboard: Dashboard
@@ -387,30 +393,26 @@
             f"Processing ownership is not supported for {self.service_connection.type.name}"
         )
         return None
 
     @staticmethod
     def _get_add_lineage_request(
         to_entity: Union[Dashboard, DashboardDataModel],
-        from_entity: Union[Table, DashboardDataModel],
+        from_entity: Union[Table, DashboardDataModel, Dashboard],
     ) -> Optional[AddLineageRequest]:
         if from_entity and to_entity:
             return AddLineageRequest(
                 edge=EntitiesEdge(
                     fromEntity=EntityReference(
                         id=from_entity.id.__root__,
-                        type="table"
-                        if isinstance(from_entity, Table)
-                        else "dashboardDataModel",
+                        type=LINEAGE_MAP[type(from_entity)],
                     ),
                     toEntity=EntityReference(
                         id=to_entity.id.__root__,
-                        type="dashboard"
-                        if isinstance(to_entity, Dashboard)
-                        else "dashboardDataModel",
+                        type=LINEAGE_MAP[type(to_entity)],
                     ),
                 )
             )
         return None
 
     def get_dashboard(self) -> Any:
         """
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -118,15 +118,15 @@
         try:
             dashboard_url = (
                 f"{self.service_connection.sandboxDomain}/page/{dashboard_details.id}"
             )
 
             dashboard_request = CreateDashboardRequest(
                 name=dashboard_details.id,
-                dashboardUrl=dashboard_url,
+                sourceUrl=dashboard_url,
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
                         service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
@@ -208,15 +208,15 @@
                     self.status.filter(chart.name, "Chart Pattern not allowed")
                     continue
                 if chart.name:
                     yield CreateChartRequest(
                         name=chart_id,
                         description=chart.description,
                         displayName=chart.name,
-                        chartUrl=chart_url,
+                        sourceUrl=chart_url,
                         service=self.context.dashboard_service.fullyQualifiedName.__root__,
                         chartType=get_standard_chart_type(chart.metadata.chartType),
                     )
                     self.status.scanned(chart.name)
             except Exception as exc:
                 name = chart.name if chart else ""
                 error = f"Error creating chart [{name}]: {exc}"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/columns.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/columns.py`

 * *Files 2% similar despite different names*

```diff
@@ -92,18 +92,21 @@
 ) -> List[Column]:
     """
     Obtain the column (measures and dimensions) from the models
     """
     columns = []
     all_fields = (model.fields.dimensions or []) + (model.fields.measures or [])
     for field in cast(Sequence[LookmlModelExploreField], all_fields):
+        type_ = LOOKER_TYPE_MAP.get(field.type, DataType.UNKNOWN)
         columns.append(
             Column(
                 name=field.name,
                 displayName=getattr(field, "label_short", field.label),
-                dataType=LOOKER_TYPE_MAP.get(field.type, DataType.UNKNOWN),
+                dataType=type_,
+                # We cannot get the inner type from the sdk of .lkml
+                arrayDataType=DataType.UNKNOWN if type_ == DataType.ARRAY else None,
                 dataTypeDisplay=field.type,
                 description=field.description,
             )
         )
 
     return columns
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/connection.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,58 +8,50 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-import os
-from typing import Optional
 
-import looker_sdk
-from looker_sdk.sdk.api40.methods import Looker40SDK
+from typing import Optional
 
+from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.dashboard.lookerConnection import (
-    LookerConnection,
+from metadata.generated.schema.entity.services.connections.messaging.kinesisConnection import (
+    KinesisConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.utils.logger import ingestion_logger
 
+logger = ingestion_logger()
 
-def get_connection(connection: LookerConnection) -> Looker40SDK:
+
+def get_connection(connection: KinesisConnection):
     """
     Create connection
     """
-    if not os.environ.get("LOOKERSDK_CLIENT_ID"):
-        os.environ["LOOKERSDK_CLIENT_ID"] = connection.clientId
-    if not os.environ.get("LOOKERSDK_CLIENT_SECRET"):
-        os.environ[
-            "LOOKERSDK_CLIENT_SECRET"
-        ] = connection.clientSecret.get_secret_value()
-    if not os.environ.get("LOOKERSDK_BASE_URL"):
-        os.environ["LOOKERSDK_BASE_URL"] = connection.hostPort
-
-    return looker_sdk.init40()
+    return AWSClient(connection.awsConfig).get_kinesis_client()
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: Looker40SDK,
-    service_connection: LookerConnection,
+    client,
+    service_connection: KinesisConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"CheckAccess": client.me}
+    test_fn = {"GetTopics": client.list_streams}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/metadata.py`

 * *Files 15% similar despite different names*

```diff
@@ -17,24 +17,25 @@
 
 Notes:
 - Filtering is applied on the Dashboard title or ID, if the title is missing
 """
 
 import traceback
 from datetime import datetime
-from typing import Iterable, List, Optional, Sequence, Set, Union, cast
+from typing import Dict, Iterable, List, Optional, Sequence, Set, Type, Union, cast
 
 from looker_sdk.sdk.api40.methods import Looker40SDK
 from looker_sdk.sdk.api40.models import Dashboard as LookerDashboard
 from looker_sdk.sdk.api40.models import (
     DashboardBase,
     DashboardElement,
     LookmlModel,
     LookmlModelExplore,
     LookmlModelNavExplore,
+    Project,
 )
 from pydantic import ValidationError
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.data.createDashboardDataModel import (
     CreateDashboardDataModelRequest,
@@ -57,32 +58,40 @@
 )
 from metadata.generated.schema.entity.services.dashboardService import (
     DashboardServiceType,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.security.credentials.bitbucketCredentials import (
+    BitBucketCredentials,
+)
 from metadata.generated.schema.security.credentials.githubCredentials import (
     GitHubCredentials,
 )
 from metadata.generated.schema.type.entityLineage import EntitiesEdge
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.generated.schema.type.usageRequest import UsageRequest
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.dashboard.dashboard_service import (
     DashboardServiceSource,
     DashboardUsage,
 )
 from metadata.ingestion.source.dashboard.looker.columns import get_columns_from_model
+from metadata.ingestion.source.dashboard.looker.links import get_path_from_link
 from metadata.ingestion.source.dashboard.looker.models import (
     Includes,
     LookMlView,
     ViewName,
 )
 from metadata.ingestion.source.dashboard.looker.parser import LkmlParser
+from metadata.readers.api_reader import ReadersCredentials
+from metadata.readers.base import Reader
+from metadata.readers.bitbucket import BitBucketReader
+from metadata.readers.credentials import get_credentials_from_url
 from metadata.readers.github import GitHubReader
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart, filter_by_datamodel
 from metadata.utils.helpers import clean_uri, get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
@@ -114,14 +123,15 @@
 def build_datamodel_name(model_name: str, explore_name: str) -> str:
     """
     Build the explore name using the model name
     """
     return clean_dashboard_name(model_name + "_" + explore_name)
 
 
+# pylint: disable=too-many-public-methods
 class LookerSource(DashboardServiceSource):
     """
     Looker Source Class.
 
     Its client uses Looker 40 from the SDK: client = looker_sdk.init40()
     """
 
@@ -133,59 +143,133 @@
         self,
         config: WorkflowSource,
         metadata_config: OpenMetadataConnection,
     ):
         super().__init__(config, metadata_config)
         self.today = datetime.now().strftime("%Y-%m-%d")
 
-        self._parser = None
         self._explores_cache = {}
+        self._repo_credentials: Optional[ReadersCredentials] = None
+        self._reader_class: Optional[Type[Reader]] = None
+        self._project_parsers: Optional[Dict[str, LkmlParser]] = None
 
     @classmethod
     def create(
         cls, config_dict: dict, metadata_config: OpenMetadataConnection
     ) -> "LookerSource":
         config = WorkflowSource.parse_obj(config_dict)
         connection: LookerConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, LookerConnection):
             raise InvalidSourceException(
                 f"Expected LookerConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
     @property
-    def parser(self) -> Optional[LkmlParser]:
-        if not self._parser and self.github_credentials:
-            self._parser = LkmlParser(reader=GitHubReader(self.github_credentials))
+    def parser(self) -> Optional[Dict[str, LkmlParser]]:
+        if self.repository_credentials:
+            return self._project_parsers
+
+        return None
 
-        return self._parser
+    @parser.setter
+    def parser(self, all_lookml_models: Sequence[LookmlModel]) -> None:
+        """
+        Initialize the project parsers.
+
+        Each LookML model is linked to a Looker Project. Each project can be
+        hosted in different GitHub repositories.
+
+        Here we will prepare the Readers for each project and the LookML parser.
+
+        We are assuming that each Git repo is based under the same owner
+        and can be accessed with the same token. If we have
+        any errors obtaining the git project information, we will default
+        to the incoming GitHub Credentials.
+        """
+        if self.repository_credentials:
+            all_projects: Set[str] = {model.project_name for model in all_lookml_models}
+            self._project_parsers: Dict[str, LkmlParser] = {
+                project_name: LkmlParser(
+                    reader=self.reader(
+                        credentials=self.get_lookml_project_credentials(
+                            project_name=project_name
+                        )
+                    )
+                )
+                for project_name in all_projects
+            }
+
+            logger.info(f"We found the following parsers:\n {self._project_parsers}")
+
+    def get_lookml_project_credentials(self, project_name: str) -> GitHubCredentials:
+        """
+        Given a lookml project, get its git URL and build the credentials
+        """
+        try:
+            project: Project = self.client.project(project_id=project_name)
+            return get_credentials_from_url(
+                original=self.repository_credentials, url=project.git_remote_url
+            )
+        except Exception as err:
+            logger.error(
+                f"Error trying to build project credentials - [{err}]. We'll use the default ones."
+            )
+            return self.repository_credentials
 
     @property
-    def github_credentials(self) -> Optional[GitHubCredentials]:
+    def reader(self) -> Optional[Type[Reader]]:
+        """
+        Depending on the type of the credentials we'll need a different reader
+        """
+        if not self._reader_class:
+
+            if self.service_connection.gitCredentials and isinstance(
+                self.service_connection.gitCredentials, GitHubCredentials
+            ):
+                self._reader_class = GitHubReader
+
+            if self.service_connection.gitCredentials and isinstance(
+                self.service_connection.gitCredentials, BitBucketCredentials
+            ):
+                self._reader_class = BitBucketReader
+
+        return self._reader_class
+
+    @property
+    def repository_credentials(self) -> Optional[ReadersCredentials]:
         """
         Check if the credentials are informed and return them.
 
         We either get GitHubCredentials or `NoGitHubCredentials`
         """
-        if self.service_connection.githubCredentials and isinstance(
-            self.service_connection.githubCredentials, GitHubCredentials
-        ):
-            return self.service_connection.githubCredentials
-        return None
+        if not self._repo_credentials:
+
+            if self.service_connection.gitCredentials and isinstance(
+                self.service_connection.gitCredentials, GitHubCredentials
+            ):
+                self._repo_credentials = self.service_connection.gitCredentials
+
+        return self._repo_credentials
 
     def list_datamodels(self) -> Iterable[LookmlModelExplore]:
         """
         Fetch explores with the SDK
         """
         if self.source_config.includeDataModels:
             # First, pick up all the LookML Models
             try:
                 all_lookml_models: Sequence[
                     LookmlModel
                 ] = self.client.all_lookml_models()
+
+                # Then, gather their information and build the parser
+                self.parser = all_lookml_models
+
+                # Finally, iterate through them to ingest Explores and Views
                 yield from self.fetch_lookml_explores(all_lookml_models)
             except Exception as err:
                 logger.debug(traceback.format_exc())
                 logger.error(f"Unexpected error fetching LookML models - {err}")
 
     def fetch_lookml_explores(
         self, all_lookml_models: Sequence[LookmlModel]
@@ -239,27 +323,29 @@
                     displayName=model.name,
                     description=model.description,
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                     dataModelType=DataModelType.LookMlExplore.value,
                     serviceType=DashboardServiceType.Looker.value,
                     columns=get_columns_from_model(model),
                     sql=self._get_explore_sql(model),
+                    # In Looker, you need to create Explores and Views within a Project
+                    project=model.project_name,
                 )
                 yield explore_datamodel
                 self.status.scanned(f"Data Model Scanned: {model.name}")
 
                 # Maybe use the project_name as key too?
                 # Save the explores for when we create the lineage with the dashboards and views
                 self._explores_cache[
                     explore_datamodel.name.__root__
                 ] = self.context.dataModel  # This is the newly created explore
 
                 # We can get VIEWs from the JOINs to know the dependencies
                 # We will only try and fetch if we have the credentials
-                if self.github_credentials:
+                if self.repository_credentials:
                     for view in model.joins:
                         if filter_by_datamodel(
                             self.source_config.dataModelFilterPattern, view.name
                         ):
                             self.status.filter(
                                 view.name, "Data model (View) filtered out."
                             )
@@ -286,19 +372,25 @@
 
     def _get_explore_sql(self, explore: LookmlModelExplore) -> Optional[str]:
         """
         If github creds are sent, we can pick the explore
         file definition and add it here
         """
         # Only look to parse if creds are in
-        if self.github_credentials:
+        if self.repository_credentials:
             try:
-                # This will only parse if the file has not been parsed yet
-                self.parser.parse_file(Includes(explore.source_file))
-                return self.parser.parsed_files.get(Includes(explore.source_file))
+                project_parser = self.parser.get(explore.project_name)
+                if project_parser:
+                    # This will only parse if the file has not been parsed yet
+                    project_parser.parse_file(
+                        Includes(get_path_from_link(explore.lookml_link))
+                    )
+                    return project_parser.parsed_files.get(
+                        Includes(get_path_from_link(explore.lookml_link))
+                    )
             except Exception as err:
                 logger.warning(f"Exception getting the model sql: {err}")
 
         return None
 
     def _process_view(self, view_name: ViewName, explore: LookmlModelExplore):
         """
@@ -306,32 +398,39 @@
         We first load the explore file from GitHub, then:
         1. Fetch the view from the GitHub files (search in includes)
         2. Yield the view as a dashboard Model
         3. Yield the lineage between the View -> Explore and Source -> View
         Every visited view, will be cached so that we don't need to process
         everything again.
         """
-        view: Optional[LookMlView] = self.parser.find_view(
-            view_name=view_name, path=Includes(explore.source_file)
-        )
 
-        if view:
-            yield CreateDashboardDataModelRequest(
-                name=build_datamodel_name(explore.model_name, view.name),
-                displayName=view.name,
-                description=view.description,
-                service=self.context.dashboard_service.fullyQualifiedName.__root__,
-                dataModelType=DataModelType.LookMlView.value,
-                serviceType=DashboardServiceType.Looker.value,
-                columns=get_columns_from_model(view),
-                sql=self.parser.parsed_files.get(Includes(view.source_file)),
+        project_parser = self.parser.get(explore.project_name)
+        if project_parser:
+
+            view: Optional[LookMlView] = project_parser.find_view(
+                view_name=view_name,
+                path=Includes(get_path_from_link(explore.lookml_link)),
             )
-            self.status.scanned(f"Data Model Scanned: {view.name}")
 
-            yield from self.add_view_lineage(view, explore)
+            if view:
+                yield CreateDashboardDataModelRequest(
+                    name=build_datamodel_name(explore.model_name, view.name),
+                    displayName=view.name,
+                    description=view.description,
+                    service=self.context.dashboard_service.fullyQualifiedName.__root__,
+                    dataModelType=DataModelType.LookMlView.value,
+                    serviceType=DashboardServiceType.Looker.value,
+                    columns=get_columns_from_model(view),
+                    sql=project_parser.parsed_files.get(Includes(view.source_file)),
+                    # In Looker, you need to create Explores and Views within a Project
+                    project=explore.project_name,
+                )
+                self.status.scanned(f"Data Model Scanned: {view.name}")
+
+                yield from self.add_view_lineage(view, explore)
 
     def add_view_lineage(
         self, view: LookMlView, explore: LookmlModelExplore
     ) -> Iterable[AddLineageRequest]:
         """
         Add the lineage source -> view -> explore
         """
@@ -439,21 +538,37 @@
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
                 for chart in self.context.charts
             ],
-            dashboardUrl=f"{clean_uri(self.service_connection.hostPort)}/dashboards/{dashboard_details.id}",
+            # Dashboards are created from the UI directly. They are not linked to a project
+            # like LookML assets, but rather just organised in folders.
+            project=self._get_dashboard_project(dashboard_details),
+            sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/dashboards/{dashboard_details.id}",
             service=self.context.dashboard_service.fullyQualifiedName.__root__,
         )
         yield dashboard_request
         self.register_record(dashboard_request=dashboard_request)
 
     @staticmethod
+    def _get_dashboard_project(dashboard_details: LookerDashboard) -> Optional[str]:
+        """
+        Get dashboard project if the folder is informed
+        """
+        try:
+            return dashboard_details.folder.name
+        except Exception as exc:
+            logger.debug(
+                f"Cannot get folder name from dashboard [{dashboard_details.title}] - [{exc}]"
+            )
+            return None
+
+    @staticmethod
     def _clean_table_name(table_name: str) -> str:
         """
         sql_table_names might be renamed when defining
         an explore. E.g., customers as cust
         :param table_name: explore table name
         :return: clean table name
         """
@@ -605,15 +720,17 @@
                     continue
 
                 yield CreateChartRequest(
                     name=chart.id,
                     displayName=chart.title or chart.id,
                     description=self.build_chart_description(chart) or None,
                     chartType=get_standard_chart_type(chart.type).value,
-                    chartUrl=f"{clean_uri(self.service_connection.hostPort)}/dashboard_elements/{chart.id}",
+                    sourceUrl=chart.query.share_url
+                    if chart.query is not None
+                    else f"{clean_uri(self.service_connection.hostPort)}/merge?mid={chart.merge_result_id}",
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                 )
                 self.status.scanned(chart.id)
 
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Error creating chart [{chart}]: {exc}")
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/looker/parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -130,7 +130,16 @@
                 return cached_view
 
             # Recursively parse inner includes
             self.find_view(view_name, include)
 
         # We might not find the view ever
         return self.get_view_from_cache(view_name)
+
+    def __repr__(self):
+        """
+        Customize string repr for logs
+        """
+        return (
+            f"Parser at [{self.reader.credentials.repositoryOwner.__root__}/"
+            f"{self.reader.credentials.repositoryName.__root__}]"
+        )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/client.py`

 * *Files 14% similar despite different names*

```diff
@@ -19,14 +19,16 @@
 
 from metadata.generated.schema.entity.services.connections.dashboard.metabaseConnection import (
     MetabaseConnection,
 )
 from metadata.ingestion.connections.test_connections import SourceConnectionException
 from metadata.ingestion.ometa.client import REST, ClientConfig
 from metadata.ingestion.source.dashboard.metabase.models import (
+    MetabaseCollection,
+    MetabaseCollectionList,
     MetabaseDashboard,
     MetabaseDashboardDetails,
     MetabaseDashboardList,
     MetabaseDatabase,
     MetabaseTable,
 )
 from metadata.utils.constants import AUTHORIZATION_HEADER, NO_ACCESS_TOKEN
@@ -93,14 +95,28 @@
                 dashboard_list = MetabaseDashboardList(dashboards=resp_dashboards)
                 return dashboard_list.dashboards
         except Exception:
             logger.debug(traceback.format_exc())
             logger.warning("Failed to fetch the dashboard list")
         return []
 
+    def get_collections_list(self) -> List[MetabaseCollection]:
+        """
+        Get List of all collections
+        """
+        try:
+            resp_collections = self.client.get("/collection")
+            if resp_collections:
+                collection_list = MetabaseCollectionList(collections=resp_collections)
+                return collection_list.collections
+        except Exception:
+            logger.debug(traceback.format_exc())
+            logger.warning("Failed to fetch the collections list")
+        return []
+
     def get_dashboard_details(
         self, dashboard_id: str
     ) -> Optional[MetabaseDashboardDetails]:
         """
         Get Dashboard Details
         """
         if not dashboard_id:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/metadata.py`

 * *Files 10% similar despite different names*

```diff
@@ -31,14 +31,15 @@
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.lineage.parser import LineageParser
 from metadata.ingestion.lineage.sql_lineage import search_table_entities
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
 from metadata.ingestion.source.dashboard.metabase.models import (
     MetabaseChart,
+    MetabaseCollection,
     MetabaseDashboard,
     MetabaseDashboardDetails,
 )
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart
 from metadata.utils.helpers import (
     clean_uri,
@@ -64,14 +65,26 @@
         connection: MetabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MetabaseConnection):
             raise InvalidSourceException(
                 f"Expected MetabaseConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
+    def __init__(
+        self,
+        config: WorkflowSource,
+        metadata_config: OpenMetadataConnection,
+    ):
+        super().__init__(config, metadata_config)
+        self.collections: List[MetabaseCollection] = []
+
+    def prepare(self):
+        self.collections = self.client.get_collections_list()
+        return super().prepare()
+
     def get_dashboards_list(self) -> Optional[List[MetabaseDashboard]]:
         """
         Get List of all dashboards
         """
         return self.client.get_dashboards_list()
 
     def get_dashboard_name(self, dashboard: MetabaseDashboard) -> str:
@@ -82,30 +95,55 @@
 
     def get_dashboard_details(self, dashboard: MetabaseDashboard) -> dict:
         """
         Get Dashboard Details
         """
         return self.client.get_dashboard_details(dashboard.id)
 
+    def _get_collection_name(self, collection_id: Optional[str]) -> Optional[str]:
+        """
+        Method to search the dataset using id in the workspace dict
+        """
+        try:
+            if collection_id:
+                collection_name = next(
+                    (
+                        collection.name
+                        for collection in self.collections
+                        if collection.id == collection_id
+                    ),
+                    None,
+                )
+                return collection_name
+        except Exception as exc:  # pylint: disable=broad-except
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Error fetching the collection details for [{collection_id}]: {exc}"
+            )
+        return None
+
     def yield_dashboard(
         self, dashboard_details: MetabaseDashboardDetails
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity
         """
         try:
             dashboard_url = (
                 f"{clean_uri(self.service_connection.hostPort)}/dashboard/{dashboard_details.id}-"
                 f"{replace_special_with(raw=dashboard_details.name.lower(), replacement='-')}"
             )
             dashboard_request = CreateDashboardRequest(
                 name=dashboard_details.id,
-                dashboardUrl=dashboard_url,
+                sourceUrl=dashboard_url,
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
+                project=self._get_collection_name(
+                    collection_id=dashboard_details.collection_id
+                ),
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
                         service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                         chart_name=chart.name.__root__,
                     )
@@ -147,15 +185,15 @@
                     self.status.filter(chart_details.name, "Chart Pattern not allowed")
                     continue
                 yield CreateChartRequest(
                     name=chart_details.id,
                     displayName=chart_details.name,
                     description=chart_details.description,
                     chartType=get_standard_chart_type(chart_details.display).value,
-                    chartUrl=chart_url,
+                    sourceUrl=chart_url,
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                 )
                 self.status.scanned(chart_details.name)
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Error creating chart [{chart}]: {exc}")
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/metabase/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/models.py`

 * *Files 18% similar despite different names*

```diff
@@ -20,20 +20,34 @@
     """
     Metabase dashboard model
     """
 
     description: Optional[str]
     name: str
     id: int
+    collection_id: Optional[str]
+
+
+class MetabaseCollection(BaseModel):
+    """
+    Metabase dashboard model
+    """
+
+    name: str
+    id: str
 
 
 class MetabaseDashboardList(BaseModel):
     dashboards: Optional[List[MetabaseDashboard]]
 
 
+class MetabaseCollectionList(BaseModel):
+    collections: Optional[List[MetabaseCollection]]
+
+
 class Native(BaseModel):
     query: Optional[str]
 
 
 class DatasetQuery(BaseModel):
     type: Optional[str]
     native: Optional[Native]
@@ -62,14 +76,15 @@
     Metabase dashboard details model
     """
 
     description: Optional[str]
     ordered_cards: List[OrderedCard]
     name: Optional[str]
     id: int
+    collection_id: Optional[str]
 
 
 class MetabaseDatabaseDetails(BaseModel):
     db: Optional[str]
 
 
 class MetabaseDatabase(BaseModel):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/mode/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,14 +32,15 @@
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.lineage.parser import LineageParser
 from metadata.ingestion.lineage.sql_lineage import search_table_entities
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
 from metadata.ingestion.source.dashboard.mode import client
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart
+from metadata.utils.helpers import clean_uri
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class ModeSource(DashboardServiceSource):
     """
@@ -85,21 +86,21 @@
 
     def yield_dashboard(
         self, dashboard_details: dict
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity
         """
+        dashboard_path = dashboard_details[client.LINKS][client.SHARE][client.HREF]
+        dashboard_url = f"{clean_uri(self.service_connection.hostPort)}{dashboard_path}"
         dashboard_request = CreateDashboardRequest(
             name=dashboard_details.get(client.TOKEN),
-            dashboardUrl=dashboard_details[client.LINKS][client.SHARE][client.HREF],
+            sourceUrl=dashboard_url,
             displayName=dashboard_details.get(client.NAME),
-            description=dashboard_details.get(client.DESCRIPTION)
-            if dashboard_details.get(client.DESCRIPTION)
-            else "",
+            description=dashboard_details.get(client.DESCRIPTION),
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
@@ -192,20 +193,23 @@
                         chart_name,
                     ):
                         self.status.filter(
                             chart_name,
                             "Chart Pattern not Allowed",
                         )
                         continue
+                    chart_path = chart[client.LINKS]["report_viz_web"][client.HREF]
+                    chart_url = (
+                        f"{clean_uri(self.service_connection.hostPort)}{chart_path}"
+                    )
                     yield CreateChartRequest(
                         name=chart.get(client.TOKEN),
                         displayName=chart_name,
-                        description="",
                         chartType=ChartType.Other,
-                        chartUrl=chart[client.LINKS]["report_viz_web"][client.HREF],
+                        sourceUrl=chart_url,
                         service=self.context.dashboard_service.fullyQualifiedName.__root__,
                     )
                     self.status.scanned(chart_name)
                 except Exception as exc:
                     name = chart_name if chart_name else ""
                     error = f"Error to yield dashboard chart [{chart}]: {exc}"
                     logger.debug(traceback.format_exc())
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/client.py`

 * *Files 6% similar despite different names*

```diff
@@ -24,16 +24,18 @@
 from metadata.ingestion.source.dashboard.powerbi.models import (
     DashboardsResponse,
     Dataset,
     DatasetResponse,
     Group,
     GroupsResponse,
     PowerBIDashboard,
+    PowerBIReport,
     PowerBiTable,
     PowerBiToken,
+    ReportsResponse,
     TablesResponse,
     Tile,
     TilesResponse,
     Workspaces,
     WorkSpaceScanResponse,
 )
 from metadata.utils.logger import utils_logger
@@ -126,14 +128,29 @@
             return response.value
         except Exception as exc:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.warning(f"Error fetching group dashboards: {exc}")
 
         return None
 
+    def fetch_all_org_reports(self, group_id: str) -> Optional[List[PowerBIReport]]:
+        """Method to fetch all powerbi reports within the group
+        Returns:
+            List[PowerBIReport]
+        """
+        try:
+            response_data = self.client.get(f"/myorg/groups/{group_id}/reports")
+            response = ReportsResponse(**response_data)
+            return response.value
+        except Exception as exc:  # pylint: disable=broad-except
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Error fetching group reports: {exc}")
+
+        return None
+
     def fetch_all_org_datasets(self, group_id: str) -> Optional[List[Dataset]]:
         """Method to fetch all powerbi datasets within the group
         Returns:
             List[Dataset]
         """
         try:
             response_data = self.client.get(f"/myorg/groups/{group_id}/datasets")
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,351 +4,292 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""PowerBI source module"""
+"""QuickSight source module"""
 
 import traceback
 from typing import Any, Iterable, List, Optional
 
+from pydantic import ValidationError
+
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.chart import Chart, ChartType
 from metadata.generated.schema.entity.data.dashboard import Dashboard
 from metadata.generated.schema.entity.data.table import Table
-from metadata.generated.schema.entity.services.connections.dashboard.powerBIConnection import (
-    PowerBIConnection,
+from metadata.generated.schema.entity.services.connections.dashboard.quickSightConnection import (
+    QuickSightConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
-from metadata.ingestion.source.dashboard.powerbi.models import Dataset, PowerBIDashboard
+from metadata.ingestion.source.dashboard.quicksight.models import DataSourceResp
 from metadata.utils import fqn
-from metadata.utils.filters import filter_by_chart, filter_by_dashboard
-from metadata.utils.helpers import clean_uri
+from metadata.utils.filters import filter_by_chart
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
+# BoundLimit for MaxResults = MaxResults >= 0 and MaxResults <= 100
+QUICKSIGHT_MAXRESULTS = 100
+
 
-class PowerbiSource(DashboardServiceSource):
+class QuicksightSource(DashboardServiceSource):
     """
-    PowerBi Source Class
+    QuickSight Source Class
     """
 
     config: WorkflowSource
-    metadata_config: OpenMetadataConnection
+    metadata: OpenMetadata
 
-    def __init__(
-        self,
-        config: WorkflowSource,
-        metadata_config: OpenMetadataConnection,
-    ):
+    def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
         super().__init__(config, metadata_config)
-        self.pagination_entity_per_page = min(
-            100, self.service_connection.pagination_entity_per_page
+        self.aws_account_id = self.service_connection.awsAccountId
+        self.dashboard_url = None
+        self.aws_region = (
+            self.config.serviceConnection.__root__.config.awsConfig.awsRegion
         )
-        self.workspace_data = []
-
-    def prepare(self):
-        if self.service_connection.useAdminApis:
-            self.get_admin_workspace_data()
-        else:
-            self.get_org_workspace_data()
-        return super().prepare()
-
-    def get_org_workspace_data(self):
-        """
-        fetch all the group workspace ids
-        """
-        groups = self.client.fetch_all_workspaces()
-        for group in groups:
-            # add the dashboards to the groups
-            group.dashboards.extend(
-                self.client.fetch_all_org_dashboards(group_id=group.id) or []
-            )
-            for dashboard in group.dashboards:
-                # add the tiles to the dashboards
-                dashboard.tiles.extend(
-                    self.client.fetch_all_org_tiles(
-                        group_id=group.id, dashboard_id=dashboard.id
-                    )
-                    or []
-                )
-
-            # add the datasets to the groups
-            group.datasets.extend(
-                self.client.fetch_all_org_datasets(group_id=group.id) or []
-            )
-            for dataset in group.datasets:
-                # add the tables to the datasets
-                dataset.tables.extend(
-                    self.client.fetch_dataset_tables(
-                        group_id=group.id, dataset_id=dataset.id
-                    )
-                    or []
-                )
-        self.workspace_data = groups
-
-    def get_admin_workspace_data(self):
-        """
-        fetch all the workspace ids
-        """
-        workspaces = self.client.fetch_all_workspaces()
-        if workspaces:
-            workspace_id_list = [workspace.id for workspace in workspaces]
-
-            # Start the scan of the available workspaces for dashboard metadata
-            workspace_paginated_list = [
-                workspace_id_list[i : i + self.pagination_entity_per_page]
-                for i in range(
-                    0, len(workspace_id_list), self.pagination_entity_per_page
-                )
-            ]
-            count = 1
-            for workspace_ids_chunk in workspace_paginated_list:
-                logger.info(
-                    f"Scanning {count}/{len(workspace_paginated_list)} set of workspaces"
-                )
-                workspace_scan = self.client.initiate_workspace_scan(
-                    workspace_ids_chunk
-                )
-
-                # Keep polling the scan status endpoint to check if scan is succeeded
-                workspace_scan_status = self.client.wait_for_scan_complete(
-                    scan_id=workspace_scan.id
-                )
-                if workspace_scan_status:
-                    response = self.client.fetch_workspace_scan_result(
-                        scan_id=workspace_scan.id
-                    )
-                    self.workspace_data.extend(
-                        [
-                            active_workspace
-                            for active_workspace in response.workspaces
-                            if active_workspace.state == "Active"
-                        ]
-                    )
-                else:
-                    logger.error("Error in fetching dashboards and charts")
-                count += 1
-        else:
-            logger.error("Unable to fetch any Powerbi workspaces")
+        self.default_args = {
+            "AwsAccountId": self.aws_account_id,
+            "MaxResults": QUICKSIGHT_MAXRESULTS,
+        }
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config = WorkflowSource.parse_obj(config_dict)
-        connection: PowerBIConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, PowerBIConnection):
+        connection: QuickSightConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, QuickSightConnection):
             raise InvalidSourceException(
-                f"Expected PowerBIConnection, but got {connection}"
+                f"Expected QuickSightConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def get_dashboard(self) -> Any:
-        """
-        Method to iterate through dashboard lists filter dashbaords & yield dashboard details
-        """
-        for workspace in self.workspace_data:
-            self.context.workspace = workspace
-            for dashboard in self.get_dashboards_list():
-                try:
-                    dashboard_details = self.get_dashboard_details(dashboard)
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(
-                        f"Cannot extract dashboard details from {dashboard}: {exc}"
-                    )
-                    continue
-                dashboard_name = self.get_dashboard_name(dashboard_details)
-
-                if filter_by_dashboard(
-                    self.source_config.dashboardFilterPattern,
-                    dashboard_name,
-                ):
-                    self.status.filter(
-                        dashboard_name,
-                        "Dashboard Fltered Out",
-                    )
-                    continue
-                yield dashboard_details
+    def _check_pagination(self, listing_method, entity_key) -> Optional[List]:
+        entity_summary_list = []
+        entity_response = listing_method(self.default_args)
+        entity_summary_list.extend(entity_response[entity_key])
+        while entity_response.get("NextToken"):
+            try:
+                copied_def_args = self.default_args.copy()
+                copied_def_args.update({"NextToken": entity_response.get("NextToken")})
+                entity_response = listing_method(copied_def_args)
+                entity_summary_list.extend(entity_response[entity_key])
+            except Exception as err:
+                logger.error(f"Pagination Failed with error: {err}")
+                logger.debug(traceback.format_exc())
+                break
+        return entity_summary_list
 
-    def get_dashboards_list(self) -> Optional[List[PowerBIDashboard]]:
+    def get_dashboards_list(self) -> Optional[List[dict]]:
         """
         Get List of all dashboards
         """
-        return self.context.workspace.dashboards
+        list_dashboards_func = lambda kwargs: self.client.list_dashboards(  # pylint: disable=unnecessary-lambda-assignment
+            **kwargs
+        )
 
-    def get_dashboard_name(self, dashboard: PowerBIDashboard) -> str:
+        dashboard_summary_list = self._check_pagination(
+            listing_method=list_dashboards_func,
+            entity_key="DashboardSummaryList",
+        )
+        dashboard_set = {
+            dashboard["DashboardId"] for dashboard in dashboard_summary_list
+        }
+        dashboards = [
+            self.client.describe_dashboard(
+                AwsAccountId=self.aws_account_id, DashboardId=dashboard_id
+            )["Dashboard"]
+            for dashboard_id in dashboard_set
+        ]
+        return dashboards
+
+    def get_dashboard_name(self, dashboard: dict) -> str:
         """
         Get Dashboard Name
         """
-        return dashboard.displayName
+        return dashboard["Name"]
 
-    def get_dashboard_details(self, dashboard: PowerBIDashboard) -> PowerBIDashboard:
+    def get_dashboard_details(self, dashboard: dict) -> dict:
         """
         Get Dashboard Details
         """
         return dashboard
 
-    def get_dashboard_url(self, workspace_id: str, dashboard_id: str) -> str:
-        """
-        Method to build the dashboard url
-        """
-        return (
-            f"{clean_uri(self.service_connection.hostPort)}/groups/"
-            f"{workspace_id}/dashboards/{dashboard_id}"
-        )
-
-    def get_chart_url(
-        self, report_id: Optional[str], workspace_id: str, dashboard_id: str
-    ) -> str:
-        """
-        Method to build the chart url
-        """
-        chart_url_postfix = (
-            f"reports/{report_id}" if report_id else f"dashboards/{dashboard_id}"
-        )
-        return (
-            f"{clean_uri(self.service_connection.hostPort)}/groups/"
-            f"{workspace_id}/{chart_url_postfix}"
-        )
-
     def yield_dashboard(
-        self, dashboard_details: PowerBIDashboard
+        self, dashboard_details: dict
     ) -> Iterable[CreateDashboardRequest]:
         """
-        Method to Get Dashboard Entity, Dashboard Charts & Lineage
+        Method to Get Dashboard Entity
         """
         dashboard_request = CreateDashboardRequest(
-            name=dashboard_details.id,
-            dashboardUrl=self.get_dashboard_url(
-                workspace_id=self.context.workspace.id,
-                dashboard_id=dashboard_details.id,
-            ),
-            displayName=dashboard_details.displayName,
-            description="",
+            name=dashboard_details["DashboardId"],
+            sourceUrl=self.dashboard_url,
+            displayName=dashboard_details["Name"],
+            description=dashboard_details["Version"].get("Description"),
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
                 for chart in self.context.charts
             ],
             service=self.context.dashboard_service.fullyQualifiedName.__root__,
         )
         yield dashboard_request
         self.register_record(dashboard_request=dashboard_request)
 
-    def yield_dashboard_lineage_details(
-        self, dashboard_details: PowerBIDashboard, db_service_name: str
-    ) -> Optional[Iterable[AddLineageRequest]]:
-        """
-        Get lineage between dashboard and data sources
-        """
-        try:
-            charts = dashboard_details.tiles
-            for chart in charts or []:
-                dataset = self.fetch_dataset_from_workspace(chart.datasetId)
-                if dataset:
-                    for table in dataset.tables:
-                        table_name = table.name
-
-                        from_fqn = fqn.build(
-                            self.metadata,
-                            entity_type=Table,
-                            service_name=db_service_name,
-                            database_name=None,
-                            schema_name=None,
-                            table_name=table_name,
-                        )
-                        from_entity = self.metadata.get_by_name(
-                            entity=Table,
-                            fqn=from_fqn,
-                        )
-                        to_fqn = fqn.build(
-                            self.metadata,
-                            entity_type=Dashboard,
-                            service_name=self.config.serviceName,
-                            dashboard_name=dashboard_details.id,
-                        )
-                        to_entity = self.metadata.get_by_name(
-                            entity=Dashboard,
-                            fqn=to_fqn,
-                        )
-                        if from_entity and to_entity:
-                            yield self._get_add_lineage_request(
-                                to_entity=to_entity, from_entity=from_entity
-                            )
-        except Exception as exc:  # pylint: disable=broad-except
-            logger.debug(traceback.format_exc())
-            logger.error(
-                f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
-            )
-
     def yield_dashboard_chart(
-        self, dashboard_details: PowerBIDashboard
+        self, dashboard_details: Any
     ) -> Optional[Iterable[CreateChartRequest]]:
         """Get chart method
+
         Args:
             dashboard_details:
         Returns:
-            Iterable[Chart]
+            Iterable[CreateChartRequest]
         """
-        charts = dashboard_details.tiles
-        for chart in charts or []:
+        # Each dashboard is guaranteed to have at least one sheet, which represents
+        # a chart in the context of QuickSight
+        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/quicksight.html#QuickSight.Client.describe_dashboard
+        charts = dashboard_details["Version"]["Sheets"]
+        for chart in charts:
             try:
-                chart_title = chart.title
-                chart_display_name = chart_title if chart_title else chart.id
                 if filter_by_chart(
-                    self.source_config.chartFilterPattern, chart_display_name
+                    self.source_config.chartFilterPattern, chart["Name"]
                 ):
-                    self.status.filter(chart_display_name, "Chart Pattern not Allowed")
+                    self.status.filter(chart["Name"], "Chart Pattern not allowed")
                     continue
+
+                self.dashboard_url = (
+                    f"https://{self.aws_region}.quicksight.aws.amazon.com/sn/dashboards"
+                    f'/{dashboard_details.get("DashboardId")}'
+                )
                 yield CreateChartRequest(
-                    name=chart.id,
-                    displayName=chart_display_name,
-                    description="",
+                    name=chart["SheetId"],
+                    displayName=chart["Name"],
                     chartType=ChartType.Other.value,
-                    chartUrl=self.get_chart_url(
-                        report_id=chart.reportId,
-                        workspace_id=self.context.workspace.id,
-                        dashboard_id=dashboard_details.id,
-                    ),
+                    sourceUrl=self.dashboard_url,
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                 )
-                self.status.scanned(chart_display_name)
+                self.status.scanned(chart["Name"])
             except Exception as exc:
-                name = chart.title
-                error = f"Error creating chart [{name}]: {exc}"
                 logger.debug(traceback.format_exc())
-                logger.warning(error)
-                self.status.failed(name, error, traceback.format_exc())
+                logger.warning(f"Error creating chart [{chart}]: {exc}")
+                continue
+
+    def yield_dashboard_lineage_details(  # pylint: disable=too-many-locals
+        self, dashboard_details: dict, db_service_name: str
+    ) -> Optional[Iterable[AddLineageRequest]]:
+        """
+        Get lineage between dashboard and data sources
+        """
+        try:
+            list_data_set_func = lambda kwargs: self.client.list_data_sets(  # pylint: disable=unnecessary-lambda-assignment
+                **kwargs
+            )
+            data_set_summary_list = self._check_pagination(
+                listing_method=list_data_set_func,
+                entity_key="DataSetSummaries",
+            )
+            dataset_ids = {
+                dataset["DataSetId"]
+                for dataset in data_set_summary_list
+                if dataset.get("Arn") in dashboard_details["Version"]["DataSetArns"]
+            }
+
+            for dataset_id in dataset_ids:
+                for data_source in list(
+                    self.client.describe_data_set(
+                        AwsAccountId=self.aws_account_id, DataSetId=dataset_id
+                    )["DataSet"]["PhysicalTableMap"].values()
+                ):
+                    try:
+                        if not data_source.get("RelationalTable"):
+                            raise KeyError(
+                                f"We currently don't support lineage to {list(data_source.keys())}"
+                            )
+                        data_source_relational_table = data_source["RelationalTable"]
+                        data_source_resp = DataSourceResp(
+                            datasource_arn=data_source_relational_table[
+                                "DataSourceArn"
+                            ],
+                            schema_name=data_source_relational_table["Schema"],
+                            table_name=data_source_relational_table["Name"],
+                        )
+                    except KeyError as err:
+                        logger.error(err)
+                        continue
+                    except ValidationError as err:
+                        logger.error(
+                            f"{err} - error while trying to fetch lineage data source"
+                        )
+                        logger.debug(traceback.format_exc())
+                        continue
+
+                    schema_name = data_source_resp.schema_name
+                    table_name = data_source_resp.table_name
+
+                    list_data_source_func = lambda kwargs: self.client.list_data_sources(  # pylint: disable=unnecessary-lambda-assignment
+                        **kwargs
+                    )
+
+                    data_source_summary_list = self._check_pagination(
+                        listing_method=list_data_source_func,
+                        entity_key="DataSources",
+                    )
 
-    def fetch_dataset_from_workspace(
-        self, dataset_id: Optional[str]
-    ) -> Optional[Dataset]:
-        """
-        Method to search the dataset using id in the workspace dict
-        """
-        if dataset_id:
-            dataset_data = next(
-                (
-                    dataset
-                    for dataset in self.context.workspace.datasets or []
-                    if dataset.id == dataset_id
-                ),
-                None,
+                    data_source_ids = [
+                        data_source_arn["DataSourceId"]
+                        for data_source_arn in data_source_summary_list
+                        if data_source_arn["Arn"] in data_source_resp.datasource_arn
+                    ]
+
+                    for data_source_id in data_source_ids:
+                        data_source_dict = self.client.describe_data_source(
+                            AwsAccountId=self.aws_account_id,
+                            DataSourceId=data_source_id,
+                        )["DataSource"]["DataSourceParameters"]
+                        for db in data_source_dict.keys():
+                            from_fqn = fqn.build(
+                                self.metadata,
+                                entity_type=Table,
+                                service_name=db_service_name,
+                                database_name=data_source_dict[db]["Database"],
+                                schema_name=schema_name,
+                                table_name=table_name,
+                                skip_es_search=True,
+                            )
+                            from_entity = self.metadata.get_by_name(
+                                entity=Table,
+                                fqn=from_fqn,
+                            )
+                            to_fqn = fqn.build(
+                                self.metadata,
+                                entity_type=Dashboard,
+                                service_name=self.config.serviceName,
+                                dashboard_name=dashboard_details["DashboardId"],
+                            )
+                            to_entity = self.metadata.get_by_name(
+                                entity=Dashboard,
+                                fqn=to_fqn,
+                            )
+                            yield self._get_add_lineage_request(
+                                to_entity=to_entity, from_entity=from_entity
+                            )
+        except Exception as exc:  # pylint: disable=broad-except
+            logger.debug(traceback.format_exc())
+            logger.error(
+                f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
             )
-            return dataset_data
-        return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/powerbi/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/models.py`

 * *Files 13% similar despite different names*

```diff
@@ -40,41 +40,75 @@
     id: str
     displayName: str
     webUrl: Optional[str]
     embedUrl: Optional[str]
     tiles: Optional[List[Tile]] = []
 
 
+class PowerBIReport(BaseModel):
+    """
+    PowerBI PowerBIReport Model
+    Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/reports/get-report#report
+    """
+
+    id: str
+    name: str
+    datasetId: Optional[str]
+
+
 class DashboardsResponse(BaseModel):
     """
     PowerBI DashboardsResponse Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/dashboards/get-dashboards-in-group
     """
 
     odata_context: str = Field(alias="@odata.context")
     value: List[PowerBIDashboard]
 
 
+class ReportsResponse(BaseModel):
+    """
+    PowerBI ReportsResponse Model
+    Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/reports/get-reports-in-group
+    """
+
+    odata_context: str = Field(alias="@odata.context")
+    value: List[PowerBIReport]
+
+
 class TilesResponse(BaseModel):
     """
     PowerBI TilesResponse Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/dashboards/get-tiles-in-group
     """
 
     odata_context: str = Field(alias="@odata.context")
     value: List[Tile]
 
 
+class PowerBiColumns(BaseModel):
+    """
+    PowerBI Column Model
+    Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/push-datasets/datasets-get-tables-in-group#column
+    """
+
+    name: str
+    dataType: Optional[str]
+    columnType: Optional[str]
+
+
 class PowerBiTable(BaseModel):
     """
     PowerBI Table Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/push-datasets/datasets-get-tables-in-group#table
     """
 
     name: str
+    columns: Optional[List[PowerBiColumns]]
+    description: Optional[str]
 
 
 class TablesResponse(BaseModel):
     """
     PowerBI TablesResponse Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/push-datasets/datasets-get-tables-in-group
     """
@@ -88,14 +122,15 @@
     PowerBI Dataset Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasets-in-group#dataset
     """
 
     id: str
     name: str
     tables: Optional[List[PowerBiTable]] = []
+    description: Optional[str]
 
 
 class DatasetResponse(BaseModel):
     """
     PowerBI DatasetResponse Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-datasets-in-group
     """
@@ -111,14 +146,15 @@
     """
 
     id: str
     name: Optional[str]
     type: Optional[str]
     state: Optional[str]
     dashboards: Optional[List[PowerBIDashboard]] = []
+    reports: Optional[List[PowerBIReport]] = []
     datasets: Optional[List[Dataset]] = []
 
 
 class GroupsResponse(BaseModel):
     """
     PowerBI GroupsResponse Model
     Definition: https://learn.microsoft.com/en-us/rest/api/power-bi/groups/get-groups
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/metadata.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,293 +4,278 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""QuickSight source module"""
-
+"""
+Redash source module
+"""
 import traceback
-from typing import Any, Iterable, List, Optional
+from typing import Iterable, List, Optional
 
-from pydantic import ValidationError
+from packaging import version
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.entity.data.chart import Chart, ChartType
-from metadata.generated.schema.entity.data.dashboard import Dashboard
+from metadata.generated.schema.entity.data.chart import Chart
+from metadata.generated.schema.entity.data.dashboard import (
+    Dashboard as LineageDashboard,
+)
 from metadata.generated.schema.entity.data.table import Table
-from metadata.generated.schema.entity.services.connections.dashboard.quickSightConnection import (
-    QuickSightConnection,
+from metadata.generated.schema.entity.services.connections.dashboard.redashConnection import (
+    RedashConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.lineage.parser import LineageParser
+from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
-from metadata.ingestion.source.dashboard.quicksight.models import DataSourceResp
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart
+from metadata.utils.helpers import clean_uri, get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 
 logger = ingestion_logger()
 
-# BoundLimit for MaxResults = MaxResults >= 0 and MaxResults <= 100
-QUICKSIGHT_MAXRESULTS = 100
+REDASH_TAG_CATEGORY = "RedashTags"
+
+INCOMPATIBLE_REDASH_VERSION = "8.0.0"
 
 
-class QuicksightSource(DashboardServiceSource):
+class RedashSource(DashboardServiceSource):
     """
-    QuickSight Source Class
+    Redash Source Class
     """
 
-    config: WorkflowSource
-    metadata: OpenMetadata
-
-    def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
+    def __init__(
+        self,
+        config: WorkflowSource,
+        metadata_config: OpenMetadataConnection,
+    ):
         super().__init__(config, metadata_config)
-        self.aws_account_id = self.service_connection.awsAccountId
-        self.dashboard_url = None
-        self.aws_region = (
-            self.config.serviceConnection.__root__.config.awsConfig.awsRegion
-        )
-        self.default_args = {
-            "AwsAccountId": self.aws_account_id,
-            "MaxResults": QUICKSIGHT_MAXRESULTS,
-        }
+        self.dashboard_list = []  # We will populate this in `prepare`
+        self.tags = []  # To create the tags before yielding final entities
 
     @classmethod
-    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        config = WorkflowSource.parse_obj(config_dict)
-        connection: QuickSightConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, QuickSightConnection):
+    def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: RedashConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, RedashConnection):
             raise InvalidSourceException(
-                f"Expected QuickSightConnection, but got {connection}"
+                f"Expected RedashConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def _check_pagination(self, listing_method, entity_key) -> Optional[List]:
-        entity_summary_list = []
-        entity_response = listing_method(self.default_args)
-        entity_summary_list.extend(entity_response[entity_key])
-        while entity_response.get("NextToken"):
-            try:
-                copied_def_args = self.default_args.copy()
-                copied_def_args.update({"NextToken": entity_response.get("NextToken")})
-                entity_response = listing_method(copied_def_args)
-                entity_summary_list.extend(entity_response[entity_key])
-            except Exception as err:
-                logger.error(f"Pagination Failed with error: {err}")
-                logger.debug(traceback.format_exc())
-                break
-        return entity_summary_list
+    def prepare(self):
+        """
+        Fetch the paginated list of dashboards and tags
+        """
+
+        self.dashboard_list = self.client.paginate(self.client.dashboards)
+
+        # Collecting all the tags
+        if self.source_config.includeTags:
+            for dashboard in self.dashboard_list:
+                self.tags.extend(dashboard.get("tags") or [])
+
+    def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
+        """
+        Fetch Dashboard Tags
+        """
+        yield from get_ometa_tag_and_classification(
+            tags=self.tags,
+            classification_name=REDASH_TAG_CATEGORY,
+            tag_description="Redash Tag",
+            classification_desciption="Tags associated with redash entities",
+            include_tags=self.source_config.includeTags,
+        )
 
     def get_dashboards_list(self) -> Optional[List[dict]]:
         """
         Get List of all dashboards
         """
-        list_dashboards_func = lambda kwargs: self.client.list_dashboards(  # pylint: disable=unnecessary-lambda-assignment
-            **kwargs
-        )
 
-        dashboard_summary_list = self._check_pagination(
-            listing_method=list_dashboards_func,
-            entity_key="DashboardSummaryList",
-        )
-        dashboard_set = {
-            dashboard["DashboardId"] for dashboard in dashboard_summary_list
-        }
-        dashboards = [
-            self.client.describe_dashboard(
-                AwsAccountId=self.aws_account_id, DashboardId=dashboard_id
-            )["Dashboard"]
-            for dashboard_id in dashboard_set
-        ]
-        return dashboards
+        return self.dashboard_list
 
     def get_dashboard_name(self, dashboard: dict) -> str:
         """
         Get Dashboard Name
         """
-        return dashboard["Name"]
+        return dashboard["name"]
 
     def get_dashboard_details(self, dashboard: dict) -> dict:
         """
         Get Dashboard Details
         """
-        return dashboard
+        return self.client.get_dashboard(dashboard["slug"])
+
+    def get_owner_details(self, dashboard_details) -> Optional[EntityReference]:
+        """Get dashboard owner
+
+        Args:
+            dashboard_details:
+        Returns:
+            Optional[EntityReference]
+        """
+        if dashboard_details.get("user") and dashboard_details["user"].get("email"):
+            user = self.metadata.get_user_by_email(
+                dashboard_details["user"].get("email")
+            )
+            if user:
+                return EntityReference(id=user.id.__root__, type="user")
+        return None
+
+    def get_dashboard_url(self, dashboard_details: dict) -> str:
+        if version.parse(self.service_connection.redashVersion) > version.parse(
+            INCOMPATIBLE_REDASH_VERSION
+        ):
+            dashboard_url = (
+                f"{clean_uri(self.service_connection.hostPort)}/dashboards"
+                f"/{dashboard_details.get('id', '')}"
+            )
+        else:
+            dashboard_url = (
+                f"{clean_uri(self.service_connection.hostPort)}/dashboards"
+                f"/{dashboard_details.get('slug', '')}"
+            )
+        return dashboard_url
 
     def yield_dashboard(
         self, dashboard_details: dict
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity
         """
-        dashboard_request = CreateDashboardRequest(
-            name=dashboard_details["DashboardId"],
-            dashboardUrl=self.dashboard_url,
-            displayName=dashboard_details["Name"],
-            description=dashboard_details["Version"].get("Description", ""),
-            charts=[
-                fqn.build(
-                    self.metadata,
-                    entity_type=Chart,
-                    service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
-                    chart_name=chart.name.__root__,
-                )
-                for chart in self.context.charts
-            ],
-            service=self.context.dashboard_service.fullyQualifiedName.__root__,
+        try:
+            dashboard_description = ""
+            for widgets in dashboard_details.get("widgets") or []:
+                dashboard_description = widgets.get("text")
+
+            dashboard_request = CreateDashboardRequest(
+                name=dashboard_details["id"],
+                displayName=dashboard_details.get("name"),
+                description=dashboard_description,
+                charts=[
+                    fqn.build(
+                        self.metadata,
+                        entity_type=Chart,
+                        service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
+                        chart_name=chart.name.__root__,
+                    )
+                    for chart in self.context.charts
+                ],
+                service=self.context.dashboard_service.fullyQualifiedName.__root__,
+                sourceUrl=self.get_dashboard_url(dashboard_details),
+                tags=get_tag_labels(
+                    metadata=self.metadata,
+                    tags=dashboard_details.get("tags"),
+                    classification_name=REDASH_TAG_CATEGORY,
+                    include_tags=self.source_config.includeTags,
+                ),
+            )
+            yield dashboard_request
+            self.register_record(dashboard_request=dashboard_request)
+
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Error to yield dashboard for {dashboard_details}: {exc}")
+
+    def yield_dashboard_lineage_details(  # pylint: disable=too-many-locals
+        self, dashboard_details: dict, db_service_name: str
+    ) -> Optional[Iterable[AddLineageRequest]]:
+        """
+        Get lineage between dashboard and data sources
+        In redash we do not get table, database_schema or database name but we do get query
+        the lineage is being generated based on the query
+        """
+
+        to_fqn = fqn.build(
+            self.metadata,
+            entity_type=LineageDashboard,
+            service_name=self.config.serviceName,
+            dashboard_name=str(dashboard_details.get("id")),
+        )
+        to_entity = self.metadata.get_by_name(
+            entity=LineageDashboard,
+            fqn=to_fqn,
         )
-        yield dashboard_request
-        self.register_record(dashboard_request=dashboard_request)
+        for widgets in dashboard_details.get("widgets") or []:
+            try:
+                visualization = widgets.get("visualization")
+                if not visualization:
+                    continue
+                if visualization.get("query", {}).get("query"):
+                    lineage_parser = LineageParser(visualization["query"]["query"])
+                    for table in lineage_parser.source_tables:
+                        table_name = str(table)
+                        database_schema_table = fqn.split_table_name(table_name)
+                        database_schema = database_schema_table.get("database_schema")
+                        database_schema_name = self.check_database_schema_name(
+                            database_schema
+                        )
+                        from_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Table,
+                            service_name=db_service_name,
+                            schema_name=database_schema_name,
+                            table_name=database_schema_table.get("table"),
+                            database_name=database_schema_table.get("database"),
+                        )
+                        from_entity = self.metadata.get_by_name(
+                            entity=Table,
+                            fqn=from_fqn,
+                        )
+                        if from_entity and to_entity:
+                            yield self._get_add_lineage_request(
+                                to_entity=to_entity, from_entity=from_entity
+                            )
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
+                )
 
     def yield_dashboard_chart(
-        self, dashboard_details: Any
+        self, dashboard_details: dict
     ) -> Optional[Iterable[CreateChartRequest]]:
-        """Get chart method
-
-        Args:
-            dashboard_details:
-        Returns:
-            Iterable[CreateChartRequest]
         """
-        # Each dashboard is guaranteed to have at least one sheet, which represents
-        # a chart in the context of QuickSight
-        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/quicksight.html#QuickSight.Client.describe_dashboard
-        charts = dashboard_details["Version"]["Sheets"]
-        for chart in charts:
+        Metod to fetch charts linked to dashboard
+        """
+        for widgets in dashboard_details.get("widgets") or []:
             try:
+                visualization = widgets.get("visualization")
+                chart_display_name = str(
+                    visualization["query"]["name"] if visualization else widgets["id"]
+                )
                 if filter_by_chart(
-                    self.source_config.chartFilterPattern, chart["Name"]
+                    self.source_config.chartFilterPattern, chart_display_name
                 ):
-                    self.status.filter(chart["Name"], "Chart Pattern not allowed")
+                    self.status.filter(chart_display_name, "Chart Pattern not allowed")
                     continue
-
-                self.dashboard_url = (
-                    f"https://{self.aws_region}.quicksight.aws.amazon.com/sn/dashboards"
-                    f'/{dashboard_details.get("DashboardId")}'
-                )
                 yield CreateChartRequest(
-                    name=chart["SheetId"],
-                    displayName=chart["Name"],
-                    description="",
-                    chartType=ChartType.Other.value,
-                    chartUrl=self.dashboard_url,
+                    name=widgets["id"],
+                    displayName=chart_display_name
+                    if visualization and visualization["query"]
+                    else "",
+                    chartType=get_standard_chart_type(
+                        visualization["type"] if visualization else ""
+                    ),
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
+                    sourceUrl=self.get_dashboard_url(dashboard_details),
+                    description=visualization["description"] if visualization else "",
                 )
-                self.status.scanned(chart["Name"])
+                self.status.scanned(f"Chart: {chart_display_name}")
             except Exception as exc:
                 logger.debug(traceback.format_exc())
-                logger.warning(f"Error creating chart [{chart}]: {exc}")
-                continue
-
-    def yield_dashboard_lineage_details(  # pylint: disable=too-many-locals
-        self, dashboard_details: dict, db_service_name: str
-    ) -> Optional[Iterable[AddLineageRequest]]:
-        """
-        Get lineage between dashboard and data sources
-        """
-        try:
-            list_data_set_func = lambda kwargs: self.client.list_data_sets(  # pylint: disable=unnecessary-lambda-assignment
-                **kwargs
-            )
-            data_set_summary_list = self._check_pagination(
-                listing_method=list_data_set_func,
-                entity_key="DataSetSummaries",
-            )
-            dataset_ids = {
-                dataset["DataSetId"]
-                for dataset in data_set_summary_list
-                if dataset.get("Arn") in dashboard_details["Version"]["DataSetArns"]
-            }
-
-            for dataset_id in dataset_ids:
-                for data_source in list(
-                    self.client.describe_data_set(
-                        AwsAccountId=self.aws_account_id, DataSetId=dataset_id
-                    )["DataSet"]["PhysicalTableMap"].values()
-                ):
-                    try:
-                        if not data_source.get("RelationalTable"):
-                            raise KeyError(
-                                f"We currently don't support lineage to {list(data_source.keys())}"
-                            )
-                        data_source_relational_table = data_source["RelationalTable"]
-                        data_source_resp = DataSourceResp(
-                            datasource_arn=data_source_relational_table[
-                                "DataSourceArn"
-                            ],
-                            schema_name=data_source_relational_table["Schema"],
-                            table_name=data_source_relational_table["Name"],
-                        )
-                    except KeyError as err:
-                        logger.error(err)
-                        continue
-                    except ValidationError as err:
-                        logger.error(
-                            f"{err} - error while trying to fetch lineage data source"
-                        )
-                        logger.debug(traceback.format_exc())
-                        continue
-
-                    schema_name = data_source_resp.schema_name
-                    table_name = data_source_resp.table_name
-
-                    list_data_source_func = lambda kwargs: self.client.list_data_sources(  # pylint: disable=unnecessary-lambda-assignment
-                        **kwargs
-                    )
-
-                    data_source_summary_list = self._check_pagination(
-                        listing_method=list_data_source_func,
-                        entity_key="DataSources",
-                    )
-
-                    data_source_ids = [
-                        data_source_arn["DataSourceId"]
-                        for data_source_arn in data_source_summary_list
-                        if data_source_arn["Arn"] in data_source_resp.datasource_arn
-                    ]
-
-                    for data_source_id in data_source_ids:
-                        data_source_dict = self.client.describe_data_source(
-                            AwsAccountId=self.aws_account_id,
-                            DataSourceId=data_source_id,
-                        )["DataSource"]["DataSourceParameters"]
-                        for db in data_source_dict.keys():
-                            from_fqn = fqn.build(
-                                self.metadata,
-                                entity_type=Table,
-                                service_name=db_service_name,
-                                database_name=data_source_dict[db]["Database"],
-                                schema_name=schema_name,
-                                table_name=table_name,
-                                skip_es_search=True,
-                            )
-                            from_entity = self.metadata.get_by_name(
-                                entity=Table,
-                                fqn=from_fqn,
-                            )
-                            to_fqn = fqn.build(
-                                self.metadata,
-                                entity_type=Dashboard,
-                                service_name=self.config.serviceName,
-                                dashboard_name=dashboard_details["DashboardId"],
-                            )
-                            to_entity = self.metadata.get_by_name(
-                                entity=Dashboard,
-                                fqn=to_fqn,
-                            )
-                            yield self._get_add_lineage_request(
-                                to_entity=to_entity, from_entity=from_entity
-                            )
-        except Exception as exc:  # pylint: disable=broad-except
-            logger.debug(traceback.format_exc())
-            logger.error(
-                f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
-            )
+                logger.warning(
+                    f"Error to yield dashboard chart for widget_id: {widgets['id']} and {dashboard_details}: {exc}"
+                )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/quicksight/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/redash/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/api_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/api_source.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,17 +15,22 @@
 import traceback
 from typing import Iterable, List, Optional
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.entity.data.chart import Chart, ChartType
 from metadata.generated.schema.entity.data.table import Table
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.ingestion.source.dashboard.superset.mixin import SupersetSourceMixin
 from metadata.utils import fqn
-from metadata.utils.helpers import clean_uri, get_standard_chart_type
+from metadata.utils.helpers import (
+    clean_uri,
+    get_database_name_for_lineage,
+    get_standard_chart_type,
+)
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class SupersetAPISource(SupersetSourceMixin):
     """
@@ -65,33 +70,32 @@
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity
         """
         dashboard_request = CreateDashboardRequest(
             name=dashboard_details["id"],
             displayName=dashboard_details["dashboard_title"],
-            description="",
-            dashboardUrl=f"{clean_uri(self.service_connection.hostPort)}{dashboard_details['url']}",
+            sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{dashboard_details['url']}",
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
                 for chart in self.context.charts
             ],
             service=self.context.dashboard_service.fullyQualifiedName.__root__,
         )
         yield dashboard_request
         self.register_record(dashboard_request=dashboard_request)
 
-    def _get_datasource_fqn_for_lineage(self, chart_json, db_service_name):
+    def _get_datasource_fqn_for_lineage(self, chart_json, db_service_entity):
         return (
-            self._get_datasource_fqn(chart_json.get("datasource_id"), db_service_name)
+            self._get_datasource_fqn(chart_json.get("datasource_id"), db_service_entity)
             if chart_json.get("datasource_id")
             else None
         )
 
     def yield_dashboard_chart(
         self, dashboard_details: dict
     ) -> Optional[Iterable[CreateChartRequest]]:
@@ -106,36 +110,48 @@
             chart = CreateChartRequest(
                 name=chart_json["id"],
                 displayName=chart_json.get("slice_name"),
                 description=chart_json.get("description"),
                 chartType=get_standard_chart_type(
                     chart_json.get("viz_type", ChartType.Other.value)
                 ),
-                chartUrl=f"{clean_uri(self.service_connection.hostPort)}{chart_json.get('url')}",
+                sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{chart_json.get('url')}",
                 service=self.context.dashboard_service.fullyQualifiedName.__root__,
             )
             yield chart
 
     def _get_datasource_fqn(
-        self, datasource_id: str, db_service_name: str
+        self, datasource_id: str, db_service_entity: DatabaseService
     ) -> Optional[str]:
-        if db_service_name:
-            try:
-                datasource_json = self.client.fetch_datasource(datasource_id)
+        try:
+            datasource_json = self.client.fetch_datasource(datasource_id)
+            if datasource_json:
                 database_json = self.client.fetch_database(
                     datasource_json["result"]["database"]["id"]
                 )
-                dataset_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Table,
-                    table_name=datasource_json["result"]["table_name"],
-                    schema_name=datasource_json["result"]["schema"],
-                    database_name=database_json["result"]["parameters"]["database"],
-                    service_name=db_service_name,
+                default_database_name = (
+                    database_json["result"]["parameters"].get("database")
+                    if database_json["result"].get("parameters")
+                    else None
                 )
-                return dataset_fqn
-            except KeyError as err:
-                logger.debug(traceback.format_exc())
-                logger.warning(
-                    f"Failed to fetch Datasource with id [{datasource_id}]: {err}"
+
+                database_name = get_database_name_for_lineage(
+                    db_service_entity, default_database_name
                 )
+
+                if database_json:
+                    dataset_fqn = fqn.build(
+                        self.metadata,
+                        entity_type=Table,
+                        table_name=datasource_json["result"]["table_name"],
+                        schema_name=datasource_json["result"].get("schema"),
+                        database_name=database_name,
+                        service_name=db_service_entity.name.__root__,
+                    )
+                return dataset_fqn
+        except KeyError as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Failed to fetch Datasource with id [{datasource_id}]: {err}"
+            )
+
         return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/connection.py`

 * *Files 0% similar despite different names*

```diff
@@ -26,15 +26,15 @@
 from metadata.generated.schema.entity.services.connections.database.mysqlConnection import (
     MysqlConnection,
 )
 from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
     PostgresConnection,
 )
 from metadata.generated.schema.entity.utils.supersetApiConnection import (
-    SupersetAPIConnection,
+    SupersetApiConnection,
 )
 from metadata.ingestion.connections.test_connections import (
     test_connection_engine_step,
     test_connection_steps,
     test_query,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
@@ -51,15 +51,15 @@
 )
 
 
 def get_connection(connection: SupersetConnection) -> SupersetAPIClient:
     """
     Create connection
     """
-    if isinstance(connection.connection, SupersetAPIConnection):
+    if isinstance(connection.connection, SupersetApiConnection):
         return SupersetAPIClient(connection)
     if isinstance(connection.connection, PostgresConnection):
         return pg_get_connection(connection=connection.connection)
     if isinstance(connection.connection, MysqlConnection):
         return mysql_get_connection(connection=connection.connection)
     return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/db_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/db_source.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,32 +12,38 @@
 Superset source module
 """
 
 import traceback
 from typing import Iterable, List, Optional
 
 from sqlalchemy.engine import Engine
+from sqlalchemy.engine.url import make_url
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.entity.data.chart import Chart, ChartType
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.source.dashboard.superset.mixin import SupersetSourceMixin
 from metadata.ingestion.source.dashboard.superset.queries import (
     FETCH_ALL_CHARTS,
     FETCH_DASHBOARDS,
 )
 from metadata.utils import fqn
-from metadata.utils.helpers import clean_uri, get_standard_chart_type
+from metadata.utils.helpers import (
+    clean_uri,
+    get_database_name_for_lineage,
+    get_standard_chart_type,
+)
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class SupersetDBSource(SupersetSourceMixin):
     """
@@ -71,33 +77,32 @@
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity
         """
         dashboard_request = CreateDashboardRequest(
             name=dashboard_details["id"],
             displayName=dashboard_details["dashboard_title"],
-            description="",
-            dashboardUrl=f"{clean_uri(self.service_connection.hostPort)}/superset/dashboard/{dashboard_details['id']}/",
+            sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/superset/dashboard/{dashboard_details['id']}/",
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
                 for chart in self.context.charts
             ],
             service=self.context.dashboard_service.fullyQualifiedName.__root__,
         )
         yield dashboard_request
         self.register_record(dashboard_request=dashboard_request)
 
-    def _get_datasource_fqn_for_lineage(self, chart_json, db_service_name):
+    def _get_datasource_fqn_for_lineage(self, chart_json, db_service_entity):
         return (
-            self._get_datasource_fqn(chart_json, db_service_name)
+            self._get_datasource_fqn(chart_json, db_service_entity)
             if chart_json.get("table_name")
             else None
         )
 
     def yield_dashboard_chart(
         self, dashboard_details: dict
     ) -> Optional[Iterable[CreateChartRequest]]:
@@ -112,39 +117,42 @@
             chart = CreateChartRequest(
                 name=chart_json["id"],
                 displayName=chart_json.get("slice_name"),
                 description=chart_json.get("description"),
                 chartType=get_standard_chart_type(
                     chart_json.get("viz_type", ChartType.Other.value)
                 ),
-                chartUrl=f"{clean_uri(self.service_connection.hostPort)}/explore/?slice_id={chart_json['id']}",
+                sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/explore/?slice_id={chart_json['id']}",
                 service=self.context.dashboard_service.fullyQualifiedName.__root__,
             )
             yield chart
 
-    def _get_database_name(self, sqa_str: str) -> str:
+    def _get_database_name(
+        self, sqa_str: str, db_service_entity: DatabaseService
+    ) -> Optional[str]:
+        default_db_name = None
         if sqa_str:
-            return sqa_str.split("/")[-1]
-        return None
+            sqa_url = make_url(sqa_str)
+            default_db_name = sqa_url.database if sqa_url else None
+        return get_database_name_for_lineage(db_service_entity, default_db_name)
 
     def _get_datasource_fqn(
-        self, chart_json: dict, db_service_name: str
+        self, chart_json: dict, db_service_entity: DatabaseService
     ) -> Optional[str]:
-        if db_service_name:
-            try:
-                dataset_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Table,
-                    table_name=chart_json.get("table_name"),
-                    database_name=self._get_database_name(
-                        chart_json.get("sqlalchemy_uri")
-                    ),
-                    schema_name=chart_json.get("schema"),
-                    service_name=db_service_name,
-                )
-                return dataset_fqn
-            except Exception as err:
-                logger.debug(traceback.format_exc())
-                logger.warning(
-                    f"Failed to fetch Datasource with id [{chart_json.get('table_name')}]: {err}"
-                )
+        try:
+            dataset_fqn = fqn.build(
+                self.metadata,
+                entity_type=Table,
+                table_name=chart_json.get("table_name"),
+                database_name=self._get_database_name(
+                    chart_json.get("sqlalchemy_uri"), db_service_entity
+                ),
+                schema_name=chart_json.get("schema"),
+                service_name=db_service_entity.name.__root__,
+            )
+            return dataset_fqn
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Failed to fetch Datasource with id [{chart_json.get('table_name')}]: {err}"
+            )
         return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 from metadata.generated.schema.entity.services.connections.dashboard.supersetConnection import (
     SupersetConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.utils.supersetApiConnection import (
-    SupersetAPIConnection,
+    SupersetApiConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.dashboard.superset.api_source import SupersetAPISource
 from metadata.ingestion.source.dashboard.superset.db_source import SupersetDBSource
@@ -37,10 +37,10 @@
     def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
         config = WorkflowSource.parse_obj(config_dict)
         connection: SupersetConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, SupersetConnection):
             raise InvalidSourceException(
                 f"Expected SupersetConnection, but got {connection}"
             )
-        if isinstance(connection.connection, SupersetAPIConnection):
+        if isinstance(connection.connection, SupersetApiConnection):
             return SupersetAPISource(config, metadata_config)
         return SupersetDBSource(config, metadata_config)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/mixin.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.services.dashboardService import (
     DashboardServiceType,
 )
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
 from metadata.utils import fqn
@@ -113,39 +114,43 @@
 
     def yield_dashboard_lineage_details(
         self, dashboard_details: dict, db_service_name: str
     ) -> Optional[Iterable[AddLineageRequest]]:
         """
         Get lineage between dashboard and data sources
         """
-        for chart_id in self._get_charts_of_dashboard(dashboard_details):
-            chart_json = self.all_charts.get(chart_id)
-            if chart_json:
-                datasource_fqn = self._get_datasource_fqn_for_lineage(
-                    chart_json, db_service_name
-                )
-                if not datasource_fqn:
-                    continue
-                from_entity = self.metadata.get_by_name(
-                    entity=Table,
-                    fqn=datasource_fqn,
-                )
-                try:
-                    dashboard_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Lineage_Dashboard,
-                        service_name=self.config.serviceName,
-                        dashboard_name=str(dashboard_details["id"]),
+        db_service_entity = self.metadata.get_by_name(
+            entity=DatabaseService, fqn=db_service_name
+        )
+        if db_service_entity:
+            for chart_id in self._get_charts_of_dashboard(dashboard_details):
+                chart_json = self.all_charts.get(chart_id)
+                if chart_json:
+                    datasource_fqn = self._get_datasource_fqn_for_lineage(
+                        chart_json, db_service_entity
                     )
-                    to_entity = self.metadata.get_by_name(
-                        entity=Lineage_Dashboard,
-                        fqn=dashboard_fqn,
+                    if not datasource_fqn:
+                        continue
+                    from_entity = self.metadata.get_by_name(
+                        entity=Table,
+                        fqn=datasource_fqn,
                     )
-                    if from_entity and to_entity:
-                        yield self._get_add_lineage_request(
-                            to_entity=to_entity, from_entity=from_entity
+                    try:
+                        dashboard_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Lineage_Dashboard,
+                            service_name=self.config.serviceName,
+                            dashboard_name=str(dashboard_details["id"]),
+                        )
+                        to_entity = self.metadata.get_by_name(
+                            entity=Lineage_Dashboard,
+                            fqn=dashboard_fqn,
+                        )
+                        if from_entity and to_entity:
+                            yield self._get_add_lineage_request(
+                                to_entity=to_entity, from_entity=from_entity
+                            )
+                    except Exception as exc:
+                        logger.debug(traceback.format_exc())
+                        logger.error(
+                            f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
                         )
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.error(
-                        f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
-                    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/superset/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/__init__.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,18 +10,14 @@
 #  limitations under the License.
 """
 Tableau source module
 """
 import traceback
 from typing import Iterable, List, Optional, Set
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.data.createDashboardDataModel import (
     CreateDashboardDataModelRequest,
 )
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.chart import Chart
@@ -35,14 +31,15 @@
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.services.dashboardService import (
     DashboardServiceType,
 )
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
@@ -52,18 +49,23 @@
     DataSource,
     DatasourceField,
     TableauDashboard,
     TableauTag,
     UpstreamTable,
 )
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
-from metadata.utils import fqn, tag_utils
+from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart, filter_by_datamodel
-from metadata.utils.helpers import clean_uri, get_standard_chart_type
+from metadata.utils.helpers import (
+    clean_uri,
+    get_database_name_for_lineage,
+    get_standard_chart_type,
+)
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 
 logger = ingestion_logger()
 
 TABLEAU_TAG_CATEGORY = "TableauTags"
 
 
 class TableauSource(DashboardServiceSource):
@@ -168,35 +170,21 @@
                 return EntityReference(id=user.id.__root__, type="user")
         return None
 
     def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
         """
         Fetch Dashboard Tags
         """
-        if self.source_config.includeTags:
-            for tag in self.tags:
-                try:
-                    classification = OMetaTagAndClassification(
-                        classification_request=CreateClassificationRequest(
-                            name=TABLEAU_TAG_CATEGORY,
-                            description="Tags associates with tableau entities",
-                        ),
-                        tag_request=CreateTagRequest(
-                            classification=TABLEAU_TAG_CATEGORY,
-                            name=tag.label,
-                            description="Tableau Tag",
-                        ),
-                    )
-                    yield classification
-                    logger.info(
-                        f"Classification {TABLEAU_TAG_CATEGORY}, Tag {tag} Ingested"
-                    )
-                except Exception as err:
-                    logger.debug(traceback.format_exc())
-                    logger.error(f"Error ingesting tag [{tag}]: {err}")
+        yield from get_ometa_tag_and_classification(
+            tags=[tag.label for tag in self.tags],
+            classification_name=TABLEAU_TAG_CATEGORY,
+            tag_description="Tableau Tag",
+            classification_desciption="Tags associated with tableau entities",
+            include_tags=self.source_config.includeTags,
+        )
 
     def yield_datamodel(
         self, dashboard_details: TableauDashboard
     ) -> Iterable[CreateDashboardDataModelRequest]:
         if self.source_config.includeDataModels:
             for data_model in dashboard_details.dataModels or []:
                 if filter_by_datamodel(
@@ -204,15 +192,14 @@
                 ):
                     self.status.filter(data_model.name, "Data model filtered out.")
                     continue
                 try:
                     data_model_request = CreateDashboardDataModelRequest(
                         name=data_model.id,
                         displayName=data_model.name,
-                        description="",
                         service=self.context.dashboard_service.fullyQualifiedName.__root__,
                         dataModelType=DataModelType.TableauDataModel.value,
                         serviceType=DashboardServiceType.Tableau.value,
                         columns=self.get_column_info(data_model),
                     )
                     yield data_model_request
                     self.status.scanned(
@@ -241,14 +228,15 @@
         topology. And they are cleared after processing each Dashboard because of the 'clear_cache' option.
         """
         try:
             dashboard_request = CreateDashboardRequest(
                 name=dashboard_details.id,
                 displayName=dashboard_details.name,
                 description=dashboard_details.description,
+                project=dashboard_details.project.name,
                 charts=[
                     fqn.build(
                         self.metadata,
                         entity_type=Chart,
                         service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                         chart_name=chart.name.__root__,
                     )
@@ -259,21 +247,21 @@
                         self.metadata,
                         entity_type=DashboardDataModel,
                         service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                         data_model_name=data_model.name.__root__,
                     )
                     for data_model in self.context.dataModels or []
                 ],
-                tags=tag_utils.get_tag_labels(
+                tags=get_tag_labels(
                     metadata=self.metadata,
                     tags=[tag.label for tag in dashboard_details.tags],
                     classification_name=TABLEAU_TAG_CATEGORY,
                     include_tags=self.source_config.includeTags,
                 ),
-                dashboardUrl=dashboard_details.webpageUrl,
+                sourceUrl=dashboard_details.webpageUrl,
                 service=self.context.dashboard_service.fullyQualifiedName.__root__,
             )
             yield dashboard_request
             self.register_record(dashboard_request=dashboard_request)
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Error to yield dashboard for {dashboard_details}: {exc}")
@@ -318,20 +306,23 @@
         Args:
             dashboard_details: Tableau Dashboard
             db_service_name: database service where look up for lineage
 
         Returns:
             Lineage request between Data Models and Database table
         """
+        db_service_entity = self.metadata.get_by_name(
+            entity=DatabaseService, fqn=db_service_name
+        )
         for datamodel in dashboard_details.dataModels or []:
             try:
                 data_model_entity = self._get_datamodel(datamodel=datamodel)
                 if data_model_entity:
                     for table in datamodel.upstreamTables or []:
-                        om_table = self._get_database_table(db_service_name, table)
+                        om_table = self._get_database_table(db_service_entity, table)
                         if om_table:
                             yield self._get_add_lineage_request(
                                 to_entity=data_model_entity, from_entity=om_table
                             )
             except Exception as err:
                 logger.debug(traceback.format_exc())
                 logger.error(
@@ -363,16 +354,16 @@
                     f"/{workbook_chart_name.chart_url_name}"
                 )
 
                 yield CreateChartRequest(
                     name=chart.id,
                     displayName=chart.name,
                     chartType=get_standard_chart_type(chart.sheetType),
-                    chartUrl=chart_url,
-                    tags=tag_utils.get_tag_labels(
+                    sourceUrl=chart_url,
+                    tags=get_tag_labels(
                         metadata=self.metadata,
                         tags=[tag.label for tag in chart.tags],
                         classification_name=TABLEAU_TAG_CATEGORY,
                         include_tags=self.source_config.includeTags,
                     ),
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                 )
@@ -387,36 +378,37 @@
         """
         try:
             self.client.sign_out()
         except ConnectionError as err:
             logger.debug(f"Error closing connection - {err}")
 
     def _get_database_table(
-        self, db_service_name: str, table: UpstreamTable
+        self, db_service_entity: DatabaseService, table: UpstreamTable
     ) -> Optional[Table]:
         """
         Get the table entity for lineage
         """
         # table.name in tableau can come as db.schema.table_name. Hence the logic to split it
         database_schema_table = fqn.split_table_name(table.name)
         database_name = (
             table.database.name
             if table.database and table.database.name
             else database_schema_table.get("database")
         )
+        database_name = get_database_name_for_lineage(db_service_entity, database_name)
         schema_name = (
             table.schema_
             if table.schema_
             else database_schema_table.get("database_schema")
         )
         table_name = database_schema_table.get("table")
         table_fqn = fqn.build(
             self.metadata,
             entity_type=Table,
-            service_name=db_service_name,
+            service_name=db_service_entity.name.__root__,
             schema_name=schema_name,
             table_name=table_name,
             database_name=database_name,
         )
         if table_fqn:
             return self.metadata.get_by_name(
                 entity=Table,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -156,14 +156,15 @@
     """
     Aux class for Dashboard object of the tableau_api_lib response
     """
 
     class Config:
         extra = Extra.allow
 
+    project: Optional[TableauBaseModel]
     description: Optional[str]
     owner: Optional[TableauOwner]
     tags: Optional[List[TableauTag]] = []
     _extract_tags = validator("tags", pre=True, allow_reuse=True)(transform_tags)
     webpageUrl: Optional[str]
     charts: Optional[List[TableauChart]]
     dataModels: List[DataSource] = []
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/dashboard/tableau/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/athena/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/athena/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/azuresql/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/azuresql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/azuresql/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/azuresql/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/connection.py`

 * *Files 9% similar despite different names*

```diff
@@ -22,16 +22,16 @@
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.services.connections.database.bigQueryConnection import (
     BigQueryConnection,
 )
-from metadata.generated.schema.security.credentials.gcsValues import (
-    GcsCredentialsValues,
+from metadata.generated.schema.security.credentials.gcpValues import (
+    GcpCredentialsValues,
     MultipleProjectId,
     SingleProjectId,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
 )
@@ -48,43 +48,43 @@
 
 def get_connection_url(connection: BigQueryConnection) -> str:
     """
     Build the connection URL and set the project
     environment variable when needed
     """
 
-    if isinstance(connection.credentials.gcsConfig, GcsCredentialsValues):
+    if isinstance(connection.credentials.gcpConfig, GcpCredentialsValues):
         if isinstance(  # pylint: disable=no-else-return
-            connection.credentials.gcsConfig.projectId, SingleProjectId
+            connection.credentials.gcpConfig.projectId, SingleProjectId
         ):
-            if not connection.credentials.gcsConfig.projectId.__root__:
-                return f"{connection.scheme.value}://{connection.credentials.gcsConfig.projectId or ''}"
+            if not connection.credentials.gcpConfig.projectId.__root__:
+                return f"{connection.scheme.value}://{connection.credentials.gcpConfig.projectId or ''}"
             if (
-                not connection.credentials.gcsConfig.privateKey
-                and connection.credentials.gcsConfig.projectId.__root__
+                not connection.credentials.gcpConfig.privateKey
+                and connection.credentials.gcpConfig.projectId.__root__
             ):
-                project_id = connection.credentials.gcsConfig.projectId.__root__
+                project_id = connection.credentials.gcpConfig.projectId.__root__
                 os.environ["GOOGLE_CLOUD_PROJECT"] = project_id
-            return f"{connection.scheme.value}://{connection.credentials.gcsConfig.projectId.__root__}"
-        elif isinstance(connection.credentials.gcsConfig.projectId, MultipleProjectId):
-            for project_id in connection.credentials.gcsConfig.projectId.__root__:
-                if not connection.credentials.gcsConfig.privateKey and project_id:
+            return f"{connection.scheme.value}://{connection.credentials.gcpConfig.projectId.__root__}"
+        elif isinstance(connection.credentials.gcpConfig.projectId, MultipleProjectId):
+            for project_id in connection.credentials.gcpConfig.projectId.__root__:
+                if not connection.credentials.gcpConfig.privateKey and project_id:
                     # Setting environment variable based on project id given by user / set in ADC
                     os.environ["GOOGLE_CLOUD_PROJECT"] = project_id
                 return f"{connection.scheme.value}://{project_id}"
             return f"{connection.scheme.value}://"
 
     return f"{connection.scheme.value}://"
 
 
 def get_connection(connection: BigQueryConnection) -> Engine:
     """
-    Prepare the engine and the GCS credentials
+    Prepare the engine and the GCP credentials
     """
-    set_google_credentials(gcs_credentials=connection.credentials)
+    set_google_credentials(gcp_credentials=connection.credentials)
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args_common,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/metadata.py`

 * *Files 8% similar despite different names*

```diff
@@ -21,22 +21,17 @@
 from sqlalchemy import inspect
 from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy.sql.sqltypes import Interval
 from sqlalchemy.types import String
 from sqlalchemy_bigquery import BigQueryDialect, _types
 from sqlalchemy_bigquery._types import _get_sqla_column_type
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
-from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.table import (
     IntervalType,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.bigQueryConnection import (
@@ -44,37 +39,40 @@
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.security.credentials.gcsCredentials import (
-    GCSCredentialsPath,
+from metadata.generated.schema.security.credentials.gcpCredentials import (
+    GcpCredentialsPath,
 )
-from metadata.generated.schema.security.credentials.gcsValues import (
-    GcsCredentialsValues,
+from metadata.generated.schema.security.credentials.gcpValues import (
+    GcpCredentialsValues,
     MultipleProjectId,
     SingleProjectId,
 )
-from metadata.generated.schema.type.tagLabel import (
-    LabelType,
-    State,
-    TagLabel,
-    TagSource,
-)
+from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.source.connections import get_connection
+from metadata.ingestion.source.database.bigquery.queries import (
+    BIGQUERY_SCHEMA_DESCRIPTION,
+)
 from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
 from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_database
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.sqlalchemy_utils import is_complex_type
+from metadata.utils.tag_utils import (
+    get_ometa_tag_and_classification,
+    get_tag_label,
+    get_tag_labels,
+)
 
 
 class BQJSON(String):
     """The SQL JSON type."""
 
     def get_col_spec(self, **kw):  # pylint: disable=unused-argument
         return "JSON"
@@ -207,54 +205,64 @@
         :return:
         """
         try:
             # Fetching labels on the databaseSchema ( dataset ) level
             dataset_obj = self.client.get_dataset(schema_name)
             if dataset_obj.labels:
                 for key, value in dataset_obj.labels.items():
-                    yield OMetaTagAndClassification(
-                        classification_request=CreateClassificationRequest(
-                            name=key,
-                            description="",
-                        ),
-                        tag_request=CreateTagRequest(
-                            classification=key,
-                            name=value,
-                            description="Bigquery Dataset Label",
-                        ),
+                    yield from get_ometa_tag_and_classification(
+                        tags=[value],
+                        classification_name=key,
+                        tag_description="Bigquery Dataset Label",
+                        classification_desciption="",
                     )
             # Fetching policy tags on the column level
             list_project_ids = [self.context.database.name.__root__]
             if not self.service_connection.taxonomyProjectID:
                 self.service_connection.taxonomyProjectID = []
             list_project_ids.extend(self.service_connection.taxonomyProjectID)
             for project_ids in list_project_ids:
                 taxonomies = PolicyTagManagerClient().list_taxonomies(
                     parent=f"projects/{project_ids}/locations/{self.service_connection.taxonomyLocation}"
                 )
                 for taxonomy in taxonomies:
                     policy_tags = PolicyTagManagerClient().list_policy_tags(
                         parent=taxonomy.name
                     )
-                    for tag in policy_tags:
-                        yield OMetaTagAndClassification(
-                            classification_request=CreateClassificationRequest(
-                                name=taxonomy.display_name,
-                                description="",
-                            ),
-                            tag_request=CreateTagRequest(
-                                classification=taxonomy.display_name,
-                                name=tag.display_name,
-                                description="Bigquery Policy Tag",
-                            ),
-                        )
+                    yield from get_ometa_tag_and_classification(
+                        tags=[tag.display_name for tag in policy_tags],
+                        classification_name=taxonomy.display_name,
+                        tag_description="Bigquery Policy Tag",
+                        classification_desciption="",
+                    )
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Skipping Policy Tag: {exc}")
 
+    def get_schema_description(self, schema_name: str) -> Optional[str]:
+        try:
+            query_resp = self.client.query(
+                BIGQUERY_SCHEMA_DESCRIPTION.format(
+                    project_id=self.client.project,
+                    region=self.service_connection.usageLocation,
+                    schema_name=schema_name,
+                )
+            )
+
+            query_result = [result.schema_description for result in query_resp.result()]
+            return query_result[0]
+        except IndexError:
+            logger.debug(f"No dataset description found for {schema_name}")
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.debug(
+                f"Failed to fetch dataset description for [{schema_name}]: {err}"
+            )
+        return ""
+
     def yield_database_schema(
         self, schema_name: str
     ) -> Iterable[CreateDatabaseSchemaRequest]:
         """
         From topology.
         Prepare a database schema request and pass it to the sink
         """
@@ -263,28 +271,23 @@
             name=schema_name,
             database=self.context.database.fullyQualifiedName,
             description=self.get_schema_description(schema_name),
         )
 
         dataset_obj = self.client.get_dataset(schema_name)
         if dataset_obj.labels:
+            database_schema_request_obj.tags = []
             for label_classification, label_tag_name in dataset_obj.labels.items():
-                database_schema_request_obj.tags = [
-                    TagLabel(
-                        tagFQN=fqn.build(
-                            self.metadata,
-                            entity_type=Tag,
-                            classification_name=label_classification,
-                            tag_name=label_tag_name,
-                        ),
-                        labelType=LabelType.Automated.value,
-                        state=State.Suggested.value,
-                        source=TagSource.Classification.value,
+                database_schema_request_obj.tags.append(
+                    get_tag_label(
+                        metadata=self.metadata,
+                        tag_name=label_tag_name,
+                        classification_name=label_classification,
                     )
-                ]
+                )
         yield database_schema_request_obj
 
     def get_tag_labels(self, table_name: str) -> Optional[List[TagLabel]]:
         """
         This will only get executed if the tags context
         is properly informed
         """
@@ -293,56 +296,49 @@
     def get_column_tag_labels(
         self, table_name: str, column: dict
     ) -> Optional[List[TagLabel]]:
         """
         This will only get executed if the tags context
         is properly informed
         """
-        if self.source_config.includeTags and column.get("policy_tags"):
-            return [
-                TagLabel(
-                    tagFQN=fqn.build(
-                        self.metadata,
-                        entity_type=Tag,
-                        classification_name=column["taxonomy"],
-                        tag_name=column["policy_tags"],
-                    ),
-                    labelType=LabelType.Automated.value,
-                    state=State.Suggested.value,
-                    source=TagSource.Classification.value,
-                )
-            ]
+        if column.get("policy_tags"):
+            return get_tag_labels(
+                metadata=self.metadata,
+                tags=[column["policy_tags"]],
+                classification_name=column["taxonomy"],
+                include_tags=self.source_config.includeTags,
+            )
         return None
 
     def set_inspector(self, database_name: str):
         self.client = Client(project=database_name)
         if isinstance(
-            self.service_connection.credentials.gcsConfig, GcsCredentialsValues
+            self.service_connection.credentials.gcpConfig, GcpCredentialsValues
         ):
-            self.service_connection.credentials.gcsConfig.projectId = SingleProjectId(
+            self.service_connection.credentials.gcpConfig.projectId = SingleProjectId(
                 __root__=database_name
             )
         self.engine = get_connection(self.service_connection)
         self.inspector = inspect(self.engine)
 
     def get_database_names(self) -> Iterable[str]:
         if isinstance(
-            self.service_connection.credentials.gcsConfig, GCSCredentialsPath
+            self.service_connection.credentials.gcpConfig, GcpCredentialsPath
         ):
             self.set_inspector(database_name=self.project_ids)
             yield self.project_ids
         elif isinstance(
-            self.service_connection.credentials.gcsConfig.projectId, SingleProjectId
+            self.service_connection.credentials.gcpConfig.projectId, SingleProjectId
         ):
             self.set_inspector(database_name=self.project_ids)
             yield self.project_ids
         elif hasattr(
-            self.service_connection.credentials.gcsConfig, "projectId"
+            self.service_connection.credentials.gcpConfig, "projectId"
         ) and isinstance(
-            self.service_connection.credentials.gcsConfig.projectId, MultipleProjectId
+            self.service_connection.credentials.gcpConfig.projectId, MultipleProjectId
         ):
             for project_id in self.project_ids:
                 database_name = project_id
                 database_fqn = fqn.build(
                     self.metadata,
                     entity_type=Database,
                     service_name=self.context.database_service.name.__root__,
@@ -430,7 +426,22 @@
         return raw_data_type.replace(", ", ",").replace(" ", ":").lower()
 
     def close(self):
         super().close()
         if self.temp_credentials:
             os.unlink(self.temp_credentials)
         os.environ.pop("GOOGLE_CLOUD_PROJECT", "")
+
+    def get_source_url(
+        self,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+        table_type: TableType,
+    ) -> Optional[str]:
+        """
+        Method to get the source url for bigquery
+        """
+        return (
+            f"https://console.cloud.google.com/bigquery?project={database_name}"
+            f"&ws=!1m5!1m4!4m3!1s{database_name}!2s{schema_name}!3s{table_name}"
+        )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/bigquery.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,39 +4,51 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-SQL Queries used during ingestion
+Bigquery System Metric Queries
 """
+from datetime import datetime
 
-import textwrap
+from pydantic import BaseModel
 
-BIGQUERY_STATEMENT = textwrap.dedent(
-    """
- SELECT
-   project_id as database_name,
-   user_email as user_name,
-   statement_type as query_type,
-   start_time,
-   end_time,
-   query as query_text,
-   null as schema_name,
-   total_slot_ms/1000 as duration
-FROM `region-{region}`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
-WHERE creation_time BETWEEN "{start_time}" AND "{end_time}"
-  {filters}
-  AND job_type = "QUERY"
-  AND state = "DONE"
-  AND IFNULL(statement_type, "NO") not in ("NO", "DROP_TABLE", "CREATE_TABLE")
-  AND query NOT LIKE '/* {{"app": "OpenMetadata", %%}} */%%'
-  AND query NOT LIKE '/* {{"app": "dbt", %%}} */%%'
-  LIMIT {result_limit}
-"""
-)
+from metadata.profiler.metrics.system.dml_operation import DatabaseDMLOperations
+
+
+class BigQueryQueryResult(BaseModel):
+    table_name: dict
+    timestamp: datetime
+    query_type: str
+    dml_statistics: dict
 
-BIGQUERY_TEST_STATEMENT = textwrap.dedent(
-    """SELECT query FROM `region-{region}`.INFORMATION_SCHEMA.JOBS_BY_PROJECT limit 1"""
-)
+
+DML_STAT_TO_DML_STATEMENT_MAPPING = {
+    "inserted_row_count": DatabaseDMLOperations.INSERT.value,
+    "deleted_row_count": DatabaseDMLOperations.DELETE.value,
+    "updated_row_count": DatabaseDMLOperations.UPDATE.value,
+}
+
+JOBS = """
+    SELECT
+        statement_type,
+        start_time,
+        destination_table,
+        dml_statistics
+    FROM
+        `region-{usage_location}`.INFORMATION_SCHEMA.JOBS
+    WHERE
+        DATE(creation_time) >= CURRENT_DATE() - 1 AND
+        destination_table.dataset_id = '{dataset_id}' AND
+        destination_table.project_id = '{project_id}' AND
+        statement_type IN (
+            '{insert}',
+            '{update}',
+            '{delete}',
+            '{merge}'
+        )
+    ORDER BY creation_time DESC;
+"""
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/bigquery/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/bigquery/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/query_parser.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Clickhouse usage module
 """
 
 import ast
+import traceback
 from abc import ABC
 from datetime import datetime
 from typing import List
 
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.services.connections.database.clickhouseConnection import (
     ClickhouseConnection,
@@ -25,14 +26,17 @@
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.database.query_parser_source import QueryParserSource
+from metadata.utils.logger import ingestion_logger
+
+logger = ingestion_logger()
 
 
 class ClickhouseQueryParserSource(QueryParserSource, ABC):
     """
     Clickhouse base for Usage and Lineage
     """
 
@@ -47,19 +51,27 @@
         return cls(config, metadata_config)
 
     @staticmethod
     def get_schema_name(data: dict) -> str:
         """
         Method to fetch schema name from row data
         """
-        schema = None
-        if data.get("schema_name"):
-            schema_list = ast.literal_eval(data["schema_name"])
-            schema = schema_list[0] if len(schema_list) == 1 else None
-        return schema
+        try:
+            if data.get("schema_name"):
+                schema_list = []
+                if isinstance(data["schema_name"], str):
+                    schema_list = ast.literal_eval(data["schema_name"])
+                elif isinstance(data["schema_name"], list):
+                    schema_list = data["schema_name"]
+                schema = schema_list[0] if len(schema_list) == 1 else None
+                return schema
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"Failed to fetch the schema name due to: {exc}")
+        return None
 
     def get_sql_statement(self, start_time: datetime, end_time: datetime) -> str:
         """
         returns sql statement to fetch query logs
         """
         return self.sql_stmt.format(
             start_time=start_time,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/clickhouse/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/column_helpers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/column_helpers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/column_type_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/column_type_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/common_db_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/common_db_source.py`

 * *Files 6% similar despite different names*

```diff
@@ -40,30 +40,25 @@
 )
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.ingestion.lineage.models import ConnectionTypeDialectMapper
-from metadata.ingestion.lineage.parser import LineageParser
-from metadata.ingestion.lineage.sql_lineage import (
-    get_column_fqn,
-    get_lineage_by_query,
-    get_lineage_via_table_entity,
-)
+from metadata.ingestion.lineage.sql_lineage import get_column_fqn
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.table_metadata import OMetaTableConstraints
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.database_service import DatabaseServiceSource
 from metadata.ingestion.source.database.sql_column_handler import SqlColumnHandlerMixin
 from metadata.ingestion.source.database.sqlalchemy_source import SqlAlchemySource
 from metadata.ingestion.source.models import TableView
 from metadata.utils import fqn
+from metadata.utils.db_utils import get_view_lineage
 from metadata.utils.filters import filter_by_table
 from metadata.utils.helpers import calculate_execution_time_generator
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
@@ -383,14 +378,20 @@
                 ),
                 columns=columns,
                 viewDefinition=view_definition,
                 databaseSchema=self.context.database_schema.fullyQualifiedName,
                 tags=self.get_tag_labels(
                     table_name=table_name
                 ),  # Pick tags from context info, if any
+                sourceUrl=self.get_source_url(
+                    table_name=table_name,
+                    schema_name=schema_name,
+                    database_name=self.context.database.name.__root__,
+                    table_type=table_type,
+                ),
             )
 
             is_partitioned, partition_details = self.get_table_partition_details(
                 table_name=table_name, schema_name=schema_name, inspector=self.inspector
             )
             if is_partitioned:
                 table_request.tableType = TableType.Partitioned.value
@@ -426,61 +427,21 @@
             self.status.failed(table_name, error, traceback.format_exc())
 
     def yield_view_lineage(self) -> Optional[Iterable[AddLineageRequest]]:
         logger.info("Processing Lineage for Views")
         for view in [
             v for v in self.context.table_views if v.view_definition is not None
         ]:
-            table_name = view.table_name
-            schema_name = view.schema_name
-            db_name = view.db_name
-            view_definition = view.view_definition
-            table_fqn = fqn.build(
-                self.metadata,
-                entity_type=Table,
+            yield from get_view_lineage(
+                view=view,
+                metadata=self.metadata,
                 service_name=self.context.database_service.name.__root__,
-                database_name=db_name,
-                schema_name=schema_name,
-                table_name=table_name,
-            )
-            table_entity = self.metadata.get_by_name(
-                entity=Table,
-                fqn=table_fqn,
+                connection_type=self.service_connection.type.value,
             )
 
-            try:
-                connection_type = str(self.service_connection.type.value)
-                dialect = ConnectionTypeDialectMapper.dialect_of(connection_type)
-                lineage_parser = LineageParser(view_definition, dialect)
-                if lineage_parser.source_tables and lineage_parser.target_tables:
-                    yield from get_lineage_by_query(
-                        self.metadata,
-                        query=view_definition,
-                        service_name=self.context.database_service.name.__root__,
-                        database_name=db_name,
-                        schema_name=schema_name,
-                        dialect=dialect,
-                    ) or []
-
-                else:
-                    yield from get_lineage_via_table_entity(
-                        self.metadata,
-                        table_entity=table_entity,
-                        service_name=self.context.database_service.name.__root__,
-                        database_name=db_name,
-                        schema_name=schema_name,
-                        query=view_definition,
-                        dialect=dialect,
-                    ) or []
-            except Exception as exc:
-                logger.debug(traceback.format_exc())
-                logger.warning(
-                    f"Could not parse query [{view_definition}] ingesting lineage failed: {exc}"
-                )
-
     def _get_foreign_constraints(
         self, table_constraints: OMetaTableConstraints
     ) -> List[TableConstraint]:
         """
         Search the referred table for foreign constraints
         and get referred column fqn
         """
@@ -557,7 +518,18 @@
 
         Returning `table` is just the default implementation.
         """
         return table
 
     def yield_table_tag(self) -> Iterable[OMetaTagAndClassification]:
         pass
+
+    def get_source_url(
+        self,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+        table_type: TableType,
+    ) -> Optional[str]:
+        """
+        By default the source url is not supported for
+        """
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/database_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/database_service.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.table import (
     Column,
     DataModel,
     Table,
     TableType,
@@ -38,20 +37,15 @@
 )
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.tagLabel import (
-    LabelType,
-    State,
-    TagLabel,
-    TagSource,
-)
+from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.ingestion.api.source import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.delete_entity import delete_entity_from_source
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.table_metadata import OMetaTableConstraints
 from metadata.ingestion.models.topology import (
     NodeStage,
@@ -59,14 +53,15 @@
     TopologyNode,
     create_source_context,
 )
 from metadata.ingestion.source.connections import get_test_connection_fn
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_schema
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.tag_utils import get_tag_label
 
 logger = ingestion_logger()
 
 
 class DataModelLink(BaseModel):
     """
     Tmp model to handle data model ingestion
@@ -106,15 +101,14 @@
                 type_=Database,
                 context="database",
                 processor="yield_database",
                 consumer=["database_service"],
             )
         ],
         children=["databaseSchema"],
-        post_process=["mark_tables_as_deleted"],
     )
     databaseSchema = TopologyNode(
         producer="get_database_schema_names",
         stages=[
             NodeStage(
                 type_=OMetaTagAndClassification,
                 context="tags",
@@ -127,14 +121,15 @@
                 type_=DatabaseSchema,
                 context="database_schema",
                 processor="yield_database_schema",
                 consumer=["database_service", "database"],
             ),
         ],
         children=["table"],
+        post_process=["mark_tables_as_deleted"],
     )
     table = TopologyNode(
         producer="get_tables_name_and_type",
         stages=[
             NodeStage(
                 type_=Table,
                 context="table",
@@ -266,29 +261,26 @@
         yield from self.get_database_schema_names()
 
     def get_tag_by_fqn(self, entity_fqn: str) -> Optional[List[TagLabel]]:
         """
         Pick up the tags registered in the context
         searching by entity FQN
         """
-        return [
-            TagLabel(
-                tagFQN=fqn.build(
-                    self.metadata,
-                    entity_type=Tag,
-                    classification_name=tag_and_category.classification_request.name.__root__,
+
+        tag_labels = []
+        for tag_and_category in self.context.tags or []:
+            if tag_and_category.fqn.__root__ == entity_fqn:
+                tag_label = get_tag_label(
+                    metadata=self.metadata,
                     tag_name=tag_and_category.tag_request.name.__root__,
-                ),
-                labelType=LabelType.Automated,
-                state=State.Suggested,
-                source=TagSource.Classification,
-            )
-            for tag_and_category in self.context.tags or []
-            if tag_and_category.fqn.__root__ == entity_fqn
-        ] or None
+                    classification_name=tag_and_category.classification_request.name.__root__,
+                )
+                if tag_label:
+                    tag_labels.append(tag_label)
+        return tag_labels or None
 
     def get_tag_labels(self, table_name: str) -> Optional[List[TagLabel]]:
         """
         This will only get executed if the tags context
         is properly informed
         """
         table_fqn = fqn.build(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/client.py`

 * *Files 17% similar despite different names*

```diff
@@ -18,34 +18,42 @@
 
 import requests
 
 from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
     DatabricksConnection,
 )
 from metadata.ingestion.ometa.client import APIError
+from metadata.ingestion.source.database.databricks.models import (
+    LineageColumnStreams,
+    LineageTableStreams,
+)
 from metadata.utils.constants import QUERY_WITH_DBT, QUERY_WITH_OM_VERSION
 from metadata.utils.helpers import datetime_to_ts
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 API_TIMEOUT = 10
+QUERIES_PATH = "/sql/history/queries"
+TABLE_LINEAGE_PATH = "/lineage-tracking/table-lineage/get"
+COLUMN_LINEAGE_PATH = "/lineage-tracking/column-lineage/get"
 
 
 class DatabricksClient:
     """
     DatabricksClient creates a Databricks connection based on DatabricksCredentials.
     """
 
     def __init__(self, config: DatabricksConnection):
         self.config = config
         base_url, *_ = self.config.hostPort.split(":")
         api_version = "/api/2.0"
         job_api_version = "/api/2.1"
         auth_token = self.config.token.get_secret_value()
-        self.base_query_url = f"https://{base_url}{api_version}/sql/history/queries"
+        self.base_url = f"https://{base_url}{api_version}"
+        self.base_query_url = f"{self.base_url}{QUERIES_PATH}"
         self.base_job_url = f"https://{base_url}{job_api_version}/jobs"
         self.jobs_list_url = f"{self.base_job_url}/list"
         self.jobs_run_list_url = f"{self.base_job_url}/runs/list"
         self.headers = {
             "Authorization": f"Bearer {auth_token}",
             "Content-Type": "application/json",
         }
@@ -204,7 +212,59 @@
                 job_runs.extend(response.get("runs" or []))
 
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.error(exc)
 
         return job_runs
+
+    def get_table_lineage(self, table_name: str) -> LineageTableStreams:
+        """
+        Method returns table lineage details
+        """
+        try:
+            data = {
+                "table_name": table_name,
+            }
+
+            response = self.client.get(
+                f"{self.base_url}{TABLE_LINEAGE_PATH}",
+                headers=self.headers,
+                data=json.dumps(data),
+                timeout=API_TIMEOUT,
+            ).json()
+            if response:
+                return LineageTableStreams(**response)
+
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(exc)
+
+        return LineageTableStreams()
+
+    def get_column_lineage(
+        self, table_name: str, column_name: str
+    ) -> LineageColumnStreams:
+        """
+        Method returns table lineage details
+        """
+        try:
+            data = {
+                "table_name": table_name,
+                "column_name": column_name,
+            }
+
+            response = self.client.get(
+                f"{self.base_url}{COLUMN_LINEAGE_PATH}",
+                headers=self.headers,
+                data=json.dumps(data),
+                timeout=API_TIMEOUT,
+            ).json()
+
+            if response:
+                return LineageColumnStreams(**response)
+
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(exc)
+
+        return LineageColumnStreams()
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/connection.py`

 * *Files 23% similar despite different names*

```diff
@@ -8,91 +8,75 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-from functools import partial
+
 from typing import Optional
 
 from sqlalchemy.engine import Engine
-from sqlalchemy.inspection import inspect
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
-    DatabricksConnection,
+from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
+    PostgresConnection,
+    SslMode,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
+    get_connection_url_common,
     init_empty_connection_arguments,
 )
-from metadata.ingestion.connections.test_connections import (
-    test_connection_engine_step,
-    test_connection_steps,
-    test_query,
-)
+from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.databricks.client import DatabricksClient
-from metadata.ingestion.source.database.databricks.queries import (
-    DATABRICKS_GET_CATALOGS,
+from metadata.ingestion.source.database.postgres.queries import (
+    POSTGRES_GET_DATABASE,
+    POSTGRES_TEST_GET_QUERIES,
+    POSTGRES_TEST_GET_TAGS,
 )
 
 
-def get_connection_url(connection: DatabricksConnection) -> str:
-    url = f"{connection.scheme.value}://token:{connection.token.get_secret_value()}@{connection.hostPort}"
-    return url
-
-
-def get_connection(connection: DatabricksConnection) -> Engine:
+def get_connection(connection: PostgresConnection) -> Engine:
     """
     Create connection
     """
-    if connection.httpPath:
+    if connection.sslMode:
         if not connection.connectionArguments:
             connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["http_path"] = connection.httpPath
-
+        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
+        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
+            connection.connectionArguments.__root__[
+                "sslrootcert"
+            ] = connection.sslConfig.__root__.certificatePath
     return create_generic_db_connection(
         connection=connection,
-        get_connection_url_fn=get_connection_url,
+        get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: DatabricksConnection,
+    service_connection: PostgresConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-
-    inspector = inspect(engine)
-    client = DatabricksClient(service_connection)
-
-    test_fn = {
-        "CheckAccess": partial(test_connection_engine_step, engine),
-        "GetSchemas": inspector.get_schema_names,
-        "GetTables": inspector.get_table_names,
-        "GetViews": inspector.get_view_names,
-        "GetDatabases": partial(
-            test_query,
-            engine=engine,
-            statement=DATABRICKS_GET_CATALOGS,
-        ),
-        "GetQueries": client.test_query_api_access,
+    queries = {
+        "GetQueries": POSTGRES_TEST_GET_QUERIES,
+        "GetDatabases": POSTGRES_GET_DATABASE,
+        "GetTags": POSTGRES_TEST_GET_TAGS,
     }
-
-    test_connection_steps(
+    test_connection_db_common(
         metadata=metadata,
-        test_fn=test_fn,
-        service_type=service_connection.type.value,
+        engine=engine,
+        service_connection=service_connection,
         automation_workflow=automation_workflow,
-        timeout_seconds=service_connection.connectionTimeout,
+        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/legacy/lineage.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,17 +21,17 @@
 )
 from metadata.ingestion.source.database.lineage_source import LineageSource
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class DatabricksLineageSource(DatabricksQueryParserSource, LineageSource):
+class DatabricksLineageLegacySource(DatabricksQueryParserSource, LineageSource):
     """
-    Databricks Lineage Source
+    Databricks Lineage Legacy Source
     """
 
     def yield_table_query(self) -> Optional[Iterator[TableQuery]]:
         data = self.client.list_query_history(
             start_date=self.start,
             end_date=self.end,
         )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/legacy/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""Clickhouse source module"""
+"""Databricks legacy source module"""
 
 import re
 import traceback
 from copy import deepcopy
 from typing import Iterable
 
 from pyhive.sqlalchemy_hive import _type_map
@@ -236,18 +236,19 @@
 DatabricksDialect.get_columns = get_columns
 DatabricksDialect.get_schema_names = get_schema_names
 DatabricksDialect.get_view_definition = get_view_definition
 DatabricksDialect.get_all_view_definitions = get_all_view_definitions
 reflection.Inspector.get_schema_names = get_schema_names_reflection
 
 
-class DatabricksSource(CommonDbSourceService):
+class DatabricksLegacySource(CommonDbSourceService):
     """
     Implements the necessary methods to extract
-    Database metadata from Databricks Source
+    Database metadata from Databricks Source using
+    the legacy hive metastore method
     """
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: DatabricksConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, DatabricksConnection):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/databricks/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -63,15 +63,15 @@
     return s3_client
 
 
 @get_datalake_client.register
 def _(config: GCSConfig):
     from google.cloud import storage
 
-    set_google_credentials(gcs_credentials=config.securityConfig)
+    set_google_credentials(gcp_credentials=config.securityConfig)
     gcs_client = storage.Client()
     return gcs_client
 
 
 @get_datalake_client.register
 def _(config: AzureConfig):
     from azure.identity import ClientSecretCredential
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -51,18 +51,24 @@
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.column_helpers import truncate_column_name
 from metadata.ingestion.source.database.database_service import DatabaseServiceSource
-from metadata.ingestion.source.database.datalake.models import DatalakeColumnWrapper
-from metadata.ingestion.source.database.datalake.utils import COMPLEX_COLUMN_SEPARATOR
+from metadata.ingestion.source.database.datalake.models import (
+    DatalakeTableSchemaWrapper,
+)
 from metadata.utils import fqn
-from metadata.utils.constants import DEFAULT_DATABASE
+from metadata.utils.constants import COMPLEX_COLUMN_SEPARATOR, DEFAULT_DATABASE
+from metadata.utils.datalake.datalake_utils import (
+    SupportedTypes,
+    clean_dataframe,
+    fetch_dataframe,
+)
 from metadata.utils.filters import filter_by_schema, filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 DATALAKE_DATA_TYPES = {
     **dict.fromkeys(["int64", "INT", "int32"], DataType.INT.value),
@@ -70,60 +76,14 @@
     **dict.fromkeys(["float64", "float32", "float"], DataType.FLOAT.value),
     "bool": DataType.BOOLEAN.value,
     **dict.fromkeys(
         ["datetime64", "timedelta[ns]", "datetime64[ns]"], DataType.DATETIME.value
     ),
 }
 
-JSON_SUPPORTED_TYPES = (".json", ".json.gz", ".json.zip")
-
-DATALAKE_SUPPORTED_FILE_TYPES = (
-    ".csv",
-    ".tsv",
-    ".parquet",
-    ".avro",
-) + JSON_SUPPORTED_TYPES
-
-
-def ometa_to_dataframe(config_source, client, table):
-    """
-    Method to get dataframe for profiling
-    """
-
-    data = None
-    if isinstance(config_source, GCSConfig):
-        data = DatalakeSource.get_gcs_files(
-            client=client,
-            key=table.name.__root__,
-            bucket_name=table.databaseSchema.name,
-        )
-    if isinstance(config_source, S3Config):
-        data = DatalakeSource.get_s3_files(
-            client=client,
-            key=table.name.__root__,
-            bucket_name=table.databaseSchema.name,
-        )
-    if isinstance(config_source, AzureConfig):
-        connection_args = config_source.securityConfig
-        data = DatalakeSource.get_azure_files(
-            client=client,
-            key=table.name.__root__,
-            container_name=table.databaseSchema.name,
-            storage_options={
-                "tenant_id": connection_args.tenantId,
-                "client_id": connection_args.clientId,
-                "client_secret": connection_args.clientSecret.get_secret_value(),
-                "account_name": connection_args.accountName,
-            },
-        )
-    if isinstance(data, DatalakeColumnWrapper):
-        data = data.dataframes
-
-    return data
-
 
 class DatalakeSource(DatabaseServiceSource):
     """
     Implements the necessary methods to extract
     Database metadata from Datalake Source
     """
 
@@ -407,181 +367,47 @@
     def yield_table(
         self, table_name_and_type: Tuple[str, str]
     ) -> Iterable[Optional[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
         table_name, table_type = table_name_and_type
         schema_name = self.context.database_schema.name.__root__
         columns = []
         try:
             table_constraints = None
-            if isinstance(self.service_connection.configSource, GCSConfig):
-                data_frame = self.get_gcs_files(
-                    client=self.client, key=table_name, bucket_name=schema_name
-                )
-            if isinstance(self.service_connection.configSource, S3Config):
-                connection_args = self.service_connection.configSource.securityConfig
-                data_frame = self.get_s3_files(
-                    client=self.client,
+            connection_args = self.service_connection.configSource.securityConfig
+            data_frame = fetch_dataframe(
+                config_source=self.service_connection.configSource,
+                client=self.client,
+                file_fqn=DatalakeTableSchemaWrapper(
                     key=table_name,
                     bucket_name=schema_name,
-                    client_kwargs=connection_args,
-                )
-            if isinstance(self.service_connection.configSource, AzureConfig):
-                connection_args = self.service_connection.configSource.securityConfig
-                storage_options = {
-                    "tenant_id": connection_args.tenantId,
-                    "client_id": connection_args.clientId,
-                    "client_secret": connection_args.clientSecret.get_secret_value(),
-                }
-                data_frame = self.get_azure_files(
-                    client=self.client,
-                    key=table_name,
-                    container_name=schema_name,
-                    storage_options=storage_options,
-                )
-            if isinstance(data_frame, DataFrame):
-                columns = self.get_columns(data_frame)
-            if isinstance(data_frame, list) and data_frame:
-                columns = self.get_columns(data_frame[0])
-            if isinstance(data_frame, DatalakeColumnWrapper):
-                columns = data_frame.columns
+                ),
+                connection_kwargs=connection_args,
+            )
+            columns = self.get_columns(data_frame[0])
             if columns:
                 table_request = CreateTableRequest(
                     name=table_name,
                     tableType=table_type,
-                    description="",
                     columns=columns,
                     tableConstraints=table_constraints if table_constraints else None,
                     databaseSchema=self.context.database_schema.fullyQualifiedName,
                 )
                 yield table_request
                 self.register_record(table_request=table_request)
         except Exception as exc:
             error = f"Unexpected exception to yield table [{table_name}]: {exc}"
             logger.debug(traceback.format_exc())
             logger.warning(error)
             self.status.failed(table_name, error, traceback.format_exc())
 
     @staticmethod
-    def get_gcs_files(client, key, bucket_name):
-        """
-        Fetch GCS Bucket files
-        """
-        from metadata.utils.gcs_utils import (  # pylint: disable=import-outside-toplevel
-            read_avro_from_gcs,
-            read_csv_from_gcs,
-            read_json_from_gcs,
-            read_parquet_from_gcs,
-            read_tsv_from_gcs,
-        )
-
-        try:
-            if key.endswith(".csv"):
-                return read_csv_from_gcs(key, bucket_name)
-
-            if key.endswith(".tsv"):
-                return read_tsv_from_gcs(key, bucket_name)
-
-            if key.endswith(JSON_SUPPORTED_TYPES):
-                return read_json_from_gcs(client, key, bucket_name)
-
-            if key.endswith(".parquet"):
-                return read_parquet_from_gcs(key, bucket_name)
-
-            if key.endswith(".avro"):
-                return read_avro_from_gcs(client, key, bucket_name)
-
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.error(
-                f"Unexpected exception to get GCS files from [{bucket_name}]: {exc}"
-            )
-        return None
-
-    @staticmethod
-    def get_azure_files(client, key, container_name, storage_options):
-        """
-        Fetch Azure Storage files
-        """
-        from metadata.utils.azure_utils import (  # pylint: disable=import-outside-toplevel
-            read_avro_from_azure,
-            read_csv_from_azure,
-            read_json_from_azure,
-            read_parquet_from_azure,
-        )
-
-        try:
-            if key.endswith(".csv"):
-                return read_csv_from_azure(client, key, container_name, storage_options)
-
-            if key.endswith(JSON_SUPPORTED_TYPES):
-                return read_json_from_azure(client, key, container_name)
-
-            if key.endswith(".parquet"):
-                return read_parquet_from_azure(
-                    client, key, container_name, storage_options
-                )
-
-            if key.endswith(".tsv"):
-                return read_csv_from_azure(
-                    client, key, container_name, storage_options, sep="\t"
-                )
-
-            if key.endswith(".avro"):
-                return read_avro_from_azure(client, key, container_name)
-
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.error(
-                f"Unexpected exception get in azure for file [{key}] for {container_name}: {exc}"
-            )
-        return None
-
-    @staticmethod
-    def get_s3_files(client, key, bucket_name, client_kwargs=None):
-        """
-        Fetch S3 Bucket files
-        """
-        from metadata.utils.s3_utils import (  # pylint: disable=import-outside-toplevel
-            read_avro_from_s3,
-            read_csv_from_s3,
-            read_json_from_s3,
-            read_parquet_from_s3,
-            read_tsv_from_s3,
-        )
-
-        try:
-            if key.endswith(".csv"):
-                return read_csv_from_s3(client, key, bucket_name)
-
-            if key.endswith(".tsv"):
-                return read_tsv_from_s3(client, key, bucket_name)
-
-            if key.endswith(JSON_SUPPORTED_TYPES):
-                return read_json_from_s3(client, key, bucket_name)
-
-            if key.endswith(".parquet"):
-                return read_parquet_from_s3(client_kwargs, key, bucket_name)
-
-            if key.endswith(".avro"):
-                return read_avro_from_s3(client, key, bucket_name)
-
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.error(
-                f"Unexpected exception to get S3 file [{key}] from bucket [{bucket_name}]: {exc}"
-            )
-        return None
-
-    @staticmethod
     def _parse_complex_column(
         data_frame,
         column,
         final_column_list: List[Column],
         complex_col_dict: dict,
         processed_complex_columns: set,
     ) -> None:
@@ -620,47 +446,62 @@
         """
         try:
             # pylint: disable=bad-str-strip-call
             column_name = str(column).strip(COMPLEX_COLUMN_SEPARATOR)
             col_hierarchy = tuple(column_name.split(COMPLEX_COLUMN_SEPARATOR))
             parent_col: Optional[Column] = None
             root_col: Optional[Column] = None
+
+            # here we are only processing col_hierarchy till [:-1]
+            # because all the column/node before -1 would be treated
+            # as a record and the column at -1 would be the column
+            # having a primitive datatype
+            # for example if col_hierarchy is ("image", "properties", "size")
+            # then image would be the record having child properties which is
+            # also a record  but the "size" will not be handled in this loop
+            # as it will be of primitive type for ex. int
             for index, col_name in enumerate(col_hierarchy[:-1]):
+
                 if complex_col_dict.get(col_hierarchy[: index + 1]):
+                    # if we have already seen this column fetch that column
                     parent_col = complex_col_dict.get(col_hierarchy[: index + 1])
                 else:
+                    # if we have not seen this column than create the column and
+                    # append to the parent if available
                     intermediate_column = Column(
                         name=truncate_column_name(col_name),
                         displayName=col_name,
                         dataType=DataType.RECORD.value,
                         children=[],
                         dataTypeDisplay=DataType.RECORD.value,
                     )
                     if parent_col:
                         parent_col.children.append(intermediate_column)
                         root_col = parent_col
                     parent_col = intermediate_column
                     complex_col_dict[col_hierarchy[: index + 1]] = parent_col
 
-            # use String by default
+            # prepare the leaf node
+            # use String as default type
             data_type = DataType.STRING.value
             if hasattr(data_frame[column], "dtypes"):
                 data_type = DATALAKE_DATA_TYPES.get(
                     data_frame[column].dtypes.name, DataType.STRING.value
                 )
             leaf_column = Column(
                 name=col_hierarchy[-1],
                 dataType=data_type,
                 dataTypeDisplay=data_type,
             )
             parent_col.children.append(leaf_column)
 
-            if col_hierarchy[0] not in processed_complex_columns and root_col:
+            # finally add the top level node in the column list
+            if col_hierarchy[0] not in processed_complex_columns:
                 processed_complex_columns.add(col_hierarchy[0])
-                final_column_list.append(root_col)
+                final_column_list.append(root_col or parent_col)
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Unexpected exception parsing column [{column}]: {exc}")
 
     @staticmethod
     def fetch_col_types(data_frame, column_name):
         data_type = DATALAKE_DATA_TYPES.get(
@@ -677,20 +518,22 @@
                 logger.warning(
                     f"Failed to disinguish data type for column {column_name}, Falling back to {data_type}, exc: {err}"
                 )
                 logger.debug(traceback.format_exc())
         return data_type
 
     @staticmethod
-    def get_columns(data_frame):
+    def get_columns(data_frame: list):
         """
         method to process column details
         """
+        data_frame = clean_dataframe(data_frame)
         cols = []
         complex_col_dict = {}
+
         processed_complex_columns = set()
         if hasattr(data_frame, "columns"):
             df_columns = list(data_frame.columns)
             for column in df_columns:
                 if COMPLEX_COLUMN_SEPARATOR in column:
                     DatalakeSource._parse_complex_column(
                         data_frame,
@@ -732,14 +575,15 @@
 
     def standardize_table_name(
         self, schema: str, table: str  # pylint: disable=unused-argument
     ) -> str:
         return table
 
     def check_valid_file_type(self, key_name):
-        if key_name.endswith(DATALAKE_SUPPORTED_FILE_TYPES):
-            return True
+        for supported_types in SupportedTypes:
+            if key_name.endswith(supported_types.value):
+                return True
         return False
 
     def close(self):
         if isinstance(self.service_connection.configSource, AzureConfig):
             self.client.close()
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/datalake/models.py`

 * *Files 14% similar despite different names*

```diff
@@ -24,8 +24,17 @@
     In case of avro files we can directly get the column details and
     we do not need the dataframe to parse the metadata but profiler
     need the dataframes hence this model binds the columns details and dataframe
     which can be used by both profiler and metadata ingestion
     """
 
     columns: Optional[List[Column]]
-    dataframes: Optional[List[Any]]  # pandas.Dataframe does not have any validators
+    dataframes: Optional[Any]  # pandas.Dataframe does not have any validators
+
+
+class DatalakeTableSchemaWrapper(BaseModel):
+    """
+    Instead of sending the whole Table model from profiler, we send only key and bucket name using this model
+    """
+
+    key: str
+    bucket_name: str
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/datalake/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/avro_dispatch.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,32 +6,45 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Module to define helper methods for datalake
+Module to define helper methods for datalake and to fetch data and metadata 
+from Avro file formats
 """
 
-import gzip
+
 import io
-import json
-import zipfile
-from typing import List, Union
+from functools import singledispatch
+from typing import Any
 
 from avro.datafile import DataFileReader
 from avro.errors import InvalidAvroBinaryEncoding
 from avro.io import DatumReader
 
 from metadata.generated.schema.entity.data.table import Column
+from metadata.generated.schema.entity.services.connections.database.datalake.azureConfig import (
+    AzureConfig,
+)
+from metadata.generated.schema.entity.services.connections.database.datalake.gcsConfig import (
+    GCSConfig,
+)
+from metadata.generated.schema.entity.services.connections.database.datalake.s3Config import (
+    S3Config,
+)
 from metadata.generated.schema.type.schema import DataTypeTopic
 from metadata.ingestion.source.database.datalake.models import DatalakeColumnWrapper
 from metadata.parsers.avro_parser import parse_avro_schema
 from metadata.utils.constants import UTF_8
+from metadata.utils.datalake.common import (
+    DatalakeFileFormatException,
+    dataframe_to_chunks,
+)
 from metadata.utils.logger import utils_logger
 
 logger = utils_logger()
 
 PD_AVRO_FIELD_MAP = {
     DataTypeTopic.BOOLEAN.value: "bool",
     DataTypeTopic.INT.value: "int",
@@ -39,72 +52,62 @@
     DataTypeTopic.FLOAT.value: "float",
     DataTypeTopic.DOUBLE.value: "float",
     DataTypeTopic.TIMESTAMP.value: "float",
     DataTypeTopic.TIMESTAMPZ.value: "float",
 }
 
 AVRO_SCHEMA = "avro.schema"
-COMPLEX_COLUMN_SEPARATOR = "_##"
 
 
 def read_from_avro(
     avro_text: bytes,
-) -> Union[DatalakeColumnWrapper, List]:
+) -> DatalakeColumnWrapper:
     """
     Method to parse the avro data from storage sources
     """
     # pylint: disable=import-outside-toplevel
     from pandas import DataFrame, Series
 
     try:
         elements = DataFileReader(io.BytesIO(avro_text), DatumReader())
         if elements.meta.get(AVRO_SCHEMA):
             return DatalakeColumnWrapper(
                 columns=parse_avro_schema(
                     schema=elements.meta.get(AVRO_SCHEMA).decode(UTF_8), cls=Column
                 ),
-                dataframes=[DataFrame.from_records(elements)],
+                dataframes=DataFrame.from_records(elements),
             )
-        return [DataFrame.from_records(elements)]
+        return DatalakeColumnWrapper(dataframes=DataFrame.from_records(elements))
     except (AssertionError, InvalidAvroBinaryEncoding):
         columns = parse_avro_schema(schema=avro_text, cls=Column)
         field_map = {
             col.name.__root__: Series(PD_AVRO_FIELD_MAP.get(col.dataType.value, "str"))
             for col in columns
         }
-        return DatalakeColumnWrapper(columns=columns, dataframes=[DataFrame(field_map)])
+        return DatalakeColumnWrapper(columns=columns, dataframes=DataFrame(field_map))
+
 
+@singledispatch
+def read_avro_dispatch(config_source: Any, key: str, **kwargs):
+    raise DatalakeFileFormatException(config_source=config_source, file_name=key)
 
-def _get_json_text(key: str, text: bytes, decode: bool) -> str:
-    if key.endswith(".gz"):
-        return gzip.decompress(text)
-    if key.endswith(".zip"):
-        with zipfile.ZipFile(io.BytesIO(text)) as zip_file:
-            return zip_file.read(zip_file.infolist()[0]).decode(UTF_8)
-    if decode:
-        return text.decode(UTF_8)
-    return text
-
-
-def read_from_json(
-    key: str, json_text: str, sample_size: int = 100, decode: bool = False
-) -> List:
+
+@read_avro_dispatch.register
+def _(_: GCSConfig, key: str, bucket_name: str, client, **kwargs):
     """
-    Read the json file from the azure container and return a dataframe
+    Read the avro file from the gcs bucket and return a dataframe
     """
+    avro_text = client.get_bucket(bucket_name).get_blob(key).download_as_string()
+    return dataframe_to_chunks(read_from_avro(avro_text).dataframes)
 
-    # pylint: disable=import-outside-toplevel
-    from pandas import json_normalize
 
-    json_text = _get_json_text(key, json_text, decode)
-    try:
-        data = json.loads(json_text)
-    except json.decoder.JSONDecodeError:
-        logger.debug("Failed to read as JSON object trying to read as JSON Lines")
-        data = [
-            json.loads(json_obj)
-            for json_obj in json_text.strip().split("\n")[:sample_size]
-        ]
-
-    if isinstance(data, list):
-        return [json_normalize(data[:sample_size], sep=COMPLEX_COLUMN_SEPARATOR)]
-    return [json_normalize(data, sep=COMPLEX_COLUMN_SEPARATOR)]
+@read_avro_dispatch.register
+def _(_: S3Config, key: str, bucket_name: str, client, **kwargs):
+    avro_text = client.get_object(Bucket=bucket_name, Key=key)["Body"].read()
+    return dataframe_to_chunks(read_from_avro(avro_text).dataframes)
+
+
+@read_avro_dispatch.register
+def _(_: AzureConfig, key: str, bucket_name: str, client, **kwargs):
+    container_client = client.get_container_client(bucket_name)
+    avro_text = container_client.get_blob_client(key).download_blob().readall()
+    return dataframe_to_chunks(read_from_avro(avro_text).dataframes)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/db2/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/db2/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/db2/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/db2/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dbt/dbt_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_service.py`

 * *Files 9% similar despite different names*

```diff
@@ -18,27 +18,34 @@
 from dbt_artifacts_parser.parser import parse_catalog, parse_manifest, parse_run_results
 
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
 from metadata.generated.schema.api.tests.createTestDefinition import (
     CreateTestDefinitionRequest,
 )
-from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
+from metadata.generated.schema.metadataIngestion.dbtPipeline import DbtPipeline
 from metadata.generated.schema.tests.basic import TestCaseResult
 from metadata.ingestion.api.source import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
     TopologyNode,
     create_source_context,
 )
 from metadata.ingestion.source.database.database_service import DataModelLink
-from metadata.utils.dbt_config import DbtFiles, DbtObjects, get_dbt_details
+from metadata.ingestion.source.database.dbt.dbt_config import get_dbt_details
+from metadata.ingestion.source.database.dbt.models import (
+    DbtFiles,
+    DbtFilteredModel,
+    DbtObjects,
+)
+from metadata.utils import fqn
+from metadata.utils.filters import filter_by_database, filter_by_schema, filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class DbtServiceTopology(ServiceTopology):
     """
@@ -102,45 +109,41 @@
             ),
         ],
     )
     process_dbt_tests = TopologyNode(
         producer="get_dbt_tests",
         stages=[
             NodeStage(
-                type_=CreateTestSuiteRequest,
-                processor="create_dbt_tests_suite",
-                ack_sink=False,
-            ),
-            NodeStage(
                 type_=CreateTestDefinitionRequest,
-                processor="create_dbt_tests_suite_definition",
+                processor="create_dbt_tests_definition",
                 ack_sink=False,
             ),
             NodeStage(
                 type_=CreateTestCaseRequest,
                 processor="create_dbt_test_case",
                 ack_sink=False,
             ),
             NodeStage(
                 type_=TestCaseResult,
-                processor="update_dbt_test_result",
+                processor="add_dbt_test_result",
                 ack_sink=False,
                 nullable=True,
             ),
         ],
     )
 
 
 class DbtServiceSource(TopologyRunnerMixin, Source, ABC):
     """
     Class for defining the topology of the DBT source
     """
 
     topology = DbtServiceTopology()
     context = create_source_context(topology)
+    source_config: DbtPipeline
 
     def remove_manifest_non_required_keys(self, manifest_dict: dict):
         """
         Method to remove the non required keys from manifest file
         """
         # To ensure smooth ingestion of data,
         # we are selectively processing the metadata, nodes, and sources from the manifest file
@@ -154,17 +157,15 @@
                 key: {}
                 for key in manifest_dict
                 if key.lower() not in required_manifest_keys
             }
         )
 
     def get_dbt_files(self) -> DbtFiles:
-        dbt_files = get_dbt_details(
-            self.source_config.dbtConfigSource  # pylint: disable=no-member
-        )
+        dbt_files = get_dbt_details(self.source_config.dbtConfigSource)
         self.context.dbt_files = dbt_files
         yield dbt_files
 
     def get_dbt_objects(self) -> DbtObjects:
         self.remove_manifest_non_required_keys(
             manifest_dict=self.context.dbt_files.dbt_manifest
         )
@@ -230,31 +231,52 @@
         """
         Prepare the DBT tests
         """
         for _, dbt_test in self.context.dbt_tests.items():
             yield dbt_test
 
     @abstractmethod
-    def create_dbt_tests_suite(self, dbt_test: dict) -> CreateTestSuiteRequest:
-        """
-        Method to add the DBT tests suites
-        """
-
-    @abstractmethod
-    def create_dbt_tests_suite_definition(
+    def create_dbt_tests_definition(
         self, dbt_test: dict
     ) -> CreateTestDefinitionRequest:
         """
         Method to add DBT test definitions
         """
 
     @abstractmethod
     def create_dbt_test_case(self, dbt_test: dict) -> CreateTestCaseRequest:
         """
         After test suite and test definitions have been processed, add the tests cases info
         """
 
     @abstractmethod
-    def update_dbt_test_result(self, dbt_test: dict):
+    def add_dbt_test_result(self, dbt_test: dict):
         """
         After test cases has been processed, add the tests results info
         """
+
+    def is_filtered(
+        self, database_name: str, schema_name: str, table_name: str
+    ) -> DbtFilteredModel:
+        """
+        Function used to identify the filtered models
+        """
+        # pylint: disable=protected-access
+        model_fqn = fqn._build(str(database_name), str(schema_name), str(table_name))
+        is_filtered = False
+        reason = None
+        message = None
+
+        if filter_by_table(self.source_config.tableFilterPattern, table_name):
+            reason = "table"
+            is_filtered = True
+        if filter_by_schema(self.source_config.schemaFilterPattern, schema_name):
+            reason = "schema"
+            is_filtered = True
+        if filter_by_database(self.source_config.databaseFilterPattern, database_name):
+            reason = "database"
+            is_filtered = True
+        if is_filtered:
+            message = f"Model Filtered due to {reason} filter pattern"
+        return DbtFilteredModel(
+            is_filtered=is_filtered, message=message, model_fqn=model_fqn
+        )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dbt/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/metadata.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,27 +8,22 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 DBT source methods.
 """
 import traceback
-from enum import Enum
+from datetime import datetime
 from typing import Iterable, List, Optional, Union
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
 from metadata.generated.schema.api.tests.createTestDefinition import (
     CreateTestDefinitionRequest,
 )
-from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
 from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.table import (
     Column,
     DataModel,
     ModelType,
     Table,
 )
@@ -38,122 +33,75 @@
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.generated.schema.tests.basic import (
+    TestCaseFailureStatus,
+    TestCaseFailureStatusType,
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testDefinition import (
     EntityType,
     TestDefinition,
     TestPlatform,
 )
-from metadata.generated.schema.tests.testSuite import TestSuite
-from metadata.generated.schema.type.basic import FullyQualifiedEntityName
+from metadata.generated.schema.type.basic import FullyQualifiedEntityName, Timestamp
 from metadata.generated.schema.type.entityLineage import EntitiesEdge
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.lineage.models import ConnectionTypeDialectMapper
 from metadata.ingestion.lineage.sql_lineage import get_lineage_by_query
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
 from metadata.ingestion.source.database.database_service import DataModelLink
+from metadata.ingestion.source.database.dbt.constants import (
+    REQUIRED_CATALOG_KEYS,
+    REQUIRED_MANIFEST_KEYS,
+    DbtCommonEnum,
+    DbtTestFailureEnum,
+    DbtTestSuccessEnum,
+    SkipResourceTypeEnum,
+)
 from metadata.ingestion.source.database.dbt.dbt_service import (
     DbtFiles,
     DbtObjects,
     DbtServiceSource,
 )
-from metadata.utils import entity_link, fqn, tag_utils
+from metadata.ingestion.source.database.dbt.dbt_utils import (
+    check_ephemeral_node,
+    check_or_create_test_suite,
+    create_test_case_parameter_definitions,
+    create_test_case_parameter_values,
+    generate_entity_link,
+    get_corrected_name,
+    get_data_model_path,
+    get_dbt_compiled_query,
+    get_dbt_model_name,
+    get_dbt_raw_query,
+)
+from metadata.utils import fqn
 from metadata.utils.elasticsearch import get_entity_from_es_result
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 
 logger = ingestion_logger()
 
-# Based on https://schemas.getdbt.com/dbt/manifest/v7/index.html
-REQUIRED_MANIFEST_KEYS = ["name", "schema", "resource_type"]
-
-# Based on https://schemas.getdbt.com/dbt/catalog/v1.json
-REQUIRED_CATALOG_KEYS = ["name", "type", "index"]
-
-NONE_KEYWORDS_LIST = ["none", "null"]
-
-
-class SkipResourceTypeEnum(Enum):
-    """
-    Enum for nodes to be skipped
-    """
-
-    ANALYSIS = "analysis"
-    TEST = "test"
-
-
-class CompiledQueriesEnum(Enum):
-    """
-    Enum for Compiled Queries
-    """
-
-    COMPILED_CODE = "compiled_code"
-    COMPILED_SQL = "compiled_sql"
-
-
-class RawQueriesEnum(Enum):
-    """
-    Enum for Raw Queries
-    """
-
-    RAW_CODE = "raw_code"
-    RAW_SQL = "raw_sql"
-
-
-class DbtTestSuccessEnum(Enum):
-    """
-    Enum for success messages of dbt tests
-    """
-
-    SUCCESS = "success"
-    PASS = "pass"
-
-
-class DbtTestFailureEnum(Enum):
-    """
-    Enum for failure message of dbt tests
-    """
-
-    FAILURE = "failure"
-    FAIL = "fail"
-
-
-class DbtCommonEnum(Enum):
-    """
-    Common enum for dbt
-    """
-
-    OWNER = "owner"
-    NODES = "nodes"
-    SOURCES = "sources"
-    RESOURCETYPE = "resource_type"
-    MANIFEST_NODE = "manifest_node"
-    UPSTREAM = "upstream"
-    RESULTS = "results"
-    TEST_SUITE_NAME = "test_suite_name"
-    DBT_TEST_SUITE = "DBT_TEST_SUITE"
-
 
 class InvalidServiceException(Exception):
     """
     The service passed in config is not found
     """
 
 
-class DbtSource(DbtServiceSource):  # pylint: disable=too-many-public-methods
+class DbtSource(DbtServiceSource):
     """
     Class defines method to extract metadata from DBT
     """
 
     def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
         super().__init__()
         self.config = config
@@ -282,15 +230,15 @@
                             f"Unable to find the node or columns in the catalog file for dbt node: {key}"
                         )
 
     def yield_dbt_tags(
         self, dbt_objects: DbtObjects
     ) -> Iterable[OMetaTagAndClassification]:
         """
-        Create and yeild tags from DBT
+        Create and yield tags from DBT
         """
         if (
             self.source_config.dbtConfigSource
             and dbt_objects.dbt_manifest
             and self.source_config.includeTags
         ):
             manifest_entities = {
@@ -328,26 +276,23 @@
                         self.metadata,
                         Tag,
                         classification_name=self.tag_classification_name,
                         tag_name=tag_name,
                     )
                     for tag_name in dbt_tags_list
                 ]
-                for tag_label in dbt_tag_labels or []:
-                    yield OMetaTagAndClassification(
-                        classification_request=CreateClassificationRequest(
-                            name=self.tag_classification_name,
-                            description="dbt classification",
-                        ),
-                        tag_request=CreateTagRequest(
-                            classification=self.tag_classification_name,
-                            name=tag_label.split(fqn.FQN_SEPARATOR)[1],
-                            description="dbt Tags",
-                        ),
-                    )
+                yield from get_ometa_tag_and_classification(
+                    tags=[
+                        tag_label.split(fqn.FQN_SEPARATOR)[1]
+                        for tag_label in dbt_tag_labels
+                    ],
+                    classification_name=self.tag_classification_name,
+                    tag_description="dbt Tags",
+                    classification_desciption="dbt classification",
+                )
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Unexpected exception creating DBT tags: {exc}")
 
     def add_dbt_tests(
         self, key: str, manifest_node, manifest_entities, dbt_objects: DbtObjects
     ) -> None:
@@ -363,14 +308,15 @@
                 item
                 for item in dbt_objects.dbt_run_results.results
                 if item.unique_id == key
             ),
             None,
         )
 
+    # pylint: disable=too-many-locals
     def yield_data_models(self, dbt_objects: DbtObjects) -> Iterable[DataModelLink]:
         """
         Yield the data models
         """
         if self.source_config.dbtConfigSource and dbt_objects.dbt_manifest:
             logger.debug("Parsing DBT Data Models")
             manifest_entities = {
@@ -397,54 +343,67 @@
                             key,
                             manifest_node=manifest_node,
                             manifest_entities=manifest_entities,
                             dbt_objects=dbt_objects,
                         )
                         continue
 
+                    # Skip the ephemeral nodes since it is not materialized
+                    if check_ephemeral_node(manifest_node):
+                        logger.debug(f"Skipping ephemeral DBT node: {key}.")
+                        continue
+
                     # Skip the analysis and test nodes
                     if manifest_node.resource_type.value in [
                         item.value for item in SkipResourceTypeEnum
                     ]:
                         logger.debug(f"Skipping DBT node: {key}.")
                         continue
 
-                    model_name = (
-                        manifest_node.alias
-                        if hasattr(manifest_node, "alias") and manifest_node.alias
-                        else manifest_node.name
+                    model_name = get_dbt_model_name(manifest_node)
+
+                    # Filter the dbt models based on filter patterns
+                    filter_model = self.is_filtered(
+                        database_name=get_corrected_name(manifest_node.database),
+                        schema_name=get_corrected_name(manifest_node.schema_),
+                        table_name=model_name,
                     )
+                    if filter_model.is_filtered:
+                        self.status.filter(filter_model.model_fqn, filter_model.message)
+                        continue
+
                     logger.debug(f"Processing DBT node: {model_name}")
 
                     catalog_node = None
                     if dbt_objects.dbt_catalog:
                         catalog_node = catalog_entities.get(key)
 
                     dbt_table_tags_list = None
                     if manifest_node.tags:
-                        dbt_table_tags_list = tag_utils.get_tag_labels(
+                        dbt_table_tags_list = get_tag_labels(
                             metadata=self.metadata,
                             tags=manifest_node.tags,
                             classification_name=self.tag_classification_name,
                             include_tags=self.source_config.includeTags,
                         )
 
-                    dbt_compiled_query = self.get_dbt_compiled_query(manifest_node)
-                    dbt_raw_query = self.get_dbt_raw_query(manifest_node)
+                    dbt_compiled_query = get_dbt_compiled_query(manifest_node)
+                    dbt_raw_query = get_dbt_raw_query(manifest_node)
 
                     # Get the table entity from ES
                     # TODO: Change to get_by_name once the postgres case sensitive calls is fixed
                     table_fqn = fqn.build(
                         self.metadata,
                         entity_type=Table,
                         service_name=self.config.serviceName,
-                        database_name=self.get_corrected_name(manifest_node.database),
-                        schema_name=self.get_corrected_name(manifest_node.schema_),
+                        database_name=get_corrected_name(manifest_node.database),
+                        schema_name=get_corrected_name(manifest_node.schema_),
                         table_name=model_name,
                     )
+
                     table_entity: Optional[
                         Union[Table, List[Table]]
                     ] = get_entity_from_es_result(
                         entity_list=self.metadata.es_search_from_fqn(
                             entity_type=Table, fqn_search_string=table_fqn
                         ),
                         fetch_multiple_entities=False,
@@ -454,17 +413,15 @@
                         data_model_link = DataModelLink(
                             table_entity=table_entity,
                             datamodel=DataModel(
                                 modelType=ModelType.DBT,
                                 description=manifest_node.description
                                 if manifest_node.description
                                 else None,
-                                path=self.get_data_model_path(
-                                    manifest_node=manifest_node
-                                ),
+                                path=get_data_model_path(manifest_node=manifest_node),
                                 rawSql=dbt_raw_query if dbt_raw_query else "",
                                 sql=dbt_compiled_query if dbt_compiled_query else "",
                                 columns=self.parse_data_model_columns(
                                     manifest_node, catalog_node
                                 ),
                                 upstream=self.parse_upstream_nodes(
                                     manifest_entities, manifest_node
@@ -486,60 +443,66 @@
                         )
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
                     logger.warning(
                         f"Unexpected exception parsing DBT node:{model_name} - {exc}"
                     )
 
-    def get_corrected_name(self, name: Optional[str]):
-        correct_name = None
-        if name:
-            correct_name = None if name.lower() in NONE_KEYWORDS_LIST else name
-        return correct_name
-
-    def get_data_model_path(self, manifest_node):
-        datamodel_path = None
-        if manifest_node.original_file_path:
-            if hasattr(manifest_node, "root_path") and manifest_node.root_path:
-                datamodel_path = (
-                    f"{manifest_node.root_path}/{manifest_node.original_file_path}"
-                )
-            else:
-                datamodel_path = manifest_node.original_file_path
-        return datamodel_path
-
     def parse_upstream_nodes(self, manifest_entities, dbt_node):
         """
         Method to fetch the upstream nodes
         """
         upstream_nodes = []
         if (
             hasattr(dbt_node, "depends_on")
             and hasattr(dbt_node.depends_on, "nodes")
             and dbt_node.depends_on
             and dbt_node.depends_on.nodes
         ):
             for node in dbt_node.depends_on.nodes:
                 try:
                     parent_node = manifest_entities[node]
-                    table_name = (
-                        parent_node.alias
-                        if hasattr(parent_node, "alias") and parent_node.alias
-                        else parent_node.name
-                    )
-                    parent_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Table,
-                        service_name=self.config.serviceName,
-                        database_name=self.get_corrected_name(parent_node.database),
-                        schema_name=self.get_corrected_name(parent_node.schema_),
+                    table_name = get_dbt_model_name(parent_node)
+
+                    filter_model = self.is_filtered(
+                        database_name=get_corrected_name(parent_node.database),
+                        schema_name=get_corrected_name(parent_node.schema_),
                         table_name=table_name,
                     )
-                    if parent_fqn:
-                        upstream_nodes.append(parent_fqn)
+                    if filter_model.is_filtered:
+                        continue
+
+                    # check if the node is an ephemeral node
+                    # Recursively store the upstream of the ephemeral node in the upstream list
+                    if check_ephemeral_node(parent_node):
+                        upstream_nodes.extend(
+                            self.parse_upstream_nodes(manifest_entities, parent_node)
+                        )
+                    else:
+                        parent_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Table,
+                            service_name=self.config.serviceName,
+                            database_name=get_corrected_name(parent_node.database),
+                            schema_name=get_corrected_name(parent_node.schema_),
+                            table_name=table_name,
+                        )
+
+                        # check if the parent table exists in OM before adding it to the upstream list
+                        # TODO: Change to get_by_name once the postgres case sensitive calls is fixed
+                        parent_table_entity: Optional[
+                            Union[Table, List[Table]]
+                        ] = get_entity_from_es_result(
+                            entity_list=self.metadata.es_search_from_fqn(
+                                entity_type=Table, fqn_search_string=parent_fqn
+                            ),
+                            fetch_multiple_entities=False,
+                        )
+                        if parent_table_entity:
+                            upstream_nodes.append(parent_fqn)
                 except Exception as exc:  # pylint: disable=broad-except
                     logger.debug(traceback.format_exc())
                     logger.warning(
                         f"Failed to parse the DBT node {node} to get upstream nodes: {exc}"
                     )
                     continue
         return upstream_nodes
@@ -577,15 +540,15 @@
                             if catalog_column
                             else manifest_column.data_type
                         ),
                         dataLength=1,
                         ordinalPosition=catalog_column.index
                         if catalog_column
                         else None,
-                        tags=tag_utils.get_tag_labels(
+                        tags=get_tag_labels(
                             metadata=self.metadata,
                             tags=manifest_column.tags,
                             classification_name=self.tag_classification_name,
                             include_tags=self.source_config.includeTags,
                         ),
                     )
                 )
@@ -724,137 +687,118 @@
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.warning(
                     f"Failed to parse the node {table_entity.fullyQualifiedName.__root__} "
                     f"to update dbt description: {exc}"
                 )
 
-    def create_dbt_tests_suite(
-        self, dbt_test: dict
-    ) -> Iterable[CreateTestSuiteRequest]:
-        """
-        Method to add the DBT tests suites
-        """
-        try:
-            manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
-            if manifest_node:
-                test_name = manifest_node.name
-                logger.debug(f"Processing DBT Tests Suite for node: {test_name}")
-                test_suite_name = manifest_node.meta.get(
-                    DbtCommonEnum.TEST_SUITE_NAME.value,
-                    DbtCommonEnum.DBT_TEST_SUITE.value,
-                )
-                test_suite_desciption = manifest_node.meta.get(
-                    "test_suite_desciption", ""
-                )
-                check_test_suite_exists = self.metadata.get_by_name(
-                    fqn=test_suite_name, entity=TestSuite
-                )
-                if not check_test_suite_exists:
-                    yield CreateTestSuiteRequest(
-                        name=test_suite_name,
-                        description=test_suite_desciption,
-                    )
-        except Exception as err:  # pylint: disable=broad-except
-            logger.debug(traceback.format_exc())
-            logger.error(f"Failed to parse the node to capture tests {err}")
-
-    def create_dbt_tests_suite_definition(
+    def create_dbt_tests_definition(
         self, dbt_test: dict
     ) -> Iterable[CreateTestDefinitionRequest]:
         """
         A Method to add DBT test definitions
         """
         try:
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
                 logger.debug(
-                    f"Processing DBT Tests Suite Definition for node: {manifest_node.name}"
+                    f"Processing DBT Tests Definition for node: {manifest_node.name}"
                 )
                 check_test_definition_exists = self.metadata.get_by_name(
                     fqn=manifest_node.name,
                     entity=TestDefinition,
                 )
                 if not check_test_definition_exists:
-                    column_name = manifest_node.column_name
-                    if column_name:
+                    entity_type = EntityType.TABLE
+                    if (
+                        hasattr(manifest_node, "column_name")
+                        and manifest_node.column_name
+                    ):
                         entity_type = EntityType.COLUMN
-                    else:
-                        entity_type = EntityType.TABLE
                     yield CreateTestDefinitionRequest(
                         name=manifest_node.name,
                         description=manifest_node.description,
                         entityType=entity_type,
                         testPlatforms=[TestPlatform.DBT],
-                        parameterDefinition=self.create_test_case_parameter_definitions(
+                        parameterDefinition=create_test_case_parameter_definitions(
                             manifest_node
                         ),
+                        displayName=None,
+                        owner=None,
                     )
         except Exception as err:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.error(f"Failed to parse the node to capture tests {err}")
 
     def create_dbt_test_case(self, dbt_test: dict) -> Iterable[CreateTestCaseRequest]:
         """
         After test suite and test definitions have been processed, add the tests cases info
         """
         try:
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
-                logger.debug(
-                    f"Processing DBT Test Case Definition for node: {manifest_node.name}"
-                )
-                entity_link_list = self.generate_entity_link(dbt_test)
+                logger.debug(f"Processing DBT Test Case for node: {manifest_node.name}")
+                entity_link_list = generate_entity_link(dbt_test)
                 for entity_link_str in entity_link_list:
-                    test_suite_name = manifest_node.meta.get(
-                        DbtCommonEnum.TEST_SUITE_NAME.value,
-                        DbtCommonEnum.DBT_TEST_SUITE.value,
+                    test_suite = check_or_create_test_suite(
+                        self.metadata, entity_link_str
                     )
                     yield CreateTestCaseRequest(
                         name=manifest_node.name,
                         description=manifest_node.description,
                         testDefinition=FullyQualifiedEntityName(
                             __root__=manifest_node.name
                         ),
                         entityLink=entity_link_str,
-                        testSuite=FullyQualifiedEntityName(__root__=test_suite_name),
-                        parameterValues=self.create_test_case_parameter_values(
-                            dbt_test
-                        ),
+                        testSuite=test_suite.fullyQualifiedName,
+                        parameterValues=create_test_case_parameter_values(dbt_test),
+                        displayName=None,
+                        owner=None,
                     )
         except Exception as err:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Failed to parse the node {manifest_node.name} to capture tests {err}"
             )
 
-    def update_dbt_test_result(self, dbt_test: dict):
+    # pylint: disable=too-many-locals
+    def add_dbt_test_result(self, dbt_test: dict):
         """
         After test cases has been processed, add the tests results info
         """
         try:
             # Process the Test Status
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
                 logger.debug(
-                    f"Processing DBT Test Case Results for node: {manifest_node.name}"
+                    f"Adding DBT Test Case Results for node: {manifest_node.name}"
                 )
                 dbt_test_result = dbt_test.get(DbtCommonEnum.RESULTS.value)
                 test_case_status = TestCaseStatus.Aborted
                 test_result_value = 0
+                test_case_failure_status = TestCaseFailureStatus()  # type: ignore
                 if dbt_test_result.status.value in [
                     item.value for item in DbtTestSuccessEnum
                 ]:
                     test_case_status = TestCaseStatus.Success
                     test_result_value = 1
                 elif dbt_test_result.status.value in [
                     item.value for item in DbtTestFailureEnum
                 ]:
                     test_case_status = TestCaseStatus.Failed
                     test_result_value = 0
+                    test_case_failure_status = TestCaseFailureStatus(
+                        testCaseFailureStatusType=TestCaseFailureStatusType.New,
+                        testCaseFailureReason=None,
+                        testCaseFailureComment=None,
+                        updatedAt=Timestamp(
+                            __root__=int(datetime.utcnow().timestamp() * 1000)
+                        ),
+                        updatedBy=None,
+                    )
 
                 # Process the Test Timings
                 dbt_test_timings = dbt_test_result.timing
                 dbt_test_completed_at = None
                 for dbt_test_timing in dbt_test_timings:
                     if dbt_test_timing.name == "execute":
                         dbt_test_completed_at = dbt_test_timing.completed_at
@@ -868,90 +812,39 @@
                     testCaseStatus=test_case_status,
                     testResultValue=[
                         TestResultValue(
                             name=dbt_test_result.unique_id,
                             value=str(test_result_value),
                         )
                     ],
+                    testCaseFailureStatus=test_case_failure_status,
+                    sampleData=None,
+                    result=None,
                 )
 
                 # Create the test case fqns and add the results
                 for table_fqn in dbt_test.get(DbtCommonEnum.UPSTREAM.value):
                     source_elements = table_fqn.split(fqn.FQN_SEPARATOR)
                     test_case_fqn = fqn.build(
                         self.metadata,
                         entity_type=TestCase,
                         service_name=self.config.serviceName,
                         database_name=source_elements[1],
                         schema_name=source_elements[2],
                         table_name=source_elements[3],
-                        column_name=manifest_node.column_name,
+                        column_name=manifest_node.column_name
+                        if hasattr(manifest_node, "column_name")
+                        else None,
                         test_case_name=manifest_node.name,
                     )
                     self.metadata.add_test_case_results(
                         test_results=test_case_result,
                         test_case_fqn=test_case_fqn,
                     )
         except Exception as err:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Failed to capture tests results for node: {manifest_node.name} {err}"
             )
 
-    def create_test_case_parameter_definitions(self, dbt_test):
-        test_case_param_definition = [
-            {
-                "name": dbt_test.test_metadata.name,
-                "displayName": dbt_test.test_metadata.name,
-                "required": False,
-            }
-        ]
-        return test_case_param_definition
-
-    def create_test_case_parameter_values(self, dbt_test):
-        manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
-        values = manifest_node.test_metadata.kwargs.get("values")
-        dbt_test_values = ""
-        if values:
-            dbt_test_values = ",".join(values)
-        test_case_param_values = [
-            {"name": manifest_node.test_metadata.name, "value": dbt_test_values}
-        ]
-        return test_case_param_values
-
-    def generate_entity_link(self, dbt_test):
-        """
-        Method returns entity link
-        """
-        manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
-        entity_link_list = [
-            entity_link.get_entity_link(
-                table_fqn=table_fqn, column_name=manifest_node.column_name
-            )
-            for table_fqn in dbt_test[DbtCommonEnum.UPSTREAM.value]
-        ]
-        return entity_link_list
-
-    def get_dbt_compiled_query(self, mnode) -> Optional[str]:
-        if (
-            hasattr(mnode, CompiledQueriesEnum.COMPILED_CODE.value)
-            and mnode.compiled_code
-        ):
-            return mnode.compiled_code
-        if (
-            hasattr(mnode, CompiledQueriesEnum.COMPILED_SQL.value)
-            and mnode.compiled_sql
-        ):
-            return mnode.compiled_sql
-        logger.debug(f"Unable to get DBT compiled query for node - {mnode.name}")
-        return None
-
-    def get_dbt_raw_query(self, mnode) -> Optional[str]:
-        if hasattr(mnode, RawQueriesEnum.RAW_CODE.value) and mnode.raw_code:
-            return mnode.raw_code
-        if hasattr(mnode, RawQueriesEnum.RAW_SQL.value) and mnode.raw_sql:
-            return mnode.raw_sql
-        logger.debug(f"Unable to get DBT compiled query for node - {mnode.name}")
-        return None
-
     def close(self):
         self.metadata.close()
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/deltalake/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/deltalake/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/deltalake/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/deltalake/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/domodatabase/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/druid/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/druid/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/druid/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/druid/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dynamodb/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/dynamodb/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -191,15 +191,14 @@
         try:
             table = self.dynamodb.Table(table_name)
             columns = self.get_columns(table.attribute_definitions)
 
             table_request = CreateTableRequest(
                 name=table_name,
                 tableType=table_type,
-                description="",
                 columns=columns,
                 tableConstraints=None,
                 databaseSchema=self.context.database_schema.fullyQualifiedName,
             )
             yield table_request
             self.register_record(table_request=table_request)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/glue/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/glue/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/glue/metadata.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Glue source methods.
 """
 import traceback
-from typing import Iterable, List, Optional, Tuple
+from typing import Iterable, Optional, Tuple
 
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
@@ -38,14 +38,20 @@
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.column_helpers import truncate_column_name
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
 from metadata.ingestion.source.database.database_service import DatabaseServiceSource
+from metadata.ingestion.source.database.glue.models import Column as GlueColumn
+from metadata.ingestion.source.database.glue.models import (
+    DatabasePage,
+    StorageDetails,
+    TablePage,
+)
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_database, filter_by_schema, filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
@@ -79,22 +85,22 @@
             )
         return cls(config, metadata_config)
 
     def _get_glue_database_and_schemas(self):
         paginator = self.glue.get_paginator("get_databases")
         paginator_response = paginator.paginate()
         for page in paginator_response:
-            yield page
+            yield DatabasePage(**page)
 
     def _get_glue_tables(self):
         schema_name = self.context.database_schema.name.__root__
         paginator = self.glue.get_paginator("get_tables")
         paginator_response = paginator.paginate(DatabaseName=schema_name)
         for page in paginator_response:
-            yield page
+            yield TablePage(**page)
 
     def get_database_names(self) -> Iterable[str]:
         """
         Default case with a single database.
 
         It might come informed - or not - from the source.
 
@@ -104,42 +110,42 @@
         Catalog ID -> Database
         """
         if self.service_connection.databaseName:
             yield self.service_connection.databaseName
         else:
             database_names = set()
             for page in self._get_glue_database_and_schemas() or []:
-                for schema in page["DatabaseList"]:
+                for schema in page.DatabaseList:
                     try:
                         database_fqn = fqn.build(
                             self.metadata,
                             entity_type=Database,
                             service_name=self.context.database_service.name.__root__,
-                            database_name=schema["CatalogId"],
+                            database_name=schema.CatalogId,
                         )
                         if filter_by_database(
                             self.config.sourceConfig.config.databaseFilterPattern,
                             database_fqn
                             if self.config.sourceConfig.config.useFqnForFiltering
-                            else schema["CatalogId"],
+                            else schema.CatalogId,
                         ):
                             self.status.filter(
                                 database_fqn,
                                 "Database (Catalog ID) Filtered Out",
                             )
                             continue
-                        if schema["CatalogId"] in database_names:
+                        if schema.CatalogId in database_names:
                             continue
-                        database_names.add(schema["CatalogId"])
+                        database_names.add(schema.CatalogId)
                     except Exception as exc:
-                        error = f"Unexpected exception to get database name [{schema}]: {exc}"
+                        error = f"Unexpected exception to get database name [{schema.CatalogId}]: {exc}"
                         logger.debug(traceback.format_exc())
                         logger.warning(error)
                         self.status.failed(
-                            schema.get("CatalogId"), error, traceback.format_exc()
+                            schema.CatalogId, error, traceback.format_exc()
                         )
             yield from database_names
 
     def yield_database(self, database_name: str) -> Iterable[CreateDatabaseRequest]:
         """
         From topology.
         Prepare a database request and pass it to the sink
@@ -150,41 +156,37 @@
         )
 
     def get_database_schema_names(self) -> Iterable[str]:
         """
         return schema names
         """
         for page in self._get_glue_database_and_schemas() or []:
-            for schema in page["DatabaseList"]:
+            for schema in page.DatabaseList:
                 try:
                     schema_fqn = fqn.build(
                         self.metadata,
                         entity_type=DatabaseSchema,
                         service_name=self.context.database_service.name.__root__,
                         database_name=self.context.database.name.__root__,
-                        schema_name=schema["Name"],
+                        schema_name=schema.Name,
                     )
                     if filter_by_schema(
                         self.config.sourceConfig.config.schemaFilterPattern,
                         schema_fqn
                         if self.config.sourceConfig.config.useFqnForFiltering
-                        else schema["Name"],
+                        else schema.Name,
                     ):
                         self.status.filter(schema_fqn, "Schema Filtered Out")
                         continue
-                    yield schema["Name"]
+                    yield schema.Name
                 except Exception as exc:
-                    error = (
-                        f"Unexpected exception to get database schema [{schema}]: {exc}"
-                    )
+                    error = f"Unexpected exception to get database schema [{schema.Name}]: {exc}"
                     logger.debug(traceback.format_exc())
                     logger.warning(error)
-                    self.status.failed(
-                        schema.get("Name"), error, traceback.format_exc()
-                    )
+                    self.status.failed(schema.Name, error, traceback.format_exc())
 
     def yield_database_schema(
         self, schema_name: str
     ) -> Iterable[CreateDatabaseSchemaRequest]:
         """
         From topology.
         Prepare a database schema request and pass it to the sink
@@ -200,79 +202,77 @@
 
         Fetches them up using the context information and
         the inspector set when preparing the db.
 
         :return: tables or views, depending on config
         """
         schema_name = self.context.database_schema.name.__root__
-        all_tables: List[dict] = []
 
         for page in self._get_glue_tables():
-            all_tables += page["TableList"]
-        for table in all_tables:
-            try:
-                table_name = table.get("Name")
-                table_name = self.standardize_table_name(schema_name, table_name)
-                table_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Table,
-                    service_name=self.context.database_service.name.__root__,
-                    database_name=self.context.database.name.__root__,
-                    schema_name=self.context.database_schema.name.__root__,
-                    table_name=table_name,
-                )
-                if filter_by_table(
-                    self.config.sourceConfig.config.tableFilterPattern,
-                    table_fqn
-                    if self.config.sourceConfig.config.useFqnForFiltering
-                    else table_name,
-                ):
-                    self.status.filter(
-                        table_fqn,
-                        "Table Filtered Out",
+            for table in page.TableList:
+                try:
+                    table_name = table.Name
+                    table_name = self.standardize_table_name(schema_name, table_name)
+                    table_fqn = fqn.build(
+                        self.metadata,
+                        entity_type=Table,
+                        service_name=self.context.database_service.name.__root__,
+                        database_name=self.context.database.name.__root__,
+                        schema_name=self.context.database_schema.name.__root__,
+                        table_name=table_name,
                     )
-                    continue
+                    if filter_by_table(
+                        self.config.sourceConfig.config.tableFilterPattern,
+                        table_fqn
+                        if self.config.sourceConfig.config.useFqnForFiltering
+                        else table_name,
+                    ):
+                        self.status.filter(
+                            table_fqn,
+                            "Table Filtered Out",
+                        )
+                        continue
+
+                    parameters = table.Parameters
 
-                parameters = table.get("Parameters")
+                    table_type: TableType = TableType.Regular
+                    if parameters and parameters.table_type == "ICEBERG":
+                        # iceberg tables need to pass a key/value pair in the DDL `'table_type'='ICEBERG'`
+                        # https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-creating-tables.html
+                        table_type = TableType.Iceberg
+                    elif table.TableType == "EXTERNAL_TABLE":
+                        table_type = TableType.External
+                    elif table.TableType == "VIRTUAL_VIEW":
+                        table_type = TableType.View
 
-                table_type: TableType = TableType.Regular
-                if parameters.get("table_type") == "ICEBERG":
-                    # iceberg tables need to pass a key/value pair in the DDL `'table_type'='ICEBERG'`
-                    # https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-creating-tables.html
-                    table_type = TableType.Iceberg
-                elif table["TableType"] == "EXTERNAL_TABLE":
-                    table_type = TableType.External
-                elif table["TableType"] == "VIRTUAL_VIEW":
-                    table_type = TableType.View
-
-                self.context.table_data = table
-                yield table_name, table_type
-            except Exception as exc:
-                error = f"Unexpected exception to get table [{table}]: {exc}"
-                logger.debug(traceback.format_exc())
-                logger.warning(error)
-                self.status.failed(table.get("Name"), error, traceback.format_exc())
+                    self.context.table_data = table
+                    yield table_name, table_type
+                except Exception as exc:
+                    error = f"Unexpected exception to get table [{table.Name}]: {exc}"
+                    logger.debug(traceback.format_exc())
+                    logger.warning(error)
+                    self.status.failed(table.Name, error, traceback.format_exc())
 
     def yield_table(
         self, table_name_and_type: Tuple[str, str]
     ) -> Iterable[Optional[CreateTableRequest]]:
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type = table_name_and_type
         table = self.context.table_data
         table_constraints = None
         try:
-            columns = self.get_columns(table["StorageDescriptor"])
+            columns = self.get_columns(table.StorageDescriptor)
 
             table_request = CreateTableRequest(
                 name=table_name,
                 tableType=table_type,
-                description=table.get("Description", ""),
+                description=table.Description,
                 columns=columns,
                 tableConstraints=table_constraints,
                 databaseSchema=self.context.database_schema.fullyQualifiedName,
             )
             yield table_request
             self.register_record(table_request=table_request)
         except Exception as exc:
@@ -280,30 +280,40 @@
             logger.debug(traceback.format_exc())
             logger.warning(error)
             self.status.failed(table_name, error, traceback.format_exc())
 
     def prepare(self):
         pass
 
-    def get_columns(self, column_data):
-        for column in column_data["Columns"]:
-            if column["Type"].lower().startswith("union"):
-                column["Type"] = column["Type"].replace(" ", "")
-            parsed_string = ColumnTypeParser._parse_datatype_string(  # pylint: disable=protected-access
-                column["Type"].lower()
+    def _get_column_object(self, column: GlueColumn) -> Column:
+        if column.Type.lower().startswith("union"):
+            column.Type = column.Type.replace(" ", "")
+        parsed_string = (
+            ColumnTypeParser._parse_datatype_string(  # pylint: disable=protected-access
+                column.Type.lower()
             )
-            if isinstance(parsed_string, list):
-                parsed_string = {}
-                parsed_string["dataTypeDisplay"] = str(column["Type"])
-                parsed_string["dataType"] = "UNION"
-            parsed_string["name"] = truncate_column_name(column["Name"])
-            parsed_string["displayName"] = column["Name"]
-            parsed_string["dataLength"] = parsed_string.get("dataLength", 1)
-            parsed_string["description"] = column.get("Comment")
-            yield Column(**parsed_string)
+        )
+        if isinstance(parsed_string, list):
+            parsed_string = {}
+            parsed_string["dataTypeDisplay"] = str(column.Type)
+            parsed_string["dataType"] = "UNION"
+        parsed_string["name"] = truncate_column_name(column.Name)
+        parsed_string["displayName"] = column.Name
+        parsed_string["dataLength"] = parsed_string.get("dataLength", 1)
+        parsed_string["description"] = column.Comment
+        return Column(**parsed_string)
+
+    def get_columns(self, column_data: StorageDetails) -> Optional[Iterable[Column]]:
+        # process table regular columns info
+        for column in column_data.Columns:
+            yield self._get_column_object(column)
+
+        # process table regular columns info
+        for column in self.context.table_data.PartitionKeys:
+            yield self._get_column_object(column)
 
     def standardize_table_name(self, _: str, table: str) -> str:
         return table[:128]
 
     def yield_view_lineage(self) -> Optional[Iterable[AddLineageRequest]]:
         yield from []
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/hive/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/hive/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/impala/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/impala/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/lineage_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/lineage_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mariadb/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mariadb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mariadb/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mariadb/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mssql/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mssql/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/mysql/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mysql/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/connection.py`

 * *Files 1% similar despite different names*

```diff
@@ -94,15 +94,15 @@
         if connection.instantClientDirectory:
             logger.info(
                 f"Initializing Oracle thick client at {connection.instantClientDirectory}"
             )
             os.environ[LD_LIB_ENV] = connection.instantClientDirectory
             oracledb.init_oracle_client()
     except DatabaseError as err:
-        logger.error(f"Could not initialize Oracle thick client: {err}")
+        logger.info(f"Could not initialize Oracle thick client: {err}")
 
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args_common,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/metadata.py`

 * *Files 6% similar despite different names*

```diff
@@ -33,15 +33,14 @@
     CommonDbSourceService,
     TableNameAndType,
 )
 from metadata.ingestion.source.database.oracle.utils import (
     _get_col_type,
     get_columns,
     get_mview_definition,
-    get_mview_definition_dialect,
     get_mview_names,
     get_mview_names_dialect,
     get_table_comment,
     get_table_names,
     get_view_definition,
 )
 from metadata.utils.logger import ingestion_logger
@@ -61,15 +60,14 @@
     }
 )
 
 OracleDialect.get_table_comment = get_table_comment
 OracleDialect.get_columns = get_columns
 OracleDialect._get_col_type = _get_col_type
 OracleDialect.get_view_definition = get_view_definition
-OracleDialect.get_mview_definition = get_mview_definition_dialect
 OracleDialect.get_all_view_definitions = get_all_view_definitions
 OracleDialect.get_all_table_comments = get_all_table_comments
 OracleDialect.get_table_names = get_table_names
 Inspector.get_mview_names = get_mview_names
 Inspector.get_mview_definition = get_mview_definition
 OracleDialect.get_mview_names = get_mview_names_dialect
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/queries.py`

 * *Files 24% similar despite different names*

```diff
@@ -25,17 +25,15 @@
 ORACLE_ALL_VIEW_DEFINITIONS = """
 SELECT
 LOWER(view_name) AS "view_name",
 LOWER(owner) AS "schema",
 DBMS_METADATA.GET_DDL('VIEW', view_name, owner) AS view_def
 FROM all_views
 WHERE owner NOT IN ('SYSTEM', 'SYS')
-"""
-
-ORACLE_ALL_MATERIALIZED_VIEW_DEFINITIONS = """
+UNION ALL
 SELECT
 LOWER(mview_name) AS "view_name",
 LOWER(owner) AS "schema",
 DBMS_METADATA.GET_DDL('MATERIALIZED_VIEW', mview_name, owner) AS view_def
 FROM all_mviews
 WHERE owner NOT IN ('SYSTEM', 'SYS')
 """
@@ -81,7 +79,28 @@
         LEFT JOIN all_col_comments{dblink} com
         ON col.table_name = com.table_name
         AND col.column_name = com.column_name
         AND col.owner = com.owner
         WHERE col.table_name = CAST(:table_name AS VARCHAR2(128))
         AND col.hidden_column = 'NO'
     """
+
+ORACLE_QUERY_HISTORY_STATEMENT = """
+SELECT 
+  NULL AS user_name,
+  NULL AS database_name,
+  NULL AS schema_name,
+  NULL AS aborted,
+  SQL_FULLTEXT AS query_text,
+  TO_TIMESTAMP(FIRST_LOAD_TIME, 'yy-MM-dd/HH24:MI:SS') AS start_time,
+  ELAPSED_TIME AS duration,
+  TO_TIMESTAMP(FIRST_LOAD_TIME, 'yy-MM-dd/HH24:MI:SS') + NUMTODSINTERVAL(ELAPSED_TIME / 1000, 'SECOND') AS end_time
+FROM gv$sql
+WHERE OBJECT_STATUS = 'VALID' 
+  {filters}
+  AND SQL_FULLTEXT NOT LIKE '/* {{"app": "OpenMetadata", %%}} */%%'
+  AND SQL_FULLTEXT NOT LIKE '/* {{"app": "dbt", %%}} */%%'
+  AND TO_TIMESTAMP(FIRST_LOAD_TIME, 'yy-MM-dd/HH24:MI:SS') >= TO_TIMESTAMP('{start_time}', 'yy-MM-dd HH24:MI:SS')
+  AND TO_TIMESTAMP(FIRST_LOAD_TIME, 'yy-MM-dd/HH24:MI:SS') + NUMTODSINTERVAL(ELAPSED_TIME / 1000, 'SECOND') < TO_TIMESTAMP('{end_time}', 'yy-MM-dd HH24:MI:SS')
+ORDER BY FIRST_LOAD_TIME DESC 
+OFFSET 0 ROWS FETCH NEXT {result_limit} ROWS ONLY
+"""
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/oracle/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,15 +17,14 @@
 from sqlalchemy import sql, util
 from sqlalchemy.dialects.oracle.base import FLOAT, INTEGER, INTERVAL, NUMBER, TIMESTAMP
 from sqlalchemy.engine import reflection
 from sqlalchemy.sql import sqltypes
 
 from metadata.ingestion.source.database.oracle.queries import (
     GET_MATERIALIZED_VIEW_NAMES,
-    ORACLE_ALL_MATERIALIZED_VIEW_DEFINITIONS,
     ORACLE_ALL_TABLE_COMMENTS,
     ORACLE_ALL_VIEW_DEFINITIONS,
     ORACLE_GET_COLUMNS,
     ORACLE_GET_TABLE_NAMES,
     ORACLE_IDENTITY_TYPE,
 )
 from metadata.utils.sqlalchemy_utils import (
@@ -69,34 +68,14 @@
         connection,
         table_name=view_name.lower(),
         schema=schema.lower() if schema else None,
         query=ORACLE_ALL_VIEW_DEFINITIONS,
     )
 
 
-@reflection.cache
-def get_mview_definition_dialect(
-    self,
-    connection,
-    view_name: str,
-    schema: str = None,
-    resolve_synonyms=False,
-    dblink="",
-    **kw,
-):
-
-    return get_view_definition_wrapper(
-        self,
-        connection,
-        table_name=view_name.lower(),
-        schema=schema.lower() if schema else None,
-        query=ORACLE_ALL_MATERIALIZED_VIEW_DEFINITIONS,
-    )
-
-
 def _get_col_type(
     self, coltype, precision, scale, length, colname
 ):  # pylint: disable=too-many-branches
     raw_type = coltype
     if coltype == "NUMBER":
         if precision is None and scale == 0:
             coltype = INTEGER()
@@ -270,10 +249,10 @@
 
     :param schema: Optional, retrieve names from a non-default schema.
         For special quoting, use :class:`.quoted_name`.
 
     """
 
     with self._operation_context() as conn:
-        return self.dialect.get_mview_definition(
+        return self.dialect.get_view_definition(
             conn, mview_name, schema, info_cache=self.info_cache
         )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/pinotdb/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/pinotdb/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/connection.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,42 +8,41 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
-    PostgresConnection,
+from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
+    RedshiftConnection,
     SslMode,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
     init_empty_connection_arguments,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.postgres.queries import (
-    POSTGRES_GET_DATABASE,
-    POSTGRES_TEST_GET_QUERIES,
-    POSTGRES_TEST_GET_TAGS,
+from metadata.ingestion.source.database.redshift.queries import (
+    REDSHIFT_GET_DATABASE_NAMES,
+    REDSHIFT_TEST_GET_QUERIES,
+    REDSHIFT_TEST_PARTITION_DETAILS,
 )
 
 
-def get_connection(connection: PostgresConnection) -> Engine:
+def get_connection(connection: RedshiftConnection) -> Engine:
     """
     Create connection
     """
     if connection.sslMode:
         if not connection.connectionArguments:
             connection.connectionArguments = init_empty_connection_arguments()
         connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
@@ -57,25 +56,25 @@
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: PostgresConnection,
+    service_connection: RedshiftConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
     queries = {
-        "GetQueries": POSTGRES_TEST_GET_QUERIES,
-        "GetDatabases": POSTGRES_GET_DATABASE,
-        "GetTags": POSTGRES_TEST_GET_TAGS,
+        "GetQueries": REDSHIFT_TEST_GET_QUERIES,
+        "GetDatabases": REDSHIFT_GET_DATABASE_NAMES,
+        "GetPartitionTableDetails": REDSHIFT_TEST_PARTITION_DETAILS,
     }
     test_connection_db_common(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
         queries=queries,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/metadata.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,18 +15,14 @@
 from collections import namedtuple
 from typing import Iterable, Tuple
 
 from sqlalchemy import sql
 from sqlalchemy.dialects.postgresql.base import PGDialect, ischema_names
 from sqlalchemy.engine.reflection import Inspector
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.table import (
     IntervalType,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
@@ -60,14 +56,15 @@
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_database
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.sqlalchemy_utils import (
     get_all_table_comments,
     get_all_view_definitions,
 )
+from metadata.utils.tag_utils import get_ometa_tag_and_classification
 
 TableKey = namedtuple("TableKey", ["schema", "table_name"])
 
 logger = ingestion_logger()
 
 
 INTERVAL_TYPE_MAP = {
@@ -210,29 +207,23 @@
         try:
             result = self.engine.execute(
                 POSTGRES_GET_ALL_TABLE_PG_POLICY.format(
                     database_name=self.context.database.name.__root__,
                     schema_name=schema_name,
                 )
             ).all()
-
             for res in result:
                 row = list(res)
                 fqn_elements = [name for name in row[2:] if name]
-                yield OMetaTagAndClassification(
-                    fqn=fqn._build(  # pylint: disable=protected-access
+                yield from get_ometa_tag_and_classification(
+                    tag_fqn=fqn._build(  # pylint: disable=protected-access
                         self.context.database_service.name.__root__, *fqn_elements
                     ),
-                    classification_request=CreateClassificationRequest(
-                        name=self.service_connection.classificationName,
-                        description="Postgres Tag Name",
-                    ),
-                    tag_request=CreateTagRequest(
-                        classification=self.service_connection.classificationName,
-                        name=row[1],
-                        description="Postgres Tag Value",
-                    ),
+                    tags=[row[1]],
+                    classification_name=self.service_connection.classificationName,
+                    tag_description="Postgres Tag Value",
+                    classification_desciption="Postgres Tag Name",
                 )
 
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Skipping Policy Tag: {exc}")
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/queries.py`

 * *Files 3% similar despite different names*

```diff
@@ -208,165 +208,180 @@
 00000cf0: 2c20 2770 2729 0a20 2020 2020 2041 4e44  , 'p').      AND
 00000d00: 2070 6764 2e64 6573 6372 6970 7469 6f6e   pgd.description
 00000d10: 2049 5320 4e4f 5420 4e55 4c4c 0a20 2020   IS NOT NULL.   
 00000d20: 2020 2041 4e44 206e 2e6e 7370 6e61 6d65     AND n.nspname
 00000d30: 203c 3e20 2770 675f 6361 7461 6c6f 6727   <> 'pg_catalog'
 00000d40: 0a20 2020 204f 5244 4552 2042 5920 2273  .    ORDER BY "s
 00000d50: 6368 656d 6122 2c20 2274 6162 6c65 5f6e  chema", "table_n
-00000d60: 616d 6522 0a22 2222 0a0a 0a50 4f53 5447  ame"."""...POSTG
-00000d70: 5245 535f 5649 4557 5f44 4546 494e 4954  RES_VIEW_DEFINIT
-00000d80: 494f 4e53 203d 2022 2222 0a53 454c 4543  IONS = """.SELEC
-00000d90: 5420 0a09 6e2e 6e73 706e 616d 6520 2273  T ..n.nspname "s
-00000da0: 6368 656d 6122 2c0a 0963 2e72 656c 6e61  chema",..c.relna
-00000db0: 6d65 2076 6965 775f 6e61 6d65 2c0a 0970  me view_name,..p
-00000dc0: 675f 6765 745f 7669 6577 6465 6628 632e  g_get_viewdef(c.
-00000dd0: 6f69 6429 2076 6965 775f 6465 660a 4652  oid) view_def.FR
-00000de0: 4f4d 2070 675f 636c 6173 7320 6320 0a4a  OM pg_class c .J
-00000df0: 4f49 4e20 7067 5f6e 616d 6573 7061 6365  OIN pg_namespace
-00000e00: 206e 204f 4e20 6e2e 6f69 6420 3d20 632e   n ON n.oid = c.
-00000e10: 7265 6c6e 616d 6573 7061 6365 200a 5748  relnamespace .WH
-00000e20: 4552 4520 632e 7265 6c6b 696e 6420 494e  ERE c.relkind IN
-00000e30: 2028 2776 272c 2027 6d27 290a 414e 4420   ('v', 'm').AND 
-00000e40: 6e2e 6e73 706e 616d 6520 6e6f 7420 696e  n.nspname not in
-00000e50: 2028 2770 675f 6361 7461 6c6f 6727 2c27   ('pg_catalog','
-00000e60: 696e 666f 726d 6174 696f 6e5f 7363 6865  information_sche
-00000e70: 6d61 2729 0a22 2222 0a0a 504f 5354 4752  ma')."""..POSTGR
-00000e80: 4553 5f47 4554 5f44 4154 4142 4153 4520  ES_GET_DATABASE 
-00000e90: 3d20 2222 220a 7365 6c65 6374 2064 6174  = """.select dat
-00000ea0: 6e61 6d65 2066 726f 6d20 7067 5f63 6174  name from pg_cat
-00000eb0: 616c 6f67 2e70 675f 6461 7461 6261 7365  alog.pg_database
-00000ec0: 0a22 2222 0a0a 504f 5354 4752 4553 5f54  ."""..POSTGRES_T
-00000ed0: 4553 545f 4745 545f 5441 4753 203d 2022  EST_GET_TAGS = "
-00000ee0: 2222 0a53 454c 4543 5420 6f69 642c 2070  "".SELECT oid, p
-00000ef0: 6f6c 6e61 6d65 2c20 7461 626c 655f 6361  olname, table_ca
-00000f00: 7461 6c6f 6720 2c20 7461 626c 655f 7363  talog , table_sc
-00000f10: 6865 6d61 202c 7461 626c 655f 6e61 6d65  hema ,table_name
-00000f20: 2020 0a46 524f 4d20 696e 666f 726d 6174    .FROM informat
-00000f30: 696f 6e5f 7363 6865 6d61 2e74 6162 6c65  ion_schema.table
-00000f40: 7320 4153 2069 740a 4a4f 494e 2028 5345  s AS it.JOIN (SE
-00000f50: 4c45 4354 2070 632e 7265 6c6e 616d 652c  LECT pc.relname,
-00000f60: 2070 702e 2a0a 2020 2020 2020 4652 4f4d   pp.*.      FROM
-00000f70: 2070 675f 706f 6c69 6379 2041 5320 7070   pg_policy AS pp
-00000f80: 0a20 2020 2020 204a 4f49 4e20 7067 5f63  .      JOIN pg_c
-00000f90: 6c61 7373 2041 5320 7063 204f 4e20 7070  lass AS pc ON pp
-00000fa0: 2e70 6f6c 7265 6c69 6420 3d20 7063 2e6f  .polrelid = pc.o
-00000fb0: 6964 0a20 2020 2020 204a 4f49 4e20 7067  id.      JOIN pg
-00000fc0: 5f6e 616d 6573 7061 6365 2061 7320 706e  _namespace as pn
-00000fd0: 204f 4e20 7063 2e72 656c 6e61 6d65 7370   ON pc.relnamesp
-00000fe0: 6163 6520 3d20 706e 2e6f 6964 2920 4153  ace = pn.oid) AS
-00000ff0: 2070 7072 204f 4e20 6974 2e74 6162 6c65   ppr ON it.table
-00001000: 5f6e 616d 6520 3d20 7070 722e 7265 6c6e  _name = ppr.reln
-00001010: 616d 650a 2020 2020 2020 4c49 4d49 5420  ame.      LIMIT 
-00001020: 310a 2222 220a 0a50 4f53 5447 5245 535f  1."""..POSTGRES_
-00001030: 5445 5354 5f47 4554 5f51 5545 5249 4553  TEST_GET_QUERIES
-00001040: 203d 2022 2222 0a20 2020 2020 2053 454c   = """.      SEL
-00001050: 4543 540a 2020 2020 2020 2020 752e 7573  ECT.        u.us
-00001060: 656e 616d 652c 0a20 2020 2020 2020 2064  ename,.        d
-00001070: 2e64 6174 6e61 6d65 2064 6174 6162 6173  .datname databas
-00001080: 655f 6e61 6d65 2c0a 2020 2020 2020 2020  e_name,.        
-00001090: 732e 7175 6572 7920 7175 6572 795f 7465  s.query query_te
-000010a0: 7874 2c0a 2020 2020 2020 2020 732e 746f  xt,.        s.to
-000010b0: 7461 6c5f 6578 6563 5f74 696d 652f 3130  tal_exec_time/10
-000010c0: 3030 2064 7572 6174 696f 6e0a 2020 2020  00 duration.    
-000010d0: 2020 4652 4f4d 0a20 2020 2020 2020 2070    FROM.        p
-000010e0: 675f 7374 6174 5f73 7461 7465 6d65 6e74  g_stat_statement
-000010f0: 7320 730a 2020 2020 2020 2020 4a4f 494e  s s.        JOIN
-00001100: 2070 675f 6361 7461 6c6f 672e 7067 5f64   pg_catalog.pg_d
-00001110: 6174 6162 6173 6520 6420 4f4e 2073 2e64  atabase d ON s.d
-00001120: 6269 6420 3d20 642e 6f69 640a 2020 2020  bid = d.oid.    
-00001130: 2020 2020 4a4f 494e 2070 675f 6361 7461      JOIN pg_cata
-00001140: 6c6f 672e 7067 5f75 7365 7220 7520 4f4e  log.pg_user u ON
-00001150: 2073 2e75 7365 7269 6420 3d20 752e 7573   s.userid = u.us
-00001160: 6573 7973 6964 0a20 2020 2020 2020 204c  esysid.        L
-00001170: 494d 4954 2031 0a20 2020 2022 2222 0a0a  IMIT 1.    """..
-00001180: 0a50 4f53 5447 5245 535f 4745 545f 4442  .POSTGRES_GET_DB
-00001190: 5f4e 414d 4553 203d 2022 2222 0a73 656c  _NAMES = """.sel
-000011a0: 6563 7420 6461 746e 616d 6520 6672 6f6d  ect datname from
-000011b0: 2070 675f 6361 7461 6c6f 672e 7067 5f64   pg_catalog.pg_d
-000011c0: 6174 6162 6173 650a 2222 220a 0a50 4f53  atabase."""..POS
-000011d0: 5447 5245 535f 434f 4c5f 4944 454e 5449  TGRES_COL_IDENTI
-000011e0: 5459 203d 2022 2222 5c0a 2020 2853 454c  TY = """\.  (SEL
-000011f0: 4543 5420 6a73 6f6e 5f62 7569 6c64 5f6f  ECT json_build_o
-00001200: 626a 6563 7428 0a20 2020 2020 2027 616c  bject(.      'al
-00001210: 7761 7973 272c 2061 2e61 7474 6964 656e  ways', a.attiden
-00001220: 7469 7479 203d 2027 6127 2c0a 2020 2020  tity = 'a',.    
-00001230: 2020 2773 7461 7274 272c 2073 2e73 6571    'start', s.seq
-00001240: 7374 6172 742c 0a20 2020 2020 2027 696e  start,.      'in
-00001250: 6372 656d 656e 7427 2c20 732e 7365 7169  crement', s.seqi
-00001260: 6e63 7265 6d65 6e74 2c0a 2020 2020 2020  ncrement,.      
-00001270: 276d 696e 7661 6c75 6527 2c20 732e 7365  'minvalue', s.se
-00001280: 716d 696e 2c0a 2020 2020 2020 276d 6178  qmin,.      'max
-00001290: 7661 6c75 6527 2c20 732e 7365 716d 6178  value', s.seqmax
-000012a0: 2c0a 2020 2020 2020 2763 6163 6865 272c  ,.      'cache',
-000012b0: 2073 2e73 6571 6361 6368 652c 0a20 2020   s.seqcache,.   
-000012c0: 2020 2027 6379 636c 6527 2c20 732e 7365     'cycle', s.se
-000012d0: 7163 7963 6c65 290a 2020 4652 4f4d 2070  qcycle).  FROM p
-000012e0: 675f 6361 7461 6c6f 672e 7067 5f73 6571  g_catalog.pg_seq
-000012f0: 7565 6e63 6520 730a 2020 4a4f 494e 2070  uence s.  JOIN p
-00001300: 675f 6361 7461 6c6f 672e 7067 5f63 6c61  g_catalog.pg_cla
-00001310: 7373 2063 206f 6e20 732e 7365 7172 656c  ss c on s.seqrel
-00001320: 6964 203d 2063 2e22 6f69 6422 0a20 2057  id = c."oid".  W
-00001330: 4845 5245 2063 2e72 656c 6b69 6e64 203d  HERE c.relkind =
-00001340: 2027 5327 0a20 2041 4e44 2061 2e61 7474   'S'.  AND a.att
-00001350: 6964 656e 7469 7479 2021 3d20 2727 0a20  identity != ''. 
-00001360: 2041 4e44 2073 2e73 6571 7265 6c69 6420   AND s.seqrelid 
-00001370: 3d20 7067 5f63 6174 616c 6f67 2e70 675f  = pg_catalog.pg_
-00001380: 6765 745f 7365 7269 616c 5f73 6571 7565  get_serial_seque
-00001390: 6e63 6528 0a20 2020 2020 2061 2e61 7474  nce(.      a.att
-000013a0: 7265 6c69 643a 3a72 6567 636c 6173 733a  relid::regclass:
-000013b0: 3a74 6578 742c 2061 2e61 7474 6e61 6d65  :text, a.attname
-000013c0: 0a20 2029 3a3a 7265 6763 6c61 7373 3a3a  .  )::regclass::
-000013d0: 6f69 640a 2020 2920 6173 2069 6465 6e74  oid.  ) as ident
-000013e0: 6974 795f 6f70 7469 6f6e 735c 0a22 2222  ity_options\."""
-000013f0: 0a0a 504f 5354 4752 4553 5f53 514c 5f43  ..POSTGRES_SQL_C
-00001400: 4f4c 554d 4e53 203d 2022 2222 0a20 2020  OLUMNS = """.   
-00001410: 2020 2020 2053 454c 4543 5420 612e 6174       SELECT a.at
-00001420: 746e 616d 652c 0a20 2020 2020 2020 2020  tname,.         
-00001430: 2020 2070 675f 6361 7461 6c6f 672e 666f     pg_catalog.fo
-00001440: 726d 6174 5f74 7970 6528 612e 6174 7474  rmat_type(a.attt
-00001450: 7970 6964 2c20 612e 6174 7474 7970 6d6f  ypid, a.atttypmo
-00001460: 6429 2c0a 2020 2020 2020 2020 2020 2020  d),.            
-00001470: 280a 2020 2020 2020 2020 2020 2020 5345  (.            SE
-00001480: 4c45 4354 2070 675f 6361 7461 6c6f 672e  LECT pg_catalog.
-00001490: 7067 5f67 6574 5f65 7870 7228 642e 6164  pg_get_expr(d.ad
-000014a0: 6269 6e2c 2064 2e61 6472 656c 6964 290a  bin, d.adrelid).
-000014b0: 2020 2020 2020 2020 2020 2020 4652 4f4d              FROM
-000014c0: 2070 675f 6361 7461 6c6f 672e 7067 5f61   pg_catalog.pg_a
-000014d0: 7474 7264 6566 2064 0a20 2020 2020 2020  ttrdef d.       
-000014e0: 2020 2020 2057 4845 5245 2064 2e61 6472       WHERE d.adr
-000014f0: 656c 6964 203d 2061 2e61 7474 7265 6c69  elid = a.attreli
-00001500: 6420 414e 4420 642e 6164 6e75 6d20 3d20  d AND d.adnum = 
-00001510: 612e 6174 746e 756d 0a20 2020 2020 2020  a.attnum.       
-00001520: 2020 2020 2041 4e44 2061 2e61 7474 6861       AND a.attha
-00001530: 7364 6566 0a20 2020 2020 2020 2020 2020  sdef.           
-00001540: 2029 2041 5320 4445 4641 554c 542c 0a20   ) AS DEFAULT,. 
-00001550: 2020 2020 2020 2020 2020 2061 2e61 7474             a.att
-00001560: 6e6f 746e 756c 6c2c 0a20 2020 2020 2020  notnull,.       
-00001570: 2020 2020 2061 2e61 7474 7265 6c69 6420       a.attrelid 
-00001580: 6173 2074 6162 6c65 5f6f 6964 2c0a 2020  as table_oid,.  
-00001590: 2020 2020 2020 2020 2020 7067 642e 6465            pgd.de
-000015a0: 7363 7269 7074 696f 6e20 6173 2063 6f6d  scription as com
-000015b0: 6d65 6e74 2c0a 2020 2020 2020 2020 2020  ment,.          
-000015c0: 2020 7b67 656e 6572 6174 6564 7d2c 0a20    {generated},. 
-000015d0: 2020 2020 2020 2020 2020 207b 6964 656e             {iden
-000015e0: 7469 7479 7d0a 2020 2020 2020 2020 4652  tity}.        FR
-000015f0: 4f4d 2070 675f 6361 7461 6c6f 672e 7067  OM pg_catalog.pg
-00001600: 5f61 7474 7269 6275 7465 2061 0a20 2020  _attribute a.   
-00001610: 2020 2020 204c 4546 5420 4a4f 494e 2070       LEFT JOIN p
-00001620: 675f 6361 7461 6c6f 672e 7067 5f64 6573  g_catalog.pg_des
-00001630: 6372 6970 7469 6f6e 2070 6764 204f 4e20  cription pgd ON 
-00001640: 280a 2020 2020 2020 2020 2020 2020 7067  (.            pg
-00001650: 642e 6f62 6a6f 6964 203d 2061 2e61 7474  d.objoid = a.att
-00001660: 7265 6c69 6420 414e 4420 7067 642e 6f62  relid AND pgd.ob
-00001670: 6a73 7562 6964 203d 2061 2e61 7474 6e75  jsubid = a.attnu
-00001680: 6d29 0a20 2020 2020 2020 2057 4845 5245  m).        WHERE
-00001690: 2061 2e61 7474 7265 6c69 6420 3d20 3a74   a.attrelid = :t
-000016a0: 6162 6c65 5f6f 6964 0a20 2020 2020 2020  able_oid.       
-000016b0: 2041 4e44 2061 2e61 7474 6e75 6d20 3e20   AND a.attnum > 
-000016c0: 3020 414e 4420 4e4f 5420 612e 6174 7469  0 AND NOT a.atti
-000016d0: 7364 726f 7070 6564 0a20 2020 2020 2020  sdropped.       
-000016e0: 204f 5244 4552 2042 5920 612e 6174 746e   ORDER BY a.attn
-000016f0: 756d 0a20 2020 2022 2222 0a0a 504f 5354  um.    """..POST
-00001700: 4752 4553 5f47 4554 5f53 4552 5645 525f  GRES_GET_SERVER_
-00001710: 5645 5253 494f 4e20 3d20 2222 220a 7368  VERSION = """.sh
-00001720: 6f77 2073 6572 7665 725f 7665 7273 696f  ow server_versio
-00001730: 6e0a 2222 220a                           n.""".
+00000d60: 616d 6522 0a22 2222 0a0a 2320 506f 7374  ame"."""..# Post
+00000d70: 6772 6573 2076 6965 7773 2064 6566 696e  gres views defin
+00000d80: 6974 696f 6e73 206f 6e6c 7920 636f 6e74  itions only cont
+00000d90: 6169 6e73 2074 6865 2073 656c 6563 7420  ains the select 
+00000da0: 7175 6572 790a 2320 6865 6e63 6520 7765  query.# hence we
+00000db0: 2061 7265 2061 7070 656e 6469 6e67 2022   are appending "
+00000dc0: 6372 6561 7465 2076 6965 7720 3c73 6368  create view <sch
+00000dd0: 656d 613e 2e3c 7461 626c 653e 2061 7320  ema>.<table> as 
+00000de0: 2220 746f 2073 656c 6563 7420 7175 6572  " to select quer
+00000df0: 790a 2320 746f 2067 656e 6572 6174 6520  y.# to generate 
+00000e00: 7468 6520 636f 6c75 6d6e 206c 6576 656c  the column level
+00000e10: 206c 696e 6561 6765 0a50 4f53 5447 5245   lineage.POSTGRE
+00000e20: 535f 5649 4557 5f44 4546 494e 4954 494f  S_VIEW_DEFINITIO
+00000e30: 4e53 203d 2022 2222 0a53 454c 4543 5420  NS = """.SELECT 
+00000e40: 0a09 6e2e 6e73 706e 616d 6520 2273 6368  ..n.nspname "sch
+00000e50: 656d 6122 2c0a 0963 2e72 656c 6e61 6d65  ema",..c.relname
+00000e60: 2076 6965 775f 6e61 6d65 2c0a 0927 6372   view_name,..'cr
+00000e70: 6561 7465 2076 6965 7720 2720 7c7c 206e  eate view ' || n
+00000e80: 2e6e 7370 6e61 6d65 207c 7c20 272e 2720  .nspname || '.' 
+00000e90: 7c7c 2063 2e72 656c 6e61 6d65 207c 7c20  || c.relname || 
+00000ea0: 2720 6173 2027 207c 7c20 7067 5f67 6574  ' as ' || pg_get
+00000eb0: 5f76 6965 7764 6566 2863 2e6f 6964 2c74  _viewdef(c.oid,t
+00000ec0: 7275 6529 2076 6965 775f 6465 660a 4652  rue) view_def.FR
+00000ed0: 4f4d 2070 675f 636c 6173 7320 6320 0a4a  OM pg_class c .J
+00000ee0: 4f49 4e20 7067 5f6e 616d 6573 7061 6365  OIN pg_namespace
+00000ef0: 206e 204f 4e20 6e2e 6f69 6420 3d20 632e   n ON n.oid = c.
+00000f00: 7265 6c6e 616d 6573 7061 6365 200a 5748  relnamespace .WH
+00000f10: 4552 4520 632e 7265 6c6b 696e 6420 494e  ERE c.relkind IN
+00000f20: 2028 2776 272c 2027 6d27 290a 414e 4420   ('v', 'm').AND 
+00000f30: 6e2e 6e73 706e 616d 6520 6e6f 7420 696e  n.nspname not in
+00000f40: 2028 2770 675f 6361 7461 6c6f 6727 2c27   ('pg_catalog','
+00000f50: 696e 666f 726d 6174 696f 6e5f 7363 6865  information_sche
+00000f60: 6d61 2729 0a22 2222 0a0a 504f 5354 4752  ma')."""..POSTGR
+00000f70: 4553 5f47 4554 5f44 4154 4142 4153 4520  ES_GET_DATABASE 
+00000f80: 3d20 2222 220a 7365 6c65 6374 2064 6174  = """.select dat
+00000f90: 6e61 6d65 2066 726f 6d20 7067 5f63 6174  name from pg_cat
+00000fa0: 616c 6f67 2e70 675f 6461 7461 6261 7365  alog.pg_database
+00000fb0: 0a22 2222 0a0a 504f 5354 4752 4553 5f54  ."""..POSTGRES_T
+00000fc0: 4553 545f 4745 545f 5441 4753 203d 2022  EST_GET_TAGS = "
+00000fd0: 2222 0a53 454c 4543 5420 6f69 642c 2070  "".SELECT oid, p
+00000fe0: 6f6c 6e61 6d65 2c20 7461 626c 655f 6361  olname, table_ca
+00000ff0: 7461 6c6f 6720 2c20 7461 626c 655f 7363  talog , table_sc
+00001000: 6865 6d61 202c 7461 626c 655f 6e61 6d65  hema ,table_name
+00001010: 2020 0a46 524f 4d20 696e 666f 726d 6174    .FROM informat
+00001020: 696f 6e5f 7363 6865 6d61 2e74 6162 6c65  ion_schema.table
+00001030: 7320 4153 2069 740a 4a4f 494e 2028 5345  s AS it.JOIN (SE
+00001040: 4c45 4354 2070 632e 7265 6c6e 616d 652c  LECT pc.relname,
+00001050: 2070 702e 2a0a 2020 2020 2020 4652 4f4d   pp.*.      FROM
+00001060: 2070 675f 706f 6c69 6379 2041 5320 7070   pg_policy AS pp
+00001070: 0a20 2020 2020 204a 4f49 4e20 7067 5f63  .      JOIN pg_c
+00001080: 6c61 7373 2041 5320 7063 204f 4e20 7070  lass AS pc ON pp
+00001090: 2e70 6f6c 7265 6c69 6420 3d20 7063 2e6f  .polrelid = pc.o
+000010a0: 6964 0a20 2020 2020 204a 4f49 4e20 7067  id.      JOIN pg
+000010b0: 5f6e 616d 6573 7061 6365 2061 7320 706e  _namespace as pn
+000010c0: 204f 4e20 7063 2e72 656c 6e61 6d65 7370   ON pc.relnamesp
+000010d0: 6163 6520 3d20 706e 2e6f 6964 2920 4153  ace = pn.oid) AS
+000010e0: 2070 7072 204f 4e20 6974 2e74 6162 6c65   ppr ON it.table
+000010f0: 5f6e 616d 6520 3d20 7070 722e 7265 6c6e  _name = ppr.reln
+00001100: 616d 650a 2020 2020 2020 4c49 4d49 5420  ame.      LIMIT 
+00001110: 310a 2222 220a 0a50 4f53 5447 5245 535f  1."""..POSTGRES_
+00001120: 5445 5354 5f47 4554 5f51 5545 5249 4553  TEST_GET_QUERIES
+00001130: 203d 2022 2222 0a20 2020 2020 2053 454c   = """.      SEL
+00001140: 4543 540a 2020 2020 2020 2020 752e 7573  ECT.        u.us
+00001150: 656e 616d 652c 0a20 2020 2020 2020 2064  ename,.        d
+00001160: 2e64 6174 6e61 6d65 2064 6174 6162 6173  .datname databas
+00001170: 655f 6e61 6d65 2c0a 2020 2020 2020 2020  e_name,.        
+00001180: 732e 7175 6572 7920 7175 6572 795f 7465  s.query query_te
+00001190: 7874 2c0a 2020 2020 2020 2020 732e 746f  xt,.        s.to
+000011a0: 7461 6c5f 6578 6563 5f74 696d 652f 3130  tal_exec_time/10
+000011b0: 3030 2064 7572 6174 696f 6e0a 2020 2020  00 duration.    
+000011c0: 2020 4652 4f4d 0a20 2020 2020 2020 2070    FROM.        p
+000011d0: 675f 7374 6174 5f73 7461 7465 6d65 6e74  g_stat_statement
+000011e0: 7320 730a 2020 2020 2020 2020 4a4f 494e  s s.        JOIN
+000011f0: 2070 675f 6361 7461 6c6f 672e 7067 5f64   pg_catalog.pg_d
+00001200: 6174 6162 6173 6520 6420 4f4e 2073 2e64  atabase d ON s.d
+00001210: 6269 6420 3d20 642e 6f69 640a 2020 2020  bid = d.oid.    
+00001220: 2020 2020 4a4f 494e 2070 675f 6361 7461      JOIN pg_cata
+00001230: 6c6f 672e 7067 5f75 7365 7220 7520 4f4e  log.pg_user u ON
+00001240: 2073 2e75 7365 7269 6420 3d20 752e 7573   s.userid = u.us
+00001250: 6573 7973 6964 0a20 2020 2020 2020 204c  esysid.        L
+00001260: 494d 4954 2031 0a20 2020 2022 2222 0a0a  IMIT 1.    """..
+00001270: 0a50 4f53 5447 5245 535f 4745 545f 4442  .POSTGRES_GET_DB
+00001280: 5f4e 414d 4553 203d 2022 2222 0a73 656c  _NAMES = """.sel
+00001290: 6563 7420 6461 746e 616d 6520 6672 6f6d  ect datname from
+000012a0: 2070 675f 6361 7461 6c6f 672e 7067 5f64   pg_catalog.pg_d
+000012b0: 6174 6162 6173 650a 2222 220a 0a50 4f53  atabase."""..POS
+000012c0: 5447 5245 535f 434f 4c5f 4944 454e 5449  TGRES_COL_IDENTI
+000012d0: 5459 203d 2022 2222 5c0a 2020 2853 454c  TY = """\.  (SEL
+000012e0: 4543 5420 6a73 6f6e 5f62 7569 6c64 5f6f  ECT json_build_o
+000012f0: 626a 6563 7428 0a20 2020 2020 2027 616c  bject(.      'al
+00001300: 7761 7973 272c 2061 2e61 7474 6964 656e  ways', a.attiden
+00001310: 7469 7479 203d 2027 6127 2c0a 2020 2020  tity = 'a',.    
+00001320: 2020 2773 7461 7274 272c 2073 2e73 6571    'start', s.seq
+00001330: 7374 6172 742c 0a20 2020 2020 2027 696e  start,.      'in
+00001340: 6372 656d 656e 7427 2c20 732e 7365 7169  crement', s.seqi
+00001350: 6e63 7265 6d65 6e74 2c0a 2020 2020 2020  ncrement,.      
+00001360: 276d 696e 7661 6c75 6527 2c20 732e 7365  'minvalue', s.se
+00001370: 716d 696e 2c0a 2020 2020 2020 276d 6178  qmin,.      'max
+00001380: 7661 6c75 6527 2c20 732e 7365 716d 6178  value', s.seqmax
+00001390: 2c0a 2020 2020 2020 2763 6163 6865 272c  ,.      'cache',
+000013a0: 2073 2e73 6571 6361 6368 652c 0a20 2020   s.seqcache,.   
+000013b0: 2020 2027 6379 636c 6527 2c20 732e 7365     'cycle', s.se
+000013c0: 7163 7963 6c65 290a 2020 4652 4f4d 2070  qcycle).  FROM p
+000013d0: 675f 6361 7461 6c6f 672e 7067 5f73 6571  g_catalog.pg_seq
+000013e0: 7565 6e63 6520 730a 2020 4a4f 494e 2070  uence s.  JOIN p
+000013f0: 675f 6361 7461 6c6f 672e 7067 5f63 6c61  g_catalog.pg_cla
+00001400: 7373 2063 206f 6e20 732e 7365 7172 656c  ss c on s.seqrel
+00001410: 6964 203d 2063 2e22 6f69 6422 0a20 2057  id = c."oid".  W
+00001420: 4845 5245 2063 2e72 656c 6b69 6e64 203d  HERE c.relkind =
+00001430: 2027 5327 0a20 2041 4e44 2061 2e61 7474   'S'.  AND a.att
+00001440: 6964 656e 7469 7479 2021 3d20 2727 0a20  identity != ''. 
+00001450: 2041 4e44 2073 2e73 6571 7265 6c69 6420   AND s.seqrelid 
+00001460: 3d20 7067 5f63 6174 616c 6f67 2e70 675f  = pg_catalog.pg_
+00001470: 6765 745f 7365 7269 616c 5f73 6571 7565  get_serial_seque
+00001480: 6e63 6528 0a20 2020 2020 2061 2e61 7474  nce(.      a.att
+00001490: 7265 6c69 643a 3a72 6567 636c 6173 733a  relid::regclass:
+000014a0: 3a74 6578 742c 2061 2e61 7474 6e61 6d65  :text, a.attname
+000014b0: 0a20 2029 3a3a 7265 6763 6c61 7373 3a3a  .  )::regclass::
+000014c0: 6f69 640a 2020 2920 6173 2069 6465 6e74  oid.  ) as ident
+000014d0: 6974 795f 6f70 7469 6f6e 735c 0a22 2222  ity_options\."""
+000014e0: 0a0a 504f 5354 4752 4553 5f53 514c 5f43  ..POSTGRES_SQL_C
+000014f0: 4f4c 554d 4e53 203d 2022 2222 0a20 2020  OLUMNS = """.   
+00001500: 2020 2020 2053 454c 4543 5420 612e 6174       SELECT a.at
+00001510: 746e 616d 652c 0a20 2020 2020 2020 2020  tname,.         
+00001520: 2020 2070 675f 6361 7461 6c6f 672e 666f     pg_catalog.fo
+00001530: 726d 6174 5f74 7970 6528 612e 6174 7474  rmat_type(a.attt
+00001540: 7970 6964 2c20 612e 6174 7474 7970 6d6f  ypid, a.atttypmo
+00001550: 6429 2c0a 2020 2020 2020 2020 2020 2020  d),.            
+00001560: 280a 2020 2020 2020 2020 2020 2020 5345  (.            SE
+00001570: 4c45 4354 2070 675f 6361 7461 6c6f 672e  LECT pg_catalog.
+00001580: 7067 5f67 6574 5f65 7870 7228 642e 6164  pg_get_expr(d.ad
+00001590: 6269 6e2c 2064 2e61 6472 656c 6964 290a  bin, d.adrelid).
+000015a0: 2020 2020 2020 2020 2020 2020 4652 4f4d              FROM
+000015b0: 2070 675f 6361 7461 6c6f 672e 7067 5f61   pg_catalog.pg_a
+000015c0: 7474 7264 6566 2064 0a20 2020 2020 2020  ttrdef d.       
+000015d0: 2020 2020 2057 4845 5245 2064 2e61 6472       WHERE d.adr
+000015e0: 656c 6964 203d 2061 2e61 7474 7265 6c69  elid = a.attreli
+000015f0: 6420 414e 4420 642e 6164 6e75 6d20 3d20  d AND d.adnum = 
+00001600: 612e 6174 746e 756d 0a20 2020 2020 2020  a.attnum.       
+00001610: 2020 2020 2041 4e44 2061 2e61 7474 6861       AND a.attha
+00001620: 7364 6566 0a20 2020 2020 2020 2020 2020  sdef.           
+00001630: 2029 2041 5320 4445 4641 554c 542c 0a20   ) AS DEFAULT,. 
+00001640: 2020 2020 2020 2020 2020 2061 2e61 7474             a.att
+00001650: 6e6f 746e 756c 6c2c 0a20 2020 2020 2020  notnull,.       
+00001660: 2020 2020 2061 2e61 7474 7265 6c69 6420       a.attrelid 
+00001670: 6173 2074 6162 6c65 5f6f 6964 2c0a 2020  as table_oid,.  
+00001680: 2020 2020 2020 2020 2020 7067 642e 6465            pgd.de
+00001690: 7363 7269 7074 696f 6e20 6173 2063 6f6d  scription as com
+000016a0: 6d65 6e74 2c0a 2020 2020 2020 2020 2020  ment,.          
+000016b0: 2020 7b67 656e 6572 6174 6564 7d2c 0a20    {generated},. 
+000016c0: 2020 2020 2020 2020 2020 207b 6964 656e             {iden
+000016d0: 7469 7479 7d0a 2020 2020 2020 2020 4652  tity}.        FR
+000016e0: 4f4d 2070 675f 6361 7461 6c6f 672e 7067  OM pg_catalog.pg
+000016f0: 5f61 7474 7269 6275 7465 2061 0a20 2020  _attribute a.   
+00001700: 2020 2020 204c 4546 5420 4a4f 494e 2070       LEFT JOIN p
+00001710: 675f 6361 7461 6c6f 672e 7067 5f64 6573  g_catalog.pg_des
+00001720: 6372 6970 7469 6f6e 2070 6764 204f 4e20  cription pgd ON 
+00001730: 280a 2020 2020 2020 2020 2020 2020 7067  (.            pg
+00001740: 642e 6f62 6a6f 6964 203d 2061 2e61 7474  d.objoid = a.att
+00001750: 7265 6c69 6420 414e 4420 7067 642e 6f62  relid AND pgd.ob
+00001760: 6a73 7562 6964 203d 2061 2e61 7474 6e75  jsubid = a.attnu
+00001770: 6d29 0a20 2020 2020 2020 2057 4845 5245  m).        WHERE
+00001780: 2061 2e61 7474 7265 6c69 6420 3d20 3a74   a.attrelid = :t
+00001790: 6162 6c65 5f6f 6964 0a20 2020 2020 2020  able_oid.       
+000017a0: 2041 4e44 2061 2e61 7474 6e75 6d20 3e20   AND a.attnum > 
+000017b0: 3020 414e 4420 4e4f 5420 612e 6174 7469  0 AND NOT a.atti
+000017c0: 7364 726f 7070 6564 0a20 2020 2020 2020  sdropped.       
+000017d0: 204f 5244 4552 2042 5920 612e 6174 746e   ORDER BY a.attn
+000017e0: 756d 0a20 2020 2022 2222 0a0a 504f 5354  um.    """..POST
+000017f0: 4752 4553 5f47 4554 5f53 4552 5645 525f  GRES_GET_SERVER_
+00001800: 5645 5253 494f 4e20 3d20 2222 220a 7368  VERSION = """.sh
+00001810: 6f77 2073 6572 7665 725f 7665 7273 696f  ow server_versio
+00001820: 6e0a 2222 220a                           n.""".
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/postgres/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/postgres/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/presto/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/presto/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/query_parser_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/query_parser_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/connection.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,74 +8,67 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
+from functools import partial
 from typing import Optional
 
-from sqlalchemy.engine import Engine
-
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
-    RedshiftConnection,
-    SslMode,
+from metadata.generated.schema.entity.services.connections.metadata.amundsenConnection import (
+    AmundsenConnection,
 )
-from metadata.ingestion.connections.builders import (
-    create_generic_db_connection,
-    get_connection_args_common,
-    get_connection_url_common,
-    init_empty_connection_arguments,
+from metadata.ingestion.connections.test_connections import (
+    SourceConnectionException,
+    test_connection_steps,
 )
-from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.redshift.queries import (
-    REDSHIFT_GET_DATABASE_NAMES,
-    REDSHIFT_TEST_GET_QUERIES,
-    REDSHIFT_TEST_PARTITION_DETAILS,
+from metadata.ingestion.source.metadata.amundsen.client import Neo4JConfig, Neo4jHelper
+from metadata.ingestion.source.metadata.amundsen.queries import (
+    NEO4J_AMUNDSEN_USER_QUERY,
 )
 
 
-def get_connection(connection: RedshiftConnection) -> Engine:
+def get_connection(connection: AmundsenConnection) -> Neo4jHelper:
     """
     Create connection
     """
-    if connection.sslMode:
-        if not connection.connectionArguments:
-            connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
-        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
-            connection.connectionArguments.__root__[
-                "sslrootcert"
-            ] = connection.sslConfig.__root__.certificatePath
-    return create_generic_db_connection(
-        connection=connection,
-        get_connection_url_fn=get_connection_url_common,
-        get_connection_args_fn=get_connection_args_common,
-    )
+    try:
+        neo4j_config = Neo4JConfig(
+            username=connection.username,
+            password=connection.password.get_secret_value(),
+            neo4j_url=connection.hostPort,
+            max_connection_life_time=connection.maxConnectionLifeTime,
+            neo4j_encrypted=connection.encrypted,
+            neo4j_validate_ssl=connection.validateSSL,
+        )
+        return Neo4jHelper(neo4j_config)
+    except Exception as exc:
+        msg = f"Unknown error connecting with {connection}: {exc}."
+        raise SourceConnectionException(msg)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    engine: Engine,
-    service_connection: RedshiftConnection,
+    client: Neo4jHelper,
+    service_connection: AmundsenConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    queries = {
-        "GetQueries": REDSHIFT_TEST_GET_QUERIES,
-        "GetDatabases": REDSHIFT_GET_DATABASE_NAMES,
-        "GetPartitionTableDetails": REDSHIFT_TEST_PARTITION_DETAILS,
+
+    test_fn = {
+        "CheckAccess": partial(client.execute_query, query=NEO4J_AMUNDSEN_USER_QUERY)
     }
-    test_connection_db_common(
+
+    test_connection_steps(
         metadata=metadata,
-        engine=engine,
-        service_connection=service_connection,
+        test_fn=test_fn,
+        service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
-        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/lineage.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,33 +5,33 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Redshift usage module
+Redshift lineage module
 
 Execute with
 
 source:
   type: redshift-lineage
   serviceName: aws_redshift_demo_2
   sourceConfig:
     config:
       queryLogDuration: 1
 sink:
   type: metadata-rest
   config: {}
 workflowConfig:
-  loggerLevel: INFO
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
-    authProvider: no-auth
-
+    authProvider: openmetadata
+    securityConfig:
+      jwtToken: "token"
 """
 
 from metadata.ingestion.source.database.lineage_source import LineageSource
 from metadata.ingestion.source.database.redshift.queries import REDSHIFT_SQL_STATEMENT
 from metadata.ingestion.source.database.redshift.query_parser import (
     RedshiftQueryParserSource,
 )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/metadata.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,556 +4,392 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""
-Redshift source ingestion
-"""
-
-import re
+"""S3 object store extraction metadata"""
+import json
+import secrets
 import traceback
-from collections import defaultdict
-from typing import Iterable, List, Optional, Tuple
-
-import sqlalchemy as sa
-from packaging.version import Version
-from sqlalchemy import inspect, sql, util
-from sqlalchemy.dialects.postgresql.base import ENUM, PGDialect
-from sqlalchemy.dialects.postgresql.base import ischema_names as pg_ischema_names
-from sqlalchemy.engine import reflection
-from sqlalchemy.engine.reflection import Inspector
-from sqlalchemy.sql import sqltypes
-from sqlalchemy_redshift.dialect import (
-    REDSHIFT_ISCHEMA_NAMES,
-    RedshiftDialect,
-    RedshiftDialectMixin,
-    RelationKey,
+from datetime import datetime, timedelta
+from enum import Enum
+from typing import Dict, Iterable, List, Optional
+
+from pandas import DataFrame
+from pydantic import ValidationError
+
+from metadata.generated.schema.api.data.createContainer import CreateContainerRequest
+from metadata.generated.schema.entity.data import container
+from metadata.generated.schema.entity.data.container import (
+    Container,
+    ContainerDataModel,
 )
-
-from metadata.generated.schema.entity.data.database import Database
-from metadata.generated.schema.entity.data.table import (
-    ConstraintType,
-    IntervalType,
-    TableConstraint,
-    TablePartition,
-    TableType,
-)
-from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
-    RedshiftConnection,
+from metadata.generated.schema.entity.data.table import Column
+from metadata.generated.schema.entity.services.connections.database.datalake.s3Config import (
+    S3Config,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
+from metadata.generated.schema.entity.services.connections.storage.s3Connection import (
+    S3Connection,
+)
+from metadata.generated.schema.metadataIngestion.storage.containerMetadataConfig import (
+    MetadataEntry,
+    StorageContainerConfig,
+)
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
-from metadata.ingestion.source.database.common_db_source import (
-    CommonDbSourceService,
-    TableNameAndType,
+from metadata.ingestion.source.database.datalake.metadata import DatalakeSource
+from metadata.ingestion.source.database.datalake.models import (
+    DatalakeTableSchemaWrapper,
 )
-from metadata.ingestion.source.database.redshift.queries import (
-    REDSHIFT_GET_ALL_RELATION_INFO,
-    REDSHIFT_GET_DATABASE_NAMES,
-    REDSHIFT_GET_SCHEMA_COLUMN_INFO,
-    REDSHIFT_PARTITION_DETAILS,
-    REDSHIFT_TABLE_COMMENTS,
+from metadata.ingestion.source.storage.s3.models import (
+    S3BucketResponse,
+    S3ContainerDetails,
 )
-from metadata.utils import fqn
-from metadata.utils.filters import filter_by_database
+from metadata.ingestion.source.storage.storage_service import StorageServiceSource
+from metadata.utils.datalake.datalake_utils import fetch_dataframe
+from metadata.utils.filters import filter_by_container
 from metadata.utils.logger import ingestion_logger
-from metadata.utils.sqlalchemy_utils import (
-    get_all_table_comments,
-    get_table_comment_wrapper,
-)
-
-sa_version = Version(sa.__version__)
 
 logger = ingestion_logger()
 
-ischema_names = pg_ischema_names
-GEOGRAPHY = create_sqlalchemy_type("GEOGRAPHY")
-ischema_names["geography"] = GEOGRAPHY
-ischema_names.update({"binary varying": sqltypes.VARBINARY})
-ischema_names.update(REDSHIFT_ISCHEMA_NAMES)
-
-
-# pylint: disable=protected-access
-@reflection.cache
-def get_columns(self, connection, table_name, schema=None, **kw):
-    """
-    Return information about columns in `table_name`.
+S3_CLIENT_ROOT_RESPONSE = "Contents"
+OPENMETADATA_TEMPLATE_FILE_NAME = "openmetadata.json"
+S3_KEY_SEPARATOR = "/"
 
-    Overrides interface
-    :meth:`~sqlalchemy.engine.interfaces.Dialect.get_columns`.
 
-    overriding the default dialect method to include the
-    distkey and sortkey info
-    """
-    cols = self._get_redshift_columns(connection, table_name, schema, **kw)
-    if not self._domains:
-        self._domains = self._load_domains(connection)
-    domains = self._domains
-    columns = []
-    for col in cols:
-        column_info = self._get_column_info(
-            name=col.name,
-            format_type=col.format_type,
-            default=col.default,
-            notnull=col.notnull,
-            domains=domains,
-            enums=[],
-            schema=col.schema,
-            encode=col.encode,
-            comment=col.comment,
-        )
-        column_info["distkey"] = col.distkey
-        column_info["sortkey"] = col.sortkey
-        column_info["system_data_type"] = col.format_type
-        columns.append(column_info)
-    return columns
+class S3Metric(Enum):
+    NUMBER_OF_OBJECTS = "NumberOfObjects"
+    BUCKET_SIZE_BYTES = "BucketSizeBytes"
 
 
-def _get_column_info(self, *args, **kwargs):
-    """
-    Get column info
-
-    Args:
-        *args:
-        **kwargs:
-    Returns
+class S3Source(StorageServiceSource):
     """
-    kwdrs = kwargs.copy()
-    encode = kwdrs.pop("encode", None)
-    if sa_version >= Version("1.3.16"):
-        kwdrs["generated"] = ""
-    if sa_version < Version("1.4.0") and "identity" in kwdrs:
-        del kwdrs["identity"]
-    elif sa_version >= Version("1.4.0") and "identity" not in kwdrs:
-        kwdrs["identity"] = None
-    column_info = super(  # pylint: disable=protected-access
-        RedshiftDialectMixin, self
-    )._get_column_info(*args, **kwdrs)
-
-    # raw_data_type is not included in column_info as
-    # redhift doesn't support complex data types directly
-    # https://docs.aws.amazon.com/redshift/latest/dg/c_Supported_data_types.html
-
-    if "info" not in column_info:
-        column_info["info"] = {}
-    if encode and encode != "none":
-        column_info["info"]["encode"] = encode
-    return column_info
-
-
-@reflection.cache
-def _get_schema_column_info(
-    self, connection, schema=None, **kw
-):  # pylint: disable=unused-argument
+    Source implementation to ingest S3 buckets data.
     """
-    Get schema column info
 
-    Args:
-        connection:
-        schema:
-        **kw:
-    Returns:
+    def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
+        super().__init__(config, metadata_config)
+        self.s3_client = self.connection.s3_client
+        self.cloudwatch_client = self.connection.cloudwatch_client
 
-    This method is responsible for fetching all the column details like
-    name, type, constraints, distkey and sortkey etc.
-    """
-    schema_clause = f"AND schema = '{schema if schema else ''}'"
-    all_columns = defaultdict(list)
-    with connection.connect() as cnct:
-        result = cnct.execute(
-            REDSHIFT_GET_SCHEMA_COLUMN_INFO.format(schema_clause=schema_clause)
-        )
-        for col in result:
-            key = RelationKey(col.table_name, col.schema, connection)
-            all_columns[key].append(col)
-    return dict(all_columns)
+        self._bucket_cache: Dict[str, Container] = {}
 
+    @classmethod
+    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: S3Connection = config.serviceConnection.__root__.config
+        if not isinstance(connection, S3Connection):
+            raise InvalidSourceException(
+                f"Expected S3StoreConnection, but got {connection}"
+            )
+        return cls(config, metadata_config)
 
-RedshiftDialectMixin._get_column_info = (  # pylint: disable=protected-access
-    _get_column_info
-)
-RedshiftDialectMixin._get_schema_column_info = (  # pylint: disable=protected-access
-    _get_schema_column_info
-)
-RedshiftDialectMixin.get_columns = get_columns
+    def get_containers(self) -> Iterable[S3ContainerDetails]:
+        bucket_results = self.fetch_buckets()
 
+        for bucket_response in bucket_results:
+            try:
 
-def _handle_array_type(attype):
-    return (
-        # strip '[]' from integer[], etc.
-        re.sub(r"\[\]$", "", attype),
-        attype.endswith("[]"),
-    )
-
-
-def _init_args(format_type):
-    args = re.search(r"\((.*)\)", format_type)
-    if args and args.group(1):
-        args = tuple(re.split(r"\s*,\s*", args.group(1)))
-    else:
-        args = ()
-    return args
-
-
-def _get_kwargs_for_time_type(kwargs, charlen, attype):
-    if charlen:
-        kwargs["precision"] = int(charlen)
-    if attype in {"timestamp with time zone", "time with time zone"}:
-        kwargs["timezone"] = True
-    else:
-        kwargs["timezone"] = False
-    return kwargs
-
-
-def _get_args_and_kwargs(charlen, attype, format_type):
-    kwargs = {}
-    args = _init_args(format_type)
-    if attype == "numeric" and charlen:
-        prec, scale = charlen.split(",")
-        args = (int(prec), int(scale))
-
-    elif attype == "double precision":
-        args = (53,)
-
-    elif attype in {
-        "timestamp with time zone",
-        "time with time zone",
-        "timestamp without time zone",
-        "time without time zone",
-        "time",
-    }:
-        kwargs = _get_kwargs_for_time_type(kwargs, charlen, attype)
-
-    elif attype == "bit varying":
-        kwargs["varying"] = True
-        if charlen:
-            args = (int(charlen),)
-
-    elif attype.startswith("interval"):
-        field_match = re.match(r"interval (.+)", attype, re.I)
-        if charlen:
-            kwargs["precision"] = int(charlen)
-        if field_match:
-            kwargs["fields"] = field_match.group(1)
-    elif charlen:
-        args = (int(charlen),)
-    return args, kwargs
-
-
-def _update_column_info(  # pylint: disable=too-many-arguments
-    default, schema, coltype, autoincrement, name, nullable, identity, comment, computed
-):
-    if default is not None:
-        match = re.search(r"""(nextval\(')([^']+)('.*$)""", default)
-        if match is not None:
-            if issubclass(
-                coltype._type_affinity,  # pylint: disable=protected-access
-                sqltypes.Integer,
-            ):
-                autoincrement = True
-            # the default is related to a Sequence
-            sch = schema
-            if "." not in match.group(2) and sch is not None:
-                # unconditionally quote the schema name.  this could
-                # later be enhanced to obey quoting rules /
-                # "quote schema"
-                default = (
-                    match.group(1)
-                    + (f'"{sch}"')
-                    + "."
-                    + match.group(2)
-                    + match.group(3)
+                # We always try to generate the parent container (the bucket)
+                yield self._generate_unstructured_container(
+                    bucket_response=bucket_response
                 )
-    column_info = {
-        "name": name,
-        "type": coltype,
-        "nullable": nullable,
-        "default": default,
-        "autoincrement": autoincrement or identity is not None,
-        "comment": comment,
-    }
-    if computed is not None:
-        column_info["computed"] = computed
-    if identity is not None:
-        column_info["identity"] = identity
-    return column_info
-
-
-def _update_coltype(coltype, args, kwargs, attype, name, is_array):
-    if coltype:
-        coltype = coltype(*args, **kwargs)
-        if is_array:
-            coltype = ischema_names["_array"](coltype)
-    else:
-        util.warn(f"Did not recognize type '{attype}' of column '{name}'")
-        coltype = sqltypes.NULLTYPE
-    return coltype
-
-
-def _update_computed_and_default(generated, default):
-    computed = None
-    if generated not in (None, "", b"\x00"):
-        computed = {
-            "sqltext": default,
-            "persisted": generated in ("s", b"s"),
-        }
-        default = None
-    return computed, default
-
-
-def _get_charlen(format_type):
-    charlen = re.search(r"\(([\d,]+)\)", format_type)
-    if charlen:
-        charlen = charlen.group(1)
-    return charlen
-
-
-@reflection.cache
-def _get_column_info(  # pylint: disable=too-many-locals,too-many-arguments, unused-argument
-    self,
-    name,
-    format_type,
-    default,
-    notnull,
-    domains,
-    enums,
-    schema,
-    comment,
-    generated,
-    identity,
-):
-    # strip (*) from character varying(5), timestamp(5)
-    # with time zone, geometry(POLYGON), etc.
-    attype = re.sub(r"\(.*\)", "", format_type)
-
-    # strip '[]' from integer[], etc. and check if an array
-    attype, is_array = _handle_array_type(attype)
-
-    # strip quotes from case sensitive enum or domain names
-    enum_or_domain_key = tuple(util.quoted_token_parser(attype))
-
-    nullable = not notnull
-    charlen = _get_charlen(format_type)
-    args, kwargs = _get_args_and_kwargs(charlen, attype, format_type)
-    if attype.startswith("interval"):
-        attype = "interval"
-    while True:
-        # looping here to suit nested domains
-        if attype in ischema_names:
-            coltype = ischema_names[attype]
-            break
-        if enum_or_domain_key in enums:
-            enum = enums[enum_or_domain_key]
-            coltype = ENUM
-            kwargs["name"] = enum["name"]
-            if not enum["visible"]:
-                kwargs["schema"] = enum["schema"]
-            args = tuple(enum["labels"])
-            break
-        if enum_or_domain_key in domains:
-            domain = domains[enum_or_domain_key]
-            attype = domain["attype"]
-            attype, is_array = _handle_array_type(attype)
-            # strip quotes from case sensitive enum or domain names
-            enum_or_domain_key = tuple(util.quoted_token_parser(attype))
-            # A table can't override a not null on the domain,
-            # but can override nullable
-            nullable = nullable and domain["nullable"]
-            if domain["default"] and not default:
-                # It can, however, override the default
-                # value, but can't set it to null.
-                default = domain["default"]
-        else:
-            coltype = None
-            break
-
-    coltype = _update_coltype(coltype, args, kwargs, attype, name, is_array)
-
-    # If a zero byte or blank string depending on driver (is also absent
-    # for older PG versions), then not a generated column. Otherwise, s =
-    # stored. (Other values might be added in the future.)
-    computed, default = _update_computed_and_default(generated, default)
-
-    # adjust the default value
-    autoincrement = False
-    column_info = _update_column_info(
-        default,
-        schema,
-        coltype,
-        autoincrement,
-        name,
-        nullable,
-        identity,
-        comment,
-        computed,
-    )
-
-    return column_info
-
-
-PGDialect._get_column_info = _get_column_info  # pylint: disable=protected-access
-
-STANDARD_TABLE_TYPES = {
-    "r": TableType.Local,
-    "e": TableType.External,
-    "v": TableType.View,
-}
-
-
-@reflection.cache
-def get_table_comment(
-    self, connection, table_name, schema=None, **kw  # pylint: disable=unused-argument
-):
-    return get_table_comment_wrapper(
-        self,
-        connection,
-        table_name=table_name,
-        schema=schema,
-        query=REDSHIFT_TABLE_COMMENTS,
-    )
-
-
-RedshiftDialect.get_all_table_comments = get_all_table_comments
-RedshiftDialect.get_table_comment = get_table_comment
-
+                self._bucket_cache[bucket_response.name] = self.context.container
 
-class RedshiftSource(CommonDbSourceService):
-    """
-    Implements the necessary methods to extract
-    Database metadata from Redshift Source
-    """
+                metadata_config = self._load_metadata_file(
+                    bucket_name=bucket_response.name
+                )
+                if metadata_config:
+                    for metadata_entry in metadata_config.entries:
+                        logger.info(
+                            f"Extracting metadata from path {metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)} "
+                            f"and generating structured container"
+                        )
+                        structured_container: Optional[
+                            S3ContainerDetails
+                        ] = self._generate_container_details(
+                            bucket_response=bucket_response,
+                            metadata_entry=metadata_entry,
+                            parent=EntityReference(
+                                id=self._bucket_cache[bucket_response.name].id.__root__,
+                                type="container",
+                            ),
+                        )
+                        if structured_container:
+                            yield structured_container
+
+            except ValidationError as err:
+                error = f"Validation error while creating Container from bucket details - {err}"
+                logger.debug(traceback.format_exc())
+                logger.warning(error)
+                self.status.failed(bucket_response.name, error, traceback.format_exc())
+            except Exception as err:
+                error = (
+                    f"Wild error while creating Container from bucket details - {err}"
+                )
+                logger.debug(traceback.format_exc())
+                logger.warning(error)
+                self.status.failed(bucket_response.name, error, traceback.format_exc())
+
+    def yield_create_container_requests(
+        self, container_details: S3ContainerDetails
+    ) -> Iterable[CreateContainerRequest]:
+        yield CreateContainerRequest(
+            name=container_details.name,
+            prefix=container_details.prefix,
+            numberOfObjects=container_details.number_of_objects,
+            size=container_details.size,
+            dataModel=container_details.data_model,
+            service=self.context.objectstore_service.fullyQualifiedName,
+            parent=container_details.parent,
+        )
 
-    def __init__(self, config, metadata_config):
-        super().__init__(config, metadata_config)
-        self.partition_details = {}
+    def _generate_container_details(
+        self,
+        bucket_response: S3BucketResponse,
+        metadata_entry: MetadataEntry,
+        parent: Optional[EntityReference] = None,
+    ) -> Optional[S3ContainerDetails]:
+        bucket_name = bucket_response.name
+        sample_key = self._get_sample_file_path(
+            bucket_name=bucket_name, metadata_entry=metadata_entry
+        )
+        # if we have a sample file to fetch a schema from
+        if sample_key:
 
-    @classmethod
-    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: RedshiftConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, RedshiftConnection):
-            raise InvalidSourceException(
-                f"Expected RedshiftConnection, but got {connection}"
+            columns = self._get_columns(
+                bucket_name=bucket_name,
+                sample_key=sample_key,
+                metadata_entry=metadata_entry,
             )
-        return cls(config, metadata_config)
+            if columns:
+                return S3ContainerDetails(
+                    name=metadata_entry.dataPath.strip(S3_KEY_SEPARATOR),
+                    prefix=f"{S3_KEY_SEPARATOR}{metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)}",
+                    creation_date=bucket_response.creation_date.isoformat(),
+                    number_of_objects=self._fetch_metric(
+                        bucket_name=bucket_name, metric=S3Metric.NUMBER_OF_OBJECTS
+                    ),
+                    size=self._fetch_metric(
+                        bucket_name=bucket_name, metric=S3Metric.BUCKET_SIZE_BYTES
+                    ),
+                    file_formats=[container.FileFormat(metadata_entry.structureFormat)],
+                    data_model=ContainerDataModel(
+                        isPartitioned=metadata_entry.isPartitioned, columns=columns
+                    ),
+                    parent=parent,
+                )
+        return None
 
-    def get_partition_details(self) -> None:
+    def _get_columns(
+        self, bucket_name: str, sample_key: str, metadata_entry: MetadataEntry
+    ) -> Optional[List[Column]]:
         """
-        Populate partition details
+        Get the columns from the file and partition information
         """
-        try:
-            self.partition_details.clear()
-            results = self.engine.execute(REDSHIFT_PARTITION_DETAILS).fetchall()
-            for row in results:
-                self.partition_details[f"{row.schema}.{row.table}"] = row.diststyle
-        except Exception as exe:
-            logger.debug(traceback.format_exc())
-            logger.debug(f"Failed to fetch partition details due: {exe}")
+        extracted_cols = self.extract_column_definitions(bucket_name, sample_key)
+        return (metadata_entry.partitionColumns or []) + (extracted_cols or [])
 
-    def query_table_names_and_types(
-        self, schema_name: str
-    ) -> Iterable[TableNameAndType]:
+    def extract_column_definitions(
+        self, bucket_name: str, sample_key: str
+    ) -> List[Column]:
         """
-        Handle custom table types
+        Extract Column related metadata from s3
         """
-
-        result = self.connection.execute(
-            sql.text(REDSHIFT_GET_ALL_RELATION_INFO),
-            {"schema": schema_name},
+        connection_args = self.service_connection.awsConfig
+        data_structure_details = fetch_dataframe(
+            config_source=S3Config(),
+            client=self.s3_client,
+            file_fqn=DatalakeTableSchemaWrapper(
+                key=sample_key, bucket_name=bucket_name
+            ),
+            connection_kwargs=connection_args,
         )
+        columns = []
+        if isinstance(data_structure_details, DataFrame):
+            columns = DatalakeSource.get_columns(data_structure_details)
+        if isinstance(data_structure_details, list) and data_structure_details:
+            columns = DatalakeSource.get_columns(data_structure_details[0])
+        return columns
+
+    def fetch_buckets(self) -> List[S3BucketResponse]:
+        results: List[S3BucketResponse] = []
+        try:
+            # No pagination required, as there is a hard 1000 limit on nr of buckets per aws account
+            for bucket in self.s3_client.list_buckets().get("Buckets") or []:
+                if filter_by_container(
+                    self.source_config.containerFilterPattern,
+                    container_name=bucket["Name"],
+                ):
+                    self.status.filter(bucket["Name"], "Bucket Filtered Out")
+                else:
+                    results.append(S3BucketResponse.parse_obj(bucket))
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Failed to fetch buckets list - {err}")
+        return results
 
-        return [
-            TableNameAndType(
-                name=name, type_=STANDARD_TABLE_TYPES.get(relkind, TableType.Regular)
+    def _fetch_metric(self, bucket_name: str, metric: S3Metric) -> float:
+        try:
+            raw_result = self.cloudwatch_client.get_metric_data(
+                MetricDataQueries=[
+                    {
+                        "Id": "total_nr_of_object_request",
+                        "MetricStat": {
+                            "Metric": {
+                                "Namespace": "AWS/S3",
+                                "MetricName": metric.value,
+                                "Dimensions": [
+                                    {"Name": "BucketName", "Value": bucket_name},
+                                    {
+                                        "Name": "StorageType",
+                                        # StandardStorage-only support for BucketSizeBytes for now
+                                        "Value": "StandardStorage"
+                                        if metric == S3Metric.BUCKET_SIZE_BYTES
+                                        else "AllStorageTypes",
+                                    },
+                                ],
+                            },
+                            "Period": 60,
+                            "Stat": "Average",
+                            "Unit": "Bytes"
+                            if metric == S3Metric.BUCKET_SIZE_BYTES
+                            else "Count",
+                        },
+                    },
+                ],
+                StartTime=datetime.now() - timedelta(days=2),
+                # metrics generated daily, ensure there is at least 1 entry
+                EndTime=datetime.now(),
+                ScanBy="TimestampDescending",
+            )
+            if raw_result["MetricDataResults"]:
+                first_metric = raw_result["MetricDataResults"][0]
+                if first_metric["StatusCode"] == "Complete" and first_metric["Values"]:
+                    return int(first_metric["Values"][0])
+        except Exception:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Failed fetching metric {metric.value} for bucket {bucket_name}, returning 0"
             )
-            for name, relkind in result
-        ]
+        return 0
 
-    def get_database_names(self) -> Iterable[str]:
-        if not self.config.serviceConnection.__root__.config.ingestAllDatabases:
-            self.inspector = inspect(self.engine)
-            self.get_partition_details()
-            yield self.config.serviceConnection.__root__.config.database
-        else:
-            results = self.connection.execute(REDSHIFT_GET_DATABASE_NAMES)
-            for res in results:
-                row = list(res)
-                new_database = row[0]
-                database_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Database,
-                    service_name=self.context.database_service.name.__root__,
-                    database_name=new_database,
+    def _load_metadata_file(self, bucket_name: str) -> Optional[StorageContainerConfig]:
+        """
+        Load the metadata template file from the root of the bucket, if it exists
+        """
+        if self._is_metadata_file_present(bucket_name=bucket_name):
+            try:
+                logger.info(
+                    f"Found metadata template file at - s3://{bucket_name}/{OPENMETADATA_TEMPLATE_FILE_NAME}"
+                )
+                response_object = self.s3_client.get_object(
+                    Bucket=bucket_name, Key=OPENMETADATA_TEMPLATE_FILE_NAME
+                )
+                content = json.load(response_object["Body"])
+                metadata_config = StorageContainerConfig.parse_obj(content)
+                return metadata_config
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Failed loading metadata file s3://{bucket_name}/{OPENMETADATA_TEMPLATE_FILE_NAME}-{exc}"
                 )
+        return None
 
-                if filter_by_database(
-                    self.source_config.databaseFilterPattern,
-                    database_fqn
-                    if self.source_config.useFqnForFiltering
-                    else new_database,
-                ):
-                    self.status.filter(database_fqn, "Database Filtered Out")
-                    continue
+    def _is_metadata_file_present(self, bucket_name: str):
+        return self.prefix_exits(
+            bucket_name=bucket_name,
+            prefix=OPENMETADATA_TEMPLATE_FILE_NAME,
+        )
 
-                try:
-                    self.set_inspector(database_name=new_database)
-                    self.get_partition_details()
-                    yield new_database
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.error(
-                        f"Error trying to connect to database {new_database}: {exc}"
-                    )
+    def _generate_unstructured_container(
+        self, bucket_response: S3BucketResponse
+    ) -> S3ContainerDetails:
+        return S3ContainerDetails(
+            name=bucket_response.name,
+            prefix=S3_KEY_SEPARATOR,
+            creation_date=bucket_response.creation_date.isoformat(),
+            number_of_objects=self._fetch_metric(
+                bucket_name=bucket_response.name, metric=S3Metric.NUMBER_OF_OBJECTS
+            ),
+            size=self._fetch_metric(
+                bucket_name=bucket_response.name, metric=S3Metric.BUCKET_SIZE_BYTES
+            ),
+            file_formats=[],  # TODO should we fetch some random files by extension here? Would it be valuable info?
+            data_model=None,
+        )
 
-    def _get_partition_key(self, diststyle: str) -> Optional[List[str]]:
+    @staticmethod
+    def _get_sample_file_prefix(metadata_entry: MetadataEntry) -> Optional[str]:
+        """
+        Return a prefix if we have structure data to read
+        """
+        result = f"{metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)}"
+        if not metadata_entry.structureFormat:
+            logger.warning(f"Ignoring un-structured metadata entry {result}")
+            return None
+        return result
+
+    def _get_sample_file_path(
+        self, bucket_name: str, metadata_entry: MetadataEntry
+    ) -> Optional[str]:
+        """
+        Given a bucket and a metadata entry, returns the full path key to a file which can then be used to infer schema
+        or None in the case of a non-structured metadata entry, or if no such keys can be found
+        """
+        prefix = self._get_sample_file_prefix(metadata_entry=metadata_entry)
+        # no objects found in the data path
+        if not self.prefix_exits(bucket_name=bucket_name, prefix=prefix):
+            logger.warning(f"Ignoring metadata entry {prefix} - no files found")
+            return None
+        # this will look only in the first 1000 files under that path (default for list_objects_v2).
+        # We'd rather not do pagination here as it would incur unwanted costs
         try:
-            regex = re.match(r"KEY\((\w+)\)", diststyle)
-            if regex:
-                return [regex.group(1)]
-        except Exception as err:
+            response = self.s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
+            candidate_keys = [
+                entry["Key"]
+                for entry in response[S3_CLIENT_ROOT_RESPONSE]
+                if entry
+                and entry.get("Key")
+                and entry["Key"].endswith(metadata_entry.structureFormat)
+            ]
+            # pick a random key out of the candidates if any were returned
+            if candidate_keys:
+                result_key = secrets.choice(candidate_keys)
+                logger.info(
+                    f"File {result_key} was picked to infer data structure from."
+                )
+                return result_key
+            logger.warning(
+                f"No sample files found in {prefix} with {metadata_entry.structureFormat} extension"
+            )
+            return None
+        except Exception:
             logger.debug(traceback.format_exc())
-            logger.warning(err)
-        return None
-
-    def get_table_partition_details(
-        self, table_name: str, schema_name: str, inspector: Inspector
-    ) -> Tuple[bool, TablePartition]:
-        diststyle = self.partition_details.get(f"{schema_name}.{table_name}")
-        if diststyle:
-            partition_details = TablePartition(
-                columns=self._get_partition_key(diststyle),
-                intervalType=IntervalType.COLUMN_VALUE,
+            logger.warning(
+                f"Error when trying to list objects in S3 bucket {bucket_name} at prefix {prefix}"
             )
-            return True, partition_details
-        return False, None
+            return None
 
-    def process_additional_table_constraints(
-        self, column: dict, table_constraints: List[TableConstraint]
-    ) -> None:
+    def prefix_exits(self, bucket_name: str, prefix: str) -> bool:
         """
-        Process DIST_KEY & SORT_KEY column properties
+        Checks if a given prefix exists in a bucket
         """
-
-        if column.get("distkey"):
-            table_constraints.append(
-                TableConstraint(
-                    constraintType=ConstraintType.DIST_KEY,
-                    columns=[column.get("name")],
-                )
+        try:
+            res = self.s3_client.list_objects_v2(
+                Bucket=bucket_name, Prefix=prefix, MaxKeys=1
             )
-
-        if column.get("sortkey"):
-            table_constraints.append(
-                TableConstraint(
-                    constraintType=ConstraintType.SORT_KEY,
-                    columns=[column.get("name")],
-                )
+            return S3_CLIENT_ROOT_RESPONSE in res
+        except Exception:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Failed when trying to check if S3 prefix {prefix} exists in bucket {bucket_name}"
             )
+            return False
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/queries.py`

 * *Files 23% similar despite different names*

```diff
@@ -225,7 +225,52 @@
 (select 1 from pg_catalog.stl_querytext limit 1)
 UNION
 (select 1 from pg_catalog.stl_query limit 1)
 """
 
 
 REDSHIFT_TEST_PARTITION_DETAILS = "select * from SVV_TABLE_INFO limit 1"
+
+
+# Redshift views definitions only contains the select query
+# hence we are appending "create view <schema>.<table> as " to select query
+# to generate the column level lineage
+REDSHIFT_GET_ALL_RELATIONS = """
+    SELECT
+        c.relkind,
+        n.oid as "schema_oid",
+        n.nspname as "schema",
+        c.oid as "rel_oid",
+        c.relname,
+        CASE c.reldiststyle
+        WHEN 0 THEN 'EVEN' WHEN 1 THEN 'KEY' WHEN 8 THEN 'ALL' END
+        AS "diststyle",
+        c.relowner AS "owner_id",
+        u.usename AS "owner_name",
+        TRIM(TRAILING ';' FROM 
+        'create view ' || n.nspname || '.' || c.relname || ' as ' ||pg_catalog.pg_get_viewdef(c.oid, true))
+        AS "view_definition",
+        pg_catalog.array_to_string(c.relacl, '\n') AS "privileges"
+    FROM pg_catalog.pg_class c
+            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace
+            JOIN pg_catalog.pg_user u ON u.usesysid = c.relowner
+    WHERE c.relkind IN ('r', 'v', 'm', 'S', 'f')
+        AND n.nspname !~ '^pg_' {schema_clause} {table_clause}
+    UNION
+    SELECT
+        'r' AS "relkind",
+        s.esoid AS "schema_oid",
+        s.schemaname AS "schema",
+        null AS "rel_oid",
+        t.tablename AS "relname",
+        null AS "diststyle",
+        s.esowner AS "owner_id",
+        u.usename AS "owner_name",
+        null AS "view_definition",
+        null AS "privileges"
+    FROM
+        svv_external_tables t
+        JOIN svv_external_schemas s ON s.schemaname = t.schemaname
+        JOIN pg_catalog.pg_user u ON u.usesysid = s.esowner
+    where 1 {schema_clause} {table_clause}
+    ORDER BY "relkind", "schema_oid", "schema";
+    """
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/redshift/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/redshift/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/salesforce/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/salesforce/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/salesforce/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/salesforce/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -186,15 +186,14 @@
                 f"sobjects/{table_name}/describe/",
                 params=None,
             )
             columns = self.get_columns(salesforce_objects["fields"])
             table_request = CreateTableRequest(
                 name=table_name,
                 tableType=table_type,
-                description="",
                 columns=columns,
                 tableConstraints=table_constraints,
                 databaseSchema=self.context.database_schema.fullyQualifiedName,
             )
             yield table_request
             self.register_record(table_request=table_request)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sample_data.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sample_data.py`

 * *Files 4% similar despite different names*

```diff
@@ -56,16 +56,18 @@
     MlStore,
 )
 from metadata.generated.schema.entity.data.pipeline import Pipeline, PipelineStatus
 from metadata.generated.schema.entity.data.table import (
     ColumnProfile,
     SystemProfile,
     Table,
+    TableData,
     TableProfile,
 )
+from metadata.generated.schema.entity.data.topic import Topic, TopicSampleData
 from metadata.generated.schema.entity.policies.policy import Policy
 from metadata.generated.schema.entity.services.connections.database.customDatabaseConnection import (
     CustomDatabaseConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
@@ -81,20 +83,21 @@
     Source as WorkflowSource,
 )
 from metadata.generated.schema.tests.basic import TestCaseResult, TestResultValue
 from metadata.generated.schema.tests.testCase import TestCase, TestCaseParameterValue
 from metadata.generated.schema.tests.testSuite import TestSuite
 from metadata.generated.schema.type.entityLineage import EntitiesEdge, LineageDetails
 from metadata.generated.schema.type.entityReference import EntityReference
-from metadata.generated.schema.type.schema import Topic
+from metadata.generated.schema.type.schema import Topic as TopicSchema
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.source import InvalidSourceException, Source
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.models.profile_data import OMetaTableProfileSampleData
 from metadata.ingestion.models.tests_data import (
+    OMetaLogicalTestSuiteSample,
     OMetaTestCaseResultsSample,
     OMetaTestCaseSample,
     OMetaTestSuiteSample,
 )
 from metadata.ingestion.models.user import OMetaUserProfile
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.parsers.schema_parsers import (
@@ -441,14 +444,22 @@
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/tests/testCaseResults.json",
                 "r",
                 encoding=UTF_8,
             )
         )
 
+        self.logical_test_suites = json.load(
+            open(  # pylint: disable=consider-using-with
+                sample_data_folder + "/tests/logicalTestSuites.json",
+                "r",
+                encoding=UTF_8,
+            )
+        )
+
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
         connection: CustomDatabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, CustomDatabaseConnection):
             raise InvalidSourceException(
@@ -474,14 +485,15 @@
         yield from self.ingest_pipeline_status()
         yield from self.ingest_mlmodels()
         yield from self.ingest_containers()
         yield from self.ingest_profiles()
         yield from self.ingest_test_suite()
         yield from self.ingest_test_case()
         yield from self.ingest_test_case_results()
+        yield from self.ingest_logical_test_suite()
 
     def ingest_teams(self):
         """
         Ingest sample teams
         """
         for team in self.teams["teams"]:
             team_to_ingest = CreateTeamRequest(
@@ -615,14 +627,35 @@
                 tableConstraints=table.get("tableConstraints"),
                 tags=table["tags"],
             )
 
             self.status.scanned(f"Table Scanned: {table_and_db.name}")
             yield table_and_db
 
+            if table.get("sampleData"):
+
+                table_fqn = fqn.build(
+                    self.metadata,
+                    entity_type=Table,
+                    service_name=self.database_service.name.__root__,
+                    database_name=db.name.__root__,
+                    schema_name=schema.name.__root__,
+                    table_name=table_and_db.name.__root__,
+                )
+
+                table_entity = self.metadata.get_by_name(entity=Table, fqn=table_fqn)
+
+                self.metadata.ingest_table_sample_data(
+                    table_entity,
+                    TableData(
+                        rows=table["sampleData"]["rows"],
+                        columns=table["sampleData"]["columns"],
+                    ),
+                )
+
     def ingest_topics(self) -> Iterable[CreateTopicRequest]:
         """
         Ingest Sample Topics
         """
         for topic in self.topics["topics"]:
             topic["service"] = EntityReference(
                 id=self.kafka_service.id, type="messagingService"
@@ -643,23 +676,39 @@
                 load_parser_fn = schema_parser_config_registry.registry.get(schema_type)
                 if not load_parser_fn:
                     raise InvalidSchemaTypeException(
                         f"Cannot find {schema_type} in parser providers registry."
                     )
                 schema_fields = load_parser_fn(topic["name"], topic["schemaText"])
 
-                create_topic.messageSchema = Topic(
+                create_topic.messageSchema = TopicSchema(
                     schemaText=topic["schemaText"],
                     schemaType=topic["schemaType"],
                     schemaFields=schema_fields,
                 )
 
             self.status.scanned(f"Topic Scanned: {create_topic.name.__root__}")
             yield create_topic
 
+            if topic.get("sampleData"):
+
+                topic_fqn = fqn.build(
+                    self.metadata,
+                    entity_type=Topic,
+                    service_name=self.kafka_service.name.__root__,
+                    topic_name=topic["name"],
+                )
+
+                topic_entity = self.metadata.get_by_name(entity=Topic, fqn=topic_fqn)
+
+                self.metadata.ingest_topic_sample_data(
+                    topic=topic_entity,
+                    sample_data=TopicSampleData(messages=topic["sampleData"]),
+                )
+
     def ingest_looker(self) -> Iterable[Entity]:
         """
         Looker sample data
         """
         for data_model in self.looker_models:
             try:
                 data_model_ev = CreateDashboardDataModelRequest(
@@ -685,30 +734,30 @@
         for chart in self.looker_charts:
             try:
                 chart_ev = CreateChartRequest(
                     name=chart["name"],
                     displayName=chart["displayName"],
                     description=chart["description"],
                     chartType=get_standard_chart_type(chart["chartType"]),
-                    chartUrl=chart["chartUrl"],
+                    sourceUrl=chart["sourceUrl"],
                     service=self.looker_service.fullyQualifiedName,
                 )
                 self.status.scanned(f"Chart Scanned: {chart_ev.name.__root__}")
                 yield chart_ev
             except ValidationError as err:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Unexpected exception ingesting chart [{chart}]: {err}")
 
         for dashboard in self.looker_dashboards:
             try:
                 dashboard_ev = CreateDashboardRequest(
                     name=dashboard["name"],
                     displayName=dashboard["displayName"],
                     description=dashboard["description"],
-                    dashboardUrl=dashboard["dashboardUrl"],
+                    sourceUrl=dashboard["sourceUrl"],
                     charts=dashboard["charts"],
                     dataModels=dashboard.get("dataModels", None),
                     service=self.looker_service.fullyQualifiedName,
                 )
                 self.status.scanned(f"Dashboard Scanned: {dashboard_ev.name.__root__}")
                 yield dashboard_ev
             except ValidationError as err:
@@ -767,15 +816,15 @@
         for chart in self.charts["charts"]:
             try:
                 chart_ev = CreateChartRequest(
                     name=chart["name"],
                     displayName=chart["displayName"],
                     description=chart["description"],
                     chartType=get_standard_chart_type(chart["chartType"]),
-                    chartUrl=chart["chartUrl"],
+                    sourceUrl=chart["sourceUrl"],
                     service=self.dashboard_service.fullyQualifiedName,
                 )
                 self.status.scanned(f"Chart Scanned: {chart_ev.name.__root__}")
                 yield chart_ev
             except ValidationError as err:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Unexpected exception ingesting chart [{chart}]: {err}")
@@ -805,31 +854,38 @@
 
     def ingest_dashboards(self) -> Iterable[CreateDashboardRequest]:
         for dashboard in self.dashboards["dashboards"]:
             dashboard_ev = CreateDashboardRequest(
                 name=dashboard["name"],
                 displayName=dashboard["displayName"],
                 description=dashboard["description"],
-                dashboardUrl=dashboard["dashboardUrl"],
+                sourceUrl=dashboard["sourceUrl"],
                 charts=dashboard["charts"],
                 dataModels=dashboard.get("dataModels", None),
                 service=self.dashboard_service.fullyQualifiedName,
             )
             self.status.scanned(f"Dashboard Scanned: {dashboard_ev.name.__root__}")
             yield dashboard_ev
 
     def ingest_pipelines(self) -> Iterable[Pipeline]:
         for pipeline in self.pipelines["pipelines"]:
+            owner = None
+            if pipeline.get("owner"):
+                user = self.metadata.get_user_by_email(email=pipeline.get("owner"))
+                if user:
+                    owner = EntityReference(id=user.id.__root__, type="user")
             pipeline_ev = CreatePipelineRequest(
                 name=pipeline["name"],
                 displayName=pipeline["displayName"],
                 description=pipeline["description"],
-                pipelineUrl=pipeline["pipelineUrl"],
+                sourceUrl=pipeline["sourceUrl"],
                 tasks=pipeline["tasks"],
                 service=self.pipeline_service.fullyQualifiedName,
+                owner=owner,
+                scheduleInterval=pipeline.get("scheduleInterval"),
             )
             yield pipeline_ev
 
     def ingest_lineage(self) -> Iterable[AddLineageRequest]:
         for edge in self.lineage:
             from_entity_ref = get_lineage_entity_ref(edge["from"], self.metadata_config)
             to_entity_ref = get_lineage_entity_ref(edge["to"], self.metadata_config)
@@ -1062,17 +1118,39 @@
     def ingest_test_suite(self) -> Iterable[OMetaTestSuiteSample]:
         """Iterate over all the testSuite and testCase and ingest them"""
         for test_suite in self.tests_suites["tests"]:
             yield OMetaTestSuiteSample(
                 test_suite=CreateTestSuiteRequest(
                     name=test_suite["testSuiteName"],
                     description=test_suite["testSuiteDescription"],
+                    executableEntityReference=test_suite["executableEntityReference"],
                 )
             )
 
+    def ingest_logical_test_suite(self) -> Iterable[OMetaLogicalTestSuiteSample]:
+        """Iterate over all the logical testSuite and testCase and ingest them"""
+        for logical_test_suite in self.logical_test_suites["tests"]:
+            test_suite = CreateTestSuiteRequest(
+                name=logical_test_suite["testSuiteName"],
+                description=logical_test_suite["testSuiteDescription"],
+            )  # type: ignore
+            test_cases: List[TestCase] = []
+            for test_case in logical_test_suite["testCases"]:
+                test_case = self.metadata.get_by_name(
+                    entity=TestCase,
+                    fqn=test_case["fqn"],
+                    fields=["testSuite", "testDefinition"],
+                )
+                if test_case:
+                    test_cases.append(test_case)
+
+            yield OMetaLogicalTestSuiteSample(
+                test_suite=test_suite, test_cases=test_cases
+            )
+
     def ingest_test_case(self) -> Iterable[OMetaTestCaseSample]:
         for test_suite in self.tests_suites["tests"]:
             suite = self.metadata.get_by_name(
                 fqn=test_suite["testSuiteName"], entity=TestSuite
             )
             for test_case in test_suite["testCases"]:
                 yield OMetaTestCaseSample(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sample_usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sample_usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/singlestore/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/singlestore/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/singlestore/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/singlestore/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -94,14 +94,17 @@
     return url
 
 
 def get_connection(connection: SnowflakeConnection) -> Engine:
     """
     Create connection
     """
+    if not connection.connectionArguments:
+        connection.connectionArguments = init_empty_connection_arguments()
+
     if connection.privateKey:
         snowflake_private_key_passphrase = (
             connection.snowflakePrivatekeyPassphrase.get_secret_value()
             if connection.snowflakePrivatekeyPassphrase
             else ""
         )
 
@@ -116,18 +119,20 @@
         )
         pkb = p_key.private_bytes(
             encoding=serialization.Encoding.DER,
             format=serialization.PrivateFormat.PKCS8,
             encryption_algorithm=serialization.NoEncryption(),
         )
 
-        if connection.privateKey:
-            if not connection.connectionArguments:
-                connection.connectionArguments = init_empty_connection_arguments()
-            connection.connectionArguments.__root__["private_key"] = pkb
+        connection.connectionArguments.__root__["private_key"] = pkb
+
+    if connection.clientSessionKeepAlive:
+        connection.connectionArguments.__root__[
+            "client_session_keep_alive"
+        ] = connection.clientSessionKeepAlive
 
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args_common,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/metadata.py`

 * *Files 7% similar despite different names*

```diff
@@ -17,18 +17,14 @@
 
 import sqlparse
 from snowflake.sqlalchemy.custom_types import VARIANT
 from snowflake.sqlalchemy.snowdialect import SnowflakeDialect, ischema_names
 from sqlalchemy.engine.reflection import Inspector
 from sqlparse.sql import Function, Identifier
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.table import (
     IntervalType,
     TablePartition,
     TableType,
 )
 from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
@@ -43,17 +39,22 @@
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
 from metadata.ingestion.source.database.common_db_source import (
     CommonDbSourceService,
     TableNameAndType,
 )
+from metadata.ingestion.source.database.snowflake.constants import (
+    SNOWFLAKE_REGION_ID_MAP,
+)
 from metadata.ingestion.source.database.snowflake.queries import (
     SNOWFLAKE_FETCH_ALL_TAGS,
     SNOWFLAKE_GET_CLUSTER_KEY,
+    SNOWFLAKE_GET_CURRENT_ACCOUNT,
+    SNOWFLAKE_GET_CURRENT_REGION,
     SNOWFLAKE_GET_DATABASE_COMMENTS,
     SNOWFLAKE_GET_DATABASES,
     SNOWFLAKE_GET_SCHEMA_COMMENTS,
     SNOWFLAKE_SESSION_TAG_QUERY,
 )
 from metadata.ingestion.source.database.snowflake.utils import (
     get_schema_columns,
@@ -65,20 +66,19 @@
     get_view_names,
     normalize_names,
 )
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_database
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.sqlalchemy_utils import get_all_table_comments
+from metadata.utils.tag_utils import get_ometa_tag_and_classification
 
-GEOGRAPHY = create_sqlalchemy_type("GEOGRAPHY")
-GEOMETRY = create_sqlalchemy_type("GEOMETRY")
 ischema_names["VARIANT"] = VARIANT
-ischema_names["GEOGRAPHY"] = GEOGRAPHY
-ischema_names["GEOMETRY"] = GEOMETRY
+ischema_names["GEOGRAPHY"] = create_sqlalchemy_type("GEOGRAPHY")
+ischema_names["GEOMETRY"] = create_sqlalchemy_type("GEOMETRY")
 
 logger = ingestion_logger()
 
 
 SnowflakeDialect._json_deserializer = json.loads  # pylint: disable=protected-access
 SnowflakeDialect.get_table_names = get_table_names
 SnowflakeDialect.get_view_names = get_view_names
@@ -292,27 +292,22 @@
                 except Exception as inner_exc:
                     logger.debug(traceback.format_exc())
                     logger.error(f"Failed to fetch tags: {inner_exc}")
 
             for res in result:
                 row = list(res)
                 fqn_elements = [name for name in row[2:] if name]
-                yield OMetaTagAndClassification(
-                    fqn=fqn._build(  # pylint: disable=protected-access
+                yield from get_ometa_tag_and_classification(
+                    tag_fqn=fqn._build(  # pylint: disable=protected-access
                         self.context.database_service.name.__root__, *fqn_elements
                     ),
-                    classification_request=CreateClassificationRequest(
-                        name=row[0],
-                        description="SNOWFLAKE TAG NAME",
-                    ),
-                    tag_request=CreateTagRequest(
-                        classification=row[0],
-                        name=row[1],
-                        description="SNOWFLAKE TAG VALUE",
-                    ),
+                    tags=[row[1]],
+                    classification_name=row[0],
+                    tag_description="SNOWFLAKE TAG VALUE",
+                    classification_desciption="SNOWFLAKE TAG NAME",
                 )
 
     def query_table_names_and_types(
         self, schema_name: str
     ) -> Iterable[TableNameAndType]:
         """
         Connect to the source database to get the table
@@ -337,7 +332,64 @@
             for table_name in self.inspector.get_table_names(
                 schema=schema_name, external_tables=True
             )
             or []
         ]
 
         return regular_tables + external_tables
+
+    def _get_current_region(self) -> Optional[str]:
+        try:
+            res = self.engine.execute(SNOWFLAKE_GET_CURRENT_REGION).one()
+            if res:
+                return res.REGION
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"Failed to fetch current region due to: {exc}")
+        return None
+
+    def _get_current_account(self) -> Optional[str]:
+        try:
+            res = self.engine.execute(SNOWFLAKE_GET_CURRENT_ACCOUNT).one()
+            if res:
+                return res.ACCOUNT
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"Failed to fetch current account due to: {exc}")
+        return None
+
+    def _clean_region_name(self, region_id: Optional[str]) -> Optional[str]:
+        """
+        Region id can be a vanilla id like "AWS_US_WEST_2"
+        and in case of multi region group it can be like "PUBLIC.AWS_US_WEST_2"
+        in such cases this method will extract vanilla region id and return the
+        region name from constant map SNOWFLAKE_REGION_ID_MAP
+
+        for more info checkout this doc:
+            https://docs.snowflake.com/en/sql-reference/functions/current_region
+        """
+        if region_id:
+            clean_region_id = region_id.split(".")[-1]
+            return SNOWFLAKE_REGION_ID_MAP.get(clean_region_id.lower())
+        return None
+
+    def get_source_url(
+        self,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+        table_type: TableType,
+    ) -> Optional[str]:
+        """
+        Method to get the source url for snowflake
+        """
+        account = self._get_current_account()
+        region_id = self._get_current_region()
+        region_name = self._clean_region_name(region_id)
+        if account and region_name:
+            tab_type = "view" if table_type == TableType.View else "table"
+            return (
+                f"https://app.snowflake.com/{region_name.lower()}/{account.lower()}/#/"
+                f"data/databases/{database_name}/schemas"
+                f"/{schema_name}/{tab_type}/{table_name}"
+            )
+        return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/queries.py`

 * *Files 4% similar despite different names*

```diff
@@ -127,7 +127,11 @@
         ic.comment,
         ic.identity_start,
         ic.identity_increment
     FROM information_schema.columns ic
     WHERE ic.table_schema=:table_schema
     ORDER BY ic.ordinal_position
 """
+
+SNOWFLAKE_GET_CURRENT_REGION = "SELECT CURRENT_REGION() AS region"
+
+SNOWFLAKE_GET_CURRENT_ACCOUNT = "SELECT CURRENT_ACCOUNT() AS account"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/snowflake/utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/snowflake/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sql_column_handler.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sql_column_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlalchemy_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlalchemy_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlite/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlite/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/sqlite/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/sqlite/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/trino/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/trino/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/usage_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/usage_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/lineage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/query_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/database/vertica/usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/vertica/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/ldap_users.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/ldap_users.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/common_broker_source.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/common_broker_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kafka/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kafka/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/connection.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,50 +8,46 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
-from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.messaging.kinesisConnection import (
-    KinesisConnection,
+from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
+    AtlasConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.utils.logger import ingestion_logger
-
-logger = ingestion_logger()
+from metadata.ingestion.source.metadata.atlas.client import AtlasClient
 
 
-def get_connection(connection: KinesisConnection):
+def get_connection(connection: AtlasConnection) -> AtlasClient:
     """
     Create connection
     """
-    return AWSClient(connection.awsConfig).get_kinesis_client()
+    return AtlasClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client,
-    service_connection: KinesisConnection,
+    client: AtlasClient,
+    service_connection: AtlasConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetTopics": client.list_streams}
+    test_fn = {"CheckAccess": client.list_entities}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/kinesis/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/messaging_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/messaging_service.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/redpanda/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/messaging/redpanda/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,67 +8,60 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-from functools import partial
+
 from typing import Optional
 
+from pydomo import Domo
+
+from metadata.clients.domo_client import DomoClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.metadata.amundsenConnection import (
-    AmundsenConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.domoPipelineConnection import (
+    DomoPipelineConnection,
 )
 from metadata.ingestion.connections.test_connections import (
     SourceConnectionException,
     test_connection_steps,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.metadata.amundsen.client import Neo4JConfig, Neo4jHelper
-from metadata.ingestion.source.metadata.amundsen.queries import (
-    NEO4J_AMUNDSEN_USER_QUERY,
-)
 
 
-def get_connection(connection: AmundsenConnection) -> Neo4jHelper:
+def get_connection(connection: DomoPipelineConnection) -> Domo:
     """
     Create connection
     """
     try:
-        neo4j_config = Neo4JConfig(
-            username=connection.username,
-            password=connection.password.get_secret_value(),
-            neo4j_url=connection.hostPort,
-            max_connection_life_time=connection.maxConnectionLifeTime,
-            neo4j_encrypted=connection.encrypted,
-            neo4j_validate_ssl=connection.validateSSL,
-        )
-        return Neo4jHelper(neo4j_config)
+        return DomoClient(connection)
     except Exception as exc:
         msg = f"Unknown error connecting with {connection}: {exc}."
         raise SourceConnectionException(msg)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: Neo4jHelper,
-    service_connection: AmundsenConnection,
+    connection: Domo,
+    service_connection: DomoPipelineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {
-        "CheckAccess": partial(client.execute_query, query=NEO4J_AMUNDSEN_USER_QUERY)
-    }
+    def custom_executor():
+        result = connection.get_pipelines()
+        return list(result)
+
+    test_fn = {"GetPipelines": custom_executor}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/metadata.py`

 * *Files 11% similar despite different names*

```diff
@@ -16,31 +16,26 @@
 import traceback
 from typing import Iterable, List, Optional
 
 from pydantic import SecretStr
 from sqlalchemy.engine.url import make_url
 
 from metadata.config.common import ConfigModel
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.services.createDatabaseService import (
     CreateDatabaseServiceRequest,
 )
 from metadata.generated.schema.api.teams.createTeam import CreateTeamRequest
 from metadata.generated.schema.api.teams.createUser import CreateUserRequest
-from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.table import Column, Table
 from metadata.generated.schema.entity.services.connections.metadata.amundsenConnection import (
     AmundsenConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
@@ -52,32 +47,31 @@
     DatabaseServiceType,
 )
 from metadata.generated.schema.entity.teams import team
 from metadata.generated.schema.entity.teams.user import User
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.source import InvalidSourceException, Source
-from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.user import OMetaUserProfile
 from metadata.ingestion.ometa.client_utils import get_chart_entities_from_id
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
 from metadata.ingestion.source.metadata.amundsen.queries import (
     NEO4J_AMUNDSEN_DASHBOARD_QUERY,
     NEO4J_AMUNDSEN_TABLE_QUERY,
     NEO4J_AMUNDSEN_USER_QUERY,
 )
 from metadata.utils import fqn
 from metadata.utils.helpers import get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.metadata_service_helper import SERVICE_TYPE_MAPPER
+from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 
 logger = ingestion_logger()
 
 
 class AmundsenConfig(ConfigModel):
     neo4j_username: Optional[str] = None
     neo4j_password: Optional[SecretStr] = None
@@ -221,30 +215,14 @@
                         columns=table_entity.columns,
                         owner=user_entity_ref,
                     )
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.error(f"Failed to create user entity [{user}]: {exc}")
 
-    def create_tags(self, tags):
-        for tag in tags:
-            classification = OMetaTagAndClassification(
-                classification_request=CreateClassificationRequest(
-                    name=AMUNDSEN_TAG_CATEGORY,
-                    description="Tags associates with amundsen entities",
-                ),
-                tag_request=CreateTagRequest(
-                    classification=AMUNDSEN_TAG_CATEGORY,
-                    name=tag,
-                    description="Amundsen Table Tag",
-                ),
-            )
-            yield classification
-            logger.info(f"Classification {classification}, Primary Tag {tag} Ingested")
-
     def _yield_create_database(self, table):
         try:
             service_entity = self.get_database_service(table["database"])
             table_name = ""
             if hasattr(service_entity.connection.config, "supportsDatabase"):
                 table_name = table["cluster"]
             else:
@@ -323,89 +301,39 @@
                     data_type
                 )
                 parsed_string["name"] = name
                 parsed_string["dataLength"] = 1
                 parsed_string["description"] = description
                 col = Column(**parsed_string)
                 columns.append(col)
-            amundsen_table_tag = OMetaTagAndClassification(
-                classification_request=CreateClassificationRequest(
-                    name=AMUNDSEN_TAG_CATEGORY,
-                    description="Tags associates with amundsen entities",
-                ),
-                tag_request=CreateTagRequest(
-                    classification=AMUNDSEN_TAG_CATEGORY,
-                    name=AMUNDSEN_TABLE_TAG,
-                    description="Amundsen Table Tag",
-                ),
-            )
-            yield amundsen_table_tag
-            amundsen_cluster_tag = OMetaTagAndClassification(
-                classification_request=CreateClassificationRequest(
-                    name=AMUNDSEN_TAG_CATEGORY,
-                    description="Tags associates with amundsen entities",
-                ),
-                tag_request=CreateTagRequest(
-                    classification=AMUNDSEN_TAG_CATEGORY,
-                    name=table["cluster"],
-                    description="Amundsen Cluster Tag",
-                ),
-            )
-            yield amundsen_cluster_tag
-            tags = [
-                TagLabel(
-                    tagFQN=fqn.build(
-                        self.metadata,
-                        Tag,
-                        classification_name=AMUNDSEN_TAG_CATEGORY,
-                        tag_name=AMUNDSEN_TABLE_TAG,
-                    ),
-                    labelType="Automated",
-                    state="Suggested",
-                    source="Classification",
-                ),
-                TagLabel(
-                    tagFQN=fqn.build(
-                        self.metadata,
-                        Tag,
-                        classification_name=AMUNDSEN_TAG_CATEGORY,
-                        tag_name=table["cluster"],
-                    ),
-                    labelType="Automated",
-                    state="Suggested",
-                    source="Classification",
-                ),
-            ]
+
+            # We are creating a couple of custom tags
+            tags = [AMUNDSEN_TABLE_TAG, table["cluster"]]
             if table["tags"]:
-                yield from self.create_tags(table["tags"])
-                tags.extend(
-                    [
-                        TagLabel(
-                            tagFQN=fqn.build(
-                                self.metadata,
-                                Tag,
-                                classification_name=AMUNDSEN_TAG_CATEGORY,
-                                tag_name=tag,
-                            ),
-                            labelType="Automated",
-                            state="Suggested",
-                            source="Classification",
-                        )
-                        for tag in table["tags"]
-                    ]
-                )
+                tags.extend(table["tags"])
+            yield from get_ometa_tag_and_classification(
+                tags=tags,
+                classification_name=AMUNDSEN_TAG_CATEGORY,
+                tag_description="Amundsen Table Tag",
+                classification_desciption="Tags associated with amundsen entities",
+            )
+
             table_request = CreateTableRequest(
                 name=table["name"],
                 tableType="Regular",
-                description=table["description"],
+                description=table.get("description"),
                 databaseSchema=self.database_schema_object.fullyQualifiedName,
-                tags=tags,
+                tags=get_tag_labels(
+                    metadata=self.metadata,
+                    tags=tags,
+                    classification_name=AMUNDSEN_TAG_CATEGORY,
+                    include_tags=True,
+                ),
                 columns=columns,
             )
-
             yield table_request
 
             self.status.scanned(table["name"])
         except Exception as exc:
             error = f"Failed to create table entity [{table}]: {exc}"
             logger.debug(traceback.format_exc())
             logger.warning(error)
@@ -429,16 +357,15 @@
         Method to process dashboard and return CreateDashboardRequest
         """
         try:
             self.status.scanned(dashboard["name"])
             yield CreateDashboardRequest(
                 name=dashboard["name"],
                 displayName=dashboard["name"],
-                description="",
-                dashboardUrl=dashboard["url"],
+                sourceUrl=dashboard["url"],
                 charts=get_chart_entities_from_id(
                     chart_ids=dashboard["chart_ids"],
                     metadata=self.metadata,
                     service_name=self.dashboard_service.name.__root__,
                 ),
                 service=self.dashboard_service.fullyQualifiedName,
             )
@@ -454,16 +381,15 @@
             dashboard["chart_ids"],
             dashboard["chart_types"],
             dashboard["chart_urls"],
         ):
             chart = CreateChartRequest(
                 name=chart_id,
                 displayName=name,
-                description="",
-                chartUrl=url,
+                sourceUrl=url,
                 chartType=get_standard_chart_type(chart_type).value,
                 service=self.dashboard_service.fullyQualifiedName,
             )
             self.status.scanned(name)
             yield chart
 
     def close(self):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/amundsen/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/connection.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,41 +13,41 @@
 Source connection handler
 """
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
-    AtlasConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.fivetranConnection import (
+    FivetranConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.metadata.atlas.client import AtlasClient
+from metadata.ingestion.source.pipeline.fivetran.client import FivetranClient
 
 
-def get_connection(connection: AtlasConnection) -> AtlasClient:
+def get_connection(connection: FivetranConnection) -> FivetranClient:
     """
     Create connection
     """
-    return AtlasClient(connection)
+    return FivetranClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: AtlasClient,
-    service_connection: AtlasConnection,
+    client: FivetranClient,
+    service_connection: FivetranConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"CheckAccess": client.list_entities}
+    test_fn = {"GetPipelines": client.list_groups}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/atlas/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/metadata.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,27 +13,21 @@
 Atlas source to extract metadata
 """
 
 import traceback
 from dataclasses import dataclass
 from typing import Any, Dict, Iterable, List
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
-from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.api.services.createDatabaseService import (
     CreateDatabaseServiceRequest,
 )
 from metadata.generated.schema.api.services.createMessagingService import (
     CreateMessagingServiceRequest,
 )
-from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.pipeline import Pipeline
 from metadata.generated.schema.entity.data.table import Column, Table
 from metadata.generated.schema.entity.data.topic import Topic
 from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
     AtlasConnection,
@@ -44,29 +38,24 @@
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.messagingService import MessagingService
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.generated.schema.type.entityLineage import EntitiesEdge
 from metadata.generated.schema.type.entityReference import EntityReference
-from metadata.generated.schema.type.tagLabel import (
-    LabelType,
-    State,
-    TagLabel,
-    TagSource,
-)
 from metadata.ingestion.api.source import InvalidSourceException, Source
-from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
 from metadata.ingestion.source.metadata.atlas.client import AtlasClient
 from metadata.utils import fqn
+from metadata.utils.helpers import get_database_name_for_lineage
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.metadata_service_helper import SERVICE_TYPE_MAPPER
+from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 
 logger = ingestion_logger()
 
 ATLAS_TAG_CATEGORY = "AtlasMetadata"
 ATLAS_TABLE_TAG = "atlas_table"
 
 
@@ -203,20 +192,24 @@
             db_entity = None
             for tbl_entity in tbl_entities:
                 try:
                     tbl_attrs = tbl_entity["attributes"]
                     db_entity = tbl_entity["relationshipAttributes"][
                         self.entity_types["Table"][name]["db"]
                     ]
+                    database_name = get_database_name_for_lineage(
+                        db_service_entity=self.service,
+                        default_db_name=db_entity["displayText"],
+                    )
 
                     database_fqn = fqn.build(
                         self.metadata,
                         entity_type=Database,
                         service_name=self.service.name.__root__,
-                        database_name=db_entity["displayText"],
+                        database_name=database_name,
                     )
                     database_object = self.metadata.get_by_name(
                         entity=Database, fqn=database_fqn
                     )
                     if db_entity.get("description", None) and database_object:
                         self.metadata.patch_description(
                             entity=Database,
@@ -225,36 +218,41 @@
                             force=True,
                         )
 
                     database_schema_fqn = fqn.build(
                         self.metadata,
                         entity_type=DatabaseSchema,
                         service_name=self.service.name.__root__,
-                        database_name=db_entity["displayText"],
+                        database_name=database_name,
                         schema_name=db_entity["displayText"],
                     )
                     database_schema_object = self.metadata.get_by_name(
                         entity=DatabaseSchema, fqn=database_schema_fqn
                     )
 
                     if db_entity.get("description", None) and database_schema_object:
                         self.metadata.patch_description(
                             entity=DatabaseSchema,
                             source=database_schema_object,
                             description=db_entity["description"],
                             force=True,
                         )
 
-                    yield self.create_tag()
+                    yield from get_ometa_tag_and_classification(
+                        tags=[ATLAS_TABLE_TAG],
+                        classification_name=ATLAS_TAG_CATEGORY,
+                        tag_description="Atlas Cluster Tag",
+                        classification_desciption="Tags associated with atlas entities",
+                    )
 
                     table_fqn = fqn.build(
                         metadata=self.metadata,
                         entity_type=Table,
                         service_name=self.service.name.__root__,
-                        database_name=db_entity["displayText"],
+                        database_name=database_name,
                         schema_name=db_entity["displayText"],
                         table_name=tbl_attrs["name"],
                     )
 
                     table_object = self.metadata.get_by_name(
                         entity=Table, fqn=table_fqn
                     )
@@ -263,70 +261,62 @@
                         if tbl_attrs.get("description", None):
                             self.metadata.patch_description(
                                 source=table_object,
                                 entity=Table,
                                 description=tbl_attrs["description"],
                                 force=True,
                             )
-
-                        tag_fqn = fqn.build(
-                            self.metadata,
-                            entity_type=Tag,
-                            classification_name=ATLAS_TAG_CATEGORY,
-                            tag_name=ATLAS_TABLE_TAG,
-                        )
-
-                        tag_label = TagLabel(
-                            tagFQN=tag_fqn,
-                            labelType=LabelType.Automated,
-                            state=State.Suggested.value,
-                            source=TagSource.Classification,
-                        )
-
-                        self.metadata.patch_tag(
-                            entity=Table, source=table_object, tag_label=tag_label
+                        yield from self.apply_table_tags(
+                            table_object=table_object, table_entity=tbl_entity
                         )
 
                     yield from self.ingest_lineage(tbl_entity["guid"], name)
 
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
                     logger.warning(
                         f"Failed to parse for database : {db_entity} - table {table}: {exc}"
                     )
 
-    def get_tags(self):
-        tags = [
-            TagLabel(
-                tagFQN=fqn.build(
-                    self.metadata,
-                    Tag,
-                    tag_category_name=ATLAS_TAG_CATEGORY,
-                    tag_name=ATLAS_TABLE_TAG,
-                ),
-                labelType="Automated",
-                state="Suggested",
-                source="Classification",
-            )
-        ]
-        return tags
-
-    def create_tag(self) -> OMetaTagAndClassification:
-        atlas_table_tag = OMetaTagAndClassification(
-            classification_request=CreateClassificationRequest(
-                name=ATLAS_TAG_CATEGORY,
-                description="Tags associates with atlas entities",
-            ),
-            tag_request=CreateTagRequest(
-                classification=ATLAS_TAG_CATEGORY,
-                name=ATLAS_TABLE_TAG,
-                description="Atlas Cluster Tag",
-            ),
+    def apply_table_tags(self, table_object: Table, table_entity: dict):
+        """
+        apply default atlas table tag
+        """
+        tag_labels = []
+        table_tags = get_tag_labels(
+            metadata=self.metadata,
+            tags=[ATLAS_TABLE_TAG],
+            classification_name=ATLAS_TAG_CATEGORY,
         )
-        return atlas_table_tag
+        if table_tags:
+            tag_labels.extend(table_tags)
+
+        # apply classification tags
+        for tag in table_entity.get("classifications", []):
+            if tag and tag.get("typeName"):
+                yield from get_ometa_tag_and_classification(
+                    tags=[tag.get("typeName", ATLAS_TABLE_TAG)],
+                    classification_name=ATLAS_TAG_CATEGORY,
+                    tag_description="Atlas Cluster Tag",
+                    classification_desciption="Tags associated with atlas entities",
+                )
+                classification_tags = get_tag_labels(
+                    metadata=self.metadata,
+                    tags=[tag.get("typeName", ATLAS_TABLE_TAG)],
+                    classification_name=ATLAS_TAG_CATEGORY,
+                )
+                if classification_tags:
+                    tag_labels.extend(classification_tags)
+
+        for tag_label in tag_labels:
+            self.metadata.patch_tag(
+                entity=Table,
+                source=table_object,
+                tag_label=tag_label,
+            )
 
     def _parse_table_columns(self, table_response, tbl_entity, name) -> List[Column]:
         om_cols = []
         col_entities = tbl_entity["relationshipAttributes"][
             self.entity_types["Table"][name]["column"]
         ]
         referred_entities = table_response["referredEntities"]
@@ -350,80 +340,85 @@
                 om_cols.append(om_column)
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Error parsing column [{col}]: {exc}")
                 continue
         return om_cols
 
-    def get_database_entity(self, database_name: str) -> Database:
-        return CreateDatabaseRequest(
-            name=database_name,
-            service=self.service.fullyQualifiedName,
-        )
-
     def ingest_lineage(self, source_guid, name) -> Iterable[AddLineageRequest]:
         """
         Fetch and ingest lineage
         """
-        lineage_response = self.atlas_client.get_lineage(source_guid)
-        lineage_relations = lineage_response["relations"]
-        tbl_entity = self.atlas_client.get_entity(lineage_response["baseEntityGuid"])
-        for key in tbl_entity["referredEntities"].keys():
-            if not tbl_entity["entities"][0]["relationshipAttributes"].get(
-                self.entity_types["Table"][name]["db"]
-            ):
-                continue
-            db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
-                self.entity_types["Table"][name]["db"]
-            ]
-            if not tbl_entity["referredEntities"].get(key):
-                continue
-            table_name = tbl_entity["referredEntities"][key]["relationshipAttributes"][
-                "table"
-            ]["displayText"]
-            from_fqn = fqn.build(
-                self.metadata,
-                entity_type=Table,
-                service_name=self.config.serviceName,
-                database_name=db_entity["displayText"],
-                schema_name=db_entity["displayText"],
-                table_name=table_name,
+        try:
+            lineage_response = self.atlas_client.get_lineage(source_guid)
+            lineage_relations = lineage_response["relations"]
+            tbl_entity = self.atlas_client.get_entity(
+                lineage_response["baseEntityGuid"]
             )
-            from_entity_ref = self.get_lineage_entity_ref(
-                from_fqn, self.metadata_config, "table"
-            )
-            for edge in lineage_relations:
-                if (
-                    lineage_response["guidEntityMap"][edge["toEntityId"]]["typeName"]
-                    == "processor"
+            for key in tbl_entity["referredEntities"].keys():
+                if not tbl_entity["entities"][0]["relationshipAttributes"].get(
+                    self.entity_types["Table"][name]["db"]
                 ):
                     continue
-
-                tbl_entity = self.atlas_client.get_entity(edge["toEntityId"])
-                for key in tbl_entity["referredEntities"]:
-                    db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
-                        self.entity_types["Table"][name]["db"]
-                    ]
-
-                    db = self.get_database_entity(db_entity["displayText"])
-                    table_name = tbl_entity["referredEntities"][key][
-                        "relationshipAttributes"
-                    ]["table"]["displayText"]
-                    to_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Table,
-                        service_name=self.config.serviceName,
-                        database_name=db.name.__root__,
-                        schema_name=db_entity["displayText"],
-                        table_name=table_name,
-                    )
-                    to_entity_ref = self.get_lineage_entity_ref(
-                        to_fqn, self.metadata_config, "table"
-                    )
-                    yield from self.yield_lineage(from_entity_ref, to_entity_ref)
+                db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
+                    self.entity_types["Table"][name]["db"]
+                ]
+                if not tbl_entity["referredEntities"].get(key):
+                    continue
+                table_name = tbl_entity["referredEntities"][key][
+                    "relationshipAttributes"
+                ]["table"]["displayText"]
+                from_fqn = fqn.build(
+                    self.metadata,
+                    entity_type=Table,
+                    service_name=self.service.name.__root__,
+                    database_name=get_database_name_for_lineage(
+                        self.service, db_entity["displayText"]
+                    ),
+                    schema_name=db_entity["displayText"],
+                    table_name=table_name,
+                )
+                from_entity_ref = self.get_lineage_entity_ref(
+                    from_fqn, self.metadata_config, "table"
+                )
+                for edge in lineage_relations:
+                    if (
+                        lineage_response["guidEntityMap"][edge["toEntityId"]][
+                            "typeName"
+                        ]
+                        == "processor"
+                    ):
+                        continue
+
+                    tbl_entity = self.atlas_client.get_entity(edge["toEntityId"])
+                    for key in tbl_entity["referredEntities"]:
+                        db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
+                            self.entity_types["Table"][name]["db"]
+                        ]
+
+                        table_name = tbl_entity["referredEntities"][key][
+                            "relationshipAttributes"
+                        ]["table"]["displayText"]
+                        to_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Table,
+                            service_name=self.service.name.__root__,
+                            database_name=get_database_name_for_lineage(
+                                self.service, db_entity["displayText"]
+                            ),
+                            schema_name=db_entity["displayText"],
+                            table_name=table_name,
+                        )
+                        to_entity_ref = self.get_lineage_entity_ref(
+                            to_fqn, self.metadata_config, "table"
+                        )
+                        yield from self.yield_lineage(from_entity_ref, to_entity_ref)
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.debug(f"failed to parse lineage due to {exc}")
 
     def get_database_service(self):
         service = self.metadata.create_or_update(
             CreateDatabaseServiceRequest(
                 name=SERVICE_TYPE_MAPPER.get("hive")["service_name"],
                 displayName=f"{self.config.serviceName}_database",
                 serviceType=SERVICE_TYPE_MAPPER.get("hive")["service_name"],
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/databricks/metadata.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,40 +4,47 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""Metadata source module"""
+"""Databricks source module"""
 
-from metadata.generated.schema.entity.services.connections.metadata.metadataESConnection import (
-    MetadataESConnection,
+from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
+    DatabricksConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.metadata.metadata import MetadataSource
+from metadata.ingestion.source.database.databricks.legacy.metadata import (
+    DatabricksLegacySource,
+)
+from metadata.ingestion.source.database.databricks.unity_catalog.metadata import (
+    DatabricksUnityCatalogSource,
+)
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class MetadataElasticsearchSource(MetadataSource):
+class DatabricksSource:
     """
-    Metadata Elasticsearch Source
-    Used for metadata to ES pipeline
+    Implements the necessary methods to extract
+    Database metadata from Databricks Source
     """
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: MetadataESConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, MetadataESConnection):
+        connection: DatabricksConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, DatabricksConnection):
             raise InvalidSourceException(
-                f"Expected MetadataESConnection, but got {connection}"
+                f"Expected DatabricksConnection, but got {connection}"
             )
-        return cls(config, metadata_config)
+        if not connection.useUnityCatalog:
+            return DatabricksLegacySource(config, metadata_config)
+        return DatabricksUnityCatalogSource(config, metadata_config)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/metadata/openmetadata/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/oracle/query_parser.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,35 +4,44 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""Metadata source module"""
-
+"""
+Oracle query parsing module
+"""
+from abc import ABC
 
+from metadata.generated.schema.entity.services.connections.database.oracleConnection import (
+    OracleConnection,
+)
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.metadata.metadata import MetadataSource
+from metadata.ingestion.source.database.query_parser_source import QueryParserSource
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class OpenmetadataSource(MetadataSource):
-    """Metadata source Class"""
+class OracleQueryParserSource(QueryParserSource, ABC):
+    """
+    Oracle base for usage and lineage
+    """
+
+    filters: str
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: OpenMetadataConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, OpenMetadataConnection):
+        connection: OracleConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, OracleConnection):
             raise InvalidSourceException(
-                f"Expected OpenMetadataConnection, but got {connection}"
+                f"Expected OracleConnection, but got {connection}"
             )
         return cls(config, metadata_config)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/uuid_encoder.py`

 * *Files 25% similar despite different names*

```diff
@@ -4,26 +4,26 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Auxiliary pydantic models used during metadata ingestion
+UUID Encoder Module
 """
-from typing import Optional
 
-from pydantic import BaseModel, Field
+import json
+from uuid import UUID
 
 
-class TableView(BaseModel):
+class UUIDEncoder(json.JSONEncoder):
     """
-    Pydantic model to define a view of a table
+    UUID Encoder class
     """
 
-    table_name: str = Field(..., description="Name of the table")
-    schema_name: str = Field(..., description="Name of the schema")
-    db_name: str = Field(..., description="Name of the Database")
-    view_definition: Optional[str] = Field(
-        None, description="Definition of the view in a specific SQL dialect"
-    )
+    def default(self, o):
+        if isinstance(o, UUID):
+            # if the obj is uuid, we simply return the value of uuid
+            return str(o)
+        return json.JSONEncoder.default(self, o)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -86,16 +86,15 @@
         """
         Returns the list of tasks linked to connection
         """
         return [
             Task(
                 name=connection["connectionId"],
                 displayName=connection["name"],
-                description="",
-                taskUrl=f"{connection_url}/status",
+                sourceUrl=f"{connection_url}/status",
             )
         ]
 
     def yield_pipeline(
         self, pipeline_details: AirbytePipelineDetails
     ) -> Iterable[CreatePipelineRequest]:
         """
@@ -107,16 +106,15 @@
             f"{clean_uri(self.service_connection.hostPort)}/workspaces"
             f"/{pipeline_details.workspace.get('workspaceId')}"
             f"/connections/{pipeline_details.connection.get('connectionId')}"
         )
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.connection.get("connectionId"),
             displayName=pipeline_details.connection.get("name"),
-            description="",
-            pipelineUrl=connection_url,
+            sourceUrl=connection_url,
             tasks=self.get_connections_jobs(
                 pipeline_details.connection, connection_url
             ),
             service=self.context.pipeline_service.fullyQualifiedName.__root__,
         )
         yield pipeline_request
         self.register_record(pipeline_request=pipeline_request)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Airflow source to extract metadata from OM UI
 """
 import traceback
-from datetime import datetime
+from datetime import datetime, timedelta
 from typing import Any, Iterable, List, Optional, cast
 
 from airflow.models import BaseOperator, DagRun, TaskInstance
 from airflow.models.serialized_dag import SerializedDagModel
 from airflow.serialization.serialized_objects import SerializedDAG
 from pydantic import BaseModel, ValidationError
 from sqlalchemy.orm import Session
@@ -46,15 +46,17 @@
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.source.pipeline.airflow.lineage_parser import get_xlets_from_dag
 from metadata.ingestion.source.pipeline.airflow.models import (
     AirflowDag,
     AirflowDagDetails,
 )
 from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
+from metadata.utils.constants import TIMEDELTA
 from metadata.utils.helpers import clean_uri, datetime_to_ts
+from metadata.utils.importer import import_from_module
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 STATUS_MAP = {
     "success": StatusType.Successful.value,
     "failed": StatusType.Failed.value,
@@ -245,28 +247,59 @@
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Wild error trying to extract status from DAG {pipeline_details.dag_id} - {exc}."
                 " Skipping status ingestion."
             )
 
+    def get_schedule_interval(self, pipeline_data) -> Optional[str]:
+        """
+        Fetch Schedule Intervals from Airflow Dags
+        """
+        schedule_interval_timetable_val = pipeline_data.get("timetable", {}).get(
+            "__var", {}
+        )
+        if schedule_interval_timetable_val:
+            # Fetch Cron as String
+            return schedule_interval_timetable_val.get("expression", None)
+        schedule_interval_val = pipeline_data.get("schedule_interval", {})
+        if schedule_interval_val:
+            type_value = schedule_interval_val.get("__type", {})
+            if type_value == TIMEDELTA:
+                var_value = schedule_interval_val.get("__var", {})
+                # types of schedule interval with timedelta
+                # timedelta(days=1) = `1 day, 0:00:00`
+                return str(timedelta(seconds=var_value))
+
+        try:
+            # If the Schedule interval is a const value like @once, @yearly etc
+            # __type sends the module path, and once instantiated
+            return import_from_module(
+                pipeline_data.get("timetable", {}).get("__type", {})
+            )().summary
+        except Exception:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Couldn't fetch schedule interval for dag {pipeline_data.get('_dag_id')}"
+            )
+        return None
+
     def get_pipelines_list(self) -> Iterable[OMSerializedDagDetails]:
         """
         List all DAGs from the metadata db.
 
         We are using the SerializedDagModel as it helps
         us retrieve all the task and inlets/outlets information
         """
 
         json_data_column = (
             SerializedDagModel._data  # For 2.3.0 onwards # pylint: disable=protected-access
             if hasattr(SerializedDagModel, "_data")
             else SerializedDagModel.data  # For 2.2.5 and 2.1.4
         )
-
         for serialized_dag in self.session.query(
             SerializedDagModel.dag_id,
             json_data_column,
             SerializedDagModel.fileloc,
         ).all():
             try:
                 data = serialized_dag[1]["dag"]
@@ -274,28 +307,36 @@
                     dag_id=serialized_dag[0],
                     fileloc=serialized_dag[2],
                     data=AirflowDag(**serialized_dag[1]),
                     max_active_runs=data.get("max_active_runs", None),
                     description=data.get("_description", None),
                     start_date=data.get("start_date", None),
                     tasks=data.get("tasks", []),
-                    owners=data.get("default_args", [])["__var"].get("email", [])
-                    if data.get("default_args")
-                    else None,
+                    schedule_interval=self.get_schedule_interval(data),
+                    owners=self.fetch_owners(data),
                 )
+
                 yield dag
             except ValidationError as err:
                 logger.debug(traceback.format_exc())
                 logger.warning(
                     f"Error building pydantic model for {serialized_dag} - {err}"
                 )
             except Exception as err:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Wild error yielding dag {serialized_dag} - {err}")
 
+    def fetch_owners(self, data) -> Optional[str]:
+        try:
+            if self.source_config.includeOwners and data.get("default_args"):
+                return data.get("default_args", [])["__var"].get("email", [])
+        except TypeError:
+            pass
+        return None
+
     def get_pipeline_name(self, pipeline_details: SerializedDAG) -> str:
         """
         Get Pipeline Name
         """
         return pipeline_details.dag_id
 
     @staticmethod
@@ -305,15 +346,15 @@
         :param dag: SerializedDAG
         :return: List of tasks
         """
         return [
             Task(
                 name=task.task_id,
                 description=task.doc_md,
-                taskUrl=(
+                sourceUrl=(
                     f"{clean_uri(host_port)}/taskinstance/list/"
                     f"?flt1_dag_id_equals={dag.dag_id}&_flt_3_task_id={task.task_id}"
                 ),
                 downstreamTasks=list(task.downstream_task_ids)
                 if task.downstream_task_ids
                 else [],
                 startDate=task.start_date.isoformat() if task.start_date else None,
@@ -364,25 +405,26 @@
         """
 
         try:
 
             pipeline_request = CreatePipelineRequest(
                 name=pipeline_details.dag_id,
                 description=pipeline_details.description,
-                pipelineUrl=f"{clean_uri(self.service_connection.hostPort)}/tree?dag_id={pipeline_details.dag_id}",
+                sourceUrl=f"{clean_uri(self.service_connection.hostPort)}/tree?dag_id={pipeline_details.dag_id}",
                 concurrency=pipeline_details.max_active_runs,
                 pipelineLocation=pipeline_details.fileloc,
                 startDate=pipeline_details.start_date.isoformat()
                 if pipeline_details.start_date
                 else None,
                 tasks=self.get_tasks_from_dag(
                     pipeline_details, self.service_connection.hostPort
                 ),
                 service=self.context.pipeline_service.fullyQualifiedName.__root__,
                 owner=self.get_owner(pipeline_details.owners),
+                scheduleInterval=pipeline_details.schedule_interval,
             )
             yield pipeline_request
             self.register_record(pipeline_request=pipeline_request)
         except TypeError as err:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Error building DAG information from {pipeline_details}. There might be Airflow version"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/airflow/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/models.py`

 * *Files 24% similar despite different names*

```diff
@@ -59,10 +59,11 @@
 
 
 class AirflowDagDetails(AirflowBaseModel):
     fileloc: str
     data: AirflowDag
     max_active_runs: Optional[int]
     description: Optional[str]
-    start_date: Optional[datetime] = None
+    start_date: Optional[datetime]
     tasks: List[Task]
-    owners: Any
+    owners: Optional[Any]
+    schedule_interval: Optional[str]
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,63 +10,63 @@
 #  limitations under the License.
 
 """
 Source connection handler
 """
 from typing import Optional
 
-from dagster_graphql import DagsterGraphQLClient
-from gql.transport.requests import RequestsHTTPTransport
-
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.dagsterConnection import (
-    DagsterConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.nifiConnection import (
+    BasicAuthentication,
+    NifiConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.dagster.queries import TEST_QUERY_GRAPHQL
-from metadata.utils.helpers import clean_uri
+from metadata.ingestion.source.pipeline.nifi.client import NifiClient
 
 
-def get_connection(connection: DagsterConnection) -> DagsterGraphQLClient:
+def get_connection(connection: NifiConnection) -> NifiClient:
     """
     Create connection
     """
-    url = clean_uri(connection.host)
-    dagster_connection = DagsterGraphQLClient(
-        url,
-        transport=RequestsHTTPTransport(
-            url=f"{url}/graphql",
-            headers={"Dagster-Cloud-Api-Token": connection.token.get_secret_value()}
-            if connection.token
+    if isinstance(connection.nifiConfig, BasicAuthentication):
+        return NifiClient(
+            host_port=connection.hostPort,
+            username=connection.nifiConfig.username,
+            password=connection.nifiConfig.password.get_secret_value()
+            if connection.nifiConfig.password
             else None,
-            timeout=connection.timeout,
-        ),
-    )
+            verify=connection.nifiConfig.verifySSL,
+        )
 
-    return dagster_connection
+    return NifiClient(
+        host_port=connection.hostPort,
+        ca_file_path=connection.nifiConfig.certificateAuthorityPath,
+        client_cert_path=connection.nifiConfig.clientCertificatePath,
+        client_key_path=connection.nifiConfig.clientkeyPath,
+    )
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: DagsterGraphQLClient,
-    service_connection: DagsterConnection,
+    client: NifiClient,
+    service_connection: NifiConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    def custom_executor_for_pipeline():
-        client._execute(TEST_QUERY_GRAPHQL)  # pylint: disable=protected-access
+    def custom_executor():
+        list(client.list_process_groups())
 
-    test_fn = {"GetPipelines": custom_executor_for_pipeline}
+    test_fn = {"GetPipelines": custom_executor}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,240 +4,220 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Dagster source to extract metadata from OM UI
+Databricks pipeline source to extract metadata
 """
+
 import traceback
-from typing import Dict, Iterable, List, Optional
+from typing import Any, Iterable, List, Optional
+
+from pydantic import ValidationError
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.pipeline import (
     PipelineStatus,
     StatusType,
     Task,
     TaskStatus,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.dagsterConnection import (
-    DagsterConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.databricksPipelineConnection import (
+    DatabricksPipelineConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
-from metadata.ingestion.source.pipeline.dagster.queries import (
-    DAGSTER_PIPELINE_DETAILS_GRAPHQL,
-    GRAPHQL_QUERY_FOR_JOBS,
-    GRAPHQL_RUNS_QUERY,
-)
 from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
-from metadata.utils import tag_utils
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
+
 STATUS_MAP = {
-    "success": StatusType.Successful.value,
-    "failure": StatusType.Failed.value,
-    "queued": StatusType.Pending.value,
+    "SUCCESS": StatusType.Successful,
+    "FAILED": StatusType.Failed,
+    "TIMEOUT": StatusType.Failed,
+    "CANCELED": StatusType.Failed,
+    "PENDING": StatusType.Pending,
+    "RUNNING": StatusType.Pending,
+    "TERMINATING": StatusType.Pending,
+    "SKIPPED": StatusType.Failed,
+    "INTERNAL_ERROR": StatusType.Failed,
 }
 
-DAGSTER_TAG_CATEGORY = "DagsterTags"
-
 
-class DagsterSource(PipelineServiceSource):
+class DatabrickspipelineSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
-    Pipeline metadata from Dagster's metadata db
+    Pipeline metadata from Databricks Jobs API
     """
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: DagsterConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, DagsterConnection):
+        connection: DatabricksPipelineConnection = (
+            config.serviceConnection.__root__.config
+        )
+        if not isinstance(connection, DatabricksPipelineConnection):
             raise InvalidSourceException(
-                f"Expected DagsterConnection, but got {connection}"
+                f"Expected DatabricksPipelineConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def get_run_list(self):
-        try:
-            # pylint: disable=protected-access
-            result = self.client._execute(DAGSTER_PIPELINE_DETAILS_GRAPHQL)
-        except ConnectionError as conerr:
-            logger.error(f"Cannot connect to dagster client {conerr}")
-            logger.debug(f"Failed due to : {traceback.format_exc()}")
+    def get_pipelines_list(self) -> Iterable[dict]:
+        """
+        Get List of all pipelines
+        """
+        for workflow in self.client.list_jobs():
+            yield workflow
 
-        return result["repositoriesOrError"]["nodes"]
+    def get_pipeline_name(self, pipeline_details: dict) -> str:
+        """
+        Get Pipeline Name
+        """
+        return pipeline_details["settings"].get("name")
 
-    def get_jobs(self, pipeline_name) -> Iterable[dict]:
+    def yield_pipeline(self, pipeline_details: Any) -> Iterable[CreatePipelineRequest]:
+        """
+        Method to Get Pipeline Entity
+        """
+        self.context.job_id_list = []
         try:
-            parameters = {
-                "selector": {
-                    "graphName": pipeline_name,
-                    "repositoryName": self.context.repository_name,
-                    "repositoryLocationName": self.context.repository_location,
-                }
-            }
-            jobs = self.client._execute(  # pylint: disable=protected-access
-                query=GRAPHQL_QUERY_FOR_JOBS, variables=parameters
+            pipeline_request = CreatePipelineRequest(
+                name=pipeline_details["job_id"],
+                displayName=pipeline_details["settings"].get("name"),
+                description=pipeline_details["settings"].get("name"),
+                tasks=self.get_tasks(pipeline_details),
+                service=self.context.pipeline_service.fullyQualifiedName.__root__,
+            )
+            yield pipeline_request
+            self.register_record(pipeline_request=pipeline_request)
+
+        except TypeError as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Error building Databricks Pipeline information from {pipeline_details}."
+                f" There might be Databricks Jobs API version incompatibilities - {err}"
+            )
+        except ValidationError as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Error building pydantic model for {pipeline_details} - {err}"
             )
-            return jobs["graphOrError"]
         except Exception as err:
-            logger.error(f"Error while getting jobs {pipeline_name} - {err}")
             logger.debug(traceback.format_exc())
+            logger.warning(f"Wild error ingesting pipeline {pipeline_details} - {err}")
 
-        return []
+    def get_tasks(self, pipeline_details: dict) -> List[Task]:
+        task_list = []
+        self.append_context(key="job_id_list", value=pipeline_details["job_id"])
 
-    def yield_pipeline(self, pipeline_details) -> Iterable[CreatePipelineRequest]:
-        """
-        Convert a DAG into a Pipeline Entity
-        :param serialized_dag: SerializedDAG from dagster metadata DB
-        :return: Create Pipeline request with tasks
-        """
-        jobs = self.get_jobs(pipeline_name=pipeline_details.get("name"))
-        task_list: List[Task] = []
-        for job in jobs["solidHandles"]:
-            down_stream_task = []
-            for tasks in job.get("solid")["inputs"]:
-                for task in tasks["dependsOn"]:
-                    down_stream_task.append(task["solid"]["name"])
-
-            task = Task(
-                name=job["handleID"],
-                displayName=job["handleID"],
-                downstreamTasks=down_stream_task,
+        downstream_tasks = self.get_downstream_tasks(
+            pipeline_details["settings"].get("tasks")
+        )
+        for task in pipeline_details["settings"].get("tasks"):
+            task_list.append(
+                Task(
+                    name=task["task_key"],
+                    displayName=task["task_key"],
+                    taskType=self.get_task_type(task),
+                    downstreamTasks=downstream_tasks.get(task["task_key"], []),
+                )
             )
 
-            task_list.append(task)
+        return task_list
 
-        pipeline_request = CreatePipelineRequest(
-            name=pipeline_details["id"].replace(":", ""),
-            displayName=pipeline_details["name"],
-            description=pipeline_details.get("description", ""),
-            tasks=task_list,
-            service=self.context.pipeline_service.fullyQualifiedName.__root__,
-            tags=tag_utils.get_tag_labels(
-                metadata=self.metadata,
-                tags=[self.context.repository_name],
-                classification_name=DAGSTER_TAG_CATEGORY,
-                include_tags=self.source_config.includeTags,
-            ),
-        )
-        yield pipeline_request
-        self.register_record(pipeline_request=pipeline_request)
+    def get_task_type(self, task):
+        task_key = "undefined_task_type"
+        for key in task.keys():
+            if key.endswith("_task"):
+                task_key = key
 
-    def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
-        if self.source_config.includeTags:
-            try:
-                classification = OMetaTagAndClassification(
-                    classification_request=CreateClassificationRequest(
-                        name=DAGSTER_TAG_CATEGORY,
-                        description="Tags associated with dagster",
-                    ),
-                    tag_request=CreateTagRequest(
-                        classification=DAGSTER_TAG_CATEGORY,
-                        name=self.context.repository_name,
-                        description="Dagster Tag",
-                    ),
-                )
+        return task_key
 
-                yield classification
-            except Exception as err:
-                logger.debug(traceback.format_exc())
-                logger.error(
-                    f"Error ingesting tag [{self.context.repository_name}]: {err}"
-                )
+    def get_downstream_tasks(self, workflow):
+        task_key_list = [task["task_key"] for task in workflow]
 
-    def get_task_runs(self, job_id, pipeline_name):
-        """
-        To get all the runs details
-        """
-        try:
-            parameters = {
-                "handleID": job_id,
-                "selector": {
-                    "pipelineName": pipeline_name,
-                    "repositoryName": self.context.repository_name,
-                    "repositoryLocationName": self.context.repository_location,
-                },
-            }
-            runs = self.client._execute(  # pylint: disable=protected-access
-                query=GRAPHQL_RUNS_QUERY, variables=parameters
-            )
+        dependent_tasks = self.get_dependent_tasks(workflow)
 
-            return runs["pipelineOrError"]
-        except Exception as err:
-            logger.error(
-                f"Error while getting runs for {job_id} - {pipeline_name} - {err}"
-            )
-            logger.debug(traceback.format_exc())
+        downstream_tasks = {}
 
-        return []
+        for task_key, task_depend_ons in dependent_tasks.items():
+            if task_depend_ons:
+                for task in task_depend_ons:
+                    if task in downstream_tasks:
+                        downstream_tasks[task].append(task_key)
+                    else:
+                        downstream_tasks[task] = [task_key]
 
-    def yield_pipeline_status(self, pipeline_details) -> OMetaPipelineStatus:
-        tasks = self.context.pipeline.tasks
-        for task in tasks:
-            runs = self.get_task_runs(
-                task.name, pipeline_name=pipeline_details.get("name")
-            )
-            for run in runs["solidHandle"]["stepStats"].get("nodes") or []:
-                task_status = TaskStatus(
-                    name=task.name,
-                    executionStatus=STATUS_MAP.get(
-                        run["status"].lower(), StatusType.Pending.value
-                    ),
-                    startTime=round(run["startTime"]),
-                    endTime=round(run["endTime"]),
-                )
+        for task in task_key_list:
+            if task not in downstream_tasks:
+                downstream_tasks[task] = []
 
-                pipeline_status = PipelineStatus(
-                    taskStatus=[task_status],
-                    executionStatus=STATUS_MAP.get(
-                        run["status"].lower(), StatusType.Pending.value
-                    ),
-                    timestamp=round(run["endTime"]),
-                )
-                pipeline_status_yield = OMetaPipelineStatus(
-                    pipeline_fqn=self.context.pipeline.fullyQualifiedName.__root__,
-                    pipeline_status=pipeline_status,
-                )
-                yield pipeline_status_yield
+        return downstream_tasks
 
-    def yield_pipeline_lineage_details(
-        self, pipeline_details
-    ) -> Optional[Iterable[AddLineageRequest]]:
-        """
-        Not implemented, as this connector does not create any lineage
-        """
+    def get_dependent_tasks(self, workflow):
+        dependent_tasks = {}
+
+        for task in workflow:
+            depends_on = task.get("depends_on")
+            if depends_on:
+                dependent_tasks[task["task_key"]] = [v["task_key"] for v in depends_on]
+            else:
+                dependent_tasks[task["task_key"]] = None
 
-    def get_pipelines_list(self) -> Dict:
-        results = self.get_run_list()
-        for result in results:
-            self.context.repository_location = result.get("location")["name"]
-            self.context.repository_name = result["name"]
-            for job in result.get("pipelines") or []:
-                yield job
+        return dependent_tasks
 
-    def get_pipeline_name(self, pipeline_details) -> str:
+    def yield_pipeline_status(self, pipeline_details) -> Iterable[OMetaPipelineStatus]:
+        for job_id in self.context.job_id_list:
+            try:
+                runs = self.client.get_job_runs(job_id=job_id)
+                for attempt in runs:
+                    for task_run in attempt["tasks"]:
+                        task_status = []
+                        task_status.append(
+                            TaskStatus(
+                                name=task_run["task_key"],
+                                executionStatus=STATUS_MAP.get(
+                                    task_run["state"].get("result_state"),
+                                    StatusType.Failed,
+                                ),
+                                startTime=task_run["start_time"],
+                                endTime=task_run["end_time"],
+                                logLink=task_run["run_page_url"],
+                            )
+                        )
+                        pipeline_status = PipelineStatus(
+                            taskStatus=task_status,
+                            timestamp=attempt["start_time"],
+                            executionStatus=STATUS_MAP.get(
+                                attempt["state"].get("result_state"),
+                                StatusType.Failed,
+                            ),
+                        )
+                        yield OMetaPipelineStatus(
+                            pipeline_fqn=self.context.pipeline.fullyQualifiedName.__root__,
+                            pipeline_status=pipeline_status,
+                        )
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(f"Failed to yield pipeline status: {exc}")
+
+    def yield_pipeline_lineage_details(
+        self, pipeline_details: Any
+    ) -> Optional[Iterable[AddLineageRequest]]:
         """
-        Get Pipeline Name
+        Get lineage between pipeline and data sources
         """
-        return pipeline_details["name"]
-
-    def test_connection(self) -> None:
-        pass
+        logger.info("Lineage is not yet supported on Databicks Pipelines")
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/dagster/queries.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/metadata.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,221 +4,228 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Databricks pipeline source to extract metadata
+Dagster source to extract metadata from OM UI
 """
-
 import traceback
-from typing import Any, Iterable, List, Optional
-
-from pydantic import ValidationError
+from typing import Iterable, List, Optional
 
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.pipeline import (
     PipelineStatus,
     StatusType,
     Task,
     TaskStatus,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.databricksPipelineConnection import (
-    DatabricksPipelineConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.dagsterConnection import (
+    DagsterConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
+from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
+from metadata.ingestion.source.pipeline.dagster.models import (
+    DagsterPipeline,
+    RunStepStats,
+    SolidHandle,
+)
 from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.tag_utils import get_ometa_tag_and_classification, get_tag_labels
 
 logger = ingestion_logger()
 
-
 STATUS_MAP = {
-    "SUCCESS": StatusType.Successful,
-    "FAILED": StatusType.Failed,
-    "TIMEOUT": StatusType.Failed,
-    "CANCELED": StatusType.Failed,
-    "PENDING": StatusType.Pending,
-    "RUNNING": StatusType.Pending,
-    "TERMINATING": StatusType.Pending,
-    "SKIPPED": StatusType.Failed,
-    "INTERNAL_ERROR": StatusType.Failed,
+    "success": StatusType.Successful.value,
+    "failure": StatusType.Failed.value,
+    "queued": StatusType.Pending.value,
 }
 
+DAGSTER_TAG_CATEGORY = "DagsterTags"
+
 
-class DatabrickspipelineSource(PipelineServiceSource):
+class DagsterSource(PipelineServiceSource):
     """
     Implements the necessary methods ot extract
-    Pipeline metadata from Databricks Jobs API
+    Pipeline metadata from Dagster's metadata db
     """
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: DatabricksPipelineConnection = (
-            config.serviceConnection.__root__.config
-        )
-        if not isinstance(connection, DatabricksPipelineConnection):
+        connection: DagsterConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, DagsterConnection):
             raise InvalidSourceException(
-                f"Expected DatabricksPipelineConnection, but got {connection}"
+                f"Expected DagsterConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def get_pipelines_list(self) -> Iterable[dict]:
+    def _get_downstream_tasks(self, job: SolidHandle) -> Optional[List[str]]:
         """
-        Get List of all pipelines
-        """
-        for workflow in self.client.list_jobs():
-            yield workflow
-
-    def get_pipeline_name(self, pipeline_details: dict) -> str:
+        Method to get downstream tasks
         """
-        Get Pipeline Name
+        down_stream_tasks = []
+        if job.solid:
+            for tasks in job.solid.inputs or []:
+                if tasks:
+                    for task in tasks.dependsOn or []:
+                        down_stream_tasks.append(task.solid.name)
+        return down_stream_tasks or None
+
+    def _get_task_list(self, pipeline_name: str) -> Optional[List[Task]]:
+        """
+        Method to collect all the tasks from dagster and return it in a task list
+        """
+        jobs = self.client.get_jobs(
+            pipeline_name=pipeline_name,
+            repository_name=self.context.repository_name,
+            repository_location=self.context.repository_location,
+        )
+        task_list: List[Task] = []
+        if jobs:
+            for job in jobs.solidHandles or []:
+                try:
+                    task = Task(
+                        name=job.handleID,
+                        displayName=job.handleID,
+                        downstreamTasks=self._get_downstream_tasks(job=job),
+                    )
+                    task_list.append(task)
+                except Exception as exc:
+                    logger.debug(traceback.format_exc())
+                    logger.warning(
+                        f"Error to fetch tasks for {pipeline_name}:{job}: {exc}"
+                    )
+
+        return task_list or None
+
+    def yield_pipeline(
+        self, pipeline_details: DagsterPipeline
+    ) -> Iterable[CreatePipelineRequest]:
+        """
+        Convert a DAG into a Pipeline Entity
+        :param serialized_dag: SerializedDAG from dagster metadata DB
+        :return: Create Pipeline request with tasks
         """
-        return pipeline_details["settings"].get("name")
 
-    def yield_pipeline(self, pipeline_details: Any) -> Iterable[CreatePipelineRequest]:
-        """
-        Method to Get Pipeline Entity
-        """
-        self.context.job_id_list = []
         try:
             pipeline_request = CreatePipelineRequest(
-                name=pipeline_details["job_id"],
-                displayName=pipeline_details["settings"].get("name"),
-                description=pipeline_details["settings"].get("name"),
-                tasks=self.get_tasks(pipeline_details),
-                pipelineUrl="",
+                name=pipeline_details.id.replace(":", ""),
+                displayName=pipeline_details.name,
+                description=pipeline_details.description,
+                tasks=self._get_task_list(pipeline_name=pipeline_details.name),
                 service=self.context.pipeline_service.fullyQualifiedName.__root__,
+                tags=get_tag_labels(
+                    metadata=self.metadata,
+                    tags=[self.context.repository_name],
+                    classification_name=DAGSTER_TAG_CATEGORY,
+                    include_tags=self.source_config.includeTags,
+                ),
             )
             yield pipeline_request
             self.register_record(pipeline_request=pipeline_request)
-
-        except TypeError as err:
-            logger.debug(traceback.format_exc())
-            logger.warning(
-                f"Error building Databricks Pipeline information from {pipeline_details}."
-                f" There might be Databricks Jobs API version incompatibilities - {err}"
-            )
-        except ValidationError as err:
+        except Exception as exc:
             logger.debug(traceback.format_exc())
-            logger.warning(
-                f"Error building pydantic model for {pipeline_details} - {err}"
-            )
-        except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Wild error ingesting pipeline {pipeline_details} - {err}")
-
-    def get_tasks(self, pipeline_details: dict) -> List[Task]:
-        task_list = []
-        self.append_context(key="job_id_list", value=pipeline_details["job_id"])
+            logger.warning(f"Error to yield pipeline for {pipeline_details}: {exc}")
 
-        downstream_tasks = self.get_downstream_tasks(
-            pipeline_details["settings"].get("tasks")
+    def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
+        yield from get_ometa_tag_and_classification(
+            tags=[self.context.repository_name],
+            classification_name=DAGSTER_TAG_CATEGORY,
+            tag_description="Dagster Tag",
+            classification_desciption="Tags associated with dagster entities",
+            include_tags=self.source_config.includeTags,
         )
-        for task in pipeline_details["settings"].get("tasks"):
-            task_list.append(
-                Task(
-                    name=task["task_key"],
-                    displayName=task["task_key"],
-                    taskType=self.get_task_type(task),
-                    downstreamTasks=downstream_tasks.get(task["task_key"], []),
-                )
-            )
-
-        return task_list
-
-    def get_task_type(self, task):
-        task_key = "undefined_task_type"
-        for key in task.keys():
-            if key.endswith("_task"):
-                task_key = key
-
-        return task_key
-
-    def get_downstream_tasks(self, workflow):
-        task_key_list = [task["task_key"] for task in workflow]
-
-        dependent_tasks = self.get_dependent_tasks(workflow)
 
-        downstream_tasks = {}
-
-        for task_key, task_depend_ons in dependent_tasks.items():
-            if task_depend_ons:
-                for task in task_depend_ons:
-                    if task in downstream_tasks:
-                        downstream_tasks[task].append(task_key)
-                    else:
-                        downstream_tasks[task] = [task_key]
-
-        for task in task_key_list:
-            if task not in downstream_tasks:
-                downstream_tasks[task] = []
-
-        return downstream_tasks
-
-    def get_dependent_tasks(self, workflow):
-        dependent_tasks = {}
-
-        for task in workflow:
-            depends_on = task.get("depends_on")
-            if depends_on:
-                dependent_tasks[task["task_key"]] = [v["task_key"] for v in depends_on]
-            else:
-                dependent_tasks[task["task_key"]] = None
+    def _get_task_status(
+        self, run: RunStepStats, task_name: str
+    ) -> Iterable[OMetaPipelineStatus]:
+        """
+        Prepare the OMetaPipelineStatus
+        """
+        try:
+            task_status = TaskStatus(
+                name=task_name,
+                executionStatus=STATUS_MAP.get(
+                    run.status.lower(), StatusType.Pending.value
+                ),
+                startTime=round(run.startTime) if run.startTime else None,
+                endTime=round(run.endTime) if run.endTime else None,
+            )
 
-        return dependent_tasks
+            pipeline_status = PipelineStatus(
+                taskStatus=[task_status],
+                executionStatus=STATUS_MAP.get(
+                    run.status.lower(), StatusType.Pending.value
+                ),
+                timestamp=round(run.endTime) if run.endTime else None,
+            )
+            pipeline_status_yield = OMetaPipelineStatus(
+                pipeline_fqn=self.context.pipeline.fullyQualifiedName.__root__,
+                pipeline_status=pipeline_status,
+            )
+            yield pipeline_status_yield
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Error to yield run status for {run}: {exc}")
 
-    def yield_pipeline_status(self, pipeline_details) -> Iterable[OMetaPipelineStatus]:
-        for job_id in self.context.job_id_list:
+    def yield_pipeline_status(
+        self, pipeline_details: DagsterPipeline
+    ) -> OMetaPipelineStatus:
+        """
+        Yield the pipeline and task status
+        """
+        for task in self.context.pipeline.tasks or []:
             try:
-                runs = self.client.get_job_runs(job_id=job_id)
-                for attempt in runs:
-                    for task_run in attempt["tasks"]:
-                        task_status = []
-                        task_status.append(
-                            TaskStatus(
-                                name=task_run["task_key"],
-                                executionStatus=STATUS_MAP.get(
-                                    task_run["state"].get("result_state"),
-                                    StatusType.Failed,
-                                ),
-                                startTime=task_run["start_time"],
-                                endTime=task_run["end_time"],
-                                logLink=task_run["run_page_url"],
-                            )
-                        )
-                        pipeline_status = PipelineStatus(
-                            taskStatus=task_status,
-                            timestamp=attempt["start_time"],
-                            executionStatus=STATUS_MAP.get(
-                                attempt["state"].get("result_state"),
-                                StatusType.Failed,
-                            ),
-                        )
-                        yield OMetaPipelineStatus(
-                            pipeline_fqn=self.context.pipeline.fullyQualifiedName.__root__,
-                            pipeline_status=pipeline_status,
-                        )
+                runs = self.client.get_task_runs(
+                    task.name,
+                    pipeline_name=pipeline_details.name,
+                    repository_name=self.context.repository_name,
+                    repository_location=self.context.repository_location,
+                )
+                for run in runs.solidHandle.stepStats.nodes or []:
+                    yield from self._get_task_status(run=run, task_name=task.name)
             except Exception as exc:
                 logger.debug(traceback.format_exc())
-                logger.error(f"Failed to yield pipeline status: {exc}")
+                logger.warning(
+                    f"Error to yield pipeline status for {pipeline_details}: {exc}"
+                )
 
     def yield_pipeline_lineage_details(
-        self, pipeline_details: Any
+        self, pipeline_details: DagsterPipeline
     ) -> Optional[Iterable[AddLineageRequest]]:
         """
-        Get lineage between pipeline and data sources
+        Not implemented, as this connector does not create any lineage
+        """
+
+    def get_pipelines_list(self) -> Iterable[DagsterPipeline]:
         """
-        logger.info("Lineage is not yet supported on Databicks Pipelines")
+        Get List of all pipelines
+        """
+
+        results = self.client.get_run_list()
+        for result in results:
+            self.context.repository_location = result.location.name
+            self.context.repository_name = result.name
+            for job in result.pipelines or []:
+                yield job
+
+    def get_pipeline_name(self, pipeline_details: DagsterPipeline) -> str:
+        """
+        Get Pipeline Name
+        """
+
+        return pipeline_details.name
+
+    def test_connection(self) -> None:
+        pass
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mongodb/connection.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,60 +8,75 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
+from functools import partial
 
+# from functools import partial
 from typing import Optional
 
-from pydomo import Domo
+from pydantic import BaseModel
+from pymongo import MongoClient
 
-from metadata.clients.domo_client import DomoClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.domoPipelineConnection import (
-    DomoPipelineConnection,
-)
-from metadata.ingestion.connections.test_connections import (
-    SourceConnectionException,
-    test_connection_steps,
+from metadata.generated.schema.entity.services.connections.database.mongoDBConnection import (
+    MongoConnectionString,
+    MongoDBConnection,
 )
+from metadata.ingestion.connections.builders import get_connection_url_common
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-def get_connection(connection: DomoPipelineConnection) -> Domo:
+def get_connection(connection: MongoDBConnection):
     """
     Create connection
     """
-    try:
-        return DomoClient(connection)
-    except Exception as exc:
-        msg = f"Unknown error connecting with {connection}: {exc}."
-        raise SourceConnectionException(msg)
+    if isinstance(connection.connectionDetails, MongoConnectionString):
+        mongo_url = connection.connectionDetails.connectionURI
+    else:
+        mongo_url = get_connection_url_common(connection.connectionDetails)
+    return MongoClient(mongo_url)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    connection: Domo,
-    service_connection: DomoPipelineConnection,
+    client: MongoClient,
+    service_connection: MongoDBConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    def custom_executor():
-        result = connection.get_pipelines()
-        return list(result)
+    class SchemaHolder(BaseModel):
+        database: Optional[str]
+
+    holder = SchemaHolder()
 
-    test_fn = {"GetPipelines": custom_executor}
+    def test_get_databases(client: MongoClient, holder: SchemaHolder):
+        for database in client.list_database_names():
+            holder.database = database
+            break
+
+    def test_get_collections(client: MongoClient, holder: SchemaHolder):
+        database = client.get_database(holder.database)
+        database.list_collection_names()
+
+    test_fn = {
+        "CheckAccess": client.server_info,
+        "GetDatabases": partial(test_get_databases, client, holder),
+        "GetCollections": partial(test_get_collections, client, holder),
+    }
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/spline/connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,41 +13,42 @@
 Source connection handler
 """
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.fivetranConnection import (
-    FivetranConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.splineConnection import (
+    SplineConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.fivetran.client import FivetranClient
+from metadata.ingestion.source.pipeline.spline.client import SplineClient
 
 
-def get_connection(connection: FivetranConnection) -> FivetranClient:
+def get_connection(connection: SplineConnection) -> SplineClient:
     """
     Create connection
     """
-    return FivetranClient(connection)
+
+    return SplineClient(config=connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: FivetranClient,
-    service_connection: FivetranConnection,
+    client: SplineClient,
+    service_connection: SplineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetPipelines": client.list_groups}
+    test_fn = {"GetPipelines": client.get_pipelines_test_connection}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_type=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -79,31 +79,28 @@
         """
         Returns the list of tasks linked to connection
         """
         return [
             Task(
                 name=pipeline_details.pipeline_name,
                 displayName=pipeline_details.pipeline_display_name,
-                description="",
             )
         ]
 
     def yield_pipeline(
         self, pipeline_details: FivetranPipelineDetails
     ) -> Iterable[CreatePipelineRequest]:
         """
         Convert a Connection into a Pipeline Entity
         :param pipeline_details: pipeline_details object from fivetran
         :return: Create Pipeline request with tasks
         """
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.pipeline_name,
             displayName=pipeline_details.pipeline_display_name,
-            description="",
-            pipelineUrl="",
             tasks=self.get_connections_jobs(pipeline_details),
             service=self.context.pipeline_service.fullyQualifiedName.__root__,
         )
         yield pipeline_request
         self.register_record(pipeline_request=pipeline_request)
 
     def yield_pipeline_status(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -94,15 +94,14 @@
         """
         Method to Get Pipeline Entity
         """
         self.job_name_list = set()
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details[NAME],
             displayName=pipeline_details[NAME],
-            description="",
             tasks=self.get_tasks(pipeline_details),
             service=self.context.pipeline_service.fullyQualifiedName.__root__,
         )
         yield pipeline_request
         self.register_record(pipeline_request=pipeline_request)
 
     def get_tasks(self, pipeline_details: Any) -> List[Task]:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/client.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/nifi/metadata.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -110,15 +110,15 @@
         that form the Pipeline
         """
         try:
             return [
                 Task(
                     name=processor.id_,
                     displayName=processor.name,
-                    taskUrl=f"{clean_uri(self.service_connection.hostPort)}{processor.uri}",
+                    sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{processor.uri}",
                     taskType=processor.type_,
                     downstreamTasks=self._get_downstream_tasks_from(
                         source_id=processor.id_,
                         connections=pipeline_details.connections,
                     ),
                 )
                 for processor in pipeline_details.processors
@@ -137,15 +137,15 @@
         Convert a Connection into a Pipeline Entity
         :param pipeline_details: pipeline_details object from Nifi
         :return: Create Pipeline request with tasks
         """
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.id_,
             displayName=pipeline_details.name,
-            pipelineUrl=f"{clean_uri(self.service_connection.hostPort)}{pipeline_details.uri}",
+            sourceUrl=f"{clean_uri(self.service_connection.hostPort)}{pipeline_details.uri}",
             tasks=self._get_tasks_from_details(pipeline_details),
             service=self.context.pipeline_service.fullyQualifiedName.__root__,
         )
         yield pipeline_request
         self.register_record(pipeline_request=pipeline_request)
 
     def yield_pipeline_status(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/pipeline/pipeline_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/pipeline/pipeline_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -140,17 +140,14 @@
             self.config.sourceConfig.config
         )
 
         self.connection = get_connection(self.service_connection)
         # Flag the connection for the test connection
         self.connection_obj = self.connection
         self.client = self.connection
-
-        # Flag the connection for the test connection
-        self.connection_obj = self.connection
         self.test_connection()
 
     @abstractmethod
     def yield_pipeline(self, pipeline_details: Any) -> Iterable[CreatePipelineRequest]:
         """
         Method to Get Pipeline Entity
         """
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/sqa_types.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/sqa_types.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/connection.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/s3/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/s3/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/source/storage/storage_service.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/storage/storage_service.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/ingestion/stage/table_usage.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/stage/table_usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/mixins/pandas/pandas_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/pandas/pandas_mixin.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,25 +11,28 @@
 
 """
 Interfaces with database for all database engine
 supporting sqlalchemy abstraction layer
 """
 import math
 import random
-from typing import List, cast
+from typing import cast
 
 from metadata.data_quality.validations.table.pandas.tableRowInsertedCountToBeBetween import (
     TableRowInsertedCountToBeBetweenValidator,
 )
 from metadata.generated.schema.entity.data.table import (
     PartitionIntervalType,
     PartitionProfilerConfig,
     ProfileSampleType,
 )
-from metadata.ingestion.source.database.datalake.metadata import ometa_to_dataframe
+from metadata.ingestion.source.database.datalake.models import (
+    DatalakeTableSchemaWrapper,
+)
+from metadata.utils.datalake.datalake_utils import fetch_dataframe
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class PandasInterfaceMixin:
     """Interface mixin grouping shared methods between test suite and profiler interfaces"""
@@ -82,24 +85,23 @@
 
     def return_ometa_dataframes_sampled(
         self, service_connection_config, client, table, profile_sample_config
     ):
         """
         returns sampled ometa dataframes
         """
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        data = ometa_to_dataframe(
+        data = fetch_dataframe(
             config_source=service_connection_config.configSource,
             client=client,
-            table=table,
+            file_fqn=DatalakeTableSchemaWrapper(
+                key=table.name.__root__, bucket_name=table.databaseSchema.name
+            ),
+            is_profiler=True,
         )
-        if isinstance(data, DataFrame):
-            data: List[DataFrame] = [data]
-        if data and isinstance(data, list):
+        if data:
             random.shuffle(data)
             # sampling data based on profiler config (if any)
             if hasattr(profile_sample_config, "profile_sample"):
                 if (
                     profile_sample_config.profile_sample_type
                     == ProfileSampleType.PERCENTAGE
                 ):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/mixins/sqalchemy/sqa_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/mixins/sqalchemy/sqa_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/parsers/avro_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/avro_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/parsers/json_schema_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/json_schema_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/parsers/protobuf_parser.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/protobuf_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/parsers/schema_parsers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/parsers/schema_parsers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/pii/__init__.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/pii/column_name_scanner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/column_name_scanner.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,80 +8,50 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Regex scanner for column names
 """
 import re
-from enum import Enum, auto
 from typing import Optional
 
 from metadata.pii.models import TagAndConfidence, TagType
 
 
-class PiiTypes(Enum):
-    """
-    PiiTypes enumerates the different types of PII data
-    """
-
-    NONE = auto()
-    UNSUPPORTED = auto()
-    PHONE = auto()
-    EMAIL = auto()
-    CREDIT_CARD = auto()
-    ADDRESS = auto()
-    ADDRESS_LOCATION = auto()
-    PERSON = auto()
-    LOCATION = auto()
-    BIRTH_DATE = auto()
-    GENDER = auto()
-    NATIONALITY = auto()
-    IP_ADDRESS = auto()
-    SSN = auto()
-    USER_NAME = auto()
-    PASSWORD = auto()
-    ETHNICITY = auto()
-    TAX_ID = auto()
-    KEY = auto()
-    BANKACC = auto()
-
-
 class ColumnNameScanner:
     """
     Column Name Scanner to scan column name
     """
 
     sensitive_regex = {
-        PiiTypes.PASSWORD: re.compile("^.*password.*$", re.IGNORECASE),
-        PiiTypes.SSN: re.compile("^.*(ssn|social).*$", re.IGNORECASE),
-        PiiTypes.CREDIT_CARD: re.compile("^.*(credit).*(card).*$", re.IGNORECASE),
-        PiiTypes.BANKACC: re.compile("^.*bank.*(acc|num).*$", re.IGNORECASE),
-        PiiTypes.EMAIL: re.compile("^.*(email|e-mail|mail).*$", re.IGNORECASE),
-        PiiTypes.USER_NAME: re.compile(
-            "^.*(user|client|person).*(name).*$", re.IGNORECASE
-        ),
-        PiiTypes.PERSON: re.compile(
+        "PASSWORD": re.compile("^.*password.*$", re.IGNORECASE),
+        "SSN": re.compile("^.*(ssn|social).*$", re.IGNORECASE),
+        "CREDIT_CARD": re.compile("^.*(credit).*(card).*$", re.IGNORECASE),
+        "BANKACC": re.compile("^.*bank.*(acc|num).*$", re.IGNORECASE),
+        "EMAIL": re.compile("^.*(email|e-mail|mail).*$", re.IGNORECASE),
+        "USER_NAME": re.compile("^.*(user|client|person).*(name).*$", re.IGNORECASE),
+        "PERSON": re.compile(
             "^.*(firstname|lastname|fullname|maidenname|nickname|name_suffix).*$",
             re.IGNORECASE,
         ),
     }
     non_sensitive_regex = {
-        PiiTypes.BIRTH_DATE: re.compile(
+        "BIRTH_DATE": re.compile(
             "^.*(date_of_birth|dateofbirth|dob|"
             "birthday|date_of_death|dateofdeath).*$",
             re.IGNORECASE,
         ),
-        PiiTypes.GENDER: re.compile("^.*(gender).*$", re.IGNORECASE),
-        PiiTypes.NATIONALITY: re.compile("^.*(nationality).*$", re.IGNORECASE),
-        PiiTypes.ADDRESS: re.compile(
+        "GENDER": re.compile("^.*(gender).*$", re.IGNORECASE),
+        "NATIONALITY": re.compile("^.*(nationality).*$", re.IGNORECASE),
+        "ADDRESS": re.compile(
             "^.*(address|city|state|county|country|"
             "zipcode|zip|postal|zone|borough).*$",
             re.IGNORECASE,
         ),
-        PiiTypes.PHONE: re.compile("^.*(phone).*$", re.IGNORECASE),
+        "PHONE": re.compile("^.*(phone).*$", re.IGNORECASE),
     }
 
     @classmethod
     def scan(cls, column_name: str) -> Optional[TagAndConfidence]:
         for pii_type_pattern in cls.sensitive_regex.values():
             if pii_type_pattern.match(column_name) is not None:
                 return TagAndConfidence(
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/pii/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/pii/ner_scanner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/ner_scanner.py`

 * *Files 25% similar despite different names*

```diff
@@ -10,40 +10,80 @@
 #  limitations under the License.
 """
 NER Scanner based on Presidio.
 
 Supported Entities https://microsoft.github.io/presidio/supported_entities/
 """
 import traceback
+from collections import defaultdict
 from enum import Enum
-from typing import Any, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Tuple
+
+from pydantic import BaseModel
 
 from metadata.pii import SPACY_EN_MODEL
 from metadata.pii.models import TagAndConfidence, TagType
 from metadata.utils.logger import pii_logger
 
 logger = pii_logger()
 
 
 class NEREntity(Enum):
+    """
+    PII Entities supported by Presidio https://microsoft.github.io/presidio/supported_entities/
+    """
+
+    # Global
     CREDIT_CARD = TagType.SENSITIVE.value
+    CRYPTO = TagType.SENSITIVE.value
+    DATE_TIME = TagType.NONSENSITIVE.value
     EMAIL_ADDRESS = TagType.SENSITIVE.value
     IBAN_CODE = TagType.SENSITIVE.value
     IP_ADDRESS = TagType.SENSITIVE.value
     NRP = TagType.NONSENSITIVE.value
     LOCATION = TagType.NONSENSITIVE.value
+    PERSON = TagType.SENSITIVE.value
     PHONE_NUMBER = TagType.NONSENSITIVE.value
     MEDICAL_LICENSE = TagType.SENSITIVE.value
-    US_DRIVER_LICENSE = TagType.SENSITIVE.value
-    DATE_TIME = TagType.NONSENSITIVE.value
     URL = TagType.NONSENSITIVE.value
+
+    # USA
     US_BANK_NUMBER = TagType.SENSITIVE.value
-    US_SSN = TagType.SENSITIVE.value
-    PERSON = TagType.SENSITIVE.value
+    US_DRIVER_LICENSE = TagType.SENSITIVE.value
+    US_ITIN = TagType.SENSITIVE.value
     US_PASSPORT = TagType.SENSITIVE.value
+    US_SSN = TagType.SENSITIVE.value
+
+    # UK
+    UK_NHS = TagType.SENSITIVE.value
+
+    # Spain
+    NIF = TagType.SENSITIVE.value
+
+    # Italy
+    IT_FISCAL_CODE = TagType.SENSITIVE.value
+    IT_DRIVER_LICENSE = TagType.SENSITIVE.value
+    IT_VAT_CODE = TagType.SENSITIVE.value
+    IT_PASSPORT = TagType.SENSITIVE.value
+    IT_IDENTITY_CARD = TagType.SENSITIVE.value
+
+    # Australia
+    AU_ABN = TagType.SENSITIVE.value
+    AU_ACN = TagType.SENSITIVE.value
+    AU_TFN = TagType.SENSITIVE.value
+    AU_MEDICARE = TagType.SENSITIVE.value
+
+
+class StringAnalysis(BaseModel):
+    """
+    Used to store results from the sample data scans for each NER Entity
+    """
+
+    score: float
+    appearances: int
 
 
 # pylint: disable=import-outside-toplevel
 class NERScanner:
     """
     Based on https://microsoft.github.io/presidio/
     """
@@ -64,51 +104,66 @@
 
         self.analyzer = AnalyzerEngine(
             nlp_engine=SpacyNlpEngine(models={"en": SPACY_EN_MODEL})
         )
 
     @staticmethod
     def get_highest_score_label(
-        labels_score, str_sample_data_rows: List[str]
-    ) -> Tuple[Optional[str], Optional[float]]:
-        most_used_label_occurrence = 0
-        label_score = None
-        for label, score in labels_score.items():
-            if score[0] == 1.0 and score[1] > len(str_sample_data_rows) * 0.8:
-                return (label, score[0])
-            if score[1] > most_used_label_occurrence:
-                label_score = (label, score[0])
-                most_used_label_occurrence = score[1]
-        return label_score or (None, None)
+        entities_score: Dict[str, StringAnalysis]
+    ) -> Tuple[str, float]:
+        top_entity = max(
+            entities_score,
+            key=lambda type_: entities_score[type_].score
+            * entities_score[type_].appearances
+            * 0.8,
+        )
+        return top_entity, entities_score[top_entity].score
 
     def scan(self, sample_data_rows: List[Any]) -> Optional[TagAndConfidence]:
         """
-        Scan the column's sample data rows and look for PII
+        Scan the column's sample data rows and look for PII.
+
+        How this works:
+        1. We create a list of strings [s1, s2, ..., sn] with each sample data row for a column
+        2. Then, for each s_i:
+          a. Run the analyzer, which will return a list of possible recognized Entities and confidence score
+             For example, the result of analyzing `123456789` gives us
+               [
+                 type: DATE_TIME, start: 0, end: 9, score: 0.85,
+                 type: US_BANK_NUMBER, start: 0, end: 9, score: 0.05,
+                 type: US_PASSPORT, start: 0, end: 9, score: 0.05,
+                 type: US_DRIVER_LICENSE, start: 0, end: 9, score: 0.01
+              ]
+          b. Each time an `Entity` appears (e.g., DATE_TIME), we store its max score and the number of appearances
+        3. After gathering all the results for each row, get the `Entity` with maximum overall score
+           and number of appearances. This gets computed as "score * appearances * 0.8", which can
+           be thought as the "score" times "weighted down appearances".
+        4. Once we have the "top" `Entity` from that column, we assign the PII label accordingly from `NEREntity`.
         """
         logger.debug("Processing '%s'", sample_data_rows)
-        labels_score = {}
+
+        # Initialize an empty dict for the given row list
+        entities_score: Dict[str, StringAnalysis] = defaultdict(
+            lambda: StringAnalysis(score=0, appearances=0)
+        )
+
         str_sample_data_rows = [str(row) for row in sample_data_rows if row is not None]
         for row in str_sample_data_rows:
             try:
                 results = self.analyzer.analyze(row, language="en")
                 for result in results:
-                    logger.debug("Found %s", result.entity_type)
-                    tag = result.entity_type
-                    if tag in labels_score:
-                        labels_score[tag] = (
-                            result.score
-                            if result.score > labels_score[tag][0]
-                            else labels_score[tag][0],
-                            labels_score[tag][1] + 1,
-                        )
-                    else:
-                        labels_score[tag] = (result.score, 1)
+                    entities_score[result.entity_type] = StringAnalysis(
+                        score=result.score
+                        if result.score > entities_score[result.entity_type].score
+                        else entities_score[result.entity_type].score,
+                        appearances=entities_score[result.entity_type].appearances + 1,
+                    )
             except Exception as exc:
                 logger.warning(f"Unknown error while processing {row} - {exc}")
                 logger.debug(traceback.format_exc())
 
-        label, score = self.get_highest_score_label(labels_score, str_sample_data_rows)
-        if label and score:
+        if entities_score:
+            label, score = self.get_highest_score_label(entities_score)
             tag_type = NEREntity.__members__.get(label, TagType.NONSENSITIVE).value
             return TagAndConfidence(tag=tag_type, confidence=score)
 
         return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/pii/processor.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/pii/processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/api/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/api/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/api/workflow.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/api/workflow.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,23 +13,21 @@
 Workflow definition for the ORM Profiler.
 
 - How to specify the source
 - How to specify the entities to run
 - How to define metrics & tests
 """
 import traceback
-from copy import deepcopy
-from typing import Iterable, List, Optional, Union, cast
+from typing import Iterable, Optional, cast
 
 from pydantic import ValidationError
-from sqlalchemy import MetaData
 
 from metadata.config.common import WorkflowExecutionError
 from metadata.generated.schema.entity.data.database import Database
-from metadata.generated.schema.entity.data.table import ColumnProfilerConfig, Table
+from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.services.connections.serviceConnection import (
@@ -49,29 +47,18 @@
 from metadata.ingestion.api.parser import parse_workflow_config_gracefully
 from metadata.ingestion.api.sink import Sink
 from metadata.ingestion.api.source import SourceStatus
 from metadata.ingestion.models.custom_types import ServiceWithConnectionType
 from metadata.ingestion.ometa.client_utils import create_ometa_client
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
-from metadata.profiler.api.models import (
-    ProfilerProcessorConfig,
-    ProfilerResponse,
-    TableConfig,
-)
-from metadata.profiler.interface.pandas.pandas_profiler_interface import (
-    PandasProfilerInterface,
-)
-from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
-from metadata.profiler.interface.sqlalchemy.sqa_profiler_interface import (
-    SQAProfilerInterface,
-)
-from metadata.profiler.metrics.registry import Metrics
+from metadata.profiler.api.models import ProfilerProcessorConfig, ProfilerResponse
 from metadata.profiler.processor.core import Profiler
-from metadata.profiler.processor.default import DefaultProfiler, get_default_metrics
+from metadata.profiler.source.base_profiler_source import BaseProfilerSource
+from metadata.profiler.source.profiler_source_factory import profiler_source_factory
 from metadata.timer.repeated_timer import RepeatedTimer
 from metadata.timer.workflow_reporter import get_ingestion_status_timer
 from metadata.utils import fqn
 from metadata.utils.class_helper import (
     get_service_class_from_service_type,
     get_service_type_from_source_type,
 )
@@ -108,15 +95,15 @@
 
         self.metadata_config: OpenMetadataConnection = (
             self.config.workflowConfig.openMetadataServerConfig
         )
         self.profiler_config = ProfilerProcessorConfig.parse_obj(
             self.config.processor.dict().get("config")
         )
-        self.metadata = OpenMetadata(self.metadata_config)
+        self.metadata = create_ometa_client(self.metadata_config)
         self._retrieve_service_connection_if_needed()
         self.test_connection()
         self.set_ingestion_pipeline_status(state=PipelineState.running)
         # Init and type the source config
         self.source_config: DatabaseServiceProfilerPipeline = cast(
             DatabaseServiceProfilerPipeline, self.config.source.sourceConfig.config
         )  # Used to satisfy type checked
@@ -164,79 +151,14 @@
         if not self._timer:
             self._timer = get_ingestion_status_timer(
                 interval=REPORTS_INTERVAL_SECONDS, logger=logger, workflow=self
             )
 
         return self._timer
 
-    def get_config_for_entity(self, entity: Table) -> Optional[TableConfig]:
-        """Get config for a specific entity
-
-        Args:
-            entity: table entity
-        """
-
-        if not self.profiler_config.tableConfig:
-            return None
-        return next(
-            (
-                table_config
-                for table_config in self.profiler_config.tableConfig
-                if table_config.fullyQualifiedName.__root__
-                == entity.fullyQualifiedName.__root__  # type: ignore
-            ),
-            None,
-        )
-
-    def get_include_columns(self, entity) -> Optional[List[ColumnProfilerConfig]]:
-        """get included columns"""
-        entity_config: Optional[TableConfig] = self.get_config_for_entity(entity)
-        if entity_config and entity_config.columnConfig:
-            return entity_config.columnConfig.includeColumns
-
-        if entity.tableProfilerConfig:
-            return entity.tableProfilerConfig.includeColumns
-
-        return None
-
-    def get_exclude_columns(self, entity) -> Optional[List[str]]:
-        """get included columns"""
-        entity_config: Optional[TableConfig] = self.get_config_for_entity(entity)
-        if entity_config and entity_config.columnConfig:
-            return entity_config.columnConfig.excludeColumns
-
-        if entity.tableProfilerConfig:
-            return entity.tableProfilerConfig.excludeColumns
-
-        return None
-
-    def create_profiler(
-        self, table_entity: Table, profiler_interface: ProfilerProtocol
-    ):
-        """Profile a single entity"""
-        if not self.profiler_config.profiler:
-            self.profiler = DefaultProfiler(
-                profiler_interface=profiler_interface,
-                include_columns=self.get_include_columns(table_entity),
-                exclude_columns=self.get_exclude_columns(table_entity),
-            )
-        else:
-            metrics = (
-                [Metrics.get(name) for name in self.profiler_config.profiler.metrics]
-                if self.profiler_config.profiler.metrics
-                else get_default_metrics(profiler_interface.table)
-            )
-
-            self.profiler = Profiler(
-                *metrics,  # type: ignore
-                profiler_interface=profiler_interface,
-                include_columns=self.get_include_columns(table_entity),
-                exclude_columns=self.get_exclude_columns(table_entity),
-            )
-
     def filter_databases(self, database: Database) -> Optional[Database]:
         """Returns filtered database entities"""
         if filter_by_database(
             self.source_config.databaseFilterPattern,
             database.name.__root__,
         ):
             self.source_status.filter(
@@ -334,109 +256,69 @@
                     database_name=database.name.__root__,
                 ),
             },  # type: ignore
         )
 
         yield from self.filter_entities(tables)
 
-    def copy_service_config(self, database) -> DatabaseService.__config__:
-        copy_service_connection_config = deepcopy(
-            self.config.source.serviceConnection.__root__.config  # type: ignore
-        )
-        if hasattr(
-            self.config.source.serviceConnection.__root__.config,  # type: ignore
-            "supportsDatabase",
-        ):
-            if hasattr(copy_service_connection_config, "database"):
-                copy_service_connection_config.database = database.name.__root__  # type: ignore
-            if hasattr(copy_service_connection_config, "catalog"):
-                copy_service_connection_config.catalog = database.name.__root__  # type: ignore
-
-        # we know we'll only be working with databaseServices, we cast the type to satisfy type checker
-        copy_service_connection_config = cast(
-            DatabaseService.__config__, copy_service_connection_config
-        )
-
-        return copy_service_connection_config
-
     def run_profiler(
-        self, entity: Table, copied_service_config, sqa_metadata=None
+        self, entity: Table, profiler_source: BaseProfilerSource
     ) -> Optional[ProfilerResponse]:
         """
         Main logic for the profiler workflow
         """
+        profiler_runner: Profiler = profiler_source.get_profiler_runner(
+            entity, self.profiler_config
+        )
+
         try:
-            profiler_interface: Union[
-                SQAProfilerInterface, PandasProfilerInterface
-            ] = ProfilerProtocol.create(
-                (
-                    copied_service_config.__class__.__name__
-                    if isinstance(copied_service_config, NON_SQA_DATABASE_CONNECTIONS)
-                    else self.config.source.serviceConnection.__root__.__class__.__name__
-                ),
-                entity,
-                self.get_config_for_entity(entity),
-                self.source_config,
-                copied_service_config,
-                create_ometa_client(self.metadata_config),
-                sqa_metadata=sqa_metadata,
-            )  # type: ignore
-            self.create_profiler(entity, profiler_interface)
-            self.profiler = cast(Profiler, self.profiler)  # satisfy type checker
-            profile: ProfilerResponse = self.profiler.process(
+            profile: ProfilerResponse = profiler_runner.process(
                 self.source_config.generateSampleData,
                 self.source_config.processPiiSensitive,
             )
-            self.profiler.close()
         except Exception as exc:
             name = entity.fullyQualifiedName.__root__
             error = f"Unexpected exception processing entity [{name}]: {exc}"
             logger.debug(traceback.format_exc())
             logger.error(error)
             self.source_status.failed(name, error, traceback.format_exc())
-            try:
-                # if we fail to instantiate a profiler_interface, we won't have a profiler_interface variable
-                self.source_status.fail_all(
-                    profiler_interface.processor_status.failures
-                )
-                self.source_status.records.extend(
-                    profiler_interface.processor_status.records
-                )
-                self.profiler.close()
-            except UnboundLocalError:
-                pass
+            self.source_status.fail_all(
+                profiler_source.interface.processor_status.failures
+            )
+            self.source_status.records.extend(
+                profiler_source.interface.processor_status.records
+            )
         else:
-            self.source_status.fail_all(profiler_interface.processor_status.failures)
+            # at this point we know we have an interface variable since we the `try` block above didn't raise
+            self.source_status.fail_all(profiler_source.interface.processor_status.failures)  # type: ignore
             self.source_status.records.extend(
-                profiler_interface.processor_status.records
+                profiler_source.interface.processor_status.records  # type: ignore
             )
             return profile
+        finally:
+            profiler_runner.close()
 
         return None
 
     def execute(self):
         """
         Run the profiling and tests
         """
         self.timer.trigger()
 
         try:
             for database in self.get_database_entities():
-                copied_service_config = self.copy_service_config(database)
-                sqa_metadata = (
-                    MetaData()
-                    if not isinstance(
-                        copied_service_config, NON_SQA_DATABASE_CONNECTIONS
-                    )
-                    else None
-                )  # we only need this for sqlalchemy based services
+                profiler_source = profiler_source_factory.create(
+                    self.config.source.type.lower(),
+                    self.config,
+                    database,
+                    self.metadata,
+                )
                 for entity in self.get_table_entities(database=database):
-                    profile = self.run_profiler(
-                        entity, copied_service_config, sqa_metadata
-                    )
+                    profile = self.run_profiler(entity, profiler_source)
                     if hasattr(self, "sink") and profile:
                         self.sink.write_record(profile)
             # At the end of the `execute`, update the associated Ingestion Pipeline status as success
             self.update_ingestion_status_at_end()
 
         # Any unhandled exception breaking the workflow should update the status
         except Exception as err:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/pandas/pandas_profiler_interface.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/pandas/profiler_interface.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,49 +25,48 @@
     TableData,
 )
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.ingestion.api.processor import ProfilerProcessorStatus
 from metadata.ingestion.source.connections import get_connection
-from metadata.ingestion.source.database.datalake.metadata import (
-    DATALAKE_DATA_TYPES,
-    DatalakeSource,
-)
+from metadata.ingestion.source.database.datalake.metadata import DatalakeSource
 from metadata.mixins.pandas.pandas_mixin import PandasInterfaceMixin
 from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
 from metadata.profiler.metrics.core import MetricTypes
 from metadata.profiler.metrics.registry import Metrics
-from metadata.profiler.processor.datalake_sampler import DatalakeSampler
+from metadata.profiler.processor.pandas.sampler import DatalakeSampler
 from metadata.utils.dispatch import valuedispatch
 from metadata.utils.logger import profiler_interface_registry_logger
 from metadata.utils.sqa_like_column import SQALikeColumn, Type
 
 logger = profiler_interface_registry_logger()
 
 
 class PandasProfilerInterface(ProfilerProtocol, PandasInterfaceMixin):
     """
     Interface to interact with registry supporting
     sqlalchemy.
     """
 
+    # pylint: disable=too-many-arguments
+
     _profiler_type: str = DatalakeConnection.__name__
 
     def __init__(
         self,
         service_connection_config,
         ometa_client,
         thread_count,
         entity,
         profile_sample_config,
         source_config,
         sample_query,
         table_partition_config=None,
-        **kwargs,
+        **_,
     ):
         """Instantiate SQA Interface object"""
         self._thread_count = thread_count
         self.table_entity = entity
         self.ometa_client = ometa_client
         self.source_config = source_config
         self.service_connection_config = service_connection_config
@@ -358,8 +357,7 @@
         return []
 
     def get_connection_client(self):
         return get_connection(self.service_connection_config).client
 
     def close(self):
         """Nothing to close with pandas"""
-        pass
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/profiler_protocol.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/profiler_protocol.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/interface/sqlalchemy/sqa_profiler_interface.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/interface/sqlalchemy/profiler_interface.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,16 +33,19 @@
 from metadata.mixins.sqalchemy.sqa_mixin import SQAInterfaceMixin
 from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
 from metadata.profiler.metrics.core import MetricTypes
 from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.metrics.static.mean import Mean
 from metadata.profiler.metrics.static.stddev import StdDev
 from metadata.profiler.metrics.static.sum import Sum
+from metadata.profiler.orm.functions.table_metric_construct import (
+    table_metric_construct_factory,
+)
 from metadata.profiler.processor.runner import QueryRunner
-from metadata.profiler.processor.sampler import Sampler
+from metadata.profiler.processor.sqlalchemy.sampler import Sampler
 from metadata.utils.custom_thread_pool import CustomThreadPoolExecutor
 from metadata.utils.dispatch import valuedispatch
 from metadata.utils.logger import profiler_interface_registry_logger
 
 logger = profiler_interface_registry_logger()
 thread_local = threading.local()
 
@@ -61,29 +64,31 @@
 
 class SQAProfilerInterface(ProfilerProtocol, SQAInterfaceMixin):
     """
     Interface to interact with registry supporting
     sqlalchemy.
     """
 
+    # pylint: disable=too-many-instance-attributes,too-many-arguments
+
     _profiler_type: str = DatabaseConnection.__name__
 
     def __init__(
         self,
         service_connection_config,
         ometa_client,
         entity,
         profile_sample_config,
         source_config,
         sample_query,
         table_partition_config,
         sqa_metadata=None,
         timeout_seconds=43200,
         thread_count=5,
-        **kwargs,
+        **_,
     ):
         """Instantiate SQA Interface object"""
         self._thread_count = thread_count
         self.table_entity = entity
         self.ometa_client = ometa_client
         self.source_config = source_config
         self.service_connection_config = service_connection_config
@@ -165,14 +170,15 @@
                     and metric not in {Sum, StdDev, Mean}
                 ]
             )
             return dict(row)
         except Exception as exc:
             msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
             handle_query_exception(msg, exc, session)
+        return None
 
     @valuedispatch
     def _get_metrics(self, *args, **kwargs):
         """Generic getter method for metrics. To be used with
         specific dispatch methods
         """
         logger.warning("Could not get metric. No function registered.")
@@ -192,19 +198,24 @@
         and returns the values
 
         Args:
             metrics: list of metrics to compute
         Returns:
             dictionnary of results
         """
+        # pylint: disable=protected-access
+
         try:
-            row = runner.select_first_from_sample(
-                *[metric().fn() for metric in metrics]
+            dialect = runner._session.get_bind().dialect.name
+            row = table_metric_construct_factory.construct(
+                dialect,
+                runner=runner,
+                metrics=metrics,
+                conn_config=self.service_connection_config,
             )
-
             if row:
                 return dict(row)
             return None
 
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(
@@ -240,30 +251,29 @@
                     metric(column).fn()
                     for metric in metrics
                     if not metric.is_window_metric()
                 ],
                 **self._is_array_column(column),
             )
             return dict(row)
-        except Exception as exc:
-            if (
-                isinstance(exc, ProgrammingError)
-                and exc.orig
-                and exc.orig.errno
-                in OVERFLOW_ERROR_CODES.get(session.bind.dialect.name)
+        except ProgrammingError as exc:
+            if exc.orig and exc.orig.errno in OVERFLOW_ERROR_CODES.get(
+                session.bind.dialect.name
             ):
                 logger.info(
                     f"Computing metrics without sum for {runner.table.__tablename__}.{column.name}"
                 )
                 return self._compute_static_metrics_wo_sum(
                     metrics, runner, session, column
                 )
 
+        except Exception as exc:
             msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
             handle_query_exception(msg, exc, session)
+        return None
 
     # pylint: disable=unused-argument
     @_get_metrics.register(MetricTypes.Query.value)
     def _(
         self,
         metric_type: str,
         metric: Metrics,
@@ -292,14 +302,15 @@
                 return {metric.name(): data}
 
             row = runner.select_first_from_query(metric_query)
             return dict(row)
         except Exception as exc:
             msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
             handle_query_exception(msg, exc, session)
+        return None
 
     # pylint: disable=unused-argument
     @_get_metrics.register(MetricTypes.Window.value)
     def _(
         self,
         metric_type: str,
         metrics: List[Metrics],
@@ -321,25 +332,25 @@
         if not metrics:
             return None
         try:
             row = runner.select_first_from_sample(
                 *[metric(column).fn() for metric in metrics],
                 **self._is_array_column(column),
             )
-        except Exception as exc:
-            if (
-                isinstance(exc, ProgrammingError)
-                and exc.orig
-                and exc.orig.errno
-                in OVERFLOW_ERROR_CODES.get(session.bind.dialect.name)
+        except ProgrammingError as exc:
+            if exc.orig and exc.orig.errno in OVERFLOW_ERROR_CODES.get(
+                session.bind.dialect.name
             ):
                 logger.info(
                     f"Skipping window metrics for {runner.table.__tablename__}.{column.name} due to overflow"
                 )
                 return None
+
+        except Exception as exc:
+
             msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
             handle_query_exception(msg, exc, session)
         if row:
             return dict(row)
         return None
 
     @_get_metrics.register(MetricTypes.System.value)
@@ -364,14 +375,15 @@
         """
         try:
             rows = metric().sql(session, conn_config=self.service_connection_config)
             return rows
         except Exception as exc:
             msg = f"Error trying to compute profile for {runner.table.__tablename__}: {exc}"
             handle_query_exception(msg, exc, session)
+        return None
 
     def _create_thread_safe_sampler(
         self,
         session,
         table,
     ):
         """Create thread safe runner"""
@@ -538,28 +550,28 @@
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Unexpected exception computing metrics: {exc}")
             self.session.rollback()
             return None
 
     def get_hybrid_metrics(
-        self, column: Column, metric: Metrics, column_results: Dict, table, **kwargs
+        self, column: Column, metric: Metrics, column_results: Dict, **kwargs
     ):
         """Given a list of metrics, compute the given results
         and returns the values
 
         Args:
             column: the column to compute the metrics against
             metrics: list of metrics to compute
         Returns:
             dictionnary of results
         """
         sampler = Sampler(
             session=self.session,
-            table=table,
+            table=kwargs.get("table"),
             sample_columns=self._get_sample_columns(),
             profile_sample_config=self.profile_sample_config,
             partition_details=self.partition_details,
             profile_sample_query=self.profile_query,
         )
         sample = sampler.random_sample()
         try:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/distinct_ratio.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/distinct_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/duplicate_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/duplicate_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/ilike_ratio.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/ilike_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/iqr.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/iqr.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/like_ratio.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/like_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/non_parametric_skew.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/non_parametric_skew.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/null_ratio.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/null_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/composed/unique_ratio.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/composed/unique_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/core.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/core.py`

 * *Files 0% similar despite different names*

```diff
@@ -224,15 +224,15 @@
         """
 
 
 class SystemMetric(Metric, ABC):
     """Abstract class for system metrics"""
 
     @abstractmethod
-    def sql(self):
+    def sql(self, session: Session, **kwargs):
         """SQL query to get system Metric"""
 
 
 class ComposedMetric(Metric, ABC):
     """
     A Metric composed by other metrics.
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/hybrid/histogram.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/hybrid/histogram.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/registry.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/column_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/column_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/column_names.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/column_names.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/count_in_set.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/count_in_set.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/distinct_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/distinct_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/ilike_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/ilike_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/like_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/like_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/max.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/max.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/max_length.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/max_length.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/mean.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/mean.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/min.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/min.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/min_length.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/min_length.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/not_like_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/not_like_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/not_regexp_match_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/not_regexp_match_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/null_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/null_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/regexp_match_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/regexp_match_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/row_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/row_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/stddev.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/stddev.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/sum.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/sum.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/static/unique_count.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/static/unique_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/system/system.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/system.py`

 * *Files 19% similar despite different names*

```diff
@@ -4,66 +4,66 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
+#  pylint: disable=unused-argument
 """
 System Metric
 """
 
 import traceback
-from collections import defaultdict, namedtuple
-from enum import Enum
+from collections import defaultdict
 from textwrap import dedent
 from typing import Dict, List, Optional
 
-import sqlparse
 from sqlalchemy import text
 from sqlalchemy.orm import DeclarativeMeta, Session
 
-from metadata.generated.schema.entity.data.table import DmlOperationType
 from metadata.generated.schema.entity.services.connections.database.bigQueryConnection import (
     BigQueryConnection,
 )
 from metadata.profiler.metrics.core import SystemMetric
+from metadata.profiler.metrics.system.dml_operation import (
+    DML_OPERATION_MAP,
+    DatabaseDMLOperations,
+)
+from metadata.profiler.metrics.system.queries.bigquery import (
+    DML_STAT_TO_DML_STATEMENT_MAPPING,
+    JOBS,
+    BigQueryQueryResult,
+)
+from metadata.profiler.metrics.system.queries.redshift import (
+    STL_QUERY,
+    get_metric_result,
+    get_query_results,
+)
+from metadata.profiler.metrics.system.queries.snowflake import (
+    INFORMATION_SCHEMA_QUERY,
+    RESULT_SCAN,
+    get_snowflake_system_queries,
+)
 from metadata.profiler.orm.registry import Dialects
 from metadata.utils.dispatch import valuedispatch
 from metadata.utils.helpers import deep_size_of_dict
 from metadata.utils.logger import profiler_logger
-from metadata.utils.profiler_utils import clean_up_query, get_snowflake_system_queries
+from metadata.utils.profiler_utils import get_value_from_cache, set_cache
 
 logger = profiler_logger()
 
 MAX_SIZE_IN_BYTES = 2 * 1024**3  # 2GB
 
 
 def recursive_dic():
     """recursive default dict"""
     return defaultdict(recursive_dic)
 
 
-class DatabaseDMLOperations(Enum):
-    """enum of supported DML operation on database engine side"""
-
-    INSERT = "INSERT"
-    UPDATE = "UPDATE"
-    DELETE = "DELETE"
-    MERGE = "MERGE"
-
-
-DML_OPERATION_MAP = {
-    DatabaseDMLOperations.INSERT.value: DmlOperationType.INSERT.value,
-    DatabaseDMLOperations.MERGE.value: DmlOperationType.UPDATE.value,
-    DatabaseDMLOperations.UPDATE.value: DmlOperationType.UPDATE.value,
-    DatabaseDMLOperations.DELETE.value: DmlOperationType.DELETE.value,
-}
-
 SYSTEM_QUERY_RESULT_CACHE = recursive_dic()
 
 
 @valuedispatch
 def get_system_metrics_for_dialect(
     dialect: str,
     session: Session,
@@ -104,108 +104,91 @@
         session (Session): session Object
         table (DeclarativeMeta): orm table
 
     Returns:
         List[Dict]:
     """
     logger.debug(f"Fetching system metrics for {dialect}")
-    dml_stat_to_dml_statement_mapping = {
-        "inserted_row_count": DatabaseDMLOperations.INSERT.value,
-        "deleted_row_count": DatabaseDMLOperations.DELETE.value,
-        "updated_row_count": DatabaseDMLOperations.UPDATE.value,
-    }
-    project_id = session.get_bind().url.host
-    dataset_id = table.__table_args__["schema"]
 
-    jobs = dedent(
-        f"""
-        SELECT
-            statement_type,
-            start_time,
-            destination_table,
-            dml_statistics
-        FROM
-            `region-{conn_config.usageLocation}`.INFORMATION_SCHEMA.JOBS
-        WHERE
-            DATE(creation_time) >= CURRENT_DATE() - 1 AND
-            destination_table.dataset_id = '{dataset_id}' AND
-            destination_table.project_id = '{project_id}' AND
-            statement_type IN (
-                '{DatabaseDMLOperations.INSERT.value}',
-                '{DatabaseDMLOperations.DELETE.value}',
-                '{DatabaseDMLOperations.UPDATE.value}',
-                '{DatabaseDMLOperations.MERGE.value}'
-            )
-        ORDER BY creation_time DESC;
-        """
-    )
+    project_id = session.get_bind().url.host
+    dataset_id = table.__table_args__["schema"]  # type: ignore
 
     metric_results: List[Dict] = []
-    QueryResult = namedtuple(
-        "QueryResult",
-        "query_type,timestamp,destination_table,dml_statistics",
+    # QueryResult = namedtuple(
+    #     "QueryResult",
+    #     "query_type,timestamp,destination_table,dml_statistics",
+    # )
+
+    jobs = get_value_from_cache(
+        SYSTEM_QUERY_RESULT_CACHE, f"{Dialects.BigQuery}.{project_id}.{dataset_id}.jobs"
     )
 
-    if (
-        "query_results"
-        in SYSTEM_QUERY_RESULT_CACHE[Dialects.BigQuery][project_id][dataset_id]
-    ):
-        # we'll try to get the cached data first
-        query_results = SYSTEM_QUERY_RESULT_CACHE[Dialects.BigQuery][project_id][
-            dataset_id
-        ]["query_results"]
-    else:
-        cursor_jobs = session.execute(text(jobs))
-        query_results = [
-            QueryResult(
-                row.statement_type,
-                row.start_time,
-                row.destination_table,
-                row.dml_statistics,
+    if not jobs:
+        cursor_jobs = session.execute(
+            text(
+                JOBS.format(
+                    usage_location=conn_config.usageLocation,
+                    dataset_id=dataset_id,
+                    project_id=project_id,
+                    insert=DatabaseDMLOperations.INSERT.value,
+                    update=DatabaseDMLOperations.UPDATE.value,
+                    delete=DatabaseDMLOperations.DELETE.value,
+                    merge=DatabaseDMLOperations.MERGE.value,
+                )
+            )
+        )
+        jobs = [
+            BigQueryQueryResult(
+                query_type=row.statement_type,
+                timestamp=row.start_time,
+                table_name=row.destination_table,
+                dml_statistics=row.dml_statistics,
             )
-            for row in cursor_jobs.fetchall()
+            for row in cursor_jobs
         ]
-        SYSTEM_QUERY_RESULT_CACHE[Dialects.BigQuery][project_id][dataset_id][
-            "query_results"
-        ] = query_results
+        set_cache(
+            SYSTEM_QUERY_RESULT_CACHE,
+            f"{Dialects.BigQuery}.{project_id}.{dataset_id}.jobs",
+            jobs,
+        )
 
-    for row_jobs in query_results:
-        if row_jobs.destination_table.get("table_id") == table.__tablename__:
+    for job in jobs:
+        if job.table_name.get("table_id") == table.__tablename__:  # type: ignore
             rows_affected = None
             try:
-                if row_jobs.query_type == DatabaseDMLOperations.INSERT.value:
-                    rows_affected = row_jobs.dml_statistics.get("inserted_row_count")
-                if row_jobs.query_type == DatabaseDMLOperations.DELETE.value:
-                    rows_affected = row_jobs.dml_statistics.get("deleted_row_count")
-                if row_jobs.query_type == DatabaseDMLOperations.UPDATE.value:
-                    rows_affected = row_jobs.dml_statistics.get("updated_row_count")
+                if job.query_type == DatabaseDMLOperations.INSERT.value:
+                    rows_affected = job.dml_statistics.get("inserted_row_count")
+                if job.query_type == DatabaseDMLOperations.DELETE.value:
+                    rows_affected = job.dml_statistics.get("deleted_row_count")
+                if job.query_type == DatabaseDMLOperations.UPDATE.value:
+                    rows_affected = job.dml_statistics.get("updated_row_count")
             except AttributeError:
                 logger.debug(traceback.format_exc())
                 rows_affected = None
 
-            if row_jobs.query_type == DatabaseDMLOperations.MERGE.value:
-                for indx, key in enumerate(row_jobs.dml_statistics):
-                    if row_jobs.dml_statistics[key] != 0:
+            if job.query_type == DatabaseDMLOperations.MERGE.value:
+                for indx, key in enumerate(job.dml_statistics):
+                    if job.dml_statistics[key] != 0:
                         metric_results.append(
                             {
                                 # Merge statement can include multiple DML operations
                                 # We are padding timestamps by 0,1,2 millisesond to avoid
                                 # duplicate timestamps
-                                "timestamp": int(row_jobs.timestamp.timestamp() * 1000)
+                                "timestamp": int(job.timestamp.timestamp() * 1000)
                                 + indx,
-                                "operation": dml_stat_to_dml_statement_mapping.get(key),
-                                "rowsAffected": row_jobs.dml_statistics[key],
+                                "operation": DML_STAT_TO_DML_STATEMENT_MAPPING.get(key),
+                                "rowsAffected": job.dml_statistics[key],
                             }
                         )
                 continue
 
             metric_results.append(
                 {
-                    "timestamp": int(row_jobs.timestamp.timestamp() * 1000),
-                    "operation": row_jobs.query_type,
+                    "timestamp": int(job.timestamp.timestamp() * 1000),
+                    "operation": job.query_type,
                     "rowsAffected": rows_affected,
                 }
             )
 
     return metric_results
 
 
@@ -225,245 +208,155 @@
         table (DeclarativeMeta): orm table
 
     Returns:
         List[Dict]:
     """
     logger.debug(f"Fetching system metrics for {dialect}")
     database = session.get_bind().url.database
-    schema = table.__table_args__["schema"]
+    schema = table.__table_args__["schema"]  # type: ignore
 
-    stl_deleted = dedent(
-        f"""
-        SELECT
-            SUM(si."rows") AS "rows",
-            sti."database",
-            sti."schema",
-            sti."table",
-            sq.text,
-            DATE_TRUNC('second', si.starttime) AS starttime
-        FROM
-            pg_catalog.stl_delete si
-            INNER JOIN  pg_catalog.svv_table_info sti ON si.tbl = sti.table_id
-            INNER JOIN pg_catalog.stl_querytext sq ON si.query = sq.query
-        WHERE
-            sti."database" = '{database}' AND
-            sti."schema" = '{schema}' AND
-            "rows" != 0 AND
-            DATE(starttime) >= CURRENT_DATE - 1
-        GROUP BY 2,3,4,5,6
-        ORDER BY 6 desc
-        """
+    metric_results: List[Dict] = []
+
+    # get inserts ddl queries
+    inserts = get_value_from_cache(
+        SYSTEM_QUERY_RESULT_CACHE, f"{Dialects.Redshift}.{database}.{schema}.inserts"
     )
+    if not inserts:
+        insert_query = STL_QUERY.format(
+            alias="si",
+            join_type="LEFT",
+            condition="sd.query is null",
+            database=database,
+            schema=schema,
+        )
+        inserts = get_query_results(
+            session,
+            insert_query,
+            DatabaseDMLOperations.INSERT.value,
+        )
+        set_cache(
+            SYSTEM_QUERY_RESULT_CACHE,
+            f"{Dialects.Redshift}.{database}.{schema}.inserts",
+            inserts,
+        )
+    metric_results.extend(get_metric_result(inserts, table.__tablename__))  # type: ignore
 
-    stl_insert = dedent(
-        f"""
-        SELECT
-            SUM(si."rows") AS "rows",
-            sti."database",
-            sti."schema",
-            sti."table",
-            sq.text,
-            DATE_TRUNC('second', si.starttime) AS starttime
-        FROM
-            pg_catalog.stl_insert si
-            INNER JOIN  pg_catalog.svv_table_info sti ON si.tbl = sti.table_id
-            INNER JOIN pg_catalog.stl_querytext sq ON si.query = sq.query
-        WHERE
-            sti."database" = '{database}' AND
-            sti."schema" = '{schema}' AND
-            "rows" != 0 AND
-            DATE(starttime) >= CURRENT_DATE - 1
-        GROUP BY 2,3,4,5,6
-        ORDER BY 6 desc
-        """
+    # get deletes ddl queries
+    deletes = get_value_from_cache(
+        SYSTEM_QUERY_RESULT_CACHE, f"{Dialects.Redshift}.{database}.{schema}.deletes"
     )
+    if not deletes:
+        delete_query = STL_QUERY.format(
+            alias="sd",
+            join_type="RIGHT",
+            condition="si.query is null",
+            database=database,
+            schema=schema,
+        )
+        deletes = get_query_results(
+            session,
+            delete_query,
+            DatabaseDMLOperations.DELETE.value,
+        )
+        set_cache(
+            SYSTEM_QUERY_RESULT_CACHE,
+            f"{Dialects.Redshift}.{database}.{schema}.deletes",
+            deletes,
+        )
+    metric_results.extend(get_metric_result(deletes, table.__tablename__))  # type: ignore
 
-    metric_results: List[Dict] = []
-    QueryResult = namedtuple(
-        "QueryResult",
-        "database_name,schema_name,table_name,query_text,timestamp,rowsAffected",
+    # get updates ddl queries
+    updates = get_value_from_cache(
+        SYSTEM_QUERY_RESULT_CACHE, f"{Dialects.Redshift}.{database}.{schema}.updates"
     )
+    if not updates:
+        update_query = STL_QUERY.format(
+            alias="si",
+            join_type="INNER",
+            condition="sd.query is not null",
+            database=database,
+            schema=schema,
+        )
+        updates = get_query_results(
+            session,
+            update_query,
+            DatabaseDMLOperations.UPDATE.value,
+        )
+        set_cache(
+            SYSTEM_QUERY_RESULT_CACHE,
+            f"{Dialects.Redshift}.{database}.{schema}.updates",
+            updates,
+        )
+    metric_results.extend(get_metric_result(updates, table.__tablename__))  # type: ignore
 
-    if (
-        "query_results_inserted"
-        in SYSTEM_QUERY_RESULT_CACHE[Dialects.Redshift][database][schema]
-    ):
-        # we'll try to get the cached data first
-        query_results_inserted = SYSTEM_QUERY_RESULT_CACHE[Dialects.Redshift][database][
-            schema
-        ]["query_results_inserted"]
-    else:
-        cursor_insert = session.execute(text(stl_insert))
-        query_results_inserted = [
-            QueryResult(
-                row.database,
-                row.schema,
-                row.table,
-                sqlparse.parse(clean_up_query(row.text))[0],
-                row.starttime,
-                row.rows,
-            )
-            for row in cursor_insert.fetchall()
-        ]
-        SYSTEM_QUERY_RESULT_CACHE[Dialects.Redshift][database][schema][
-            "query_results_inserted"
-        ] = query_results_inserted
-
-    if (
-        "query_results_deleted"
-        in SYSTEM_QUERY_RESULT_CACHE[Dialects.Redshift][database][schema]
-    ):
-        # we'll try to get the cached data first
-        query_results_deleted = SYSTEM_QUERY_RESULT_CACHE[Dialects.Redshift][database][
-            schema
-        ]["query_results_deleted"]
-    else:
-        cursor_deleted = session.execute(text(stl_deleted))
-        query_results_deleted = [
-            QueryResult(
-                row.database,
-                row.schema,
-                row.table,
-                sqlparse.parse(clean_up_query(row.text))[0],
-                row.starttime,
-                row.rows,
-            )
-            for row in cursor_deleted.fetchall()
-        ]
-        SYSTEM_QUERY_RESULT_CACHE[Dialects.Redshift][database][schema][
-            "query_results_deleted"
-        ] = query_results_deleted
-
-    for row_inserted in query_results_inserted:
-        if row_inserted.table_name == table.__tablename__:
-            query_text = row_inserted.query_text
-            operation = next(
-                (
-                    token.value.upper()
-                    for token in query_text.tokens
-                    if token.ttype is sqlparse.tokens.DML
-                    and token.value.upper()
-                    in DmlOperationType._member_names_  # pylint: disable=protected-access
-                ),
-                None,
-            )
-            if operation:
-                metric_results.append(
-                    {
-                        "timestamp": int(row_inserted.timestamp.timestamp() * 1000),
-                        "operation": operation,
-                        "rowsAffected": row_inserted.rowsAffected,
-                    }
-                )
+    return metric_results
 
-    for row_deleted in query_results_deleted:
-        if row_deleted.table_name == table.__tablename__:
-            query_text = row_deleted.query_text
-            operation = next(
-                (
-                    token.value.upper()
-                    for token in query_text.tokens
-                    if token.ttype is sqlparse.tokens.DML and token.value != "UPDATE"
-                ),
-                None,
-            )
 
-            if operation:
-                metric_results.append(
-                    {
-                        "timestamp": int(row_deleted.timestamp.timestamp() * 1000),
-                        "operation": operation,
-                        "rowsAffected": row_deleted.rowsAffected,
-                    }
-                )
+@get_system_metrics_for_dialect.register(Dialects.Snowflake)
+def _(
+    dialect: str,
+    session: Session,
+    table: DeclarativeMeta,
+    *args,
+    **kwargs,
+) -> Optional[List[Dict]]:
+    """Fetch system metrics for Snowflake. query_history will return maximum 10K rows in one request.
+    We'll be fetching all the queries ran for the past 24 hours and filtered on specific query types
+    (INSERTS, MERGE, DELETE, UPDATE).
 
-    return metric_results
+    :waring: Unlike redshift and bigquery results are not cached as we'll be looking
+    at DDL for each table
 
+    To get the number of rows affected we'll use the specific query ID.
 
-# @get_system_metrics_for_dialect.register(Dialects.Snowflake)
-# def _(
-#     dialect: str,
-#     session: Session,
-#     table: DeclarativeMeta,
-#     *args,
-#     **kwargs,
-# ) -> Optional[List[Dict]]:
-#     """Fetch system metrics for Snowflake. query_history will return maximum 10K rows in one request.
-#     We'll be fetching all the queries ran for the past 24 hours and filtered on specific query types
-#     (INSERTS, MERGE, DELETE, UPDATE).
-
-#     To get the number of rows affected we'll use the specific query ID.
-
-#     Args:
-#         dialect (str): dialect
-#         session (Session): session object
-
-#     Returns:
-#         Dict: system metric
-#     """
-#     logger.debug(f"Fetching system metrics for {dialect}")
-#     database = session.get_bind().url.database
-#     schema = table.__table_args__["schema"]
-
-#     metric_results: List[Dict] = []
-
-#     information_schema_query_history = f"""
-#         SELECT * FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
-#         WHERE
-#         start_time>= DATEADD('DAY', -1, CURRENT_TIMESTAMP)
-#         AND QUERY_TYPE IN (
-#             '{DatabaseDMLOperations.INSERT.value}',
-#             '{DatabaseDMLOperations.UPDATE.value}',
-#             '{DatabaseDMLOperations.DELETE.value}',
-#             '{DatabaseDMLOperations.MERGE.value}'
-#         )
-#         AND EXECUTION_STATUS = 'SUCCESS';
-#     """
-#     result_scan = """
-#     SELECT *
-#     FROM TABLE(RESULT_SCAN('{query_id}'));
-#     """
-
-#     if (
-#         "query_results"
-#         in SYSTEM_QUERY_RESULT_CACHE[Dialects.Snowflake][database][schema]
-#     ):
-#         # we'll try to get the cached data first
-#         query_results = SYSTEM_QUERY_RESULT_CACHE[Dialects.Snowflake][database][schema][
-#             "query_results"
-#         ]
-#     else:
-#         rows = session.execute(text(information_schema_query_history))
-#         query_results = []
-#         for row in rows:
-#             result = get_snowflake_system_queries(row, database, schema)
-#             if result:
-#                 query_results.append(result)
-#         SYSTEM_QUERY_RESULT_CACHE[Dialects.Snowflake][database][schema][
-#             "query_results"
-#         ] = query_results
-
-#     for query_result in query_results:
-#         if table.__tablename__.lower() == query_result.table_name:
-#             cursor_for_result_scan = session.execute(
-#                 text(dedent(result_scan.format(query_id=query_result.query_id)))
-#             )
-#             row_for_result_scan = cursor_for_result_scan.first()
-
-#             metric_results.append(
-#                 {
-#                     "timestamp": int(query_result.timestamp.timestamp() * 1000),
-#                     "operation": DML_OPERATION_MAP.get(query_result.query_type),
-#                     "rowsAffected": row_for_result_scan[0]
-#                     if row_for_result_scan
-#                     else None,
-#                 }
-#             )
+    Args:
+        dialect (str): dialect
+        session (Session): session object
 
-#     return metric_results
+    Returns:
+        Dict: system metric
+    """
+    logger.debug(f"Fetching system metrics for {dialect}")
+    database = session.get_bind().url.database
+    schema = table.__table_args__["schema"]  # type: ignore
+
+    metric_results: List[Dict] = []
+
+    rows = session.execute(
+        text(
+            INFORMATION_SCHEMA_QUERY.format(
+                tablename=table.__tablename__,  # type: ignore
+                insert=DatabaseDMLOperations.INSERT.value,
+                update=DatabaseDMLOperations.UPDATE.value,
+                delete=DatabaseDMLOperations.DELETE.value,
+                merge=DatabaseDMLOperations.MERGE.value,
+            )
+        )
+    )
+    query_results = []
+    for row in rows:
+        result = get_snowflake_system_queries(row, database, schema)
+        if result:
+            query_results.append(result)
+
+    for query_result in query_results:
+        cursor_for_result_scan = session.execute(
+            text(dedent(RESULT_SCAN.format(query_id=query_result.query_id)))
+        )
+        row_for_result_scan = cursor_for_result_scan.first()
+
+        metric_results.append(
+            {
+                "timestamp": int(query_result.timestamp.timestamp() * 1000),
+                "operation": DML_OPERATION_MAP.get(query_result.query_type),
+                "rowsAffected": row_for_result_scan[0] if row_for_result_scan else None,
+            }
+        )
+
+    return metric_results
 
 
 class System(SystemMetric):
     """System metric class to fetch:
         1. freshness
         2. affected rows
 
@@ -509,12 +402,12 @@
             )
 
         conn_config = kwargs.get("conn_config")
 
         system_metrics = get_system_metrics_for_dialect(
             session.get_bind().dialect.name,
             session=session,
-            table=self.table,
+            table=self.table,  # pylint: disable=no-member
             conn_config=conn_config,
         )
         self._manage_cache()
         return system_metrics
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/first_quartile.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/first_quartile.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/median.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/median.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/metrics/window/third_quartile.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/window/third_quartile.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/converter.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/converter.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/concat.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/concat.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/conn_test.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/conn_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -46,7 +46,12 @@
     return "SELECT SESSION_USER()"
 
 
 @compiles(ConnTestFn, Dialects.Db2)
 @compiles(ConnTestFn, Dialects.IbmDbSa)
 def _(*_, **__):
     return "SELECT 42 FROM SYSIBM.SYSDUMMY1;"
+
+
+@compiles(ConnTestFn, Dialects.Hana)
+def _(*_, **__):
+    return "SELECT 42 FROM DUMMY"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/datetime.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/datetime.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/length.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/length.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,14 +43,15 @@
 @compiles(LenFn, Dialects.Athena)
 @compiles(LenFn, Dialects.Trino)
 @compiles(LenFn, Dialects.Presto)
 @compiles(LenFn, Dialects.BigQuery)
 @compiles(LenFn, Dialects.Oracle)
 @compiles(LenFn, Dialects.IbmDbSa)
 @compiles(LenFn, Dialects.Db2)
+@compiles(LenFn, Dialects.Hana)
 def _(element, compiler, **kw):
     return "LENGTH(%s)" % compiler.process(element.clauses, **kw)
 
 
 @compiles(LenFn, Dialects.Postgres)
 def _(element, compiler, **kw):
     return "LENGTH(CAST(%s AS text))" % compiler.process(element.clauses, **kw)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/median.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/median.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 
 
 @compiles(MedianFn, Dialects.ClickHouse)
 def _(elements, compiler, **kwargs):
     col, _, percentile = [
         compiler.process(element, **kwargs) for element in elements.clauses
     ]
-    return "quantile(%s)(%s)" % (percentile, col)
+    return "if(isNaN(quantile(%s)(%s)),null,quantile(%s)(%s))" % ((percentile, col) * 2)
 
 
 # pylint: disable=unused-argument
 @compiles(MedianFn, Dialects.Athena)
 @compiles(MedianFn, Dialects.Trino)
 @compiles(MedianFn, Dialects.Presto)
 def _(elements, compiler, **kwargs):
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/modulo.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/modulo.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,14 +55,15 @@
 @compiles(ModuloFn, Dialects.MySQL)
 @compiles(ModuloFn, Dialects.Oracle)
 @compiles(ModuloFn, Dialects.Presto)
 @compiles(ModuloFn, Dialects.Trino)
 @compiles(ModuloFn, Dialects.IbmDbSa)
 @compiles(ModuloFn, Dialects.Db2)
 @compiles(ModuloFn, Dialects.Vertica)
+@compiles(ModuloFn, Dialects.Hana)
 def _(element, compiler, **kw):
     """Modulo function for specific dialect"""
     value, base = validate_and_compile(element, compiler, **kw)
     return f"MOD({value}, {base})"
 
 
 @compiles(ModuloFn, Dialects.ClickHouse)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/random_num.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/random_num.py`

 * *Files 5% similar despite different names*

```diff
@@ -42,14 +42,15 @@
 
 
 @compiles(RandomNumFn, Dialects.Hive)
 @compiles(RandomNumFn, Dialects.Impala)
 @compiles(RandomNumFn, Dialects.MySQL)
 @compiles(RandomNumFn, Dialects.IbmDbSa)
 @compiles(RandomNumFn, Dialects.Db2)
+@compiles(RandomNumFn, Dialects.Hana)
 def _(*_, **__):
     return "ABS(RAND()) * 100"
 
 
 @compiles(RandomNumFn, Dialects.BigQuery)
 def _(*_, **__):
     return "CAST(100*RAND() AS INT64)"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/functions/sum.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/functions/sum.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/registry.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/registry.py`

 * *Files 0% similar despite different names*

```diff
@@ -50,14 +50,15 @@
     BigQuery = "bigquery"
     ClickHouse = "clickhouse"
     Databricks = "databricks"
     Db2 = "db2"
     Druid = "druid"
     DynamoDB = "dynamoDB"
     Glue = "glue"
+    Hana = "hana"
     Hive = b"hive"  # Hive requires bytes
     Impala = "impala"
     IbmDbSa = "ibm_db_sa"
     MariaDB = "mariadb"
     MSSQL = "mssql"
     MySQL = "mysql"
     Oracle = "oracle"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/bytea_to_string.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/bytea_to_string.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/custom_array.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/custom_array.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/custom_timestamp.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/custom_timestamp.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/hex_byte_string.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/hex_byte_string.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/orm/types/uuid.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/orm/types/uuid.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/core.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/core.py`

 * *Files 0% similar despite different names*

```diff
@@ -194,15 +194,15 @@
         """
         for attrs, val in profile.tableProfile:
             if attrs not in {"timestamp", "profileSample", "profileSampleType"} and val:
                 return profile
 
         for col_element in profile.columnProfile:
             for attrs, val in col_element:
-                if attrs not in {"timestamp", "name"} and val:
+                if attrs not in {"timestamp", "name"} and val is not None:
                     return profile
 
         raise RuntimeError(
             f"No profile data computed for {self.profiler_interface.table_entity.fullyQualifiedName.__root__}"
         )
 
     @property
@@ -560,14 +560,16 @@
                 )
             ]
 
             table_profile = TableProfile(
                 timestamp=self.profile_date,
                 columnCount=self._table_results.get("columnCount"),
                 rowCount=self._table_results.get(RowCount.name()),
+                createDateTime=self._table_results.get("createDateTime"),
+                sizeInByte=self._table_results.get("sizeInBytes"),
                 profileSample=self.profile_sample_config.profile_sample
                 if self.profile_sample_config
                 else None,
                 profileSampleType=self.profile_sample_config.profile_sample_type
                 if self.profile_sample_config
                 else None,
             )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/datalake_sampler.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/pandas/sampler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/default.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/default.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/handle_partition.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/handle_partition.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/models.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/runner.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/processor/sampler.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/processor/sqlalchemy/sampler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/registry.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/sink/file.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/sink/file.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/profiler/sink/metadata_rest.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/sink/metadata_rest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/readers/base.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/base.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/readers/github.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/github.py`

 * *Files 21% similar despite different names*

```diff
@@ -17,58 +17,36 @@
 from typing import Any, Dict
 
 import requests
 
 from metadata.generated.schema.security.credentials.githubCredentials import (
     GitHubCredentials,
 )
-from metadata.readers.base import Reader, ReadException
+from metadata.readers.api_reader import ApiReader
+from metadata.readers.base import ReadException
 from metadata.utils.constants import UTF_8
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 HOST = "https://api.github.com"
 
 
 class UrlParts(Enum):
     REPOS = "repos"
     CONTENTS = "contents"
 
 
-class GitHubReader(Reader):
+class GitHubReader(ApiReader):
     """
     Handle calls to the GitHub API against a repo
     """
 
-    def __init__(self, credentials: GitHubCredentials):
-        self.credentials = credentials
-
-        self._auth_headers = None
-
-    @property
-    def auth_headers(self) -> Dict[str, str]:
-        """
-        Build the headers to authenticate
-        to the API
-        """
-        if self._auth_headers is None:
-            self._auth_headers = {
-                "Authorization": f"Bearer {self.credentials.token.get_secret_value()}"
-            }
-
-        return self._auth_headers
-
-    @staticmethod
-    def _build_url(*parts: str):
-        """
-        Build URL parts
-        """
-        return "/".join(parts)
+    credentials: GitHubCredentials
 
     @staticmethod
     def _decode_content(json_response: Dict[str, Any]) -> str:
         """
         Return the content of the response
 
         If no `content` there, throw the KeyError
@@ -76,22 +54,24 @@
         return base64.b64decode(json_response["content"]).decode(UTF_8)
 
     def read(self, path: str) -> str:
         """
         Read a file from a GitHub Repo and return its
         contents as a string
         https://docs.github.com/en/rest/repos/contents?apiVersion=2022-11-28#get-repository-content
+
+        This does not care if the path starts with `/` or not.
         """
         try:
             res = requests.get(
                 self._build_url(
                     HOST,
                     UrlParts.REPOS.value,
-                    self.credentials.repositoryOwner,
-                    self.credentials.repositoryName,
+                    self.credentials.repositoryOwner.__root__,
+                    self.credentials.repositoryName.__root__,
                     UrlParts.CONTENTS.value,
                     path,
                 ),
                 headers=self.auth_headers,
                 timeout=30,
             )
             if res.status_code == 200:
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/readers/local.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/readers/local.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/timer/repeated_timer.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/timer/repeated_timer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/timer/workflow_reporter.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/timer/workflow_reporter.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/azure_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/mongodb/metadata.py`

 * *Files 27% similar despite different names*

```diff
@@ -4,97 +4,97 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Utils module to convert different file types from azure file system into a dataframe
+MongoDB source methods.
 """
 
-import gzip
-import io
 import traceback
-import zipfile
-from typing import Any
+from typing import Dict, List, Union
 
-import pandas as pd
+from pymongo.errors import OperationFailure
 
-from metadata.ingestion.source.database.datalake.utils import (
-    read_from_avro,
-    read_from_json,
+from metadata.generated.schema.entity.services.connections.database.mongoDBConnection import (
+    MongoDBConnection,
 )
-from metadata.utils.logger import utils_logger
-
-logger = utils_logger()
-
-
-def _get_json_text(key: str, text: str) -> str:
-    if key.endswith(".gz"):
-        return gzip.decompress(text)
-    if key.endswith(".zip"):
-        with zipfile.ZipFile(io.BytesIO(text)) as zip_file:
-            return zip_file.read(zip_file.infolist()[0]).decode("utf-8")
-    return text
-
-
-def get_file_text(client: Any, key: str, container_name: str):
-    container_client = client.get_container_client(container_name)
-    blob_client = container_client.get_blob_client(key)
-    return blob_client.download_blob().readall()
-
-
-def read_csv_from_azure(
-    client: Any, key: str, container_name: str, storage_options: dict, sep: str = ","
-):
-    """
-    Read the csv file from the azure container and return a dataframe
-    """
-    try:
-        account_url = (
-            f"abfs://{container_name}@{client.account_name}.dfs.core.windows.net/{key}"
-        )
-        dataframe = pd.read_csv(account_url, storage_options=storage_options, sep=sep)
-        return dataframe
-    except Exception as exc:
-        logger.debug(traceback.format_exc())
-        logger.warning(f"Error reading CSV from ADLS - {exc}")
-        return None
-
-
-def read_json_from_azure(client: Any, key: str, container_name: str, sample_size=100):
-    """
-    Read the json file from the azure container and return a dataframe
-    """
-    json_text = get_file_text(client=client, key=key, container_name=container_name)
-    return read_from_json(
-        key=key, json_text=json_text, sample_size=sample_size, decode=True
-    )
-
+from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
+    OpenMetadataConnection,
+)
+from metadata.generated.schema.metadataIngestion.workflow import (
+    Source as WorkflowSource,
+)
+from metadata.ingestion.api.source import InvalidSourceException
+from metadata.ingestion.source.database.common_nosql_source import (
+    SAMPLE_SIZE,
+    CommonNoSQLSource,
+)
+from metadata.utils.logger import ingestion_logger
 
-def read_parquet_from_azure(
-    client: Any, key: str, container_name: str, storage_options: dict
-):
-    """
-    Read the parquet file from the container and return a dataframe
-    """
-    try:
-        account_url = (
-            f"abfs://{container_name}@{client.account_name}.dfs.core.windows.net/{key}"
-        )
-        dataframe = pd.read_parquet(account_url, storage_options=storage_options)
-        return dataframe
-    except Exception as exc:
-        logger.debug(traceback.format_exc())
-        logger.warning(f"Error reading parquet file from azure - {exc}")
-        return None
+logger = ingestion_logger()
 
 
-def read_avro_from_azure(client: Any, key: str, container_name: str):
-    """
-    Read the avro file from the gcs bucket and return a dataframe
+class MongodbSource(CommonNoSQLSource):
     """
-    return read_from_avro(
-        get_file_text(client=client, key=key, container_name=container_name)
-    )
+    Implements the necessary methods to extract
+    Database metadata from Dynamo Source
+    """
+
+    def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
+        super().__init__(config, metadata_config)
+        self.mongodb = self.connection_obj
+
+    @classmethod
+    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: MongoDBConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, MongoDBConnection):
+            raise InvalidSourceException(
+                f"Expected MongoDBConnection, but got {connection}"
+            )
+        return cls(config, metadata_config)
+
+    def get_schema_name_list(self) -> List[str]:
+        """
+        Method to get list of schema names available within NoSQL db
+        need to be overridden by sources
+        """
+        try:
+            return self.mongodb.list_database_names()
+        except Exception as exp:
+            logger.debug(f"Failed to list database names: {exp}")
+            logger.debug(traceback.format_exc())
+        return []
+
+    def get_table_name_list(self, schema_name: str) -> List[str]:
+        """
+        Method to get list of table names available within schema db
+        need to be overridden by sources
+        """
+        try:
+            database = self.mongodb.get_database(schema_name)
+            return database.list_collection_names()
+        except Exception as exp:
+            logger.debug(
+                f"Failed to list collection names for schema [{schema_name}]: {exp}"
+            )
+            logger.debug(traceback.format_exc())
+        return []
+
+    def get_table_columns_dict(
+        self, schema_name: str, table_name: str
+    ) -> Union[List[Dict], Dict]:
+        """
+        Method to get actual data available within table
+        need to be overridden by sources
+        """
+        try:
+            database = self.mongodb[schema_name]
+            collection = database.get_collection(table_name)
+            return list(collection.find().limit(SAMPLE_SIZE))
+        except OperationFailure as opf:
+            logger.debug(f"Failed to read collection [{table_name}]: {opf}")
+            logger.debug(traceback.format_exc())
+        return []
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/class_helper.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/class_helper.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/client_version.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/client_version.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/constants.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/constants.py`

 * *Files 21% similar despite different names*

```diff
@@ -14,25 +14,34 @@
 """
 
 DOT = "_DOT_"
 TEN_MIN = 10 * 60
 UTF_8 = "utf-8"
 CHUNKSIZE = 200000
 DEFAULT_DATABASE = "default"
-
+BUILDER_PASSWORD_ATTR = "password"
+TIMEDELTA = "timedelta"
+COMPLEX_COLUMN_SEPARATOR = "_##"
 
 ES_SOURCE_TO_ES_OBJ_ARGS = {
     "caCerts": "ca_certs",
     "regionName": "region_name",
     "timeout": "timeout",
     "useAwsCredentials": "use_AWS_credentials",
     "useSSL": "use_ssl",
     "verifyCerts": "verify_certs",
 }
 
+ES_SOURCE_IGNORE_KEYS = {
+    "searchIndexMappingLanguage",
+    "batchSize",
+    "recreateIndex",
+    "type",
+}
+
 QUERY_WITH_OM_VERSION = '/* {"app": "OpenMetadata"'
 
 QUERY_WITH_DBT = '/* {"app": "dbt"'
 
 AUTHORIZATION_HEADER = "Authorization"
 
 NO_ACCESS_TOKEN = "no_token"
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/credentials.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/credentials.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,31 +15,31 @@
 import json
 import os
 import tempfile
 from typing import Dict
 
 from cryptography.hazmat.primitives import serialization
 
-from metadata.generated.schema.security.credentials.gcsCredentials import (
-    GCSCredentials,
-    GCSCredentialsPath,
+from metadata.generated.schema.security.credentials.gcpCredentials import (
+    GCPCredentials,
+    GcpCredentialsPath,
 )
-from metadata.generated.schema.security.credentials.gcsValues import (
-    GcsCredentialsValues,
+from metadata.generated.schema.security.credentials.gcpValues import (
+    GcpCredentialsValues,
 )
 from metadata.utils.logger import utils_logger
 
 logger = utils_logger()
 
 GOOGLE_CREDENTIALS = "GOOGLE_APPLICATION_CREDENTIALS"
 
 
-class InvalidGcsConfigException(Exception):
+class InvalidGcpConfigException(Exception):
     """
-    Raised when we have errors trying to set GCS credentials
+    Raised when we have errors trying to set GCP credentials
     """
 
 
 class InvalidPrivateKeyException(Exception):
     """
     If the key cannot be serialised
     """
@@ -67,71 +67,71 @@
     with tempfile.NamedTemporaryFile(delete=False) as temp_file:
         cred_json = json.dumps(credentials, indent=4, separators=(",", ": "))
         temp_file.write(cred_json.encode())
 
         return temp_file.name
 
 
-def build_google_credentials_dict(gcs_values: GcsCredentialsValues) -> Dict[str, str]:
+def build_google_credentials_dict(gcp_values: GcpCredentialsValues) -> Dict[str, str]:
     """
-    Given GcsCredentialsValues, build a dictionary as the JSON file
-    downloaded from GCS with the service_account
-    :param gcs_values: GCS credentials
+    Given GcPCredentialsValues, build a dictionary as the JSON file
+    downloaded from GCP with the service_account
+    :param gcp_values: GCP credentials
     :return: Dictionary with credentials
     """
-    private_key_str = gcs_values.privateKey.get_secret_value()
+    private_key_str = gcp_values.privateKey.get_secret_value()
     # adding the replace string here to escape line break if passed from env
     private_key_str = private_key_str.replace("\\n", "\n")
     validate_private_key(private_key_str)
 
     return {
-        "type": gcs_values.type,
-        "project_id": gcs_values.projectId.__root__,
-        "private_key_id": gcs_values.privateKeyId,
+        "type": gcp_values.type,
+        "project_id": gcp_values.projectId.__root__,
+        "private_key_id": gcp_values.privateKeyId,
         "private_key": private_key_str,
-        "client_email": gcs_values.clientEmail,
-        "client_id": gcs_values.clientId,
-        "auth_uri": str(gcs_values.authUri),
-        "token_uri": str(gcs_values.tokenUri),
-        "auth_provider_x509_cert_url": str(gcs_values.authProviderX509CertUrl),
-        "client_x509_cert_url": str(gcs_values.clientX509CertUrl),
+        "client_email": gcp_values.clientEmail,
+        "client_id": gcp_values.clientId,
+        "auth_uri": str(gcp_values.authUri),
+        "token_uri": str(gcp_values.tokenUri),
+        "auth_provider_x509_cert_url": str(gcp_values.authProviderX509CertUrl),
+        "client_x509_cert_url": str(gcp_values.clientX509CertUrl),
     }
 
 
-def set_google_credentials(gcs_credentials: GCSCredentials) -> None:
+def set_google_credentials(gcp_credentials: GCPCredentials) -> None:
     """
-    Set GCS credentials environment variable
-    :param gcs_credentials: GCSCredentials
+    Set GCP credentials environment variable
+    :param gcp_credentials: GCPCredentials
     """
-    if isinstance(gcs_credentials.gcsConfig, GCSCredentialsPath):
-        os.environ[GOOGLE_CREDENTIALS] = str(gcs_credentials.gcsConfig.__root__)
+    if isinstance(gcp_credentials.gcpConfig, GcpCredentialsPath):
+        os.environ[GOOGLE_CREDENTIALS] = str(gcp_credentials.gcpConfig.__root__)
         return
 
-    if gcs_credentials.gcsConfig.projectId is None:
+    if gcp_credentials.gcpConfig.projectId is None:
         logger.info(
             "No credentials available, using the current environment permissions authenticated via gcloud SDK."
         )
         return
 
-    if isinstance(gcs_credentials.gcsConfig, GcsCredentialsValues):
+    if isinstance(gcp_credentials.gcpConfig, GcpCredentialsValues):
         if (
-            gcs_credentials.gcsConfig.projectId
-            and not gcs_credentials.gcsConfig.privateKey
+            gcp_credentials.gcpConfig.projectId
+            and not gcp_credentials.gcpConfig.privateKey
         ):
             logger.info(
                 "Overriding default projectid, using the current environment permissions authenticated via gcloud SDK."
             )
             return
-        credentials_dict = build_google_credentials_dict(gcs_credentials.gcsConfig)
+        credentials_dict = build_google_credentials_dict(gcp_credentials.gcpConfig)
         tmp_credentials_file = create_credential_tmp_file(credentials=credentials_dict)
         os.environ[GOOGLE_CREDENTIALS] = tmp_credentials_file
         return
 
-    raise InvalidGcsConfigException(
-        f"Error trying to set GCS credentials with {gcs_credentials}."
+    raise InvalidGcpConfigException(
+        f"Error trying to set GCP credentials with {gcp_credentials}."
         " Check https://docs.open-metadata.org/connectors/database/bigquery "
     )
 
 
 def generate_http_basic_token(username, password):
     """
     Generates a HTTP basic token from username and password
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/custom_thread_pool.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/custom_thread_pool.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/dbt_config.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,19 +10,21 @@
 #  limitations under the License.
 """
 Hosts the singledispatch to get DBT files
 """
 import json
 import traceback
 from functools import singledispatch
-from typing import Any, Optional, Tuple
+from typing import Optional, Tuple
 
 import requests
-from pydantic import BaseModel
 
+from metadata.generated.schema.metadataIngestion.dbtconfig.dbtAzureConfig import (
+    DbtAzureConfig,
+)
 from metadata.generated.schema.metadataIngestion.dbtconfig.dbtCloudConfig import (
     DbtCloudConfig,
 )
 from metadata.generated.schema.metadataIngestion.dbtconfig.dbtGCSConfig import (
     DbtGcsConfig,
 )
 from metadata.generated.schema.metadataIngestion.dbtconfig.dbtHttpConfig import (
@@ -30,35 +32,25 @@
 )
 from metadata.generated.schema.metadataIngestion.dbtconfig.dbtLocalConfig import (
     DbtLocalConfig,
 )
 from metadata.generated.schema.metadataIngestion.dbtconfig.dbtS3Config import (
     DbtS3Config,
 )
+from metadata.ingestion.source.database.dbt.constants import (
+    DBT_CATALOG_FILE_NAME,
+    DBT_MANIFEST_FILE_NAME,
+    DBT_RUN_RESULTS_FILE_NAME,
+)
+from metadata.ingestion.source.database.dbt.models import DbtFiles
 from metadata.utils.credentials import set_google_credentials
 from metadata.utils.logger import ometa_logger
 
 logger = ometa_logger()
 
-DBT_CATALOG_FILE_NAME = "catalog.json"
-DBT_MANIFEST_FILE_NAME = "manifest.json"
-DBT_RUN_RESULTS_FILE_NAME = "run_results.json"
-
-
-class DbtFiles(BaseModel):
-    dbt_catalog: Optional[dict]
-    dbt_manifest: dict
-    dbt_run_results: Optional[dict]
-
-
-class DbtObjects(BaseModel):
-    dbt_catalog: Optional[Any]
-    dbt_manifest: Any
-    dbt_run_results: Optional[Any]
-
 
 class DBTConfigException(Exception):
     """
     Raise when encountering errors while extacting dbt files
     """
 
 
@@ -277,15 +269,15 @@
     dbt_catalog = None
     dbt_manifest = None
     dbt_run_results = None
     try:
         bucket_name, prefix = get_dbt_prefix_config(config)
         from google.cloud import storage  # pylint: disable=import-outside-toplevel
 
-        set_google_credentials(gcs_credentials=config.dbtSecurityConfig)
+        set_google_credentials(gcp_credentials=config.dbtSecurityConfig)
         client = storage.Client()
         if not bucket_name:
             buckets = client.list_buckets()
         else:
             buckets = [client.get_bucket(bucket_name)]
         for bucket in buckets:
             if prefix:
@@ -312,14 +304,87 @@
     except DBTConfigException as exc:
         raise exc
     except Exception as exc:
         logger.debug(traceback.format_exc())
         raise DBTConfigException(f"Error fetching dbt files from gcs: {exc}")
 
 
+@get_dbt_details.register
+def _(config: DbtAzureConfig):
+    dbt_catalog = None
+    dbt_manifest = None
+    dbt_run_results = None
+    try:
+        bucket_name, prefix = get_dbt_prefix_config(config)
+        from azure.identity import (  # pylint: disable=import-outside-toplevel
+            ClientSecretCredential,
+        )
+        from azure.storage.blob import (  # pylint: disable=import-outside-toplevel
+            BlobServiceClient,
+        )
+
+        azure_client = BlobServiceClient(
+            f"https://{config.dbtSecurityConfig.accountName}.blob.core.windows.net/",
+            credential=ClientSecretCredential(
+                config.dbtSecurityConfig.tenantId,
+                config.dbtSecurityConfig.clientId,
+                config.dbtSecurityConfig.clientSecret.get_secret_value(),
+            ),
+        )
+
+        if not bucket_name:
+            container_dicts = azure_client.list_containers()
+            containers = [
+                azure_client.get_container_client(container["name"])
+                for container in container_dicts
+            ]
+        else:
+            container_client = azure_client.get_container_client(bucket_name)
+            containers = [container_client]
+        for container_client in containers:
+            if prefix:
+                blob_list = container_client.list_blobs(name_starts_with=prefix)
+            else:
+                blob_list = container_client.list_blobs()
+            for blob in blob_list:
+                if DBT_MANIFEST_FILE_NAME in blob.name:
+                    logger.debug(f"{DBT_MANIFEST_FILE_NAME} found")
+                    dbt_manifest = (
+                        container_client.download_blob(blob.name)
+                        .readall()
+                        .decode("utf-8")
+                    )
+                if DBT_CATALOG_FILE_NAME in blob.name:
+                    logger.debug(f"{DBT_CATALOG_FILE_NAME} found")
+                    dbt_catalog = (
+                        container_client.download_blob(blob.name)
+                        .readall()
+                        .decode("utf-8")
+                    )
+                if DBT_RUN_RESULTS_FILE_NAME in blob.name:
+                    logger.debug(f"{DBT_RUN_RESULTS_FILE_NAME} found")
+                    dbt_run_results = (
+                        container_client.download_blob(blob.name)
+                        .readall()
+                        .decode("utf-8")
+                    )
+        if not dbt_manifest:
+            raise DBTConfigException("Manifest file not found in Azure")
+        return DbtFiles(
+            dbt_catalog=json.loads(dbt_catalog) if dbt_catalog else None,
+            dbt_manifest=json.loads(dbt_manifest),
+            dbt_run_results=json.loads(dbt_run_results) if dbt_run_results else None,
+        )
+    except DBTConfigException as exc:
+        raise exc
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        raise DBTConfigException(f"Error fetching dbt files from Azure: {exc}")
+
+
 def get_dbt_prefix_config(config) -> Tuple[Optional[str], Optional[str]]:
     """
     Return (bucket, prefix) tuple
     """
     if config.dbtPrefixConfig:
         return (
             config.dbtPrefixConfig.dbtBucketName,
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/dispatch.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/dispatch.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/elasticsearch.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/elasticsearch.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/entity_link.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/entity_link.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/filters.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/filters.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/fqn.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/fqn.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,22 +10,21 @@
 #  limitations under the License.
 """
 Handle FQN building and splitting logic.
 Filter information has been taken from the
 ES indexes definitions
 """
 import re
-from collections import namedtuple
 from typing import Dict, List, Optional, Type, TypeVar, Union
 
 from antlr4.CommonTokenStream import CommonTokenStream
 from antlr4.error.ErrorStrategy import BailErrorStrategy
 from antlr4.InputStream import InputStream
 from antlr4.tree.Tree import ParseTreeWalker
-from pydantic import BaseModel
+from pydantic import BaseModel, Field
 
 from metadata.antlr.split_listener import FqnSplitListener
 from metadata.generated.antlr.FqnLexer import FqnLexer
 from metadata.generated.antlr.FqnParser import FqnParser
 from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.chart import Chart
 from metadata.generated.schema.entity.data.dashboard import Dashboard
@@ -51,14 +50,23 @@
 
 class FQNBuildingException(Exception):
     """
     Raise for inconsistencies when building the FQN
     """
 
 
+class SplitTestCaseFqn(BaseModel):
+    service: str
+    database: str
+    schema_: str = Field(alias="schema")
+    table: str
+    column: Optional[str]
+    test_case: Optional[str]
+
+
 def split(s: str) -> List[str]:  # pylint: disable=invalid-name
     """
     Equivalent of Java's FullyQualifiedName#split
     """
     lexer = FqnLexer(InputStream(s))
     stream = CommonTokenStream(lexer)
     parser = FqnParser(stream)
@@ -365,15 +373,15 @@
 def _(
     _: OpenMetadata,  # ES Search not enabled for TestCase
     *,
     service_name: str,
     database_name: str,
     schema_name: str,
     table_name: str,
-    column_name: str,
+    column_name: Optional[str],
     test_case_name: str,
 ) -> str:
     if column_name:
         return _build(
             service_name,
             database_name,
             schema_name,
@@ -417,44 +425,48 @@
     # Pad None to the left until size of list is 3
     full_details: List[Optional[str]] = ([None] * (3 - len(details))) + details
 
     database, database_schema, table = full_details
     return {"database": database, "database_schema": database_schema, "table": table}
 
 
-def split_test_case_fqn(test_case_fqn: str) -> Dict[str, Optional[str]]:
+def split_test_case_fqn(test_case_fqn: str) -> SplitTestCaseFqn:
     """given a test case fqn split each element
 
     Args:
         test_case_fqn (str): test case fqn
 
     Returns:
         Dict[str, Optional[str]]:
     """
-    SplitTestCaseFqn = namedtuple(
-        "SplitTestCaseFqn", "service database schema table column test_case"
-    )
     details = split(test_case_fqn)
     if len(details) < 5:
         raise ValueError(
             f"{test_case_fqn} does not appear to be a valid test_case fqn "
         )
     if len(details) != 6:
-        details.insert(4, None)
+        details.insert(4, None)  # type: ignore
 
     (  # pylint: disable=unbalanced-tuple-unpacking
         service,
         database,
         schema,
         table,
         column,
         test_case,
     ) = details
 
-    return SplitTestCaseFqn(service, database, schema, table, column, test_case)
+    return SplitTestCaseFqn(
+        service=service,
+        database=database,
+        schema=schema,
+        table=table,
+        column=column,
+        test_case=test_case,
+    )
 
 
 def build_es_fqn_search_string(
     database_name: str, schema_name, service_name, table_name
 ) -> str:
     """
     Builds FQN search string for ElasticSearch
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/gcs_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/datalake/json_dispatch.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,96 +6,92 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Utils module to convert different file types from gcs buckets into a dataframe
+Module to define helper methods for datalake and to fetch data and metadata 
+from Json file formats
 """
+import gzip
+import io
+import json
+import zipfile
+from functools import singledispatch
+from typing import Any, List
 
-import traceback
-from typing import Any
-
-import gcsfs
-import pandas as pd
-from pandas import DataFrame
-from pyarrow.parquet import ParquetFile
-
-from metadata.ingestion.source.database.datalake.utils import (
-    read_from_avro,
-    read_from_json,
+from metadata.generated.schema.entity.services.connections.database.datalake.azureConfig import (
+    AzureConfig,
+)
+from metadata.generated.schema.entity.services.connections.database.datalake.gcsConfig import (
+    GCSConfig,
+)
+from metadata.generated.schema.entity.services.connections.database.datalake.s3Config import (
+    S3Config,
 )
-from metadata.utils.constants import CHUNKSIZE
+from metadata.utils.constants import COMPLEX_COLUMN_SEPARATOR, UTF_8
+from metadata.utils.datalake.common import DatalakeFileFormatException
 from metadata.utils.logger import utils_logger
 
 logger = utils_logger()
 
 
-def get_file_text(client: Any, key: str, bucket_name: str):
-    bucket = client.get_bucket(bucket_name)
-    return bucket.get_blob(key).download_as_string()
+def _get_json_text(key: str, text: bytes, decode: bool) -> str:
+    if key.endswith(".gz"):
+        return gzip.decompress(text)
+    if key.endswith(".zip"):
+        with zipfile.ZipFile(io.BytesIO(text)) as zip_file:
+            return zip_file.read(zip_file.infolist()[0]).decode(UTF_8)
+    if decode:
+        return text.decode(UTF_8)
+    return text
 
 
-def read_csv_from_gcs(  # pylint: disable=inconsistent-return-statements
-    key: str, bucket_name: str
-) -> DataFrame:
+def read_from_json(
+    key: str, json_text: str, decode: bool = False, is_profiler: bool = False, **_
+) -> List:
     """
-    Read the csv file from the gcs bucket and return a dataframe
+    Read the json file from the azure container and return a dataframe
     """
 
-    try:
-        chunk_list = []
-        with pd.read_csv(
-            f"gs://{bucket_name}/{key}", sep=",", chunksize=CHUNKSIZE
-        ) as reader:
-            for chunks in reader:
-                chunk_list.append(chunks)
-        return chunk_list
-    except Exception as exc:
-        logger.debug(traceback.format_exc())
-        logger.warning(f"Error reading CSV from GCS - {exc}")
+    # pylint: disable=import-outside-toplevel
+    from pandas import json_normalize
 
+    from metadata.utils.datalake.common import dataframe_to_chunks
 
-def read_tsv_from_gcs(  # pylint: disable=inconsistent-return-statements
-    key: str, bucket_name: str
-) -> DataFrame:
-    """
-    Read the tsv file from the gcs bucket and return a dataframe
-    """
+    json_text = _get_json_text(key, json_text, decode)
     try:
-        chunk_list = []
-        with pd.read_csv(
-            f"gs://{bucket_name}/{key}", sep="\t", chunksize=CHUNKSIZE
-        ) as reader:
-            for chunks in reader:
-                chunk_list.append(chunks)
-        return chunk_list
-    except Exception as exc:
-        logger.debug(traceback.format_exc())
-        logger.warning(f"Error reading CSV from GCS - {exc}")
+        data = json.loads(json_text)
+    except json.decoder.JSONDecodeError:
+        logger.debug("Failed to read as JSON object trying to read as JSON Lines")
+        data = [json.loads(json_obj) for json_obj in json_text.strip().split("\n")]
+    if is_profiler:
+        return dataframe_to_chunks(json_normalize(data))
+    return dataframe_to_chunks(json_normalize(data, sep=COMPLEX_COLUMN_SEPARATOR))
+
+
+@singledispatch
+def read_json_dispatch(config_source: Any, key: str, **kwargs):
+    raise DatalakeFileFormatException(config_source=config_source, file_name=key)
 
 
-def read_json_from_gcs(client: Any, key: str, bucket_name: str) -> DataFrame:
+@read_json_dispatch.register
+def _(_: GCSConfig, key: str, bucket_name: str, client, **kwargs):
     """
     Read the json file from the gcs bucket and return a dataframe
     """
-    json_text = get_file_text(client=client, key=key, bucket_name=bucket_name)
-    return read_from_json(key=key, json_text=json_text, decode=True)
+    json_text = client.get_bucket(bucket_name).get_blob(key).download_as_string()
+    return read_from_json(key=key, json_text=json_text, decode=True, **kwargs)
 
 
-def read_parquet_from_gcs(key: str, bucket_name: str) -> DataFrame:
-    """
-    Read the parquet file from the gcs bucket and return a dataframe
-    """
-
-    gcs = gcsfs.GCSFileSystem()
-    file = gcs.open(f"gs://{bucket_name}/{key}")
-    return [ParquetFile(file).read().to_pandas()]
+@read_json_dispatch.register
+def _(_: S3Config, key: str, bucket_name: str, client, **kwargs):
+    json_text = client.get_object(Bucket=bucket_name, Key=key)["Body"].read()
+    return read_from_json(key=key, json_text=json_text, decode=True, **kwargs)
 
 
-def read_avro_from_gcs(client: Any, key: str, bucket_name: str) -> DataFrame:
-    """
-    Read the avro file from the gcs bucket and return a dataframe
-    """
-    avro_text = get_file_text(client=client, key=key, bucket_name=bucket_name)
-    return read_from_avro(avro_text)
+@read_json_dispatch.register
+def _(_: AzureConfig, key: str, bucket_name: str, client, **kwargs):
+    container_client = client.get_container_client(bucket_name)
+    json_text = container_client.get_blob_client(key).download_blob().readall()
+    return read_from_json(key=key, json_text=json_text, decode=True, **kwargs)
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/helpers.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/helpers.py`

 * *Files 16% similar despite different names*

```diff
@@ -20,17 +20,22 @@
 import sys
 from datetime import datetime, timedelta
 from functools import wraps
 from math import floor, log
 from time import perf_counter
 from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
 
+import sqlparse
+from sqlparse.sql import Statement
+
 from metadata.generated.schema.entity.data.chart import ChartType
 from metadata.generated.schema.entity.data.table import Column, Table
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.type.tagLabel import TagLabel
+from metadata.utils.constants import DEFAULT_DATABASE
 from metadata.utils.logger import utils_logger
 
 logger = utils_logger()
 
 
 class BackupRestoreArgs:
     def __init__(  # pylint: disable=too-many-arguments
@@ -383,7 +388,68 @@
             if isinstance(obj, type_):
                 size += sum(map(sizeof, handler(obj)))
                 break
 
         return size
 
     return sizeof(obj)
+
+
+def is_safe_sql_query(sql_query: str) -> bool:
+    """Validate SQL query
+    Args:
+        sql_query (str): SQL query
+    Returns:
+        bool
+    """
+
+    forbiden_token = {
+        "CREATE",
+        "ALTER",
+        "DROP",
+        "TRUNCATE",
+        "COMMENT",
+        "RENAME",
+        "INSERT",
+        "UPDATE",
+        "DELETE",
+        "MERGE",
+        "CALL",
+        "EXPLAIN PLAN",
+        "LOCK TABLE",
+        "UNLOCK TABLE",
+        "GRANT",
+        "REVOKE",
+        "COMMIT",
+        "ROLLBACK",
+        "SAVEPOINT",
+        "SET TRANSACTION",
+    }
+
+    parsed_queries: Tuple[Statement] = sqlparse.parse(sql_query)
+    for parsed_query in parsed_queries:
+        validation = [
+            token.normalized in forbiden_token for token in parsed_query.tokens
+        ]
+        if any(validation):
+            return False
+    return True
+
+
+def get_database_name_for_lineage(
+    db_service_entity: DatabaseService, default_db_name: str
+) -> Optional[str]:
+    # If the database service supports multiple db or
+    # database service connection details are not available
+    # then pick the database name available from api response
+    if db_service_entity.connection is None or hasattr(
+        db_service_entity.connection.config, "supportsDatabase"
+    ):
+        return default_db_name
+
+    # otherwise if it is an single db source then use "databaseName"
+    # and if databaseName field is not available or is empty then use
+    # "default" as database name
+    return (
+        db_service_entity.connection.config.__dict__.get("databaseName")
+        or DEFAULT_DATABASE
+    )
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/importer.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/importer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/logger.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/logger.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/lru_cache.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/lru_cache.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/metadata_service_helper.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/metadata_service_helper.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/partition.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/partition.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/profiler_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/profiler/metrics/system/queries/snowflake.py`

 * *Files 26% similar despite different names*

```diff
@@ -5,112 +5,95 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-"""Profiler utils class and functions"""
+"""
+Snowflake System Metric Queries and query operations
+"""
 
 import re
-from collections import namedtuple
 from typing import Optional
 
-import sqlparse
 from sqlalchemy.engine.row import Row
-from sqlparse.sql import Identifier
 
-from metadata.utils.sqa_utils import is_array
+from metadata.utils.logger import profiler_logger
+from metadata.utils.profiler_utils import QueryResult, get_identifiers_from_string
 
+logger = profiler_logger()
 
-class ColumnLike:
-    """We don't have column information at this stage (only metric entities)
-    we'll create a column like onject with the attributes needed in handle_array()
+INFORMATION_SCHEMA_QUERY = """
+    SELECT * FROM "SNOWFLAKE"."ACCOUNT_USAGE"."QUERY_HISTORY"
+    WHERE
+    start_time>= DATEADD('DAY', -1, CURRENT_TIMESTAMP)
+    AND QUERY_TEXT ILIKE '%{tablename}%'
+    AND QUERY_TYPE IN (
+        '{insert}',
+        '{update}',
+        '{delete}',
+        '{merge}'
+    )
+    AND EXECUTION_STATUS = 'SUCCESS';
+"""
 
-    Attrs:
-        is_array (bool): is array or not
-        array_col (Optional[str]): column name for the array column
+RESULT_SCAN = """
+    SELECT *
+    FROM TABLE(RESULT_SCAN('{query_id}'));
     """
 
-    def __init__(self, _is_array: bool, _array_col: Optional[str]) -> None:
-        self._is_array = _is_array
-        self._array_col = _array_col
-
-    @classmethod
-    def create(cls, kwargs: dict) -> "ColumnLike":
-        """instantiate the class with the required logic
-
-        Args:
-            is_array (bool): is array or not
-            array_col (Optional[str]): column name for the array column
-
-        Returns:
-            ColumnLike: ColumnLike isntante
-        """
-        try:
-            return cls(is_array(kwargs), kwargs.pop("array_col"))
-        except KeyError:
-            return cls(False, None)
-
-
-def clean_up_query(query: str) -> str:
-    """remove comments and newlines from query"""
-    return sqlparse.format(query, strip_comments=True).replace("\\n", "")
-
 
 def get_snowflake_system_queries(
     row: Row, database: str, schema: str
-) -> Optional["QueryResult"]:
+) -> Optional[QueryResult]:
     """get snowflake system queries for a specific database and schema. Parsing the query
-    is the only reliable way to get the DDL operation as fields in the table are not.
+    is the only reliable way to get the DDL operation as fields in the table are not. If parsing
+    fails we'll fall back to regex lookup
+
+    1. Parse the query and check if we have an Identifier
+    2.
 
     Args:
         row (dict): row from the snowflake system queries table
         database (str): database name
         schema (str): schema name
     Returns:
         QueryResult: namedtuple with the query result
     """
 
-    QueryResult = namedtuple(
-        "QueryResult",
-        "query_id,database_name,schema_name,table_name,query_text,query_type,timestamp",
-    )
-
     try:
-        parsed_query = sqlparse.parse(clean_up_query(row.query_text))[0]
-        identifier = next(
-            (
-                query_el
-                for query_el in parsed_query.tokens
-                if isinstance(query_el, Identifier)
-            ),
-            None,
-        )
-        if not identifier:
+        logger.debug(f"Trying to parse query:\n{row.QUERY_TEXT}\n")
+
+        pattern = r"(?:(INSERT\s*INTO\s*|INSERT\s*OVERWRITE\s*INTO\s*|UPDATE\s*|MERGE\s*INTO\s*|DELETE\s*FROM\s*))([\w._\"]+)(?=[\s*\n])"  # pylint: disable=line-too-long
+        match = re.match(pattern, row.QUERY_TEXT, re.IGNORECASE)
+        try:
+            identifier = match.group(2)
+        except (IndexError, AttributeError):
+            logger.debug("Could not find identifier in query. Skipping row.")
             return None
-        values = identifier.value.split(".")
-        database_name, schema_name, table_name = ([None] * (3 - len(values))) + values
+
+        database_name, schema_name, table_name = get_identifiers_from_string(identifier)
 
         if not all([database_name, schema_name, table_name]):
+            logger.debug(
+                "Missing database, schema, or table. Can't link operation to table entity in OpenMetadata."
+            )
             return None
 
-        # clean up table name
-        table_name = re.sub(r"\s.*", "", table_name).strip()
-
         if (
             database.lower() == database_name.lower()
             and schema.lower() == schema_name.lower()
         ):
             return QueryResult(
-                row.query_id,
-                database_name.lower(),
-                schema_name.lower(),
-                table_name.lower(),
-                parsed_query,
-                row.query_type,
-                row.start_time,
+                query_id=row.QUERY_ID,
+                database_name=database_name.lower(),
+                schema_name=schema_name.lower(),
+                table_name=table_name.lower(),
+                query_text=row.QUERY_TEXT,
+                query_type=row.QUERY_TYPE,
+                timestamp=row.START_TIME,
             )
     except Exception:
         return None
 
     return None
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/aws_based_secrets_manager.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/aws_based_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/aws_secrets_manager.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/aws_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/external_secrets_manager.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/external_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/noop_secrets_manager.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/noop_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/secrets/secrets_manager.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/secrets/secrets_manager.py`

 * *Files 13% similar despite different names*

```diff
@@ -15,16 +15,14 @@
 from abc import abstractmethod
 
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.singleton import Singleton
 
 logger = ingestion_logger()
 
-SECRET_MANAGER_AIRFLOW_CONF = "openmetadata_secrets_manager"
-
 
 class SecretsManager(metaclass=Singleton):
     """
     Abstract class implemented by different secrets' manager providers.
 
     It contains a set of auxiliary methods for adding missing fields which have been encrypted in the secrets' manager
     providers.
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/singleton.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/singleton.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/sqa_like_column.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/sqa_like_column.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/sqa_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/sqa_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/sqlalchemy_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/sqlalchemy_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/ssl_registry.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/ssl_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/test_suite.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/test_suite.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/time_utils.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/time_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/timeout.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/timeout.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/uuid_encoder.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/links.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,26 +4,27 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-UUID Encoder Module
+LookML Link handler
 """
-
-import json
-from uuid import UUID
+from urllib.parse import unquote, urlparse
 
 
-class UUIDEncoder(json.JSONEncoder):
-    """
-    UUID Encoder class
+def get_path_from_link(link: str) -> str:
     """
+    Given the `lookml_link` property from an explore,
+    get the source file path to fetch the file from Git.
 
-    def default(self, o):
-        if isinstance(o, UUID):
-            # if the obj is uuid, we simply return the value of uuid
-            return str(o)
-        return json.JSONEncoder.default(self, o)
+    Note that we cannot directly use the `source_file`
+    property since it does not give us the actual path,
+    only the file name.
+
+    The usual shape will be:
+    /projects/<projectId>/files/<path>?params
+    """
+    parsed = urlparse(unquote(link))
+    return parsed.path.split("/files/")[-1]
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/utils/workflow_output_handler.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/utils/workflow_output_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/metadata/workflow/workflow_status_mixin.py` & `openmetadata-ingestion-1.1.0.0.dev0/src/metadata/workflow/workflow_status_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/PKG-INFO` & `openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: openmetadata-ingestion
-Version: 1.0.5.0
+Version: 1.1.0.0.dev0
 Summary: Ingestion Framework for OpenMetadata
 Home-page: https://open-metadata.org/
 Author: OpenMetadata Committers
 License: Apache License 2.0
 Project-URL: Documentation, https://docs.open-metadata.org/
 Project-URL: Source, https://github.com/open-metadata/OpenMetadata
 Requires-Python: >=3.7
@@ -40,14 +40,15 @@
 Provides-Extra: hive
 Provides-Extra: impala
 Provides-Extra: kafka
 Provides-Extra: kinesis
 Provides-Extra: ldap-users
 Provides-Extra: looker
 Provides-Extra: mlflow
+Provides-Extra: mongo
 Provides-Extra: mssql
 Provides-Extra: mssql-odbc
 Provides-Extra: mysql
 Provides-Extra: nifi
 Provides-Extra: okta
 Provides-Extra: oracle
 Provides-Extra: pinotdb
@@ -57,14 +58,15 @@
 Provides-Extra: pymssql
 Provides-Extra: quicksight
 Provides-Extra: redash
 Provides-Extra: redpanda
 Provides-Extra: redshift
 Provides-Extra: sagemaker
 Provides-Extra: salesforce
+Provides-Extra: sap-hana
 Provides-Extra: singlestore
 Provides-Extra: sklearn
 Provides-Extra: snowflake
 Provides-Extra: superset
 Provides-Extra: tableau
 Provides-Extra: trino
 Provides-Extra: vertica
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/SOURCES.txt` & `openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -53,24 +53,27 @@
 src/metadata/data_insight/sink/__init__.py
 src/metadata/data_insight/sink/metadata_rest.py
 src/metadata/data_quality/__init__.py
 src/metadata/data_quality/api/__init__.py
 src/metadata/data_quality/api/models.py
 src/metadata/data_quality/api/workflow.py
 src/metadata/data_quality/interface/__init__.py
-src/metadata/data_quality/interface/test_suite_protocol.py
+src/metadata/data_quality/interface/test_suite_interface.py
+src/metadata/data_quality/interface/test_suite_interface_factory.py
 src/metadata/data_quality/interface/pandas/__init__.py
 src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
 src/metadata/data_quality/interface/sqlalchemy/__init__.py
 src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
 src/metadata/data_quality/runner/__init__.py
 src/metadata/data_quality/runner/core.py
 src/metadata/data_quality/runner/models.py
 src/metadata/data_quality/sink/__init__.py
 src/metadata/data_quality/sink/metadata_rest.py
+src/metadata/data_quality/source/base_test_suite_source.py
+src/metadata/data_quality/source/test_suite_source_factory.py
 src/metadata/data_quality/validations/__init__.py
 src/metadata/data_quality/validations/base_test_handler.py
 src/metadata/data_quality/validations/validator.py
 src/metadata/data_quality/validations/column/__init__.py
 src/metadata/data_quality/validations/column/base/__init__.py
 src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
 src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
@@ -148,14 +151,16 @@
 src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
 src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
 src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
 src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
 src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
 src/metadata/examples/workflows/airbyte.yaml
 src/metadata/examples/workflows/airflow.yaml
+src/metadata/examples/workflows/airflow_backend.yaml
+src/metadata/examples/workflows/airflow_postgres.yaml
 src/metadata/examples/workflows/amundsen.yaml
 src/metadata/examples/workflows/athena.yaml
 src/metadata/examples/workflows/athena_lineage.yaml
 src/metadata/examples/workflows/athena_usage.yaml
 src/metadata/examples/workflows/atlas.yaml
 src/metadata/examples/workflows/azuresql.yaml
 src/metadata/examples/workflows/bigquery.yaml
@@ -167,43 +172,46 @@
 src/metadata/examples/workflows/clickhouse_usage.yaml
 src/metadata/examples/workflows/dagster.yaml
 src/metadata/examples/workflows/data_insight.yaml
 src/metadata/examples/workflows/databricks.yaml
 src/metadata/examples/workflows/databricks_lineage.yaml
 src/metadata/examples/workflows/databricks_pipeline.yaml
 src/metadata/examples/workflows/databricks_usage.yaml
-src/metadata/examples/workflows/datalake.yaml
+src/metadata/examples/workflows/datalake_azure.yaml
+src/metadata/examples/workflows/datalake_gcs.yaml
 src/metadata/examples/workflows/datalake_profiler.yaml
+src/metadata/examples/workflows/datalake_s3.yaml
 src/metadata/examples/workflows/db2.yaml
 src/metadata/examples/workflows/db2_profiler.yaml
 src/metadata/examples/workflows/dbt.yaml
 src/metadata/examples/workflows/deltalake.yaml
 src/metadata/examples/workflows/domodashboard.yaml
 src/metadata/examples/workflows/dynamodb.yaml
 src/metadata/examples/workflows/fivetran.yaml
 src/metadata/examples/workflows/glue.yaml
 src/metadata/examples/workflows/gluepipeline.yaml
 src/metadata/examples/workflows/hive.yaml
 src/metadata/examples/workflows/impala.yaml
 src/metadata/examples/workflows/kafka.yaml
 src/metadata/examples/workflows/kinesis.yaml
-src/metadata/examples/workflows/ldap_user_to_catalog.yaml
 src/metadata/examples/workflows/looker.yaml
 src/metadata/examples/workflows/mariadb.yaml
 src/metadata/examples/workflows/metabase.yaml
-src/metadata/examples/workflows/migrate_source.yaml
 src/metadata/examples/workflows/mlflow.yaml
 src/metadata/examples/workflows/mode.yaml
+src/metadata/examples/workflows/mongodb.yaml
 src/metadata/examples/workflows/mssql.yaml
 src/metadata/examples/workflows/mssql_lineage.yaml
 src/metadata/examples/workflows/mssql_usage.yaml
 src/metadata/examples/workflows/mysql.yaml
 src/metadata/examples/workflows/mysql_profiler.yaml
 src/metadata/examples/workflows/openmetadata.yaml
 src/metadata/examples/workflows/oracle.yaml
+src/metadata/examples/workflows/oracle_lineage.yaml
+src/metadata/examples/workflows/oracle_usage.yaml
 src/metadata/examples/workflows/pinotdb.yaml
 src/metadata/examples/workflows/postgres.yaml
 src/metadata/examples/workflows/postgres_lineage.yaml
 src/metadata/examples/workflows/postgres_usage.yaml
 src/metadata/examples/workflows/powerbi.yaml
 src/metadata/examples/workflows/presto.yaml
 src/metadata/examples/workflows/query_log_usage.yaml
@@ -216,26 +224,30 @@
 src/metadata/examples/workflows/redshift_usage.yaml
 src/metadata/examples/workflows/sagemaker.yaml
 src/metadata/examples/workflows/salesforce.yaml
 src/metadata/examples/workflows/singlestore.yaml
 src/metadata/examples/workflows/snowflake.yaml
 src/metadata/examples/workflows/snowflake_lineage.yaml
 src/metadata/examples/workflows/snowflake_usage.yaml
+src/metadata/examples/workflows/spline.yaml
 src/metadata/examples/workflows/sqlite.yaml
 src/metadata/examples/workflows/superset.yaml
 src/metadata/examples/workflows/tableau.yaml
 src/metadata/examples/workflows/test_suite.yaml
 src/metadata/examples/workflows/trino.yaml
 src/metadata/examples/workflows/vertica.yaml
 src/metadata/generated/antlr/EntityLinkLexer.py
 src/metadata/generated/antlr/EntityLinkListener.py
 src/metadata/generated/antlr/EntityLinkParser.py
 src/metadata/generated/antlr/FqnLexer.py
 src/metadata/generated/antlr/FqnListener.py
 src/metadata/generated/antlr/FqnParser.py
+src/metadata/generated/antlr/JdbcUriLexer.py
+src/metadata/generated/antlr/JdbcUriListener.py
+src/metadata/generated/antlr/JdbcUriParser.py
 src/metadata/generated/schema/analytics/__init__.py
 src/metadata/generated/schema/analytics/basic.py
 src/metadata/generated/schema/analytics/reportData.py
 src/metadata/generated/schema/analytics/webAnalyticEvent.py
 src/metadata/generated/schema/analytics/webAnalyticEventData.py
 src/metadata/generated/schema/analytics/reportDataType/__init__.py
 src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
@@ -301,14 +313,15 @@
 src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
 src/metadata/generated/schema/api/teams/__init__.py
 src/metadata/generated/schema/api/teams/createRole.py
 src/metadata/generated/schema/api/teams/createTeam.py
 src/metadata/generated/schema/api/teams/createUser.py
 src/metadata/generated/schema/api/tests/__init__.py
 src/metadata/generated/schema/api/tests/createCustomMetric.py
+src/metadata/generated/schema/api/tests/createLogicalTestCases.py
 src/metadata/generated/schema/api/tests/createTestCase.py
 src/metadata/generated/schema/api/tests/createTestDefinition.py
 src/metadata/generated/schema/api/tests/createTestSuite.py
 src/metadata/generated/schema/auth/__init__.py
 src/metadata/generated/schema/auth/basicAuth.py
 src/metadata/generated/schema/auth/basicLoginRequest.py
 src/metadata/generated/schema/auth/changePasswordRequest.py
@@ -333,19 +346,21 @@
 src/metadata/generated/schema/configuration/applicationConfiguration.py
 src/metadata/generated/schema/configuration/authConfig.py
 src/metadata/generated/schema/configuration/authenticationConfiguration.py
 src/metadata/generated/schema/configuration/authorizerConfiguration.py
 src/metadata/generated/schema/configuration/changeEventConfiguration.py
 src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
 src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
+src/metadata/generated/schema/configuration/extensionConfiguration.py
 src/metadata/generated/schema/configuration/fernetConfiguration.py
 src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
 src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
 src/metadata/generated/schema/configuration/ldapConfiguration.py
 src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
+src/metadata/generated/schema/configuration/slackAppConfiguration.py
 src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
 src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
 src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
 src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
 src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
 src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
 src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
@@ -442,31 +457,38 @@
 src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
 src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
 src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
 src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
 src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
 src/metadata/generated/schema/entity/services/connections/database/impalaConnection.py
 src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
+src/metadata/generated/schema/entity/services/connections/database/mongoDBConnection.py
 src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
 src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
 src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
 src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
 src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
 src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
 src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
 src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
+src/metadata/generated/schema/entity/services/connections/database/sapHanaConnection.py
 src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
 src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
 src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
 src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
 src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
+src/metadata/generated/schema/entity/services/connections/database/common/__init__.py
+src/metadata/generated/schema/entity/services/connections/database/common/basicAuth.py
+src/metadata/generated/schema/entity/services/connections/database/common/iamAuthConfig.py
 src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
 src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
 src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
 src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
+src/metadata/generated/schema/entity/services/connections/database/mongoDB/__init__.py
+src/metadata/generated/schema/entity/services/connections/database/mongoDB/mongoDBValues.py
 src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
 src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
 src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
 src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
 src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
 src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
 src/metadata/generated/schema/entity/services/connections/messaging/saslMechanismType.py
@@ -487,14 +509,15 @@
 src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
+src/metadata/generated/schema/entity/services/connections/pipeline/splineConnection.py
 src/metadata/generated/schema/entity/services/connections/storage/__init__.py
 src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py
 src/metadata/generated/schema/entity/services/connections/storage/customStorageConnection.py
 src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
 src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
 src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
 src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
@@ -528,14 +551,15 @@
 src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
 src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
 src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
 src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
 src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
 src/metadata/generated/schema/metadataIngestion/workflow.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
+src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtAzureConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
 src/metadata/generated/schema/metadataIngestion/storage/__init__.py
@@ -553,18 +577,21 @@
 src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
 src/metadata/generated/schema/security/client/samlSSOClientConfig.py
 src/metadata/generated/schema/security/credentials/__init__.py
 src/metadata/generated/schema/security/credentials/accessTokenAuth.py
 src/metadata/generated/schema/security/credentials/awsCredentials.py
 src/metadata/generated/schema/security/credentials/azureCredentials.py
 src/metadata/generated/schema/security/credentials/basicAuth.py
-src/metadata/generated/schema/security/credentials/gcsCredentials.py
-src/metadata/generated/schema/security/credentials/gcsValues.py
+src/metadata/generated/schema/security/credentials/bitbucketCredentials.py
+src/metadata/generated/schema/security/credentials/gcpCredentials.py
+src/metadata/generated/schema/security/credentials/gcpValues.py
+src/metadata/generated/schema/security/credentials/gitCredentials.py
 src/metadata/generated/schema/security/credentials/githubCredentials.py
 src/metadata/generated/schema/security/secrets/__init__.py
+src/metadata/generated/schema/security/secrets/secretsManagerClientLoader.py
 src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
 src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
 src/metadata/generated/schema/security/ssl/__init__.py
 src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
 src/metadata/generated/schema/security/ssl/verifySSLConfig.py
 src/metadata/generated/schema/settings/__init__.py
 src/metadata/generated/schema/settings/settings.py
@@ -586,14 +613,15 @@
 src/metadata/generated/schema/type/csvFile.py
 src/metadata/generated/schema/type/csvImportResult.py
 src/metadata/generated/schema/type/dailyCount.py
 src/metadata/generated/schema/type/databaseConnectionConfig.py
 src/metadata/generated/schema/type/entityHistory.py
 src/metadata/generated/schema/type/entityLineage.py
 src/metadata/generated/schema/type/entityReference.py
+src/metadata/generated/schema/type/entityReferenceList.py
 src/metadata/generated/schema/type/entityRelationship.py
 src/metadata/generated/schema/type/entityUsage.py
 src/metadata/generated/schema/type/filterPattern.py
 src/metadata/generated/schema/type/function.py
 src/metadata/generated/schema/type/include.py
 src/metadata/generated/schema/type/jdbcConnection.py
 src/metadata/generated/schema/type/paging.py
@@ -710,14 +738,15 @@
 src/metadata/ingestion/source/dashboard/dashboard_service.py
 src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
 src/metadata/ingestion/source/dashboard/domodashboard/connection.py
 src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
 src/metadata/ingestion/source/dashboard/looker/__init__.py
 src/metadata/ingestion/source/dashboard/looker/columns.py
 src/metadata/ingestion/source/dashboard/looker/connection.py
+src/metadata/ingestion/source/dashboard/looker/links.py
 src/metadata/ingestion/source/dashboard/looker/metadata.py
 src/metadata/ingestion/source/dashboard/looker/models.py
 src/metadata/ingestion/source/dashboard/looker/parser.py
 src/metadata/ingestion/source/dashboard/metabase/__init__.py
 src/metadata/ingestion/source/dashboard/metabase/client.py
 src/metadata/ingestion/source/dashboard/metabase/connection.py
 src/metadata/ingestion/source/dashboard/metabase/metadata.py
@@ -753,14 +782,15 @@
 src/metadata/ingestion/source/dashboard/tableau/metadata.py
 src/metadata/ingestion/source/dashboard/tableau/models.py
 src/metadata/ingestion/source/dashboard/tableau/queries.py
 src/metadata/ingestion/source/database/__init__.py
 src/metadata/ingestion/source/database/column_helpers.py
 src/metadata/ingestion/source/database/column_type_parser.py
 src/metadata/ingestion/source/database/common_db_source.py
+src/metadata/ingestion/source/database/common_nosql_source.py
 src/metadata/ingestion/source/database/database_service.py
 src/metadata/ingestion/source/database/lineage_source.py
 src/metadata/ingestion/source/database/query_parser_source.py
 src/metadata/ingestion/source/database/sample_data.py
 src/metadata/ingestion/source/database/sample_usage.py
 src/metadata/ingestion/source/database/sql_column_handler.py
 src/metadata/ingestion/source/database/sqlalchemy_source.py
@@ -790,28 +820,38 @@
 src/metadata/ingestion/source/database/clickhouse/query_parser.py
 src/metadata/ingestion/source/database/clickhouse/usage.py
 src/metadata/ingestion/source/database/databricks/__init__.py
 src/metadata/ingestion/source/database/databricks/client.py
 src/metadata/ingestion/source/database/databricks/connection.py
 src/metadata/ingestion/source/database/databricks/lineage.py
 src/metadata/ingestion/source/database/databricks/metadata.py
+src/metadata/ingestion/source/database/databricks/models.py
 src/metadata/ingestion/source/database/databricks/queries.py
 src/metadata/ingestion/source/database/databricks/query_parser.py
 src/metadata/ingestion/source/database/databricks/usage.py
+src/metadata/ingestion/source/database/databricks/legacy/__init__.py
+src/metadata/ingestion/source/database/databricks/legacy/lineage.py
+src/metadata/ingestion/source/database/databricks/legacy/metadata.py
+src/metadata/ingestion/source/database/databricks/unity_catalog/__init__.py
+src/metadata/ingestion/source/database/databricks/unity_catalog/lineage.py
+src/metadata/ingestion/source/database/databricks/unity_catalog/metadata.py
 src/metadata/ingestion/source/database/datalake/__init__.py
 src/metadata/ingestion/source/database/datalake/connection.py
 src/metadata/ingestion/source/database/datalake/metadata.py
 src/metadata/ingestion/source/database/datalake/models.py
-src/metadata/ingestion/source/database/datalake/utils.py
 src/metadata/ingestion/source/database/db2/__init__.py
 src/metadata/ingestion/source/database/db2/connection.py
 src/metadata/ingestion/source/database/db2/metadata.py
 src/metadata/ingestion/source/database/dbt/__init__.py
+src/metadata/ingestion/source/database/dbt/constants.py
+src/metadata/ingestion/source/database/dbt/dbt_config.py
 src/metadata/ingestion/source/database/dbt/dbt_service.py
+src/metadata/ingestion/source/database/dbt/dbt_utils.py
 src/metadata/ingestion/source/database/dbt/metadata.py
+src/metadata/ingestion/source/database/dbt/models.py
 src/metadata/ingestion/source/database/deltalake/__init__.py
 src/metadata/ingestion/source/database/deltalake/connection.py
 src/metadata/ingestion/source/database/deltalake/metadata.py
 src/metadata/ingestion/source/database/domodatabase/__init__.py
 src/metadata/ingestion/source/database/domodatabase/connection.py
 src/metadata/ingestion/source/database/domodatabase/metadata.py
 src/metadata/ingestion/source/database/domodatabase/models.py
@@ -820,41 +860,48 @@
 src/metadata/ingestion/source/database/druid/metadata.py
 src/metadata/ingestion/source/database/dynamodb/__init__.py
 src/metadata/ingestion/source/database/dynamodb/connection.py
 src/metadata/ingestion/source/database/dynamodb/metadata.py
 src/metadata/ingestion/source/database/glue/__init__.py
 src/metadata/ingestion/source/database/glue/connection.py
 src/metadata/ingestion/source/database/glue/metadata.py
+src/metadata/ingestion/source/database/glue/models.py
 src/metadata/ingestion/source/database/hive/__init__.py
 src/metadata/ingestion/source/database/hive/connection.py
 src/metadata/ingestion/source/database/hive/metadata.py
 src/metadata/ingestion/source/database/hive/queries.py
 src/metadata/ingestion/source/database/impala/__init__.py
 src/metadata/ingestion/source/database/impala/connection.py
 src/metadata/ingestion/source/database/impala/metadata.py
 src/metadata/ingestion/source/database/impala/queries.py
 src/metadata/ingestion/source/database/mariadb/__init__.py
 src/metadata/ingestion/source/database/mariadb/connection.py
 src/metadata/ingestion/source/database/mariadb/metadata.py
+src/metadata/ingestion/source/database/mongodb/__init__.py
+src/metadata/ingestion/source/database/mongodb/connection.py
+src/metadata/ingestion/source/database/mongodb/metadata.py
 src/metadata/ingestion/source/database/mssql/__init__.py
 src/metadata/ingestion/source/database/mssql/connection.py
 src/metadata/ingestion/source/database/mssql/lineage.py
 src/metadata/ingestion/source/database/mssql/metadata.py
 src/metadata/ingestion/source/database/mssql/queries.py
 src/metadata/ingestion/source/database/mssql/query_parser.py
 src/metadata/ingestion/source/database/mssql/usage.py
 src/metadata/ingestion/source/database/mssql/utils.py
 src/metadata/ingestion/source/database/mysql/__init__.py
 src/metadata/ingestion/source/database/mysql/connection.py
 src/metadata/ingestion/source/database/mysql/metadata.py
 src/metadata/ingestion/source/database/mysql/utils.py
 src/metadata/ingestion/source/database/oracle/__init__.py
 src/metadata/ingestion/source/database/oracle/connection.py
+src/metadata/ingestion/source/database/oracle/lineage.py
 src/metadata/ingestion/source/database/oracle/metadata.py
 src/metadata/ingestion/source/database/oracle/queries.py
+src/metadata/ingestion/source/database/oracle/query_parser.py
+src/metadata/ingestion/source/database/oracle/usage.py
 src/metadata/ingestion/source/database/oracle/utils.py
 src/metadata/ingestion/source/database/pinotdb/__init__.py
 src/metadata/ingestion/source/database/pinotdb/connection.py
 src/metadata/ingestion/source/database/pinotdb/metadata.py
 src/metadata/ingestion/source/database/postgres/__init__.py
 src/metadata/ingestion/source/database/postgres/connection.py
 src/metadata/ingestion/source/database/postgres/lineage.py
@@ -873,22 +920,27 @@
 src/metadata/ingestion/source/database/redshift/__init__.py
 src/metadata/ingestion/source/database/redshift/connection.py
 src/metadata/ingestion/source/database/redshift/lineage.py
 src/metadata/ingestion/source/database/redshift/metadata.py
 src/metadata/ingestion/source/database/redshift/queries.py
 src/metadata/ingestion/source/database/redshift/query_parser.py
 src/metadata/ingestion/source/database/redshift/usage.py
+src/metadata/ingestion/source/database/redshift/utils.py
 src/metadata/ingestion/source/database/salesforce/__init__.py
 src/metadata/ingestion/source/database/salesforce/connection.py
 src/metadata/ingestion/source/database/salesforce/metadata.py
+src/metadata/ingestion/source/database/saphana/__init__.py
+src/metadata/ingestion/source/database/saphana/connection.py
+src/metadata/ingestion/source/database/saphana/metadata.py
 src/metadata/ingestion/source/database/singlestore/__init__.py
 src/metadata/ingestion/source/database/singlestore/connection.py
 src/metadata/ingestion/source/database/singlestore/metadata.py
 src/metadata/ingestion/source/database/snowflake/__init__.py
 src/metadata/ingestion/source/database/snowflake/connection.py
+src/metadata/ingestion/source/database/snowflake/constants.py
 src/metadata/ingestion/source/database/snowflake/lineage.py
 src/metadata/ingestion/source/database/snowflake/metadata.py
 src/metadata/ingestion/source/database/snowflake/queries.py
 src/metadata/ingestion/source/database/snowflake/query_parser.py
 src/metadata/ingestion/source/database/snowflake/usage.py
 src/metadata/ingestion/source/database/snowflake/utils.py
 src/metadata/ingestion/source/database/sqlite/__init__.py
@@ -915,28 +967,25 @@
 src/metadata/ingestion/source/messaging/kinesis/connection.py
 src/metadata/ingestion/source/messaging/kinesis/metadata.py
 src/metadata/ingestion/source/messaging/kinesis/models.py
 src/metadata/ingestion/source/messaging/redpanda/__init__.py
 src/metadata/ingestion/source/messaging/redpanda/connection.py
 src/metadata/ingestion/source/messaging/redpanda/metadata.py
 src/metadata/ingestion/source/metadata/__init__.py
-src/metadata/ingestion/source/metadata/metadata.py
 src/metadata/ingestion/source/metadata/amundsen/__init__.py
 src/metadata/ingestion/source/metadata/amundsen/client.py
 src/metadata/ingestion/source/metadata/amundsen/connection.py
 src/metadata/ingestion/source/metadata/amundsen/metadata.py
 src/metadata/ingestion/source/metadata/amundsen/queries.py
 src/metadata/ingestion/source/metadata/atlas/__init__.py
 src/metadata/ingestion/source/metadata/atlas/client.py
 src/metadata/ingestion/source/metadata/atlas/connection.py
 src/metadata/ingestion/source/metadata/atlas/metadata.py
 src/metadata/ingestion/source/metadata/metadata_elasticsearch/__init__.py
 src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py
-src/metadata/ingestion/source/metadata/openmetadata/__init__.py
-src/metadata/ingestion/source/metadata/openmetadata/metadata.py
 src/metadata/ingestion/source/mlmodel/__init__.py
 src/metadata/ingestion/source/mlmodel/mlmodel_service.py
 src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
 src/metadata/ingestion/source/mlmodel/mlflow/connection.py
 src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
 src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
 src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
@@ -949,16 +998,18 @@
 src/metadata/ingestion/source/pipeline/airbyte/metadata.py
 src/metadata/ingestion/source/pipeline/airflow/__init__.py
 src/metadata/ingestion/source/pipeline/airflow/connection.py
 src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
 src/metadata/ingestion/source/pipeline/airflow/metadata.py
 src/metadata/ingestion/source/pipeline/airflow/models.py
 src/metadata/ingestion/source/pipeline/dagster/__init__.py
+src/metadata/ingestion/source/pipeline/dagster/client.py
 src/metadata/ingestion/source/pipeline/dagster/connection.py
 src/metadata/ingestion/source/pipeline/dagster/metadata.py
+src/metadata/ingestion/source/pipeline/dagster/models.py
 src/metadata/ingestion/source/pipeline/dagster/queries.py
 src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
 src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
 src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
 src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
 src/metadata/ingestion/source/pipeline/domopipeline/connection.py
 src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
@@ -969,14 +1020,20 @@
 src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
 src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
 src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
 src/metadata/ingestion/source/pipeline/nifi/__init__.py
 src/metadata/ingestion/source/pipeline/nifi/client.py
 src/metadata/ingestion/source/pipeline/nifi/connection.py
 src/metadata/ingestion/source/pipeline/nifi/metadata.py
+src/metadata/ingestion/source/pipeline/spline/__init__.py
+src/metadata/ingestion/source/pipeline/spline/client.py
+src/metadata/ingestion/source/pipeline/spline/connection.py
+src/metadata/ingestion/source/pipeline/spline/metadata.py
+src/metadata/ingestion/source/pipeline/spline/models.py
+src/metadata/ingestion/source/pipeline/spline/utils.py
 src/metadata/ingestion/source/storage/__init__.py
 src/metadata/ingestion/source/storage/storage_service.py
 src/metadata/ingestion/source/storage/s3/__init__.py
 src/metadata/ingestion/source/storage/s3/connection.py
 src/metadata/ingestion/source/storage/s3/metadata.py
 src/metadata/ingestion/source/storage/s3/models.py
 src/metadata/ingestion/stage/__init__.py
@@ -997,17 +1054,20 @@
 src/metadata/pii/ner_scanner.py
 src/metadata/pii/processor.py
 src/metadata/profiler/__init__.py
 src/metadata/profiler/registry.py
 src/metadata/profiler/api/__init__.py
 src/metadata/profiler/api/models.py
 src/metadata/profiler/api/workflow.py
+src/metadata/profiler/interface/__init__.py
 src/metadata/profiler/interface/profiler_protocol.py
-src/metadata/profiler/interface/pandas/pandas_profiler_interface.py
-src/metadata/profiler/interface/sqlalchemy/sqa_profiler_interface.py
+src/metadata/profiler/interface/pandas/__init__.py
+src/metadata/profiler/interface/pandas/profiler_interface.py
+src/metadata/profiler/interface/sqlalchemy/__init__.py
+src/metadata/profiler/interface/sqlalchemy/profiler_interface.py
 src/metadata/profiler/metrics/__init__.py
 src/metadata/profiler/metrics/core.py
 src/metadata/profiler/metrics/registry.py
 src/metadata/profiler/metrics/composed/__init__.py
 src/metadata/profiler/metrics/composed/distinct_ratio.py
 src/metadata/profiler/metrics/composed/duplicate_count.py
 src/metadata/profiler/metrics/composed/ilike_ratio.py
@@ -1034,15 +1094,21 @@
 src/metadata/profiler/metrics/static/not_regexp_match_count.py
 src/metadata/profiler/metrics/static/null_count.py
 src/metadata/profiler/metrics/static/regexp_match_count.py
 src/metadata/profiler/metrics/static/row_count.py
 src/metadata/profiler/metrics/static/stddev.py
 src/metadata/profiler/metrics/static/sum.py
 src/metadata/profiler/metrics/static/unique_count.py
+src/metadata/profiler/metrics/system/__init__.py
+src/metadata/profiler/metrics/system/dml_operation.py
 src/metadata/profiler/metrics/system/system.py
+src/metadata/profiler/metrics/system/queries/__init__.py
+src/metadata/profiler/metrics/system/queries/bigquery.py
+src/metadata/profiler/metrics/system/queries/redshift.py
+src/metadata/profiler/metrics/system/queries/snowflake.py
 src/metadata/profiler/metrics/window/__init__.py
 src/metadata/profiler/metrics/window/first_quartile.py
 src/metadata/profiler/metrics/window/median.py
 src/metadata/profiler/metrics/window/third_quartile.py
 src/metadata/profiler/orm/__init__.py
 src/metadata/profiler/orm/converter.py
 src/metadata/profiler/orm/registry.py
@@ -1051,79 +1117,96 @@
 src/metadata/profiler/orm/functions/conn_test.py
 src/metadata/profiler/orm/functions/datetime.py
 src/metadata/profiler/orm/functions/length.py
 src/metadata/profiler/orm/functions/median.py
 src/metadata/profiler/orm/functions/modulo.py
 src/metadata/profiler/orm/functions/random_num.py
 src/metadata/profiler/orm/functions/sum.py
+src/metadata/profiler/orm/functions/table_metric_construct.py
 src/metadata/profiler/orm/types/__init__.py
 src/metadata/profiler/orm/types/bytea_to_string.py
 src/metadata/profiler/orm/types/custom_array.py
 src/metadata/profiler/orm/types/custom_timestamp.py
 src/metadata/profiler/orm/types/hex_byte_string.py
 src/metadata/profiler/orm/types/uuid.py
 src/metadata/profiler/processor/__init__.py
 src/metadata/profiler/processor/core.py
-src/metadata/profiler/processor/datalake_sampler.py
 src/metadata/profiler/processor/default.py
 src/metadata/profiler/processor/handle_partition.py
 src/metadata/profiler/processor/models.py
 src/metadata/profiler/processor/runner.py
-src/metadata/profiler/processor/sampler.py
+src/metadata/profiler/processor/pandas/__init__.py
+src/metadata/profiler/processor/pandas/sampler.py
+src/metadata/profiler/processor/sqlalchemy/__init__.py
+src/metadata/profiler/processor/sqlalchemy/sampler.py
 src/metadata/profiler/sink/__init__.py
 src/metadata/profiler/sink/file.py
 src/metadata/profiler/sink/metadata_rest.py
+src/metadata/profiler/source/__init__.py
+src/metadata/profiler/source/base_profiler_source.py
+src/metadata/profiler/source/profiler_source_factory.py
+src/metadata/profiler/source/bigquery/__init__.py
+src/metadata/profiler/source/bigquery/profiler_source.py
 src/metadata/readers/__init__.py
+src/metadata/readers/api_reader.py
 src/metadata/readers/base.py
+src/metadata/readers/bitbucket.py
+src/metadata/readers/credentials.py
 src/metadata/readers/github.py
 src/metadata/readers/local.py
 src/metadata/timer/__init__.py
 src/metadata/timer/repeated_timer.py
 src/metadata/timer/workflow_reporter.py
 src/metadata/utils/__init__.py
-src/metadata/utils/azure_utils.py
 src/metadata/utils/class_helper.py
 src/metadata/utils/client_version.py
 src/metadata/utils/constants.py
 src/metadata/utils/credentials.py
 src/metadata/utils/custom_thread_pool.py
-src/metadata/utils/dbt_config.py
+src/metadata/utils/db_utils.py
 src/metadata/utils/dispatch.py
 src/metadata/utils/elasticsearch.py
 src/metadata/utils/entity_link.py
 src/metadata/utils/filters.py
 src/metadata/utils/fqn.py
-src/metadata/utils/gcs_utils.py
 src/metadata/utils/helpers.py
 src/metadata/utils/importer.py
 src/metadata/utils/logger.py
 src/metadata/utils/lru_cache.py
 src/metadata/utils/metadata_service_helper.py
 src/metadata/utils/partition.py
 src/metadata/utils/profiler_utils.py
-src/metadata/utils/s3_utils.py
 src/metadata/utils/singleton.py
 src/metadata/utils/sqa_like_column.py
 src/metadata/utils/sqa_utils.py
 src/metadata/utils/sqlalchemy_utils.py
 src/metadata/utils/ssl_registry.py
 src/metadata/utils/tag_utils.py
 src/metadata/utils/test_suite.py
 src/metadata/utils/time_utils.py
 src/metadata/utils/timeout.py
 src/metadata/utils/uuid_encoder.py
 src/metadata/utils/workflow_output_handler.py
+src/metadata/utils/datalake/__init__.py
+src/metadata/utils/datalake/avro_dispatch.py
+src/metadata/utils/datalake/common.py
+src/metadata/utils/datalake/csv_tsv_dispatch.py
+src/metadata/utils/datalake/datalake_utils.py
+src/metadata/utils/datalake/json_dispatch.py
+src/metadata/utils/datalake/parquet_dispatch.py
 src/metadata/utils/secrets/__init__.py
 src/metadata/utils/secrets/aws_based_secrets_manager.py
 src/metadata/utils/secrets/aws_secrets_manager.py
 src/metadata/utils/secrets/aws_ssm_secrets_manager.py
 src/metadata/utils/secrets/external_secrets_manager.py
 src/metadata/utils/secrets/noop_secrets_manager.py
 src/metadata/utils/secrets/secrets_manager.py
 src/metadata/utils/secrets/secrets_manager_factory.py
+src/metadata/utils/secrets/client/__init__.py
+src/metadata/utils/secrets/client/loader.py
 src/metadata/workflow/__init__.py
 src/metadata/workflow/workflow_status_mixin.py
 src/openmetadata_ingestion.egg-info/PKG-INFO
 src/openmetadata_ingestion.egg-info/SOURCES.txt
 src/openmetadata_ingestion.egg-info/dependency_links.txt
 src/openmetadata_ingestion.egg-info/entry_points.txt
 src/openmetadata_ingestion.egg-info/not-zip-safe
```

### Comparing `openmetadata-ingestion-1.0.5.0/src/openmetadata_ingestion.egg-info/requires.txt` & `openmetadata-ingestion-1.1.0.0.dev0/src/openmetadata_ingestion.egg-info/requires.txt`

 * *Files 6% similar despite different names*

```diff
@@ -1,143 +1,147 @@
+chardet==4.0.0
+importlib-metadata~=4.12.0
+cryptography
+pymysql>=1.0.2
+cached-property==1.5.2
 avro~=1.11
-python-jose~=3.3
-wheel~=0.38.4
+openmetadata-sqllineage>=1.0.4
 jsonpatch==1.32
-croniter~=1.3.0
-setuptools~=65.6.3
+tabulate==0.9.0
 typing-inspect
-requests>=2.23
-Jinja2>=2.11.3
+typing_extensions<=4.5.0
 sqlalchemy<2,>=1.4.0
-importlib-metadata~=4.12.0
-chardet==4.0.0
-grpcio-tools>=1.47.2
-commonregex
-mypy_extensions>=0.4.3
+wheel~=0.38.4
+python-dateutil>=2.8.1
+email-validator>=1.0.3
+PyYAML
+typing-compat~=0.1.0
 pydantic~=1.10
-requests-aws4auth~=1.1
-boto3<2.0,>=1.20
 antlr4-python3-runtime==4.9.2
-idna<3,>=2.5
-google-auth>=1.33.0
-jsonschema
-PyYAML
+python-jose~=3.3
+requests>=2.23
+requests-aws4auth~=1.1
+croniter~=1.3.0
+grpcio-tools>=1.47.2
 google>=3.0.0
-pymysql>=1.0.2
-openmetadata-sqllineage>=1.0.4
-email-validator>=1.0.3
-tabulate==0.9.0
-cryptography
-typing_extensions<=4.5.0
-typing-compat~=0.1.0
+mypy_extensions>=0.4.3
 memory-profiler
-python-dateutil>=2.8.1
-cached-property==1.5.2
+jsonschema
+commonregex
+google-auth>=1.33.0
+boto3<2.0,>=1.20
+Jinja2>=2.11.3
+setuptools~=65.6.3
+idna<3,>=2.5
 
 [airflow]
 apache-airflow==2.3.3
 
 [all]
-wheel~=0.38.4
-jsonpatch==1.32
-pinotdb~=0.3
-google-cloud
-alembic~=1.10.2
-croniter~=1.3.0
-requests>=2.23
-Jinja2>=2.11.3
-sqlalchemy<2,>=1.4.0
-importlib-metadata~=4.12.0
-psycopg2-binary
-grpcio-tools>=1.47.2
-commonregex
-mypy_extensions>=0.4.3
-adlfs>=2022.2.0
-impyla[kerberos]~=0.18.0
-cachetools
-requests-aws4auth~=1.1
-confluent_kafka==1.8.2
-boto3<2.0,>=1.20
-dagster_graphql~=1.1
-idna<3,>=2.5
-google-auth>=1.33.0
-PyYAML
-google-cloud-storage==1.43.0
-pydruid>=0.6.5
-simple_salesforce==1.11.4
-sasl~=0.3
-azure-storage-blob~=12.14
+clickhouse-driver~=0.2
+chardet==4.0.0
 scikit-learn~=1.0
-openmetadata-sqllineage>=1.0.4
 dbt-artifacts-parser
-azure-storage-blob
-sqlalchemy-databricks~=0.1
-tabulate==0.9.0
-pydomo~=0.3
-typing_extensions<=4.5.0
-looker-sdk>=22.20.0
-memory-profiler
-python-dateutil>=2.8.1
-mlflow-skinny~=1.30
-tableau-api-lib~=0.1
+impyla~=0.18.0
+msal~=1.2
+importlib-metadata~=4.12.0
+cryptography
+elasticsearch==7.13.1
+sqlalchemy-redshift==0.8.12
+pinotdb~=0.3
+sqlalchemy-vertica[vertica-python]>=0.0.5
 avro~=1.11
-PyAthena[sqlalchemy]
-pymssql==2.2.5
-delta-spark<=2.3.0
-thrift<1,>=0.13
 sqlalchemy-bigquery>=1.2.2
-python-jose~=3.3
-sqlalchemy-pytds~=0.3
-pyodbc<5,>=4.0.35
-neo4j~=5.3.0
-azure-identity
-oracledb~=1.2
-setuptools~=65.6.3
-s3fs==0.4.2
-pyhive~=0.6
 typing-inspect
-presidio-analyzer==2.2.32
-python_on_whales==0.55.0
-snowflake-sqlalchemy~=1.4
-chardet==4.0.0
+azure-identity
+dagster_graphql~=1.1
+sqlalchemy-pytds~=0.3
+thrift<1,>=0.13
+wheel~=0.38.4
+python-dateutil>=2.8.1
+azure-storage-blob
+google-cloud
+mlflow-skinny~=1.30
+psycopg2-binary
 gcsfs==2022.11.0
 pydantic~=1.10
-clickhouse-driver~=0.2
+delta-spark<=2.3.0
+packaging==21.3
 antlr4-python3-runtime==4.9.2
-elasticsearch==7.13.1
-google-cloud-logging
+python-jose~=3.3
+trino[sqlalchemy]
+looker-sdk>=22.20.0
+fastavro>=1.2.0
+mypy_extensions>=0.4.3
+pyodbc<5,>=4.0.35
+confluent_kafka==2.1.1
+memory-profiler
+hdbcli
 cx_Oracle<9,>=8.3.0
-jsonschema
+google-auth>=1.33.0
+sasl~=0.3
+protobuf
+google-cloud-datacatalog==3.6.2
+pydruid>=0.6.5
+simple_salesforce==1.11.4
+presidio-analyzer==2.2.32
+Jinja2>=2.11.3
+pyhive~=0.6
+databricks-sdk~=0.1
+adlfs>=2022.2.0
+okta~=2.3
+s3fs==0.4.2
+pymysql>=1.0.2
+cached-property==1.5.2
+python_on_whales==0.55.0
 thrift-sasl~=0.4
+openmetadata-sqllineage>=1.0.4
+jsonpatch==1.32
+snowflake-sqlalchemy~=1.4
+tabulate==0.9.0
+boto3<2.0,>=1.20
+typing_extensions<=4.5.0
+impyla[kerberos]~=0.18.0
 ldap3==2.9.1
-trino[sqlalchemy]
+alembic~=1.10.2
+sqlalchemy-hana
+pydomo~=0.3
+sqlalchemy<2,>=1.4.0
+neo4j~=5.3.0
 spacy==3.5.0
-google>=3.0.0
-clickhouse-sqlalchemy~=0.2
-packaging==21.3
-pymysql>=1.0.2
-sqlalchemy-redshift==0.8.12
-presto-types-parser>=0.0.2
-azure-identity~=1.12
-google-cloud-datacatalog==3.6.2
-sqlalchemy-vertica[vertica-python]>=0.0.5
-impyla~=0.18.0
-python-snappy~=0.6.1
 email-validator>=1.0.3
-pyarrow~=10.0
-protobuf
-cryptography
-okta~=2.3
 lkml~=1.3
-fastavro>=1.2.0
+pymssql==2.2.5
+PyYAML
+azure-identity~=1.12
+oracledb~=1.2
 typing-compat~=0.1.0
+pyarrow~=10.0
+requests>=2.23
+requests-aws4auth~=1.1
+sqlalchemy-databricks~=0.1
+croniter~=1.3.0
+grpcio-tools>=1.47.2
+tableau-api-lib~=0.1
+google>=3.0.0
 GeoAlchemy2~=0.12
-msal~=1.2
+google-cloud-logging
+azure-storage-blob~=12.14
+google-cloud-storage==1.43.0
+cachetools
 pandas==1.3.5
-cached-property==1.5.2
+presto-types-parser>=0.0.2
+jsonschema
+commonregex
+python-snappy~=0.6.1
+PyAthena[sqlalchemy]
+clickhouse-sqlalchemy~=0.2
+pymongo~=4.3
+setuptools~=65.6.3
+idna<3,>=2.5
 
 [amundsen]
 neo4j~=5.3.0
 
 [athena]
 PyAthena[sqlalchemy]
 
@@ -151,117 +155,120 @@
 
 [backup]
 boto3<2.0,>=1.20
 azure-storage-blob
 azure-identity
 
 [base]
+chardet==4.0.0
+importlib-metadata~=4.12.0
+cryptography
+pymysql>=1.0.2
+cached-property==1.5.2
 avro~=1.11
-python-jose~=3.3
-wheel~=0.38.4
+openmetadata-sqllineage>=1.0.4
 jsonpatch==1.32
-croniter~=1.3.0
-setuptools~=65.6.3
+tabulate==0.9.0
 typing-inspect
-requests>=2.23
-Jinja2>=2.11.3
+typing_extensions<=4.5.0
 sqlalchemy<2,>=1.4.0
-importlib-metadata~=4.12.0
-chardet==4.0.0
-grpcio-tools>=1.47.2
-commonregex
-mypy_extensions>=0.4.3
+wheel~=0.38.4
+python-dateutil>=2.8.1
+email-validator>=1.0.3
+PyYAML
+typing-compat~=0.1.0
 pydantic~=1.10
-requests-aws4auth~=1.1
-boto3<2.0,>=1.20
 antlr4-python3-runtime==4.9.2
-idna<3,>=2.5
-google-auth>=1.33.0
-jsonschema
-PyYAML
+python-jose~=3.3
+requests>=2.23
+requests-aws4auth~=1.1
+croniter~=1.3.0
+grpcio-tools>=1.47.2
 google>=3.0.0
-pymysql>=1.0.2
-openmetadata-sqllineage>=1.0.4
-email-validator>=1.0.3
-tabulate==0.9.0
-cryptography
-typing_extensions<=4.5.0
-typing-compat~=0.1.0
+mypy_extensions>=0.4.3
 memory-profiler
-python-dateutil>=2.8.1
-cached-property==1.5.2
+jsonschema
+commonregex
+google-auth>=1.33.0
+boto3<2.0,>=1.20
+Jinja2>=2.11.3
+setuptools~=65.6.3
+idna<3,>=2.5
 
 [bigquery]
-google-cloud-logging
 sqlalchemy-bigquery>=1.2.2
+google-cloud-logging
+cachetools
 pyarrow~=10.0
 google-cloud-datacatalog==3.6.2
-cachetools
 
 [clickhouse]
-clickhouse-sqlalchemy~=0.2
 clickhouse-driver~=0.2
+clickhouse-sqlalchemy~=0.2
 
 [dagster]
-psycopg2-binary
-GeoAlchemy2~=0.12
-pymysql>=1.0.2
 dagster_graphql~=1.1
+pymysql>=1.0.2
+GeoAlchemy2~=0.12
+psycopg2-binary
 
 [data-insight]
 elasticsearch==7.13.1
 
 [databricks]
 sqlalchemy-databricks~=0.1
+databricks-sdk~=0.1
 
 [datalake-azure]
 boto3<2.0,>=1.20
-python-snappy~=0.6.1
+pandas==1.3.5
 azure-storage-blob~=12.14
+pyarrow~=10.0
+python-snappy~=0.6.1
 adlfs>=2022.2.0
-pandas==1.3.5
 azure-identity~=1.12
-pyarrow~=10.0
 
 [datalake-gcs]
 boto3<2.0,>=1.20
-python-snappy~=0.6.1
 pandas==1.3.5
+python-snappy~=0.6.1
+pyarrow~=10.0
 gcsfs==2022.11.0
 google-cloud-storage==1.43.0
-pyarrow~=10.0
 
 [datalake-s3]
 s3fs==0.4.2
-boto3<2.0,>=1.20
-python-snappy~=0.6.1
 pandas==1.3.5
+python-snappy~=0.6.1
 pyarrow~=10.0
+boto3<2.0,>=1.20
 
 [db2]
 ibm-db-sa~=0.3
 
 [dbt]
 google-cloud
-boto3<2.0,>=1.20
 dbt-artifacts-parser
+azure-storage-blob~=12.14
 google-cloud-storage==1.43.0
+azure-identity~=1.12
+boto3<2.0,>=1.20
 
 [deltalake]
 delta-spark<=2.3.0
 
 [dev]
-pre-commit
-docker
-black==22.3.0
-pycln
 twine
-pylint
-datamodel-code-generator==0.15.0
+pycln
 isort
+black==22.3.0
+datamodel-code-generator==0.15.0
+pre-commit
+pylint
+docker
 
 [docker]
 python_on_whales==0.55.0
 
 [domo]
 pydomo~=0.3
 
@@ -277,48 +284,52 @@
 [glue]
 boto3<2.0,>=1.20
 
 [great-expectations]
 great-expectations~=0.16.0
 
 [hive]
+impyla~=0.18.0
 pyhive~=0.6
-thrift<1,>=0.13
-thrift-sasl~=0.4
-sasl~=0.3
 presto-types-parser>=0.0.2
-impyla~=0.18.0
+sasl~=0.3
+thrift-sasl~=0.4
+thrift<1,>=0.13
 
 [impala]
 sasl~=0.3
 thrift<1,>=0.13
+thrift-sasl~=0.4
 impyla[kerberos]~=0.18.0
 presto-types-parser>=0.0.2
-thrift-sasl~=0.4
 
 [kafka]
-avro~=1.11
 fastavro>=1.2.0
-confluent_kafka==1.8.2
 protobuf
+confluent_kafka==2.1.1
+avro~=1.11
 grpcio-tools>=1.47.2
 
 [kinesis]
 boto3<2.0,>=1.20
 
 [ldap-users]
 ldap3==2.9.1
 
 [looker]
-looker-sdk>=22.20.0
 lkml~=1.3
+looker-sdk>=22.20.0
 
 [mlflow]
-mlflow-skinny~=1.30
 alembic~=1.10.2
+mlflow-skinny~=1.30
+
+[mongo]
+pandas==1.3.5
+pymongo~=4.3
 
 [mssql]
 sqlalchemy-pytds~=0.3
 
 [mssql-odbc]
 pyodbc<5,>=4.0.35
 
@@ -339,18 +350,18 @@
 pandas==1.3.5
 presidio-analyzer==2.2.32
 
 [pinotdb]
 pinotdb~=0.3
 
 [postgres]
-psycopg2-binary
-GeoAlchemy2~=0.12
 pymysql>=1.0.2
 packaging==21.3
+GeoAlchemy2~=0.12
+psycopg2-binary
 
 [powerbi]
 msal~=1.2
 
 [presto]
 pyhive~=0.6
 presto-types-parser>=0.0.2
@@ -361,31 +372,35 @@
 [quicksight]
 boto3<2.0,>=1.20
 
 [redash]
 packaging==21.3
 
 [redpanda]
-avro~=1.11
 fastavro>=1.2.0
-confluent_kafka==1.8.2
 protobuf
+confluent_kafka==2.1.1
+avro~=1.11
 grpcio-tools>=1.47.2
 
 [redshift]
-psycopg2-binary
 sqlalchemy-redshift==0.8.12
 GeoAlchemy2~=0.12
+psycopg2-binary
 
 [sagemaker]
 boto3<2.0,>=1.20
 
 [salesforce]
 simple_salesforce==1.11.4
 
+[sap-hana]
+hdbcli
+sqlalchemy-hana
+
 [singlestore]
 pymysql>=1.0.2
 
 [sklearn]
 scikit-learn~=1.0
 
 [snowflake]
@@ -393,21 +408,21 @@
 
 [superset]
 
 [tableau]
 tableau-api-lib~=0.1
 
 [test]
-pytest-order
-moto==4.0.8
 coverage
-great-expectations~=0.16.0
+dbt-artifacts-parser
 pytest==7.0.0
+great-expectations~=0.16.0
 pytest-cov
+moto==4.0.8
 apache-airflow==2.3.3
-dbt-artifacts-parser
+pytest-order
 
 [trino]
 trino[sqlalchemy]
 
 [vertica]
 sqlalchemy-vertica[vertica-python]>=0.0.5
```


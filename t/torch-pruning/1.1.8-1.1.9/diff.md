# Comparing `tmp/torch-pruning-1.1.8.tar.gz` & `tmp/torch-pruning-1.1.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "torch-pruning-1.1.8.tar", last modified: Fri May 26 14:55:04 2023, max compression
+gzip compressed data, was "torch-pruning-1.1.9.tar", last modified: Mon Jun 26 06:40:55 2023, max compression
```

## Comparing `torch-pruning-1.1.8.tar` & `torch-pruning-1.1.9.tar`

 * *Command `'html2text {}'` failed with exit code 1. (No output)*

```diff
@@ -26,16 +26,16 @@
 00000190: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000001a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000001b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000001c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000001d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000001e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000001f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00000200: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-00000210: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+00000200: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+00000210: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 00000220: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000260: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000270: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000280: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -59,23 +59,23 @@
 000003a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000003b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000003c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000003d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000003e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000003f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000400: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00000410: 312e 382f 0000 0000 0000 0000 0000 0000  1.8/............
+00000410: 312e 392f 0000 0000 0000 0000 0000 0000  1.9/............
 00000420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000460: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 00000470: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00000480: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-00000490: 3131 3000 3031 3337 3135 0020 3500 0000  110.013715. 5...
+00000480: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+00000490: 3336 3700 3031 3337 3334 0020 3500 0000  367.013734. 5...
 000004a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000004b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000004c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000004d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000004e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000004f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000500: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -122,16 +122,16 @@
 00000790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000007a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000007b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000007c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000007d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000007e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000007f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00000800: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00000810: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00000800: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00000810: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00000820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -155,23 +155,23 @@
 000009a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000009b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000009c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000009d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000009e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000009f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000a00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00000a10: 312e 382f 4c49 4345 4e53 4500 0000 0000  1.8/LICENSE.....
+00000a10: 312e 392f 4c49 4345 4e53 4500 0000 0000  1.9/LICENSE.....
 00000a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000a60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00000a70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00000a80: 3030 3032 3035 3500 3134 3433 3431 3434  0002055.14434144
-00000a90: 3037 3100 3031 3437 3332 0020 3000 0000  071.014732. 0...
+00000a80: 3030 3032 3035 3500 3134 3434 3632 3331  0002055.14446231
+00000a90: 3335 3300 3031 3437 3336 0020 3000 0000  353.014736. 0...
 00000aa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000ab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000ac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000ad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000ae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000af0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00000b00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -314,16 +314,16 @@
 00001390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000013a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000013b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000013c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000013d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000013e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000013f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00001400: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-00001410: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+00001400: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+00001410: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 00001420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001460: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001470: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001480: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -347,23 +347,23 @@
 000015a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000015b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000015c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000015d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000015e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000015f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001600: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00001610: 312e 382f 504b 472d 494e 464f 0000 0000  1.8/PKG-INFO....
+00001610: 312e 392f 504b 472d 494e 464f 0000 0000  1.9/PKG-INFO....
 00001620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001660: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00001670: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00001680: 3030 3531 3031 3300 3134 3433 3431 3434  0051013.14434144
-00001690: 3131 3000 3031 3530 3132 0020 3000 0000  110.015012. 0...
+00001680: 3030 3531 3037 3600 3134 3434 3632 3331  0051076.14446231
+00001690: 3336 3700 3031 3530 3432 0020 3000 0000  367.015042. 0...
 000016a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000016b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000016c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000016d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000016e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000016f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001700: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -381,15 +381,15 @@
 000017c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000017f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00001800: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 00001810: 3a20 322e 310a 4e61 6d65 3a20 746f 7263  : 2.1.Name: torc
 00001820: 682d 7072 756e 696e 670a 5665 7273 696f  h-pruning.Versio
-00001830: 6e3a 2031 2e31 2e38 0a53 756d 6d61 7279  n: 1.1.8.Summary
+00001830: 6e3a 2031 2e31 2e39 0a53 756d 6d61 7279  n: 1.1.9.Summary
 00001840: 3a20 5374 7275 6374 7572 616c 2050 7275  : Structural Pru
 00001850: 6e69 6e67 2066 6f72 204d 6f64 656c 2041  ning for Model A
 00001860: 6363 656c 6572 6174 696f 6e2e 0a48 6f6d  cceleration..Hom
 00001870: 652d 7061 6765 3a20 6874 7470 733a 2f2f  e-page: https://
 00001880: 6769 7468 7562 2e63 6f6d 2f56 6169 6e46  github.com/VainF
 00001890: 2f54 6f72 6368 2d50 7275 6e69 6e67 0a41  /Torch-Pruning.A
 000018a0: 7574 686f 723a 2047 6f6e 6766 616e 2046  uthor: Gongfan F
@@ -466,15 +466,15 @@
 00001d10: 2068 7265 663d 2268 7474 7073 3a2f 2f67   href="https://g
 00001d20: 6974 6875 622e 636f 6d2f 5661 696e 462f  ithub.com/VainF/
 00001d30: 546f 7263 682d 5072 756e 696e 672f 7265  Torch-Pruning/re
 00001d40: 6c65 6173 6573 2f6c 6174 6573 7422 3e3c  leases/latest"><
 00001d50: 696d 6720 7372 633d 2268 7474 7073 3a2f  img src="https:/
 00001d60: 2f69 6d67 2e73 6869 656c 6473 2e69 6f2f  /img.shields.io/
 00001d70: 6261 6467 652f 4c61 7465 7374 2532 3056  badge/Latest%20V
-00001d80: 6572 7369 6f6e 2d31 2e31 2e38 2d33 6635  ersion-1.1.8-3f5
+00001d80: 6572 7369 6f6e 2d31 2e31 2e39 2d33 6635  ersion-1.1.9-3f5
 00001d90: 3162 352e 7376 6722 2061 6c74 3d22 4c61  1b5.svg" alt="La
 00001da0: 7465 7374 2056 6572 7369 6f6e 223e 3c2f  test Version"></
 00001db0: 613e 0a20 203c 6120 6872 6566 3d22 6874  a>.  <a href="ht
 00001dc0: 7470 733a 2f2f 636f 6c61 622e 7265 7365  tps://colab.rese
 00001dd0: 6172 6368 2e67 6f6f 676c 652e 636f 6d2f  arch.google.com/
 00001de0: 6472 6976 652f 3154 5276 454c 5144 4e6a  drive/1TRvELQDNj
 00001df0: 3950 774d 2d45 4552 5762 4633 4951 4f79  9PwM-EERWbF3IQOy
@@ -634,1074 +634,1074 @@
 00002790: 3230 3233 2e30 342e 3231 204a 6f69 6e20  2023.04.21 Join 
 000027a0: 6f75 7220 5465 6c65 6772 616d 206f 7220  our Telegram or 
 000027b0: 5765 6368 6174 2067 726f 7570 2066 6f72  Wechat group for
 000027c0: 2063 6173 7561 6c20 6469 7363 7573 7369   casual discussi
 000027d0: 6f6e 733a 0a20 202a 2054 656c 6567 7261  ons:.  * Telegra
 000027e0: 6d3a 2068 7474 7073 3a2f 2f74 2e6d 652f  m: https://t.me/
 000027f0: 2b4e 776a 6242 444e 3261 6f31 6c5a 6a5a  +NwjbBDN2ao1lZjZ
-00002800: 6c0a 2020 2a20 5765 6368 6174 3a20 3c69  l.  * Wechat: <i
+00002800: 6c0a 2020 2a20 5765 4368 6174 3a20 3c69  l.  * WeChat: <i
 00002810: 6d67 2077 6964 7468 3d22 3130 3022 2061  mg width="100" a
 00002820: 6c74 3d22 696d 6167 6522 2073 7263 3d22  lt="image" src="
 00002830: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
 00002840: 6f6d 2f56 6169 6e46 2f54 6f72 6368 2d50  om/VainF/Torch-P
 00002850: 7275 6e69 6e67 2f61 7373 6574 732f 3138  runing/assets/18
-00002860: 3539 3232 3131 2f35 3932 6161 3035 342d  592211/592aa054-
-00002870: 3762 3737 2d34 3334 632d 3935 3931 2d32  7b77-434c-9591-2
-00002880: 3762 3633 3538 3439 6535 3322 3e0a 0a50  7b635849e53">..P
-00002890: 6c65 6173 6520 646f 206e 6f74 2068 6573  lease do not hes
-000028a0: 6974 6174 6520 746f 206f 7065 6e20 6120  itate to open a 
-000028b0: 5b64 6973 6375 7373 696f 6e5d 2868 7474  [discussion](htt
-000028c0: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-000028d0: 5661 696e 462f 546f 7263 682d 5072 756e  VainF/Torch-Prun
-000028e0: 696e 672f 6469 7363 7573 7369 6f6e 7329  ing/discussions)
-000028f0: 206f 7220 5b69 7373 7565 5d28 6874 7470   or [issue](http
-00002900: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
-00002910: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
-00002920: 6e67 2f69 7373 7565 7329 2069 6620 796f  ng/issues) if yo
-00002930: 7520 656e 636f 756e 7465 7220 616e 7920  u encounter any 
-00002940: 7072 6f62 6c65 6d73 2077 6974 6820 7468  problems with th
-00002950: 6520 6c69 6272 6172 7920 6f72 2074 6865  e library or the
-00002960: 2070 6170 6572 2e0a 0a0a 2323 2320 2a2a   paper....### **
-00002970: 4665 6174 7572 6573 3a2a 2a0a 2d20 5b78  Features:**.- [x
-00002980: 5d20 5374 7275 6374 7572 616c 2070 7275  ] Structural pru
-00002990: 6e69 6e67 2066 6f72 2043 4e4e 732c 2054  ning for CNNs, T
-000029a0: 7261 6e73 666f 726d 6572 732c 2044 6574  ransformers, Det
-000029b0: 6563 746f 7273 2c20 4c61 6e67 7561 6765  ectors, Language
-000029c0: 204d 6f64 656c 7320 616e 6420 4469 6666   Models and Diff
-000029d0: 7573 696f 6e20 4d6f 6465 6c73 2e20 506c  usion Models. Pl
-000029e0: 6561 7365 2072 6566 6572 2074 6f20 7468  ease refer to th
-000029f0: 6520 5b50 7275 6e61 6269 6c69 7479 2042  e [Prunability B
-00002a00: 656e 6368 6d61 726b 5d28 6265 6e63 686d  enchmark](benchm
-00002a10: 6172 6b73 2f70 7275 6e61 6269 6c69 7479  arks/prunability
-00002a20: 292e 0a2d 205b 785d 2048 6967 682d 6c65  )..- [x] High-le
-00002a30: 7665 6c20 7072 756e 6572 733a 205b 4d61  vel pruners: [Ma
-00002a40: 676e 6974 7564 6550 7275 6e65 725d 2868  gnitudePruner](h
-00002a50: 7474 7073 3a2f 2f61 7278 6976 2e6f 7267  ttps://arxiv.org
-00002a60: 2f61 6273 2f31 3630 382e 3038 3731 3029  /abs/1608.08710)
-00002a70: 2c20 5b42 4e53 6361 6c65 5072 756e 6572  , [BNScalePruner
-00002a80: 5d28 6874 7470 733a 2f2f 6172 7869 762e  ](https://arxiv.
-00002a90: 6f72 672f 6162 732f 3137 3038 2e30 3635  org/abs/1708.065
-00002aa0: 3139 292c 205b 4772 6f75 704e 6f72 6d50  19), [GroupNormP
-00002ab0: 7275 6e65 725d 2868 7474 7073 3a2f 2f61  runer](https://a
-00002ac0: 7278 6976 2e6f 7267 2f61 6273 2f32 3330  rxiv.org/abs/230
-00002ad0: 312e 3132 3930 3029 2c20 5261 6e64 6f6d  1.12900), Random
-00002ae0: 5072 756e 6572 2c20 6574 632e 0a2d 205b  Pruner, etc..- [
-00002af0: 785d 2049 6d70 6f72 7461 6e63 6520 4372  x] Importance Cr
-00002b00: 6974 6572 6961 3a20 4c2d 7020 4e6f 726d  iteria: L-p Norm
-00002b10: 2c20 5461 796c 6f72 2c20 5261 6e64 6f6d  , Taylor, Random
-00002b20: 2c20 424e 5363 616c 696e 672c 2065 7463  , BNScaling, etc
-00002b30: 2e0a 2d20 5b78 5d20 4465 7065 6e64 656e  ..- [x] Dependen
-00002b40: 6379 2047 7261 7068 2066 6f72 2064 6570  cy Graph for dep
-00002b50: 656e 6465 6e63 7920 6d6f 6465 6c69 6e67  endency modeling
-00002b60: 2e0a 2d20 5b78 5d20 5375 7070 6f72 7465  ..- [x] Supporte
-00002b70: 6420 6d6f 6475 6c65 733a 204c 696e 6561  d modules: Linea
-00002b80: 722c 2028 5472 616e 7370 6f73 6564 2920  r, (Transposed) 
-00002b90: 436f 6e76 2c20 4e6f 726d 616c 697a 6174  Conv, Normalizat
-00002ba0: 696f 6e2c 2050 5265 4c55 2c20 456d 6265  ion, PReLU, Embe
-00002bb0: 6464 696e 672c 204d 756c 7469 6865 6164  dding, Multihead
-00002bc0: 4174 7465 6e74 696f 6e2c 206e 6e2e 5061  Attention, nn.Pa
-00002bd0: 7261 6d65 7465 7273 2061 6e64 205b 6375  rameters and [cu
-00002be0: 7374 6f6d 697a 6564 206d 6f64 756c 6573  stomized modules
-00002bf0: 5d28 7465 7374 732f 7465 7374 5f63 7573  ](tests/test_cus
-00002c00: 746f 6d69 7a65 645f 6c61 7965 722e 7079  tomized_layer.py
-00002c10: 292e 0a2d 205b 785d 2053 7570 706f 7274  )..- [x] Support
-00002c20: 6564 206f 7065 7261 746f 7273 3a20 7370  ed operators: sp
-00002c30: 6c69 742c 2063 6f6e 6361 7465 6e61 7469  lit, concatenati
-00002c40: 6f6e 2c20 736b 6970 2063 6f6e 6e65 6374  on, skip connect
-00002c50: 696f 6e2c 2066 6c61 7474 656e 2c20 7265  ion, flatten, re
-00002c60: 7368 6170 652c 2076 6965 772c 2061 6c6c  shape, view, all
-00002c70: 2065 6c65 6d65 6e74 2d77 6973 6520 6f70   element-wise op
-00002c80: 732c 2065 7463 2e0a 2d20 5b78 5d20 5b4c  s, etc..- [x] [L
-00002c90: 6f77 2d6c 6576 656c 2070 7275 6e69 6e67  ow-level pruning
-00002ca0: 2066 756e 6374 696f 6e73 5d28 746f 7263   functions](torc
-00002cb0: 685f 7072 756e 696e 672f 7072 756e 6572  h_pruning/pruner
-00002cc0: 2f66 756e 6374 696f 6e2e 7079 290a 2d20  /function.py).- 
-00002cd0: 5b78 5d20 5b42 656e 6368 6d61 726b 735d  [x] [Benchmarks]
-00002ce0: 2862 656e 6368 6d61 726b 7329 2061 6e64  (benchmarks) and
-00002cf0: 205b 7475 746f 7269 616c 735d 2874 7574   [tutorials](tut
-00002d00: 6f72 6961 6c73 290a 2d20 5b78 5d20 4120  orials).- [x] A 
-00002d10: 5b72 6573 6f75 7263 6520 6c69 7374 5d28  [resource list](
-00002d20: 7072 6163 7469 6361 6c5f 7374 7275 6374  practical_struct
-00002d30: 7572 616c 5f70 7275 6e69 6e67 2e6d 6429  ural_pruning.md)
-00002d40: 2066 6f72 2070 7261 6374 6963 616c 2073   for practical s
-00002d50: 7472 7563 7472 7561 6c20 7072 756e 696e  tructrual prunin
-00002d60: 672e 0a20 200a 2323 2320 2a2a 544f 444f  g..  .### **TODO
-00002d70: 204c 6973 743a 2a2a 0a2d 205b 205d 2041   List:**.- [ ] A
-00002d80: 2073 7472 6f6e 6720 6261 7365 6c69 6e65   strong baseline
-00002d90: 2077 6974 6820 6261 6773 206f 6620 7472   with bags of tr
-00002da0: 6963 6b73 2066 726f 6d20 6578 6973 7469  icks from existi
-00002db0: 6e67 206d 6574 686f 6473 2e0a 2d20 5b20  ng methods..- [ 
-00002dc0: 5d20 4120 6265 6e63 686d 6172 6b20 666f  ] A benchmark fo
-00002dd0: 7220 5b54 6f72 6368 7669 7369 6f6e 5d28  r [Torchvision](
-00002de0: 6874 7470 733a 2f2f 7079 746f 7263 682e  https://pytorch.
-00002df0: 6f72 672f 7669 7369 6f6e 2f73 7461 626c  org/vision/stabl
-00002e00: 652f 6d6f 6465 6c73 2e68 746d 6c29 2063  e/models.html) c
-00002e10: 6f6d 7061 7469 6269 6c69 7479 2028 2a2a  ompatibility (**
-00002e20: 3831 2f38 353d 3935 2e33 252a 2a2c 203a  81/85=95.3%**, :
-00002e30: 6865 6176 795f 6368 6563 6b5f 6d61 726b  heavy_check_mark
-00002e40: 3a29 2061 6e64 205b 7469 6d6d 5d28 6874  :) and [timm](ht
-00002e50: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-00002e60: 2f68 7567 6769 6e67 6661 6365 2f70 7974  /huggingface/pyt
-00002e70: 6f72 6368 2d69 6d61 6765 2d6d 6f64 656c  orch-image-model
-00002e80: 7329 2063 6f6d 7061 7469 6269 6c69 7479  s) compatibility
-00002e90: 2e0a 2d20 5b20 5d20 5072 756e 696e 6720  ..- [ ] Pruning 
-00002ea0: 6672 6f6d 2053 6372 6174 6368 202f 2061  from Scratch / a
-00002eb0: 7420 496e 6974 6961 6c69 7a61 7469 6f6e  t Initialization
-00002ec0: 2e0a 2d20 5b20 5d20 4d6f 7265 2068 6967  ..- [ ] More hig
-00002ed0: 682d 6c65 7665 6c20 7072 756e 6572 7320  h-level pruners 
-00002ee0: 6c69 6b65 205b 4669 7368 6572 5072 756e  like [FisherPrun
-00002ef0: 6572 5d28 6874 7470 733a 2f2f 6172 7869  er](https://arxi
-00002f00: 762e 6f72 672f 6162 732f 3231 3038 2e30  v.org/abs/2108.0
-00002f10: 3037 3038 292c 205b 4772 6f77 696e 6752  0708), [GrowingR
-00002f20: 6567 5d28 6874 7470 733a 2f2f 6172 7869  eg](https://arxi
-00002f30: 762e 6f72 672f 6162 732f 3230 3132 2e30  v.org/abs/2012.0
-00002f40: 3932 3433 292c 2065 7463 2e0a 2d20 5b20  9243), etc..- [ 
-00002f50: 5d20 4d6f 7265 2054 7261 6e73 666f 726d  ] More Transform
-00002f60: 6572 7320 6c69 6b65 2056 6973 696f 6e20  ers like Vision 
-00002f70: 5472 616e 7366 6f72 6d65 7273 2028 3a68  Transformers (:h
-00002f80: 6561 7679 5f63 6865 636b 5f6d 6172 6b3a  eavy_check_mark:
-00002f90: 292c 2053 7769 6e20 5472 616e 7366 6f72  ), Swin Transfor
-00002fa0: 6d65 7273 2c20 506f 6f6c 466f 726d 6572  mers, PoolFormer
-00002fb0: 732e 0a2d 205b 205d 2042 6c6f 636b 2f4c  s..- [ ] Block/L
-00002fc0: 6179 6572 2f44 6570 7468 2050 7275 6e69  ayer/Depth Pruni
-00002fd0: 6e67 0a2d 205b 205d 2050 7275 6e69 6e67  ng.- [ ] Pruning
-00002fe0: 2062 656e 6368 6d61 726b 7320 666f 7220   benchmarks for 
-00002ff0: 4349 4641 522c 2049 6d61 6765 4e65 7420  CIFAR, ImageNet 
-00003000: 616e 6420 434f 434f 2e0a 0a23 2320 496e  and COCO...## In
-00003010: 7374 616c 6c61 7469 6f6e 0a0a 546f 7263  stallation..Torc
-00003020: 682d 5072 756e 696e 6720 6973 2063 6f6d  h-Pruning is com
-00003030: 7061 7469 626c 6520 7769 7468 2050 7954  patible with PyT
-00003040: 6f72 6368 2031 2e78 2061 6e64 2032 2e78  orch 1.x and 2.x
-00003050: 2e20 2a2a 5079 546f 7263 6820 312e 3132  . **PyTorch 1.12
-00003060: 2e31 2069 7320 7265 636f 6d6d 656e 6465  .1 is recommende
-00003070: 6421 2a2a 0a0a 6060 6062 6173 680a 7069  d!**..```bash.pi
-00003080: 7020 696e 7374 616c 6c20 746f 7263 682d  p install torch-
-00003090: 7072 756e 696e 6720 2320 7631 2e31 2e38  pruning # v1.1.8
-000030a0: 0a60 6060 0a6f 720a 6060 6062 6173 680a  .```.or.```bash.
-000030b0: 6769 7420 636c 6f6e 6520 6874 7470 733a  git clone https:
-000030c0: 2f2f 6769 7468 7562 2e63 6f6d 2f56 6169  //github.com/Vai
-000030d0: 6e46 2f54 6f72 6368 2d50 7275 6e69 6e67  nF/Torch-Pruning
-000030e0: 2e67 6974 0a60 6060 0a0a 2323 2051 7569  .git.```..## Qui
-000030f0: 636b 7374 6172 740a 2020 0a48 6572 6520  ckstart.  .Here 
-00003100: 7765 2070 726f 7669 6465 2061 2071 7569  we provide a qui
-00003110: 636b 2073 7461 7274 2066 6f72 2054 6f72  ck start for Tor
-00003120: 6368 2d50 7275 6e69 6e67 2e20 4d6f 7265  ch-Pruning. More
-00003130: 2065 7870 6c61 696e 6564 2064 6574 6169   explained detai
-00003140: 6c73 2063 616e 2062 6520 666f 756e 6420  ls can be found 
-00003150: 696e 205b 7475 746f 7261 6c73 5d28 2e2f  in [tutorals](./
-00003160: 7475 746f 7269 616c 732f 290a 0a23 2323  tutorials/)..###
-00003170: 2030 2e20 486f 7720 4974 2057 6f72 6b73   0. How It Works
-00003180: 0a0a 496e 2073 7472 7563 7475 7261 6c20  ..In structural 
-00003190: 7072 756e 696e 672c 202a 2a60 6047 726f  pruning, **``Gro
-000031a0: 7570 6060 2069 7320 7468 6520 6d69 6e69  up`` is the mini
-000031b0: 6d61 6c20 7265 6d6f 7661 626c 6520 756e  mal removable un
-000031c0: 6974 2077 6974 6869 6e20 6465 6570 206e  it within deep n
-000031d0: 6574 776f 726b 732a 2a2e 2045 6163 6820  etworks**. Each 
-000031e0: 6772 6f75 7020 636f 6e74 6169 6e73 2073  group contains s
-000031f0: 6576 6572 616c 2069 6e74 6572 6465 7065  everal interdepe
-00003200: 6e64 656e 7420 6c61 7965 7273 2074 6861  ndent layers tha
-00003210: 7420 6d75 7374 2062 6520 7072 756e 6564  t must be pruned
-00003220: 2073 696d 756c 7461 6e65 6f75 736c 7920   simultaneously 
-00003230: 746f 206d 6169 6e74 6169 6e20 7468 6520  to maintain the 
-00003240: 696e 7465 6772 6974 7920 6f66 2074 6865  integrity of the
-00003250: 2072 6573 756c 7469 6e67 2073 7472 7563   resulting struc
-00003260: 7475 7265 732e 2048 6f77 6576 6572 2c20  tures. However, 
-00003270: 6465 6570 206e 6574 776f 726b 7320 6f66  deep networks of
-00003280: 7465 6e20 7072 6573 656e 7420 636f 6d70  ten present comp
-00003290: 6c65 7820 6465 7065 6e64 656e 6369 6573  lex dependencies
-000032a0: 2061 6d6f 6e67 206c 6179 6572 732c 206d   among layers, m
-000032b0: 616b 696e 6720 7374 7275 6374 7572 616c  aking structural
-000032c0: 2070 7275 6e69 6e67 2061 2063 6861 6c6c   pruning a chall
-000032d0: 656e 6769 6e67 2065 6e64 6561 766f 722e  enging endeavor.
-000032e0: 2054 6869 7320 776f 726b 2061 6464 7265   This work addre
-000032f0: 7373 6573 2074 6869 7320 6368 616c 6c65  sses this challe
-00003300: 6e67 6520 6279 206f 6666 6572 696e 6720  nge by offering 
-00003310: 616e 2061 7574 6f6d 6174 6564 206d 6563  an automated mec
-00003320: 6861 6e69 736d 2c20 6060 4465 7047 7261  hanism, ``DepGra
-00003330: 7068 6060 2c20 666f 7220 7061 7261 6d65  ph``, for parame
-00003340: 7465 7220 6772 6f75 7069 6e67 2c20 7768  ter grouping, wh
-00003350: 6963 6820 6661 6369 6c69 7461 7465 7320  ich facilitates 
-00003360: 6566 666f 7274 6c65 7373 2070 7275 6e69  effortless pruni
-00003370: 6e67 2066 6f72 2061 2077 6964 6520 7261  ng for a wide ra
-00003380: 6e67 6520 6f66 2064 6565 7020 6e65 7477  nge of deep netw
-00003390: 6f72 6b73 2e0a 0a3c 6469 7620 616c 6967  orks...<div alig
-000033a0: 6e3d 2263 656e 7465 7222 3e0a 3c69 6d67  n="center">.<img
-000033b0: 2073 7263 3d22 6173 7365 7473 2f64 6570   src="assets/dep
-000033c0: 2e70 6e67 2220 7769 6474 683d 2231 3030  .png" width="100
-000033d0: 2522 3e0a 3c2f 6469 763e 0a0a 2323 2320  %">.</div>..### 
-000033e0: 312e 2041 204d 696e 696d 616c 2045 7861  1. A Minimal Exa
-000033f0: 6d70 6c65 0a0a 6060 6070 7974 686f 6e0a  mple..```python.
-00003400: 696d 706f 7274 2074 6f72 6368 0a66 726f  import torch.fro
-00003410: 6d20 746f 7263 6876 6973 696f 6e2e 6d6f  m torchvision.mo
-00003420: 6465 6c73 2069 6d70 6f72 7420 7265 736e  dels import resn
-00003430: 6574 3138 0a69 6d70 6f72 7420 746f 7263  et18.import torc
-00003440: 685f 7072 756e 696e 6720 6173 2074 700a  h_pruning as tp.
-00003450: 0a6d 6f64 656c 203d 2072 6573 6e65 7431  .model = resnet1
-00003460: 3828 7072 6574 7261 696e 6564 3d54 7275  8(pretrained=Tru
-00003470: 6529 2e65 7661 6c28 290a 0a23 2031 2e20  e).eval()..# 1. 
-00003480: 6275 696c 6420 6465 7065 6e64 656e 6379  build dependency
-00003490: 2067 7261 7068 2066 6f72 2072 6573 6e65   graph for resne
-000034a0: 7431 380a 4447 203d 2074 702e 4465 7065  t18.DG = tp.Depe
-000034b0: 6e64 656e 6379 4772 6170 6828 292e 6275  ndencyGraph().bu
-000034c0: 696c 645f 6465 7065 6e64 656e 6379 286d  ild_dependency(m
-000034d0: 6f64 656c 2c20 6578 616d 706c 655f 696e  odel, example_in
-000034e0: 7075 7473 3d74 6f72 6368 2e72 616e 646e  puts=torch.randn
-000034f0: 2831 2c33 2c32 3234 2c32 3234 2929 0a0a  (1,3,224,224))..
-00003500: 2320 322e 2053 7065 6369 6679 2074 6865  # 2. Specify the
-00003510: 2074 6f2d 6265 2d70 7275 6e65 6420 6368   to-be-pruned ch
-00003520: 616e 6e65 6c73 2e20 4865 7265 2077 6520  annels. Here we 
-00003530: 7072 756e 6520 7468 6f73 6520 6368 616e  prune those chan
-00003540: 6e65 6c73 2069 6e64 6578 6564 2062 7920  nels indexed by 
-00003550: 5b32 2c20 362c 2039 5d2e 0a67 726f 7570  [2, 6, 9]..group
-00003560: 203d 2044 472e 6765 745f 7072 756e 696e   = DG.get_prunin
-00003570: 675f 6772 6f75 7028 206d 6f64 656c 2e63  g_group( model.c
-00003580: 6f6e 7631 2c20 7470 2e70 7275 6e65 5f63  onv1, tp.prune_c
-00003590: 6f6e 765f 6f75 745f 6368 616e 6e65 6c73  onv_out_channels
-000035a0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
-000035b0: 2029 0a0a 2320 332e 2070 7275 6e65 2061   )..# 3. prune a
-000035c0: 6c6c 2067 726f 7570 6564 206c 6179 6572  ll grouped layer
-000035d0: 7320 7468 6174 2061 7265 2063 6f75 706c  s that are coupl
-000035e0: 6564 2077 6974 6820 6d6f 6465 6c2e 636f  ed with model.co
-000035f0: 6e76 3120 2869 6e63 6c75 6465 6429 2e0a  nv1 (included)..
-00003600: 6966 2044 472e 6368 6563 6b5f 7072 756e  if DG.check_prun
-00003610: 696e 675f 6772 6f75 7028 6772 6f75 7029  ing_group(group)
-00003620: 3a20 2320 6176 6f69 6420 6675 6c6c 2070  : # avoid full p
-00003630: 7275 6e69 6e67 2c20 692e 652e 2c20 6368  runing, i.e., ch
-00003640: 616e 6e65 6c73 3d30 2e0a 2020 2020 6772  annels=0..    gr
-00003650: 6f75 702e 7072 756e 6528 290a 2020 2020  oup.prune().    
-00003660: 0a23 2034 2e20 5361 7665 2026 204c 6f61  .# 4. Save & Loa
-00003670: 640a 6d6f 6465 6c2e 7a65 726f 5f67 7261  d.model.zero_gra
-00003680: 6428 2920 2320 5765 2064 6f6e 2774 2077  d() # We don't w
-00003690: 616e 7420 746f 2073 746f 7265 2067 7261  ant to store gra
-000036a0: 6469 656e 7420 696e 666f 726d 6174 696f  dient informatio
-000036b0: 6e0a 746f 7263 682e 7361 7665 286d 6f64  n.torch.save(mod
-000036c0: 656c 2c20 276d 6f64 656c 2e70 7468 2729  el, 'model.pth')
-000036d0: 2023 2077 6974 686f 7574 202e 7374 6174   # without .stat
-000036e0: 655f 6469 6374 0a6d 6f64 656c 203d 2074  e_dict.model = t
-000036f0: 6f72 6368 2e6c 6f61 6428 276d 6f64 656c  orch.load('model
-00003700: 2e70 7468 2729 2023 206c 6f61 6420 7468  .pth') # load th
-00003710: 6520 6d6f 6465 6c20 6f62 6a65 6374 0a60  e model object.`
-00003720: 6060 0a20 200a 5468 6520 6162 6f76 6520  ``.  .The above 
-00003730: 6578 616d 706c 6520 6465 6d6f 6e73 7472  example demonstr
-00003740: 6174 6573 2074 6865 2066 756e 6461 6d65  ates the fundame
-00003750: 6e74 616c 2070 7275 6e69 6e67 2070 6970  ntal pruning pip
-00003760: 656c 696e 6520 7573 696e 6720 4465 7047  eline using DepG
-00003770: 7261 7068 2e20 5468 6520 7461 7267 6574  raph. The target
-00003780: 206c 6179 6572 2072 6573 6e65 742e 636f   layer resnet.co
-00003790: 6e76 3120 6973 2063 6f75 706c 6564 2077  nv1 is coupled w
-000037a0: 6974 6820 7365 7665 7261 6c20 6c61 7965  ith several laye
-000037b0: 7273 2c20 7768 6963 6820 7265 7175 6972  rs, which requir
-000037c0: 6573 2073 696d 756c 7461 6e65 6f75 7320  es simultaneous 
-000037d0: 7265 6d6f 7661 6c20 696e 2073 7472 7563  removal in struc
-000037e0: 7475 7261 6c20 7072 756e 696e 672e 204c  tural pruning. L
-000037f0: 6574 2773 2070 7269 6e74 2074 6865 2067  et's print the g
-00003800: 726f 7570 2061 6e64 206f 6273 6572 7665  roup and observe
-00003810: 2068 6f77 2061 2070 7275 6e69 6e67 206f   how a pruning o
-00003820: 7065 7261 7469 6f6e 2022 7472 6967 6765  peration "trigge
-00003830: 7273 2220 6f74 6865 7220 6f6e 6573 2e20  rs" other ones. 
-00003840: 496e 2074 6865 2066 6f6c 6c6f 7769 6e67  In the following
-00003850: 206f 7574 7075 7473 2c20 6060 4120 3d3e   outputs, ``A =>
-00003860: 2042 6060 206d 6561 6e73 2074 6865 2070   B`` means the p
-00003870: 7275 6e69 6e67 206f 7065 7261 7469 6f6e  runing operation
-00003880: 2060 6041 6060 2074 7269 6767 6572 7320   ``A`` triggers 
-00003890: 7468 6520 7072 756e 696e 6720 6f70 6572  the pruning oper
-000038a0: 6174 696f 6e20 6060 4260 602e 2067 726f  ation ``B``. gro
-000038b0: 7570 5b30 5d20 7265 6665 7273 2074 6f20  up[0] refers to 
-000038c0: 7468 6520 7072 756e 696e 6720 726f 6f74  the pruning root
-000038d0: 2069 6e20 6060 4447 2e67 6574 5f70 7275   in ``DG.get_pru
-000038e0: 6e69 6e67 5f67 726f 7570 6060 2e0a 0a60  ning_group``...`
-000038f0: 6060 0a2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ``.-------------
-00003900: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00003910: 2d2d 2d0a 2020 2020 2020 2020 2020 5072  ---.          Pr
-00003920: 756e 696e 6720 4772 6f75 700a 2d2d 2d2d  uning Group.----
+00002860: 3539 3232 3131 2f31 6131 3261 3634 652d  592211/1a12a64e-
+00002870: 6139 3465 2d34 6137 342d 3839 3039 2d38  a94e-4a74-8909-8
+00002880: 6663 3038 3836 6636 3366 3222 3e0a 0a0a  fc0886f63f2">...
+00002890: 0a50 6c65 6173 6520 646f 206e 6f74 2068  .Please do not h
+000028a0: 6573 6974 6174 6520 746f 206f 7065 6e20  esitate to open 
+000028b0: 6120 5b64 6973 6375 7373 696f 6e5d 2868  a [discussion](h
+000028c0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+000028d0: 6d2f 5661 696e 462f 546f 7263 682d 5072  m/VainF/Torch-Pr
+000028e0: 756e 696e 672f 6469 7363 7573 7369 6f6e  uning/discussion
+000028f0: 7329 206f 7220 5b69 7373 7565 5d28 6874  s) or [issue](ht
+00002900: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
+00002910: 2f56 6169 6e46 2f54 6f72 6368 2d50 7275  /VainF/Torch-Pru
+00002920: 6e69 6e67 2f69 7373 7565 7329 2069 6620  ning/issues) if 
+00002930: 796f 7520 656e 636f 756e 7465 7220 616e  you encounter an
+00002940: 7920 7072 6f62 6c65 6d73 2077 6974 6820  y problems with 
+00002950: 7468 6520 6c69 6272 6172 7920 6f72 2074  the library or t
+00002960: 6865 2070 6170 6572 2e0a 0a0a 2323 2320  he paper....### 
+00002970: 2a2a 4665 6174 7572 6573 3a2a 2a0a 2d20  **Features:**.- 
+00002980: 5b78 5d20 5374 7275 6374 7572 616c 2070  [x] Structural p
+00002990: 7275 6e69 6e67 2066 6f72 2043 4e4e 732c  runing for CNNs,
+000029a0: 2054 7261 6e73 666f 726d 6572 732c 2044   Transformers, D
+000029b0: 6574 6563 746f 7273 2c20 4c61 6e67 7561  etectors, Langua
+000029c0: 6765 204d 6f64 656c 7320 616e 6420 4469  ge Models and Di
+000029d0: 6666 7573 696f 6e20 4d6f 6465 6c73 2e20  ffusion Models. 
+000029e0: 506c 6561 7365 2072 6566 6572 2074 6f20  Please refer to 
+000029f0: 7468 6520 5b50 7275 6e61 6269 6c69 7479  the [Prunability
+00002a00: 2042 656e 6368 6d61 726b 5d28 6265 6e63   Benchmark](benc
+00002a10: 686d 6172 6b73 2f70 7275 6e61 6269 6c69  hmarks/prunabili
+00002a20: 7479 292e 0a2d 205b 785d 2048 6967 682d  ty)..- [x] High-
+00002a30: 6c65 7665 6c20 7072 756e 6572 733a 205b  level pruners: [
+00002a40: 4d61 676e 6974 7564 6550 7275 6e65 725d  MagnitudePruner]
+00002a50: 2868 7474 7073 3a2f 2f61 7278 6976 2e6f  (https://arxiv.o
+00002a60: 7267 2f61 6273 2f31 3630 382e 3038 3731  rg/abs/1608.0871
+00002a70: 3029 2c20 5b42 4e53 6361 6c65 5072 756e  0), [BNScalePrun
+00002a80: 6572 5d28 6874 7470 733a 2f2f 6172 7869  er](https://arxi
+00002a90: 762e 6f72 672f 6162 732f 3137 3038 2e30  v.org/abs/1708.0
+00002aa0: 3635 3139 292c 205b 4772 6f75 704e 6f72  6519), [GroupNor
+00002ab0: 6d50 7275 6e65 725d 2868 7474 7073 3a2f  mPruner](https:/
+00002ac0: 2f61 7278 6976 2e6f 7267 2f61 6273 2f32  /arxiv.org/abs/2
+00002ad0: 3330 312e 3132 3930 3029 2c20 5261 6e64  301.12900), Rand
+00002ae0: 6f6d 5072 756e 6572 2c20 6574 632e 0a2d  omPruner, etc..-
+00002af0: 205b 785d 2049 6d70 6f72 7461 6e63 6520   [x] Importance 
+00002b00: 4372 6974 6572 6961 3a20 4c2d 7020 4e6f  Criteria: L-p No
+00002b10: 726d 2c20 5461 796c 6f72 2c20 5261 6e64  rm, Taylor, Rand
+00002b20: 6f6d 2c20 424e 5363 616c 696e 672c 2065  om, BNScaling, e
+00002b30: 7463 2e0a 2d20 5b78 5d20 4465 7065 6e64  tc..- [x] Depend
+00002b40: 656e 6379 2047 7261 7068 2066 6f72 2064  ency Graph for d
+00002b50: 6570 656e 6465 6e63 7920 6d6f 6465 6c69  ependency modeli
+00002b60: 6e67 2e0a 2d20 5b78 5d20 5375 7070 6f72  ng..- [x] Suppor
+00002b70: 7465 6420 6d6f 6475 6c65 733a 204c 696e  ted modules: Lin
+00002b80: 6561 722c 2028 5472 616e 7370 6f73 6564  ear, (Transposed
+00002b90: 2920 436f 6e76 2c20 4e6f 726d 616c 697a  ) Conv, Normaliz
+00002ba0: 6174 696f 6e2c 2050 5265 4c55 2c20 456d  ation, PReLU, Em
+00002bb0: 6265 6464 696e 672c 204d 756c 7469 6865  bedding, Multihe
+00002bc0: 6164 4174 7465 6e74 696f 6e2c 206e 6e2e  adAttention, nn.
+00002bd0: 5061 7261 6d65 7465 7273 2061 6e64 205b  Parameters and [
+00002be0: 6375 7374 6f6d 697a 6564 206d 6f64 756c  customized modul
+00002bf0: 6573 5d28 7465 7374 732f 7465 7374 5f63  es](tests/test_c
+00002c00: 7573 746f 6d69 7a65 645f 6c61 7965 722e  ustomized_layer.
+00002c10: 7079 292e 0a2d 205b 785d 2053 7570 706f  py)..- [x] Suppo
+00002c20: 7274 6564 206f 7065 7261 746f 7273 3a20  rted operators: 
+00002c30: 7370 6c69 742c 2063 6f6e 6361 7465 6e61  split, concatena
+00002c40: 7469 6f6e 2c20 736b 6970 2063 6f6e 6e65  tion, skip conne
+00002c50: 6374 696f 6e2c 2066 6c61 7474 656e 2c20  ction, flatten, 
+00002c60: 7265 7368 6170 652c 2076 6965 772c 2061  reshape, view, a
+00002c70: 6c6c 2065 6c65 6d65 6e74 2d77 6973 6520  ll element-wise 
+00002c80: 6f70 732c 2065 7463 2e0a 2d20 5b78 5d20  ops, etc..- [x] 
+00002c90: 5b4c 6f77 2d6c 6576 656c 2070 7275 6e69  [Low-level pruni
+00002ca0: 6e67 2066 756e 6374 696f 6e73 5d28 746f  ng functions](to
+00002cb0: 7263 685f 7072 756e 696e 672f 7072 756e  rch_pruning/prun
+00002cc0: 6572 2f66 756e 6374 696f 6e2e 7079 290a  er/function.py).
+00002cd0: 2d20 5b78 5d20 5b42 656e 6368 6d61 726b  - [x] [Benchmark
+00002ce0: 735d 2862 656e 6368 6d61 726b 7329 2061  s](benchmarks) a
+00002cf0: 6e64 205b 7475 746f 7269 616c 735d 2874  nd [tutorials](t
+00002d00: 7574 6f72 6961 6c73 290a 2d20 5b78 5d20  utorials).- [x] 
+00002d10: 4120 5b72 6573 6f75 7263 6520 6c69 7374  A [resource list
+00002d20: 5d28 7072 6163 7469 6361 6c5f 7374 7275  ](practical_stru
+00002d30: 6374 7572 616c 5f70 7275 6e69 6e67 2e6d  ctural_pruning.m
+00002d40: 6429 2066 6f72 2070 7261 6374 6963 616c  d) for practical
+00002d50: 2073 7472 7563 7472 7561 6c20 7072 756e   structrual prun
+00002d60: 696e 672e 0a20 200a 2323 2320 2a2a 544f  ing..  .### **TO
+00002d70: 444f 204c 6973 743a 2a2a 0a2d 205b 205d  DO List:**.- [ ]
+00002d80: 2041 2073 7472 6f6e 6720 6261 7365 6c69   A strong baseli
+00002d90: 6e65 2077 6974 6820 6261 6773 206f 6620  ne with bags of 
+00002da0: 7472 6963 6b73 2066 726f 6d20 6578 6973  tricks from exis
+00002db0: 7469 6e67 206d 6574 686f 6473 2e0a 2d20  ting methods..- 
+00002dc0: 5b20 5d20 4120 6265 6e63 686d 6172 6b20  [ ] A benchmark 
+00002dd0: 666f 7220 5b54 6f72 6368 7669 7369 6f6e  for [Torchvision
+00002de0: 5d28 6874 7470 733a 2f2f 7079 746f 7263  ](https://pytorc
+00002df0: 682e 6f72 672f 7669 7369 6f6e 2f73 7461  h.org/vision/sta
+00002e00: 626c 652f 6d6f 6465 6c73 2e68 746d 6c29  ble/models.html)
+00002e10: 2063 6f6d 7061 7469 6269 6c69 7479 2028   compatibility (
+00002e20: 2a2a 3831 2f38 353d 3935 2e33 252a 2a2c  **81/85=95.3%**,
+00002e30: 203a 6865 6176 795f 6368 6563 6b5f 6d61   :heavy_check_ma
+00002e40: 726b 3a29 2061 6e64 205b 7469 6d6d 5d28  rk:) and [timm](
+00002e50: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
+00002e60: 6f6d 2f68 7567 6769 6e67 6661 6365 2f70  om/huggingface/p
+00002e70: 7974 6f72 6368 2d69 6d61 6765 2d6d 6f64  ytorch-image-mod
+00002e80: 656c 7329 2063 6f6d 7061 7469 6269 6c69  els) compatibili
+00002e90: 7479 2e0a 2d20 5b20 5d20 5072 756e 696e  ty..- [ ] Prunin
+00002ea0: 6720 6672 6f6d 2053 6372 6174 6368 202f  g from Scratch /
+00002eb0: 2061 7420 496e 6974 6961 6c69 7a61 7469   at Initializati
+00002ec0: 6f6e 2e0a 2d20 5b20 5d20 4d6f 7265 2068  on..- [ ] More h
+00002ed0: 6967 682d 6c65 7665 6c20 7072 756e 6572  igh-level pruner
+00002ee0: 7320 6c69 6b65 205b 4669 7368 6572 5072  s like [FisherPr
+00002ef0: 756e 6572 5d28 6874 7470 733a 2f2f 6172  uner](https://ar
+00002f00: 7869 762e 6f72 672f 6162 732f 3231 3038  xiv.org/abs/2108
+00002f10: 2e30 3037 3038 292c 205b 4772 6f77 696e  .00708), [Growin
+00002f20: 6752 6567 5d28 6874 7470 733a 2f2f 6172  gReg](https://ar
+00002f30: 7869 762e 6f72 672f 6162 732f 3230 3132  xiv.org/abs/2012
+00002f40: 2e30 3932 3433 292c 2065 7463 2e0a 2d20  .09243), etc..- 
+00002f50: 5b20 5d20 4d6f 7265 2054 7261 6e73 666f  [ ] More Transfo
+00002f60: 726d 6572 7320 6c69 6b65 2056 6973 696f  rmers like Visio
+00002f70: 6e20 5472 616e 7366 6f72 6d65 7273 2028  n Transformers (
+00002f80: 3a68 6561 7679 5f63 6865 636b 5f6d 6172  :heavy_check_mar
+00002f90: 6b3a 292c 2053 7769 6e20 5472 616e 7366  k:), Swin Transf
+00002fa0: 6f72 6d65 7273 2c20 506f 6f6c 466f 726d  ormers, PoolForm
+00002fb0: 6572 732e 0a2d 205b 205d 2042 6c6f 636b  ers..- [ ] Block
+00002fc0: 2f4c 6179 6572 2f44 6570 7468 2050 7275  /Layer/Depth Pru
+00002fd0: 6e69 6e67 0a2d 205b 205d 2050 7275 6e69  ning.- [ ] Pruni
+00002fe0: 6e67 2062 656e 6368 6d61 726b 7320 666f  ng benchmarks fo
+00002ff0: 7220 4349 4641 522c 2049 6d61 6765 4e65  r CIFAR, ImageNe
+00003000: 7420 616e 6420 434f 434f 2e0a 0a23 2320  t and COCO...## 
+00003010: 496e 7374 616c 6c61 7469 6f6e 0a0a 546f  Installation..To
+00003020: 7263 682d 5072 756e 696e 6720 6973 2063  rch-Pruning is c
+00003030: 6f6d 7061 7469 626c 6520 7769 7468 2050  ompatible with P
+00003040: 7954 6f72 6368 2031 2e78 2061 6e64 2032  yTorch 1.x and 2
+00003050: 2e78 2e20 2a2a 5079 546f 7263 6820 312e  .x. **PyTorch 1.
+00003060: 3132 2e31 2069 7320 7265 636f 6d6d 656e  12.1 is recommen
+00003070: 6465 6421 2a2a 0a0a 6060 6062 6173 680a  ded!**..```bash.
+00003080: 7069 7020 696e 7374 616c 6c20 746f 7263  pip install torc
+00003090: 682d 7072 756e 696e 6720 2320 7631 2e31  h-pruning # v1.1
+000030a0: 2e39 0a60 6060 0a6f 720a 6060 6062 6173  .9.```.or.```bas
+000030b0: 680a 6769 7420 636c 6f6e 6520 6874 7470  h.git clone http
+000030c0: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
+000030d0: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
+000030e0: 6e67 2e67 6974 0a60 6060 0a0a 2323 2051  ng.git.```..## Q
+000030f0: 7569 636b 7374 6172 740a 2020 0a48 6572  uickstart.  .Her
+00003100: 6520 7765 2070 726f 7669 6465 2061 2071  e we provide a q
+00003110: 7569 636b 2073 7461 7274 2066 6f72 2054  uick start for T
+00003120: 6f72 6368 2d50 7275 6e69 6e67 2e20 4d6f  orch-Pruning. Mo
+00003130: 7265 2065 7870 6c61 696e 6564 2064 6574  re explained det
+00003140: 6169 6c73 2063 616e 2062 6520 666f 756e  ails can be foun
+00003150: 6420 696e 205b 7475 746f 7261 6c73 5d28  d in [tutorals](
+00003160: 2e2f 7475 746f 7269 616c 732f 290a 0a23  ./tutorials/)..#
+00003170: 2323 2030 2e20 486f 7720 4974 2057 6f72  ## 0. How It Wor
+00003180: 6b73 0a0a 496e 2073 7472 7563 7475 7261  ks..In structura
+00003190: 6c20 7072 756e 696e 672c 2061 2060 6047  l pruning, a ``G
+000031a0: 726f 7570 6060 2069 7320 6465 6669 6e65  roup`` is define
+000031b0: 6420 6173 2074 6865 206d 696e 696d 616c  d as the minimal
+000031c0: 2072 656d 6f76 6162 6c65 2075 6e69 7420   removable unit 
+000031d0: 7769 7468 696e 2064 6565 7020 6e65 7477  within deep netw
+000031e0: 6f72 6b73 2e20 4561 6368 2067 726f 7570  orks. Each group
+000031f0: 2063 6f6e 7369 7374 7320 6f66 206d 756c   consists of mul
+00003200: 7469 706c 6520 696e 7465 7264 6570 656e  tiple interdepen
+00003210: 6465 6e74 206c 6179 6572 7320 7468 6174  dent layers that
+00003220: 206e 6565 6420 746f 2062 6520 7072 756e   need to be prun
+00003230: 6564 2073 696d 756c 7461 6e65 6f75 736c  ed simultaneousl
+00003240: 7920 696e 206f 7264 6572 2074 6f20 7072  y in order to pr
+00003250: 6573 6572 7665 2074 6865 2069 6e74 6567  eserve the integ
+00003260: 7269 7479 206f 6620 7468 6520 7265 7375  rity of the resu
+00003270: 6c74 696e 6720 7374 7275 6374 7572 6573  lting structures
+00003280: 2e20 486f 7765 7665 722c 2064 6565 7020  . However, deep 
+00003290: 6e65 7477 6f72 6b73 206f 6674 656e 2065  networks often e
+000032a0: 7868 6962 6974 2069 6e74 7269 6361 7465  xhibit intricate
+000032b0: 2064 6570 656e 6465 6e63 6965 7320 616d   dependencies am
+000032c0: 6f6e 6720 6c61 7965 7273 2c20 706f 7369  ong layers, posi
+000032d0: 6e67 2061 2073 6967 6e69 6669 6361 6e74  ng a significant
+000032e0: 2063 6861 6c6c 656e 6765 2066 6f72 2073   challenge for s
+000032f0: 7472 7563 7475 7261 6c20 7072 756e 696e  tructural prunin
+00003300: 672e 2054 6869 7320 776f 726b 2074 6163  g. This work tac
+00003310: 6b6c 6573 2074 6869 7320 6368 616c 6c65  kles this challe
+00003320: 6e67 6520 6279 2069 6e74 726f 6475 6369  nge by introduci
+00003330: 6e67 2061 6e20 6175 746f 6d61 7465 6420  ng an automated 
+00003340: 6d65 6368 616e 6973 6d20 6361 6c6c 6564  mechanism called
+00003350: 2060 6044 6570 4772 6170 6860 602c 2077   ``DepGraph``, w
+00003360: 6869 6368 2065 6e61 626c 6573 2065 6666  hich enables eff
+00003370: 6f72 746c 6573 7320 7061 7261 6d65 7465  ortless paramete
+00003380: 7220 6772 6f75 7069 6e67 2061 6e64 2066  r grouping and f
+00003390: 6163 696c 6974 6174 6573 2070 7275 6e69  acilitates pruni
+000033a0: 6e67 2066 6f72 2061 2064 6976 6572 7365  ng for a diverse
+000033b0: 2072 616e 6765 206f 6620 6465 6570 206e   range of deep n
+000033c0: 6574 776f 726b 732e 0a0a 3c64 6976 2061  etworks...<div a
+000033d0: 6c69 676e 3d22 6365 6e74 6572 223e 0a3c  lign="center">.<
+000033e0: 696d 6720 7372 633d 2261 7373 6574 732f  img src="assets/
+000033f0: 6465 702e 706e 6722 2077 6964 7468 3d22  dep.png" width="
+00003400: 3130 3025 223e 0a3c 2f64 6976 3e0a 0a23  100%">.</div>..#
+00003410: 2323 2031 2e20 4120 4d69 6e69 6d61 6c20  ## 1. A Minimal 
+00003420: 4578 616d 706c 650a 0a60 6060 7079 7468  Example..```pyth
+00003430: 6f6e 0a69 6d70 6f72 7420 746f 7263 680a  on.import torch.
+00003440: 6672 6f6d 2074 6f72 6368 7669 7369 6f6e  from torchvision
+00003450: 2e6d 6f64 656c 7320 696d 706f 7274 2072  .models import r
+00003460: 6573 6e65 7431 380a 696d 706f 7274 2074  esnet18.import t
+00003470: 6f72 6368 5f70 7275 6e69 6e67 2061 7320  orch_pruning as 
+00003480: 7470 0a0a 6d6f 6465 6c20 3d20 7265 736e  tp..model = resn
+00003490: 6574 3138 2870 7265 7472 6169 6e65 643d  et18(pretrained=
+000034a0: 5472 7565 292e 6576 616c 2829 0a0a 2320  True).eval()..# 
+000034b0: 312e 2062 7569 6c64 2064 6570 656e 6465  1. build depende
+000034c0: 6e63 7920 6772 6170 6820 666f 7220 7265  ncy graph for re
+000034d0: 736e 6574 3138 0a44 4720 3d20 7470 2e44  snet18.DG = tp.D
+000034e0: 6570 656e 6465 6e63 7947 7261 7068 2829  ependencyGraph()
+000034f0: 2e62 7569 6c64 5f64 6570 656e 6465 6e63  .build_dependenc
+00003500: 7928 6d6f 6465 6c2c 2065 7861 6d70 6c65  y(model, example
+00003510: 5f69 6e70 7574 733d 746f 7263 682e 7261  _inputs=torch.ra
+00003520: 6e64 6e28 312c 332c 3232 342c 3232 3429  ndn(1,3,224,224)
+00003530: 290a 0a23 2032 2e20 5370 6563 6966 7920  )..# 2. Specify 
+00003540: 7468 6520 746f 2d62 652d 7072 756e 6564  the to-be-pruned
+00003550: 2063 6861 6e6e 656c 732e 2048 6572 6520   channels. Here 
+00003560: 7765 2070 7275 6e65 2074 686f 7365 2063  we prune those c
+00003570: 6861 6e6e 656c 7320 696e 6465 7865 6420  hannels indexed 
+00003580: 6279 205b 322c 2036 2c20 395d 2e0a 6772  by [2, 6, 9]..gr
+00003590: 6f75 7020 3d20 4447 2e67 6574 5f70 7275  oup = DG.get_pru
+000035a0: 6e69 6e67 5f67 726f 7570 2820 6d6f 6465  ning_group( mode
+000035b0: 6c2e 636f 6e76 312c 2074 702e 7072 756e  l.conv1, tp.prun
+000035c0: 655f 636f 6e76 5f6f 7574 5f63 6861 6e6e  e_conv_out_chann
+000035d0: 656c 732c 2069 6478 733d 5b32 2c20 362c  els, idxs=[2, 6,
+000035e0: 2039 5d20 290a 0a23 2033 2e20 7072 756e   9] )..# 3. prun
+000035f0: 6520 616c 6c20 6772 6f75 7065 6420 6c61  e all grouped la
+00003600: 7965 7273 2074 6861 7420 6172 6520 636f  yers that are co
+00003610: 7570 6c65 6420 7769 7468 206d 6f64 656c  upled with model
+00003620: 2e63 6f6e 7631 2028 696e 636c 7564 6564  .conv1 (included
+00003630: 292e 0a69 6620 4447 2e63 6865 636b 5f70  )..if DG.check_p
+00003640: 7275 6e69 6e67 5f67 726f 7570 2867 726f  runing_group(gro
+00003650: 7570 293a 2023 2061 766f 6964 2066 756c  up): # avoid ful
+00003660: 6c20 7072 756e 696e 672c 2069 2e65 2e2c  l pruning, i.e.,
+00003670: 2063 6861 6e6e 656c 733d 302e 0a20 2020   channels=0..   
+00003680: 2067 726f 7570 2e70 7275 6e65 2829 0a20   group.prune(). 
+00003690: 2020 200a 2320 342e 2053 6176 6520 2620     .# 4. Save & 
+000036a0: 4c6f 6164 0a6d 6f64 656c 2e7a 6572 6f5f  Load.model.zero_
+000036b0: 6772 6164 2829 2023 2057 6520 646f 6e27  grad() # We don'
+000036c0: 7420 7761 6e74 2074 6f20 7374 6f72 6520  t want to store 
+000036d0: 6772 6164 6965 6e74 2069 6e66 6f72 6d61  gradient informa
+000036e0: 7469 6f6e 0a74 6f72 6368 2e73 6176 6528  tion.torch.save(
+000036f0: 6d6f 6465 6c2c 2027 6d6f 6465 6c2e 7074  model, 'model.pt
+00003700: 6827 2920 2320 7769 7468 6f75 7420 2e73  h') # without .s
+00003710: 7461 7465 5f64 6963 740a 6d6f 6465 6c20  tate_dict.model 
+00003720: 3d20 746f 7263 682e 6c6f 6164 2827 6d6f  = torch.load('mo
+00003730: 6465 6c2e 7074 6827 2920 2320 6c6f 6164  del.pth') # load
+00003740: 2074 6865 206d 6f64 656c 206f 626a 6563   the model objec
+00003750: 740a 6060 600a 2020 0a54 6865 2061 626f  t.```.  .The abo
+00003760: 7665 2065 7861 6d70 6c65 2064 656d 6f6e  ve example demon
+00003770: 7374 7261 7465 7320 7468 6520 6675 6e64  strates the fund
+00003780: 616d 656e 7461 6c20 7072 756e 696e 6720  amental pruning 
+00003790: 7069 7065 6c69 6e65 2075 7369 6e67 2044  pipeline using D
+000037a0: 6570 4772 6170 682e 2054 6865 2074 6172  epGraph. The tar
+000037b0: 6765 7420 6c61 7965 7220 7265 736e 6574  get layer resnet
+000037c0: 2e63 6f6e 7631 2069 7320 636f 7570 6c65  .conv1 is couple
+000037d0: 6420 7769 7468 2073 6576 6572 616c 206c  d with several l
+000037e0: 6179 6572 732c 2077 6869 6368 2072 6571  ayers, which req
+000037f0: 7569 7265 7320 7369 6d75 6c74 616e 656f  uires simultaneo
+00003800: 7573 2072 656d 6f76 616c 2069 6e20 7374  us removal in st
+00003810: 7275 6374 7572 616c 2070 7275 6e69 6e67  ructural pruning
+00003820: 2e20 4c65 7427 7320 7072 696e 7420 7468  . Let's print th
+00003830: 6520 6772 6f75 7020 616e 6420 6f62 7365  e group and obse
+00003840: 7276 6520 686f 7720 6120 7072 756e 696e  rve how a prunin
+00003850: 6720 6f70 6572 6174 696f 6e20 2274 7269  g operation "tri
+00003860: 6767 6572 7322 206f 7468 6572 206f 6e65  ggers" other one
+00003870: 732e 2049 6e20 7468 6520 666f 6c6c 6f77  s. In the follow
+00003880: 696e 6720 6f75 7470 7574 732c 2060 6041  ing outputs, ``A
+00003890: 203d 3e20 4260 6020 6d65 616e 7320 7468   => B`` means th
+000038a0: 6520 7072 756e 696e 6720 6f70 6572 6174  e pruning operat
+000038b0: 696f 6e20 6060 4160 6020 7472 6967 6765  ion ``A`` trigge
+000038c0: 7273 2074 6865 2070 7275 6e69 6e67 206f  rs the pruning o
+000038d0: 7065 7261 7469 6f6e 2060 6042 6060 2e20  peration ``B``. 
+000038e0: 6772 6f75 705b 305d 2072 6566 6572 7320  group[0] refers 
+000038f0: 746f 2074 6865 2070 7275 6e69 6e67 2072  to the pruning r
+00003900: 6f6f 7420 696e 2060 6044 472e 6765 745f  oot in ``DG.get_
+00003910: 7072 756e 696e 675f 6772 6f75 7060 602e  pruning_group``.
+00003920: 0a0a 6060 600a 2d2d 2d2d 2d2d 2d2d 2d2d  ..```.----------
 00003930: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00003940: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 0a5b 305d  ------------.[0]
-00003950: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
-00003960: 656c 7320 6f6e 2063 6f6e 7631 2028 436f  els on conv1 (Co
-00003970: 6e76 3264 2833 2c20 3634 2c20 6b65 726e  nv2d(3, 64, kern
-00003980: 656c 5f73 697a 653d 2837 2c20 3729 2c20  el_size=(7, 7), 
-00003990: 7374 7269 6465 3d28 322c 2032 292c 2070  stride=(2, 2), p
-000039a0: 6164 6469 6e67 3d28 332c 2033 292c 2062  adding=(3, 3), b
-000039b0: 6961 733d 4661 6c73 6529 2920 3d3e 2070  ias=False)) => p
-000039c0: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-000039d0: 7320 6f6e 2063 6f6e 7631 2028 436f 6e76  s on conv1 (Conv
-000039e0: 3264 2833 2c20 3634 2c20 6b65 726e 656c  2d(3, 64, kernel
-000039f0: 5f73 697a 653d 2837 2c20 3729 2c20 7374  _size=(7, 7), st
-00003a00: 7269 6465 3d28 322c 2032 292c 2070 6164  ride=(2, 2), pad
-00003a10: 6469 6e67 3d28 332c 2033 292c 2062 6961  ding=(3, 3), bia
-00003a20: 733d 4661 6c73 6529 292c 2069 6478 733d  s=False)), idxs=
-00003a30: 5b32 2c20 362c 2039 5d20 2850 7275 6e69  [2, 6, 9] (Pruni
-00003a40: 6e67 2052 6f6f 7429 0a5b 315d 2070 7275  ng Root).[1] pru
-00003a50: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
-00003a60: 6f6e 2063 6f6e 7631 2028 436f 6e76 3264  on conv1 (Conv2d
-00003a70: 2833 2c20 3634 2c20 6b65 726e 656c 5f73  (3, 64, kernel_s
-00003a80: 697a 653d 2837 2c20 3729 2c20 7374 7269  ize=(7, 7), stri
-00003a90: 6465 3d28 322c 2032 292c 2070 6164 6469  de=(2, 2), paddi
-00003aa0: 6e67 3d28 332c 2033 292c 2062 6961 733d  ng=(3, 3), bias=
-00003ab0: 4661 6c73 6529 2920 3d3e 2070 7275 6e65  False)) => prune
-00003ac0: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
-00003ad0: 2062 6e31 2028 4261 7463 684e 6f72 6d32   bn1 (BatchNorm2
-00003ae0: 6428 3634 2c20 6570 733d 3165 2d30 352c  d(64, eps=1e-05,
-00003af0: 206d 6f6d 656e 7475 6d3d 302e 312c 2061   momentum=0.1, a
-00003b00: 6666 696e 653d 5472 7565 2c20 7472 6163  ffine=True, trac
-00003b10: 6b5f 7275 6e6e 696e 675f 7374 6174 733d  k_running_stats=
-00003b20: 5472 7565 2929 2c20 6964 7873 3d5b 322c  True)), idxs=[2,
-00003b30: 2036 2c20 395d 0a5b 325d 2070 7275 6e65   6, 9].[2] prune
-00003b40: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
-00003b50: 2062 6e31 2028 4261 7463 684e 6f72 6d32   bn1 (BatchNorm2
-00003b60: 6428 3634 2c20 6570 733d 3165 2d30 352c  d(64, eps=1e-05,
-00003b70: 206d 6f6d 656e 7475 6d3d 302e 312c 2061   momentum=0.1, a
-00003b80: 6666 696e 653d 5472 7565 2c20 7472 6163  ffine=True, trac
-00003b90: 6b5f 7275 6e6e 696e 675f 7374 6174 733d  k_running_stats=
-00003ba0: 5472 7565 2929 203d 3e20 7072 756e 655f  True)) => prune_
-00003bb0: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-00003bc0: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f32  _ElementWiseOp_2
-00003bd0: 3028 5265 6c75 4261 636b 7761 7264 3029  0(ReluBackward0)
-00003be0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
-00003bf0: 0a5b 335d 2070 7275 6e65 5f6f 7574 5f63  .[3] prune_out_c
-00003c00: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
-00003c10: 656e 7457 6973 654f 705f 3230 2852 656c  entWiseOp_20(Rel
-00003c20: 7542 6163 6b77 6172 6430 2920 3d3e 2070  uBackward0) => p
-00003c30: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-00003c40: 7320 6f6e 205f 456c 656d 656e 7457 6973  s on _ElementWis
-00003c50: 654f 705f 3139 284d 6178 506f 6f6c 3244  eOp_19(MaxPool2D
-00003c60: 5769 7468 496e 6469 6365 7342 6163 6b77  WithIndicesBackw
-00003c70: 6172 6430 292c 2069 6478 733d 5b32 2c20  ard0), idxs=[2, 
-00003c80: 362c 2039 5d0a 5b34 5d20 7072 756e 655f  6, 9].[4] prune_
-00003c90: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-00003ca0: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f31  _ElementWiseOp_1
-00003cb0: 3928 4d61 7850 6f6f 6c32 4457 6974 6849  9(MaxPool2DWithI
-00003cc0: 6e64 6963 6573 4261 636b 7761 7264 3029  ndicesBackward0)
-00003cd0: 203d 3e20 7072 756e 655f 6f75 745f 6368   => prune_out_ch
-00003ce0: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
-00003cf0: 6e74 5769 7365 4f70 5f31 3828 4164 6442  ntWiseOp_18(AddB
-00003d00: 6163 6b77 6172 6430 292c 2069 6478 733d  ackward0), idxs=
-00003d10: 5b32 2c20 362c 2039 5d0a 5b35 5d20 7072  [2, 6, 9].[5] pr
-00003d20: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-00003d30: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
-00003d40: 4f70 5f31 3928 4d61 7850 6f6f 6c32 4457  Op_19(MaxPool2DW
-00003d50: 6974 6849 6e64 6963 6573 4261 636b 7761  ithIndicesBackwa
-00003d60: 7264 3029 203d 3e20 7072 756e 655f 696e  rd0) => prune_in
-00003d70: 5f63 6861 6e6e 656c 7320 6f6e 206c 6179  _channels on lay
-00003d80: 6572 312e 302e 636f 6e76 3120 2843 6f6e  er1.0.conv1 (Con
-00003d90: 7632 6428 3634 2c20 3634 2c20 6b65 726e  v2d(64, 64, kern
-00003da0: 656c 5f73 697a 653d 2833 2c20 3329 2c20  el_size=(3, 3), 
-00003db0: 7374 7269 6465 3d28 312c 2031 292c 2070  stride=(1, 1), p
-00003dc0: 6164 6469 6e67 3d28 312c 2031 292c 2062  adding=(1, 1), b
-00003dd0: 6961 733d 4661 6c73 6529 292c 2069 6478  ias=False)), idx
-00003de0: 733d 5b32 2c20 362c 2039 5d0a 5b36 5d20  s=[2, 6, 9].[6] 
-00003df0: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-00003e00: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-00003e10: 7365 4f70 5f31 3828 4164 6442 6163 6b77  seOp_18(AddBackw
-00003e20: 6172 6430 2920 3d3e 2070 7275 6e65 5f6f  ard0) => prune_o
-00003e30: 7574 5f63 6861 6e6e 656c 7320 6f6e 206c  ut_channels on l
-00003e40: 6179 6572 312e 302e 626e 3220 2842 6174  ayer1.0.bn2 (Bat
-00003e50: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
-00003e60: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
-00003e70: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
-00003e80: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
-00003e90: 5f73 7461 7473 3d54 7275 6529 292c 2069  _stats=True)), i
-00003ea0: 6478 733d 5b32 2c20 362c 2039 5d0a 5b37  dxs=[2, 6, 9].[7
-00003eb0: 5d20 7072 756e 655f 6f75 745f 6368 616e  ] prune_out_chan
-00003ec0: 6e65 6c73 206f 6e20 5f45 6c65 6d65 6e74  nels on _Element
-00003ed0: 5769 7365 4f70 5f31 3828 4164 6442 6163  WiseOp_18(AddBac
-00003ee0: 6b77 6172 6430 2920 3d3e 2070 7275 6e65  kward0) => prune
-00003ef0: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
-00003f00: 205f 456c 656d 656e 7457 6973 654f 705f   _ElementWiseOp_
-00003f10: 3137 2852 656c 7542 6163 6b77 6172 6430  17(ReluBackward0
-00003f20: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
-00003f30: 5d0a 5b38 5d20 7072 756e 655f 6f75 745f  ].[8] prune_out_
-00003f40: 6368 616e 6e65 6c73 206f 6e20 5f45 6c65  channels on _Ele
-00003f50: 6d65 6e74 5769 7365 4f70 5f31 3728 5265  mentWiseOp_17(Re
-00003f60: 6c75 4261 636b 7761 7264 3029 203d 3e20  luBackward0) => 
-00003f70: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-00003f80: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-00003f90: 7365 4f70 5f31 3628 4164 6442 6163 6b77  seOp_16(AddBackw
-00003fa0: 6172 6430 292c 2069 6478 733d 5b32 2c20  ard0), idxs=[2, 
-00003fb0: 362c 2039 5d0a 5b39 5d20 7072 756e 655f  6, 9].[9] prune_
-00003fc0: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-00003fd0: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f31  _ElementWiseOp_1
-00003fe0: 3728 5265 6c75 4261 636b 7761 7264 3029  7(ReluBackward0)
-00003ff0: 203d 3e20 7072 756e 655f 696e 5f63 6861   => prune_in_cha
-00004000: 6e6e 656c 7320 6f6e 206c 6179 6572 312e  nnels on layer1.
-00004010: 312e 636f 6e76 3120 2843 6f6e 7632 6428  1.conv1 (Conv2d(
-00004020: 3634 2c20 3634 2c20 6b65 726e 656c 5f73  64, 64, kernel_s
-00004030: 697a 653d 2833 2c20 3329 2c20 7374 7269  ize=(3, 3), stri
-00004040: 6465 3d28 312c 2031 292c 2070 6164 6469  de=(1, 1), paddi
-00004050: 6e67 3d28 312c 2031 292c 2062 6961 733d  ng=(1, 1), bias=
-00004060: 4661 6c73 6529 292c 2069 6478 733d 5b32  False)), idxs=[2
-00004070: 2c20 362c 2039 5d0a 5b31 305d 2070 7275  , 6, 9].[10] pru
-00004080: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
-00004090: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
-000040a0: 705f 3136 2841 6464 4261 636b 7761 7264  p_16(AddBackward
-000040b0: 3029 203d 3e20 7072 756e 655f 6f75 745f  0) => prune_out_
-000040c0: 6368 616e 6e65 6c73 206f 6e20 6c61 7965  channels on laye
-000040d0: 7231 2e31 2e62 6e32 2028 4261 7463 684e  r1.1.bn2 (BatchN
-000040e0: 6f72 6d32 6428 3634 2c20 6570 733d 3165  orm2d(64, eps=1e
-000040f0: 2d30 352c 206d 6f6d 656e 7475 6d3d 302e  -05, momentum=0.
-00004100: 312c 2061 6666 696e 653d 5472 7565 2c20  1, affine=True, 
-00004110: 7472 6163 6b5f 7275 6e6e 696e 675f 7374  track_running_st
-00004120: 6174 733d 5472 7565 2929 2c20 6964 7873  ats=True)), idxs
-00004130: 3d5b 322c 2036 2c20 395d 0a5b 3131 5d20  =[2, 6, 9].[11] 
-00004140: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-00004150: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-00004160: 7365 4f70 5f31 3628 4164 6442 6163 6b77  seOp_16(AddBackw
-00004170: 6172 6430 2920 3d3e 2070 7275 6e65 5f6f  ard0) => prune_o
-00004180: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
-00004190: 456c 656d 656e 7457 6973 654f 705f 3135  ElementWiseOp_15
-000041a0: 2852 656c 7542 6163 6b77 6172 6430 292c  (ReluBackward0),
-000041b0: 2069 6478 733d 5b32 2c20 362c 2039 5d0a   idxs=[2, 6, 9].
-000041c0: 5b31 325d 2070 7275 6e65 5f6f 7574 5f63  [12] prune_out_c
-000041d0: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
-000041e0: 656e 7457 6973 654f 705f 3135 2852 656c  entWiseOp_15(Rel
-000041f0: 7542 6163 6b77 6172 6430 2920 3d3e 2070  uBackward0) => p
-00004200: 7275 6e65 5f69 6e5f 6368 616e 6e65 6c73  rune_in_channels
-00004210: 206f 6e20 6c61 7965 7232 2e30 2e64 6f77   on layer2.0.dow
-00004220: 6e73 616d 706c 652e 3020 2843 6f6e 7632  nsample.0 (Conv2
-00004230: 6428 3634 2c20 3132 382c 206b 6572 6e65  d(64, 128, kerne
-00004240: 6c5f 7369 7a65 3d28 312c 2031 292c 2073  l_size=(1, 1), s
-00004250: 7472 6964 653d 2832 2c20 3229 2c20 6269  tride=(2, 2), bi
-00004260: 6173 3d46 616c 7365 2929 2c20 6964 7873  as=False)), idxs
-00004270: 3d5b 322c 2036 2c20 395d 0a5b 3133 5d20  =[2, 6, 9].[13] 
-00004280: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-00004290: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-000042a0: 7365 4f70 5f31 3528 5265 6c75 4261 636b  seOp_15(ReluBack
-000042b0: 7761 7264 3029 203d 3e20 7072 756e 655f  ward0) => prune_
-000042c0: 696e 5f63 6861 6e6e 656c 7320 6f6e 206c  in_channels on l
-000042d0: 6179 6572 322e 302e 636f 6e76 3120 2843  ayer2.0.conv1 (C
-000042e0: 6f6e 7632 6428 3634 2c20 3132 382c 206b  onv2d(64, 128, k
-000042f0: 6572 6e65 6c5f 7369 7a65 3d28 332c 2033  ernel_size=(3, 3
-00004300: 292c 2073 7472 6964 653d 2832 2c20 3229  ), stride=(2, 2)
-00004310: 2c20 7061 6464 696e 673d 2831 2c20 3129  , padding=(1, 1)
-00004320: 2c20 6269 6173 3d46 616c 7365 2929 2c20  , bias=False)), 
-00004330: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
-00004340: 3134 5d20 7072 756e 655f 6f75 745f 6368  14] prune_out_ch
-00004350: 616e 6e65 6c73 206f 6e20 6c61 7965 7231  annels on layer1
-00004360: 2e31 2e62 6e32 2028 4261 7463 684e 6f72  .1.bn2 (BatchNor
-00004370: 6d32 6428 3634 2c20 6570 733d 3165 2d30  m2d(64, eps=1e-0
-00004380: 352c 206d 6f6d 656e 7475 6d3d 302e 312c  5, momentum=0.1,
-00004390: 2061 6666 696e 653d 5472 7565 2c20 7472   affine=True, tr
-000043a0: 6163 6b5f 7275 6e6e 696e 675f 7374 6174  ack_running_stat
-000043b0: 733d 5472 7565 2929 203d 3e20 7072 756e  s=True)) => prun
-000043c0: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
-000043d0: 6e20 6c61 7965 7231 2e31 2e63 6f6e 7632  n layer1.1.conv2
-000043e0: 2028 436f 6e76 3264 2836 342c 2036 342c   (Conv2d(64, 64,
-000043f0: 206b 6572 6e65 6c5f 7369 7a65 3d28 332c   kernel_size=(3,
-00004400: 2033 292c 2073 7472 6964 653d 2831 2c20   3), stride=(1, 
-00004410: 3129 2c20 7061 6464 696e 673d 2831 2c20  1), padding=(1, 
-00004420: 3129 2c20 6269 6173 3d46 616c 7365 2929  1), bias=False))
-00004430: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
-00004440: 0a5b 3135 5d20 7072 756e 655f 6f75 745f  .[15] prune_out_
-00004450: 6368 616e 6e65 6c73 206f 6e20 6c61 7965  channels on laye
-00004460: 7231 2e30 2e62 6e32 2028 4261 7463 684e  r1.0.bn2 (BatchN
-00004470: 6f72 6d32 6428 3634 2c20 6570 733d 3165  orm2d(64, eps=1e
-00004480: 2d30 352c 206d 6f6d 656e 7475 6d3d 302e  -05, momentum=0.
-00004490: 312c 2061 6666 696e 653d 5472 7565 2c20  1, affine=True, 
-000044a0: 7472 6163 6b5f 7275 6e6e 696e 675f 7374  track_running_st
-000044b0: 6174 733d 5472 7565 2929 203d 3e20 7072  ats=True)) => pr
-000044c0: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-000044d0: 206f 6e20 6c61 7965 7231 2e30 2e63 6f6e   on layer1.0.con
-000044e0: 7632 2028 436f 6e76 3264 2836 342c 2036  v2 (Conv2d(64, 6
-000044f0: 342c 206b 6572 6e65 6c5f 7369 7a65 3d28  4, kernel_size=(
-00004500: 332c 2033 292c 2073 7472 6964 653d 2831  3, 3), stride=(1
-00004510: 2c20 3129 2c20 7061 6464 696e 673d 2831  , 1), padding=(1
-00004520: 2c20 3129 2c20 6269 6173 3d46 616c 7365  , 1), bias=False
-00004530: 2929 2c20 6964 7873 3d5b 322c 2036 2c20  )), idxs=[2, 6, 
-00004540: 395d 0a2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  9].-------------
-00004550: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00004560: 2d2d 2d0a 6060 600a 466f 7220 6d6f 7265  ---.```.For more
-00004570: 2064 6574 6169 6c73 2061 626f 7574 2067   details about g
-00004580: 726f 7570 696e 672c 2070 6c65 6173 6520  rouping, please 
-00004590: 7265 6665 7220 746f 205b 7475 746f 7269  refer to [tutori
-000045a0: 616c 732f 3220 2d20 4578 706c 6f72 696e  als/2 - Explorin
-000045b0: 6720 4465 7065 6e64 656e 6379 2047 726f  g Dependency Gro
-000045c0: 7570 735d 2868 7474 7073 3a2f 2f67 6974  ups](https://git
-000045d0: 6875 622e 636f 6d2f 5661 696e 462f 546f  hub.com/VainF/To
-000045e0: 7263 682d 5072 756e 696e 672f 626c 6f62  rch-Pruning/blob
-000045f0: 2f6d 6173 7465 722f 7475 746f 7269 616c  /master/tutorial
-00004600: 732f 3225 3230 2d25 3230 4578 706c 6f72  s/2%20-%20Explor
-00004610: 696e 6725 3230 4465 7065 6e64 656e 6379  ing%20Dependency
-00004620: 2532 3047 726f 7570 732e 6970 796e 6229  %20Groups.ipynb)
-00004630: 0a20 200a 2323 2323 2048 6f77 2074 6f20  .  .#### How to 
-00004640: 7363 616e 2061 6c6c 2067 726f 7570 7320  scan all groups 
-00004650: 2841 6476 616e 6365 6429 3a0a 5765 2063  (Advanced):.We c
-00004660: 616e 2075 7365 2060 6044 472e 6765 745f  an use ``DG.get_
-00004670: 616c 6c5f 6772 6f75 7073 2869 676e 6f72  all_groups(ignor
-00004680: 6564 5f6c 6179 6572 732c 2072 6f6f 745f  ed_layers, root_
-00004690: 6d6f 6475 6c65 5f74 7970 6573 2960 6020  module_types)`` 
-000046a0: 746f 2073 6361 6e20 616c 6c20 6772 6f75  to scan all grou
-000046b0: 7073 2073 6571 7565 6e74 6961 6c6c 792e  ps sequentially.
-000046c0: 2045 6163 6820 6772 6f75 7020 7769 6c6c   Each group will
-000046d0: 2062 6567 696e 2077 6974 6820 6120 6c61   begin with a la
-000046e0: 7965 7220 7468 6174 206d 6174 6368 6573  yer that matches
-000046f0: 2061 2074 7970 6520 696e 2074 6865 2022   a type in the "
-00004700: 726f 6f74 5f6d 6f64 756c 655f 7479 7065  root_module_type
-00004710: 7322 2070 6172 616d 6574 6572 2e20 4e6f  s" parameter. No
-00004720: 7465 2074 6861 7420 4447 2e67 6574 5f61  te that DG.get_a
-00004730: 6c6c 5f67 726f 7570 7320 6973 206f 6e6c  ll_groups is onl
-00004740: 7920 7265 7370 6f6e 7369 626c 6520 666f  y responsible fo
-00004750: 7220 6772 6f75 7069 6e67 2061 6e64 2064  r grouping and d
-00004760: 6f65 7320 6e6f 7420 6861 7665 2061 6e79  oes not have any
-00004770: 206b 6e6f 776c 6564 6765 206f 7220 756e   knowledge or un
-00004780: 6465 7273 7461 6e64 696e 6720 6f66 2077  derstanding of w
-00004790: 6869 6368 2070 6172 616d 6574 6572 7320  hich parameters 
-000047a0: 7368 6f75 6c64 2062 6520 7072 756e 6564  should be pruned
-000047b0: 2e20 5468 6572 6566 6f72 652c 2069 7420  . Therefore, it 
-000047c0: 6973 206e 6563 6573 7361 7279 2074 6f20  is necessary to 
-000047d0: 7370 6563 6966 7920 7468 6520 7072 756e  specify the prun
-000047e0: 696e 6720 6964 7873 2075 7369 6e67 2020  ing idxs using  
-000047f0: 6060 6772 6f75 702e 7072 756e 6528 6964  ``group.prune(id
-00004800: 7873 3d69 6478 7329 6060 2e0a 0a60 6060  xs=idxs)``...```
-00004810: 7079 7468 6f6e 0a66 6f72 2067 726f 7570  python.for group
-00004820: 2069 6e20 4447 2e67 6574 5f61 6c6c 5f67   in DG.get_all_g
-00004830: 726f 7570 7328 6967 6e6f 7265 645f 6c61  roups(ignored_la
-00004840: 7965 7273 3d5b 6d6f 6465 6c2e 636f 6e76  yers=[model.conv
-00004850: 315d 2c20 726f 6f74 5f6d 6f64 756c 655f  1], root_module_
-00004860: 7479 7065 733d 5b6e 6e2e 436f 6e76 3264  types=[nn.Conv2d
-00004870: 2c20 6e6e 2e4c 696e 6561 725d 293a 0a20  , nn.Linear]):. 
-00004880: 2020 2023 2068 616e 646c 6520 6772 6f75     # handle grou
-00004890: 7073 2069 6e20 7365 7175 656e 7469 616c  ps in sequential
-000048a0: 206f 7264 6572 0a20 2020 2069 6478 7320   order.    idxs 
-000048b0: 3d20 5b32 2c34 2c36 5d20 2320 796f 7572  = [2,4,6] # your
-000048c0: 2070 7275 6e69 6e67 2069 6e64 6963 6573   pruning indices
-000048d0: 0a20 2020 2067 726f 7570 2e70 7275 6e65  .    group.prune
-000048e0: 2869 6478 733d 6964 7873 290a 2020 2020  (idxs=idxs).    
-000048f0: 7072 696e 7428 6772 6f75 7029 0a60 6060  print(group).```
-00004900: 0a0a 2323 2320 322e 2048 6967 682d 6c65  ..### 2. High-le
-00004910: 7665 6c20 5072 756e 6572 730a 0a4c 6576  vel Pruners..Lev
-00004920: 6572 6167 696e 6720 7468 6520 4465 7065  eraging the Depe
-00004930: 6e64 656e 6379 4772 6170 682c 2077 6520  ndencyGraph, we 
-00004940: 6465 7665 6c6f 7065 6420 7365 7665 7261  developed severa
-00004950: 6c20 6869 6768 2d6c 6576 656c 2070 7275  l high-level pru
-00004960: 6e65 7273 2069 6e20 7468 6973 2072 6570  ners in this rep
-00004970: 6f73 6974 6f72 7920 746f 2066 6163 696c  ository to facil
-00004980: 6974 6174 6520 6566 666f 7274 6c65 7373  itate effortless
-00004990: 2070 7275 6e69 6e67 2e20 4279 2073 7065   pruning. By spe
-000049a0: 6369 6679 696e 6720 7468 6520 6465 7369  cifying the desi
-000049b0: 7265 6420 6368 616e 6e65 6c20 7370 6172  red channel spar
-000049c0: 7369 7479 2c20 796f 7520 6361 6e20 7072  sity, you can pr
-000049d0: 756e 6520 7468 6520 656e 7469 7265 206d  une the entire m
-000049e0: 6f64 656c 2061 6e64 2066 696e 652d 7475  odel and fine-tu
-000049f0: 6e65 2069 7420 7573 696e 6720 796f 7572  ne it using your
-00004a00: 206f 776e 2074 7261 696e 696e 6720 636f   own training co
-00004a10: 6465 2e20 466f 7220 6465 7461 696c 6564  de. For detailed
-00004a20: 2069 6e66 6f72 6d61 7469 6f6e 206f 6e20   information on 
-00004a30: 7468 6973 2070 726f 6365 7373 2c20 706c  this process, pl
-00004a40: 6561 7365 2072 6566 6572 2074 6f20 5b74  ease refer to [t
-00004a50: 6869 7320 7475 746f 7269 616c 5d28 6874  his tutorial](ht
-00004a60: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-00004a70: 2f56 6169 6e46 2f54 6f72 6368 2d50 7275  /VainF/Torch-Pru
-00004a80: 6e69 6e67 2f62 6c6f 622f 6d61 7374 6572  ning/blob/master
-00004a90: 2f74 7574 6f72 6961 6c73 2f31 2532 302d  /tutorials/1%20-
-00004aa0: 2532 3043 7573 746f 6d69 7a65 2532 3059  %20Customize%20Y
-00004ab0: 6f75 7225 3230 4f77 6e25 3230 5072 756e  our%20Own%20Prun
-00004ac0: 6572 732e 6970 796e 6229 2c20 7768 6963  ers.ipynb), whic
-00004ad0: 6820 7368 6f77 7320 686f 7720 746f 2069  h shows how to i
-00004ae0: 6d70 6c65 6d65 6e74 2061 205b 736c 696d  mplement a [slim
-00004af0: 6d69 6e67 5d28 6874 7470 733a 2f2f 6172  ming](https://ar
-00004b00: 7869 762e 6f72 672f 6162 732f 3137 3038  xiv.org/abs/1708
-00004b10: 2e30 3635 3139 2920 7072 756e 6572 2066  .06519) pruner f
-00004b20: 726f 6d20 7363 7261 7463 682e 2041 6464  rom scratch. Add
-00004b30: 6974 696f 6e61 6c6c 792c 2079 6f75 2063  itionally, you c
-00004b40: 616e 2066 696e 6420 6d6f 7265 2070 7261  an find more pra
-00004b50: 6374 6963 616c 2065 7861 6d70 6c65 7320  ctical examples 
-00004b60: 696e 205b 6265 6e63 686d 6172 6b73 2f6d  in [benchmarks/m
-00004b70: 6169 6e2e 7079 5d28 6265 6e63 686d 6172  ain.py](benchmar
-00004b80: 6b73 2f6d 6169 6e2e 7079 292e 0a0a 6060  ks/main.py)...``
-00004b90: 6070 7974 686f 6e0a 696d 706f 7274 2074  `python.import t
-00004ba0: 6f72 6368 0a66 726f 6d20 746f 7263 6876  orch.from torchv
-00004bb0: 6973 696f 6e2e 6d6f 6465 6c73 2069 6d70  ision.models imp
-00004bc0: 6f72 7420 7265 736e 6574 3138 0a69 6d70  ort resnet18.imp
-00004bd0: 6f72 7420 746f 7263 685f 7072 756e 696e  ort torch_prunin
-00004be0: 6720 6173 2074 700a 0a6d 6f64 656c 203d  g as tp..model =
-00004bf0: 2072 6573 6e65 7431 3828 7072 6574 7261   resnet18(pretra
-00004c00: 696e 6564 3d54 7275 6529 0a0a 2320 496d  ined=True)..# Im
-00004c10: 706f 7274 616e 6365 2063 7269 7465 7269  portance criteri
-00004c20: 610a 6578 616d 706c 655f 696e 7075 7473  a.example_inputs
-00004c30: 203d 2074 6f72 6368 2e72 616e 646e 2831   = torch.randn(1
-00004c40: 2c20 332c 2032 3234 2c20 3232 3429 0a69  , 3, 224, 224).i
-00004c50: 6d70 203d 2074 702e 696d 706f 7274 616e  mp = tp.importan
-00004c60: 6365 2e54 6179 6c6f 7249 6d70 6f72 7461  ce.TaylorImporta
-00004c70: 6e63 6528 290a 0a69 676e 6f72 6564 5f6c  nce()..ignored_l
-00004c80: 6179 6572 7320 3d20 5b5d 0a66 6f72 206d  ayers = [].for m
-00004c90: 2069 6e20 6d6f 6465 6c2e 6d6f 6475 6c65   in model.module
-00004ca0: 7328 293a 0a20 2020 2069 6620 6973 696e  s():.    if isin
-00004cb0: 7374 616e 6365 286d 2c20 746f 7263 682e  stance(m, torch.
-00004cc0: 6e6e 2e4c 696e 6561 7229 2061 6e64 206d  nn.Linear) and m
-00004cd0: 2e6f 7574 5f66 6561 7475 7265 7320 3d3d  .out_features ==
-00004ce0: 2031 3030 303a 0a20 2020 2020 2020 2069   1000:.        i
-00004cf0: 676e 6f72 6564 5f6c 6179 6572 732e 6170  gnored_layers.ap
-00004d00: 7065 6e64 286d 2920 2320 444f 204e 4f54  pend(m) # DO NOT
-00004d10: 2070 7275 6e65 2074 6865 2066 696e 616c   prune the final
-00004d20: 2063 6c61 7373 6966 6965 7221 0a0a 6974   classifier!..it
-00004d30: 6572 6174 6976 655f 7374 6570 7320 3d20  erative_steps = 
-00004d40: 3520 2320 7072 6f67 7265 7373 6976 6520  5 # progressive 
-00004d50: 7072 756e 696e 670a 7072 756e 6572 203d  pruning.pruner =
-00004d60: 2074 702e 7072 756e 6572 2e4d 6167 6e69   tp.pruner.Magni
-00004d70: 7475 6465 5072 756e 6572 280a 2020 2020  tudePruner(.    
-00004d80: 6d6f 6465 6c2c 0a20 2020 2065 7861 6d70  model,.    examp
-00004d90: 6c65 5f69 6e70 7574 732c 0a20 2020 2069  le_inputs,.    i
-00004da0: 6d70 6f72 7461 6e63 653d 696d 702c 0a20  mportance=imp,. 
-00004db0: 2020 2069 7465 7261 7469 7665 5f73 7465     iterative_ste
-00004dc0: 7073 3d69 7465 7261 7469 7665 5f73 7465  ps=iterative_ste
-00004dd0: 7073 2c0a 2020 2020 6368 5f73 7061 7273  ps,.    ch_spars
-00004de0: 6974 793d 302e 352c 2023 2072 656d 6f76  ity=0.5, # remov
-00004df0: 6520 3530 2520 6368 616e 6e65 6c73 2c20  e 50% channels, 
-00004e00: 5265 734e 6574 3138 203d 207b 3634 2c20  ResNet18 = {64, 
-00004e10: 3132 382c 2032 3536 2c20 3531 327d 203d  128, 256, 512} =
-00004e20: 3e20 5265 734e 6574 3138 5f48 616c 6620  > ResNet18_Half 
-00004e30: 3d20 7b33 322c 2036 342c 2031 3238 2c20  = {32, 64, 128, 
-00004e40: 3235 367d 0a20 2020 2069 676e 6f72 6564  256}.    ignored
-00004e50: 5f6c 6179 6572 733d 6967 6e6f 7265 645f  _layers=ignored_
-00004e60: 6c61 7965 7273 2c0a 290a 0a62 6173 655f  layers,.)..base_
-00004e70: 6d61 6373 2c20 6261 7365 5f6e 7061 7261  macs, base_npara
-00004e80: 6d73 203d 2074 702e 7574 696c 732e 636f  ms = tp.utils.co
-00004e90: 756e 745f 6f70 735f 616e 645f 7061 7261  unt_ops_and_para
-00004ea0: 6d73 286d 6f64 656c 2c20 6578 616d 706c  ms(model, exampl
-00004eb0: 655f 696e 7075 7473 290a 666f 7220 6920  e_inputs).for i 
-00004ec0: 696e 2072 616e 6765 2869 7465 7261 7469  in range(iterati
-00004ed0: 7665 5f73 7465 7073 293a 0a20 2020 2069  ve_steps):.    i
-00004ee0: 6620 6973 696e 7374 616e 6365 2869 6d70  f isinstance(imp
-00004ef0: 2c20 7470 2e69 6d70 6f72 7461 6e63 652e  , tp.importance.
-00004f00: 5461 796c 6f72 496d 706f 7274 616e 6365  TaylorImportance
-00004f10: 293a 0a20 2020 2020 2020 2023 2054 6179  ):.        # Tay
-00004f20: 6c6f 7220 6578 7061 6e73 696f 6e20 7265  lor expansion re
-00004f30: 7175 6972 6573 2067 7261 6469 656e 7473  quires gradients
-00004f40: 2066 6f72 2069 6d70 6f72 7461 6e63 6520   for importance 
-00004f50: 6573 7469 6d61 7469 6f6e 0a20 2020 2020  estimation.     
-00004f60: 2020 206c 6f73 7320 3d20 6d6f 6465 6c28     loss = model(
-00004f70: 6578 616d 706c 655f 696e 7075 7473 292e  example_inputs).
-00004f80: 7375 6d28 2920 2320 6120 6475 6d6d 7920  sum() # a dummy 
-00004f90: 6c6f 7373 2066 6f72 2054 6179 6c6f 7249  loss for TaylorI
-00004fa0: 6d70 6f72 7461 6e63 650a 2020 2020 2020  mportance.      
-00004fb0: 2020 6c6f 7373 2e62 6163 6b77 6172 6428    loss.backward(
-00004fc0: 2920 2320 6265 666f 7265 2070 7275 6e65  ) # before prune
-00004fd0: 722e 7374 6570 2829 0a20 2020 2070 7275  r.step().    pru
-00004fe0: 6e65 722e 7374 6570 2829 0a20 2020 206d  ner.step().    m
-00004ff0: 6163 732c 206e 7061 7261 6d73 203d 2074  acs, nparams = t
-00005000: 702e 7574 696c 732e 636f 756e 745f 6f70  p.utils.count_op
-00005010: 735f 616e 645f 7061 7261 6d73 286d 6f64  s_and_params(mod
-00005020: 656c 2c20 6578 616d 706c 655f 696e 7075  el, example_inpu
-00005030: 7473 290a 2020 2020 2320 6669 6e65 7475  ts).    # finetu
-00005040: 6e65 2079 6f75 7220 6d6f 6465 6c20 6865  ne your model he
-00005050: 7265 0a20 2020 2023 2066 696e 6574 756e  re.    # finetun
-00005060: 6528 6d6f 6465 6c29 0a20 2020 2023 202e  e(model).    # .
-00005070: 2e2e 0a60 6060 0a0a 2323 2323 2053 7061  ...```..#### Spa
-00005080: 7273 6520 5472 6169 6e69 6e67 0a53 6f6d  rse Training.Som
-00005090: 6520 7072 756e 6572 7320 6c69 6b65 205b  e pruners like [
-000050a0: 424e 5363 616c 6550 7275 6e65 725d 2868  BNScalePruner](h
-000050b0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
-000050c0: 6d2f 5661 696e 462f 546f 7263 682d 5072  m/VainF/Torch-Pr
-000050d0: 756e 696e 672f 626c 6f62 2f64 6435 3939  uning/blob/dd599
-000050e0: 3231 3336 3564 3732 6163 6232 3835 3764  21365d72acb2857d
-000050f0: 3364 3734 6637 3563 3033 6534 3737 3036  3d74f75c03e47706
-00005100: 3066 622f 746f 7263 685f 7072 756e 696e  0fb/torch_prunin
-00005110: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
-00005120: 686d 732f 6261 7463 686e 6f72 6d5f 7363  hms/batchnorm_sc
-00005130: 616c 655f 7072 756e 6572 2e70 7923 4c34  ale_pruner.py#L4
-00005140: 3529 2061 6e64 205b 4772 6f75 704e 6f72  5) and [GroupNor
-00005150: 6d50 7275 6e65 725d 2868 7474 7073 3a2f  mPruner](https:/
-00005160: 2f67 6974 6875 622e 636f 6d2f 5661 696e  /github.com/Vain
-00005170: 462f 546f 7263 682d 5072 756e 696e 672f  F/Torch-Pruning/
-00005180: 626c 6f62 2f64 6435 3939 3231 3336 3564  blob/dd59921365d
-00005190: 3732 6163 6232 3835 3764 3364 3734 6637  72acb2857d3d74f7
-000051a0: 3563 3033 6534 3737 3036 3066 622f 746f  5c03e477060fb/to
-000051b0: 7263 685f 7072 756e 696e 672f 7072 756e  rch_pruning/prun
-000051c0: 6572 2f61 6c67 6f72 6974 686d 732f 6772  er/algorithms/gr
-000051d0: 6f75 705f 6e6f 726d 5f70 7275 6e65 722e  oup_norm_pruner.
-000051e0: 7079 234c 3533 2920 7265 7175 6972 6520  py#L53) require 
-000051f0: 7370 6172 7365 2074 7261 696e 696e 6720  sparse training 
-00005200: 6265 666f 7265 2070 7275 6e69 6e67 2e20  before pruning. 
-00005210: 5468 6973 2063 616e 2062 6520 6561 7369  This can be easi
-00005220: 6c79 2061 6368 6965 7665 6420 6279 2069  ly achieved by i
-00005230: 6e73 6572 7469 6e67 206a 7573 7420 6f6e  nserting just on
-00005240: 6520 6c69 6e65 206f 6620 636f 6465 2060  e line of code `
-00005250: 6070 7275 6e65 722e 7265 6775 6c61 7269  `pruner.regulari
-00005260: 7a65 286d 6f64 656c 2960 6020 696e 2079  ze(model)`` in y
-00005270: 6f75 7220 7472 6169 6e69 6e67 2073 6372  our training scr
-00005280: 6970 742e 2054 6865 2070 7275 6e65 7220  ipt. The pruner 
-00005290: 7769 6c6c 2075 7064 6174 6520 7468 6520  will update the 
-000052a0: 6772 6164 6965 6e74 206f 6620 7472 6169  gradient of trai
-000052b0: 6e61 626c 6520 7061 7261 6d65 7465 7273  nable parameters
-000052c0: 2e0a 6060 6070 7974 686f 6e0a 666f 7220  ..```python.for 
-000052d0: 6570 6f63 6820 696e 2072 616e 6765 2865  epoch in range(e
-000052e0: 706f 6368 7329 3a0a 2020 2020 6d6f 6465  pochs):.    mode
-000052f0: 6c2e 7472 6169 6e28 290a 2020 2020 666f  l.train().    fo
-00005300: 7220 692c 2028 6461 7461 2c20 7461 7267  r i, (data, targ
-00005310: 6574 2920 696e 2065 6e75 6d65 7261 7465  et) in enumerate
-00005320: 2874 7261 696e 5f6c 6f61 6465 7229 3a0a  (train_loader):.
-00005330: 2020 2020 2020 2020 6461 7461 2c20 7461          data, ta
-00005340: 7267 6574 203d 2064 6174 612e 746f 2864  rget = data.to(d
-00005350: 6576 6963 6529 2c20 7461 7267 6574 2e74  evice), target.t
-00005360: 6f28 6465 7669 6365 290a 2020 2020 2020  o(device).      
-00005370: 2020 6f70 7469 6d69 7a65 722e 7a65 726f    optimizer.zero
-00005380: 5f67 7261 6428 290a 2020 2020 2020 2020  _grad().        
-00005390: 6f75 7420 3d20 6d6f 6465 6c28 6461 7461  out = model(data
-000053a0: 290a 2020 2020 2020 2020 6c6f 7373 203d  ).        loss =
-000053b0: 2046 2e63 726f 7373 5f65 6e74 726f 7079   F.cross_entropy
-000053c0: 286f 7574 2c20 7461 7267 6574 290a 2020  (out, target).  
-000053d0: 2020 2020 2020 6c6f 7373 2e62 6163 6b77        loss.backw
-000053e0: 6172 6428 290a 2020 2020 2020 2020 7072  ard().        pr
-000053f0: 756e 6572 2e72 6567 756c 6172 697a 6528  uner.regularize(
-00005400: 6d6f 6465 6c29 2023 203c 3d3d 2066 6f72  model) # <== for
-00005410: 2073 7061 7273 6520 6c65 6172 6e69 6e67   sparse learning
-00005420: 0a20 2020 2020 2020 206f 7074 696d 697a  .        optimiz
-00005430: 6572 2e73 7465 7028 290a 6060 600a 0a23  er.step().```..#
-00005440: 2323 2320 496e 7465 7261 6374 6976 6520  ### Interactive 
-00005450: 5072 756e 696e 6720 2841 6476 616e 6365  Pruning (Advance
-00005460: 6429 0a41 6c6c 2068 6967 682d 6c65 7665  d).All high-leve
-00005470: 6c20 7072 756e 6572 7320 7375 7070 6f72  l pruners suppor
-00005480: 7420 696e 7465 7261 6374 6976 6520 7072  t interactive pr
-00005490: 756e 696e 672e 2055 7365 2060 6070 7275  uning. Use ``pru
-000054a0: 6e65 722e 7374 6570 2869 6e74 6572 6163  ner.step(interac
-000054b0: 7469 7665 3d54 7275 6529 6060 2074 6f20  tive=True)`` to 
-000054c0: 6765 7420 616c 6c20 6772 6f75 7073 2061  get all groups a
-000054d0: 6e64 2069 6e74 6572 6163 7469 7665 6c79  nd interactively
-000054e0: 2070 7275 6e65 2074 6865 6d20 6279 2063   prune them by c
-000054f0: 616c 6c69 6e67 2060 6067 726f 7570 2e70  alling ``group.p
-00005500: 7275 6e65 2829 6060 2e20 5468 6973 2066  rune()``. This f
-00005510: 6561 7475 7265 2069 7320 7573 6566 756c  eature is useful
-00005520: 2069 6620 796f 7520 7761 6e74 2074 6f20   if you want to 
-00005530: 636f 6e74 726f 6c2f 6d6f 6e69 746f 7220  control/monitor 
-00005540: 7468 6520 7072 756e 696e 6720 7072 6f63  the pruning proc
-00005550: 6573 732e 0a0a 6060 6070 7974 686f 6e0a  ess...```python.
-00005560: 666f 7220 6920 696e 2072 616e 6765 2869  for i in range(i
-00005570: 7465 7261 7469 7665 5f73 7465 7073 293a  terative_steps):
-00005580: 0a20 2020 2066 6f72 2067 726f 7570 2069  .    for group i
-00005590: 6e20 7072 756e 6572 2e73 7465 7028 696e  n pruner.step(in
-000055a0: 7465 7261 6374 6976 653d 5472 7565 293a  teractive=True):
-000055b0: 2023 2057 6172 6e69 6e67 3a20 6772 6f75   # Warning: grou
-000055c0: 7073 206d 7573 7420 6265 2068 616e 646c  ps must be handl
-000055d0: 6564 2073 6571 7565 6e74 6961 6c6c 792e  ed sequentially.
-000055e0: 2044 6f20 6e6f 7420 6b65 6570 2074 6865   Do not keep the
-000055f0: 6d20 6173 2061 206c 6973 742e 0a20 2020  m as a list..   
-00005600: 2020 2020 2070 7269 6e74 2867 726f 7570       print(group
-00005610: 2920 0a20 2020 2020 2020 2023 2064 6f20  ) .        # do 
-00005620: 7768 6174 6576 6572 2079 6f75 206c 696b  whatever you lik
-00005630: 6520 7769 7468 2074 6865 2067 726f 7570  e with the group
-00005640: 200a 2020 2020 2020 2020 6465 702c 2069   .        dep, i
-00005650: 6478 7320 3d20 6772 6f75 705b 305d 2023  dxs = group[0] #
-00005660: 2067 6574 2074 6865 2069 6478 730a 2020   get the idxs.  
-00005670: 2020 2020 2020 7461 7267 6574 5f6d 6f64        target_mod
-00005680: 756c 6520 3d20 6465 702e 7461 7267 6574  ule = dep.target
-00005690: 2e6d 6f64 756c 6520 2320 6765 7420 7468  .module # get th
-000056a0: 6520 726f 6f74 206d 6f64 756c 650a 2020  e root module.  
-000056b0: 2020 2020 2020 7072 756e 696e 675f 666e        pruning_fn
-000056c0: 203d 2064 6570 2e68 616e 646c 6572 2023   = dep.handler #
-000056d0: 2067 6574 2074 6865 2070 7275 6e69 6e67   get the pruning
-000056e0: 2066 756e 6374 696f 6e0a 2020 2020 2020   function.      
-000056f0: 200a 2020 2020 2020 2020 2320 446f 6e27   .        # Don'
-00005700: 7420 666f 7267 6574 2074 6f20 7072 756e  t forget to prun
-00005710: 6520 7468 6520 6772 6f75 700a 2020 2020  e the group.    
-00005720: 2020 2020 6772 6f75 702e 7072 756e 6528      group.prune(
-00005730: 290a 2020 2020 2020 2020 2020 0a20 2020  ).          .   
-00005740: 2020 2020 2023 2067 726f 7570 2e70 7275       # group.pru
-00005750: 6e65 2869 6478 733d 5b30 2c20 322c 2036  ne(idxs=[0, 2, 6
-00005760: 5d29 2023 2049 7420 6973 2065 7665 6e20  ]) # It is even 
-00005770: 706f 7373 6962 6c65 2074 6f20 6368 616e  possible to chan
-00005780: 6765 2074 6865 2070 7275 6e69 6e67 2062  ge the pruning b
-00005790: 6568 6176 696f 7572 2077 6974 6820 7468  ehaviour with th
-000057a0: 6520 6964 7873 2070 6172 616d 6574 6572  e idxs parameter
-000057b0: 0a20 2020 206d 6163 732c 206e 7061 7261  .    macs, npara
-000057c0: 6d73 203d 2074 702e 7574 696c 732e 636f  ms = tp.utils.co
-000057d0: 756e 745f 6f70 735f 616e 645f 7061 7261  unt_ops_and_para
-000057e0: 6d73 286d 6f64 656c 2c20 6578 616d 706c  ms(model, exampl
-000057f0: 655f 696e 7075 7473 290a 2020 2020 2320  e_inputs).    # 
-00005800: 6669 6e65 7475 6e65 2079 6f75 7220 6d6f  finetune your mo
-00005810: 6465 6c20 6865 7265 0a20 2020 2023 2066  del here.    # f
-00005820: 696e 6574 756e 6528 6d6f 6465 6c29 0a20  inetune(model). 
-00005830: 2020 2023 202e 2e2e 0a60 6060 0a0a 2323     # ....```..##
-00005840: 2323 2047 726f 7570 2d6c 6576 656c 2050  ## Group-level P
-00005850: 7275 6e69 6e67 0a0a 5769 7468 2044 6570  runing..With Dep
-00005860: 4772 6170 682c 2069 7420 6973 2065 6173  Graph, it is eas
-00005870: 7920 746f 2064 6573 6967 6e20 736f 6d65  y to design some
-00005880: 2022 6772 6f75 702d 6c65 7665 6c22 2063   "group-level" c
-00005890: 7269 7465 7269 6120 746f 2065 7374 696d  riteria to estim
-000058a0: 6174 6520 7468 6520 696d 706f 7274 616e  ate the importan
-000058b0: 6365 206f 6620 6120 7768 6f6c 6520 6772  ce of a whole gr
-000058c0: 6f75 7020 7261 7468 6572 2074 6861 6e20  oup rather than 
-000058d0: 6120 7369 6e67 6c65 206c 6179 6572 2e20  a single layer. 
-000058e0: 496e 2054 6f72 6368 2d70 7275 6e69 6e67  In Torch-pruning
-000058f0: 2c20 616c 6c20 7072 756e 6572 7320 776f  , all pruners wo
-00005900: 726b 2069 6e20 7468 6520 6772 6f75 7020  rk in the group 
-00005910: 6c65 7665 6c2e 0a0a 3c64 6976 2061 6c69  level...<div ali
-00005920: 676e 3d22 6365 6e74 6572 223e 0a3c 696d  gn="center">.<im
-00005930: 6720 7372 633d 2261 7373 6574 732f 6772  g src="assets/gr
-00005940: 6f75 705f 7370 6172 7369 7479 2e70 6e67  oup_sparsity.png
-00005950: 2220 7769 6474 683d 2238 3025 223e 0a3c  " width="80%">.<
-00005960: 2f64 6976 3e0a 0a23 2323 2033 2e20 5361  /div>..### 3. Sa
-00005970: 7665 2026 204c 6f61 640a 2020 2020 2020  ve & Load.      
-00005980: 2020 2020 0a54 6865 2066 6f6c 6c6f 7769      .The followi
-00005990: 6e67 2073 6372 6970 7420 7361 7665 7320  ng script saves 
-000059a0: 7468 6520 7768 6f6c 6520 6d6f 6465 6c20  the whole model 
-000059b0: 6f62 6a65 6374 2028 7374 7275 6374 7572  object (structur
-000059c0: 652b 7765 6967 6874 7329 2061 7320 6120  e+weights) as a 
-000059d0: 276d 6f64 656c 2e70 7468 272e 200a 6060  'model.pth'. .``
-000059e0: 6070 7974 686f 6e0a 6d6f 6465 6c2e 7a65  `python.model.ze
-000059f0: 726f 5f67 7261 6428 2920 2320 5765 2064  ro_grad() # We d
-00005a00: 6f6e 2774 2077 616e 7420 746f 2073 746f  on't want to sto
-00005a10: 7265 2067 7261 6469 656e 7420 696e 666f  re gradient info
-00005a20: 726d 6174 696f 6e0a 746f 7263 682e 7361  rmation.torch.sa
-00005a30: 7665 286d 6f64 656c 2c20 276d 6f64 656c  ve(model, 'model
-00005a40: 2e70 7468 2729 2023 2077 6974 686f 7574  .pth') # without
-00005a50: 202e 7374 6174 655f 6469 6374 0a6d 6f64   .state_dict.mod
-00005a60: 656c 203d 2074 6f72 6368 2e6c 6f61 6428  el = torch.load(
-00005a70: 276d 6f64 656c 2e70 7468 2729 2023 206c  'model.pth') # l
-00005a80: 6f61 6420 7468 6520 7072 756e 6564 206d  oad the pruned m
-00005a90: 6f64 656c 0a60 6060 0a0a 2a2a 4578 7065  odel.```..**Expe
-00005aa0: 7269 6d65 6e74 616c 2046 6561 7475 7265  rimental Feature
-00005ab0: 732a 2a3a 2052 652d 6372 6561 7465 2070  s**: Re-create p
-00005ac0: 7275 6e65 6420 6d6f 6465 6c73 2066 726f  runed models fro
-00005ad0: 6d20 756e 7072 756e 6564 206f 6e65 7320  m unpruned ones 
-00005ae0: 7573 696e 6720 6060 7470 2e73 7461 7465  using ``tp.state
-00005af0: 5f64 6963 7460 6020 616e 6420 6060 7470  _dict`` and ``tp
-00005b00: 2e6c 6f61 645f 7374 6174 655f 6469 6374  .load_state_dict
-00005b10: 6060 2e0a 6060 6070 7974 686f 6e0a 2320  ``..```python.# 
-00005b20: 7361 7665 2074 6865 2070 7275 6e65 6420  save the pruned 
-00005b30: 7374 6174 655f 6469 6374 2c20 7768 6963  state_dict, whic
-00005b40: 6820 696e 636c 7564 6573 2062 6f74 6820  h includes both 
-00005b50: 7072 756e 6564 2070 6172 616d 6574 6572  pruned parameter
-00005b60: 7320 616e 6420 6d6f 6469 6669 6564 2061  s and modified a
-00005b70: 7474 7269 6275 7465 730a 7374 6174 655f  ttributes.state_
-00005b80: 6469 6374 203d 2074 702e 7374 6174 655f  dict = tp.state_
-00005b90: 6469 6374 2870 7275 6e65 645f 6d6f 6465  dict(pruned_mode
-00005ba0: 6c29 2023 2074 6865 2070 7275 6e65 6420  l) # the pruned 
-00005bb0: 6d6f 6465 6c2c 2065 2e67 2e2c 2061 2072  model, e.g., a r
-00005bc0: 6573 6e65 742d 3138 2d68 616c 660a 746f  esnet-18-half.to
-00005bd0: 7263 682e 7361 7665 2873 7461 7465 5f64  rch.save(state_d
-00005be0: 6963 742c 2027 7072 756e 6564 2e70 7468  ict, 'pruned.pth
-00005bf0: 2729 0a0a 2320 6372 6561 7465 2061 206e  ')..# create a n
-00005c00: 6577 206d 6f64 656c 2c20 652e 672e 2072  ew model, e.g. r
-00005c10: 6573 6e65 7431 380a 6e65 775f 6d6f 6465  esnet18.new_mode
-00005c20: 6c20 3d20 7265 736e 6574 3138 2829 2e65  l = resnet18().e
-00005c30: 7661 6c28 290a 0a23 206c 6f61 6420 7468  val()..# load th
-00005c40: 6520 7072 756e 6564 2073 7461 7465 5f64  e pruned state_d
-00005c50: 6963 7420 696e 746f 2074 6865 2075 6e70  ict into the unp
-00005c60: 7275 6e65 6420 6d6f 6465 6c2e 0a6c 6f61  runed model..loa
-00005c70: 6465 645f 7374 6174 655f 6469 6374 203d  ded_state_dict =
-00005c80: 2074 6f72 6368 2e6c 6f61 6428 2770 7275   torch.load('pru
-00005c90: 6e65 642e 7074 6827 2c20 6d61 705f 6c6f  ned.pth', map_lo
-00005ca0: 6361 7469 6f6e 3d27 6370 7527 290a 7470  cation='cpu').tp
-00005cb0: 2e6c 6f61 645f 7374 6174 655f 6469 6374  .load_state_dict
-00005cc0: 286e 6577 5f6d 6f64 656c 2c20 7374 6174  (new_model, stat
-00005cd0: 655f 6469 6374 3d6c 6f61 6465 645f 7374  e_dict=loaded_st
-00005ce0: 6174 655f 6469 6374 290a 7072 696e 7428  ate_dict).print(
-00005cf0: 6e65 775f 6d6f 6465 6c29 2023 2054 6869  new_model) # Thi
-00005d00: 7320 7769 6c6c 2062 6520 6120 7072 756e  s will be a prun
-00005d10: 6564 206d 6f64 656c 2e0a 6060 600a 5265  ed model..```.Re
-00005d20: 6665 7220 746f 205b 7465 7374 732f 7465  fer to [tests/te
-00005d30: 7374 5f73 6572 6961 6c69 7a61 7469 6f6e  st_serialization
-00005d40: 2e70 795d 2874 6573 7473 2f74 6573 745f  .py](tests/test_
-00005d50: 7365 7269 616c 697a 6174 696f 6e2e 7079  serialization.py
-00005d60: 2920 666f 7220 616e 2056 6954 2065 7861  ) for an ViT exa
-00005d70: 6d70 6c65 2e20 496e 2074 6869 7320 6578  mple. In this ex
-00005d80: 616d 706c 652c 2077 6520 7769 6c6c 2070  ample, we will p
-00005d90: 7275 6e65 2074 6865 206d 6f64 656c 2061  rune the model a
-00005da0: 6e64 206d 6f64 6966 7920 736f 6d65 2061  nd modify some a
-00005db0: 7474 7269 6275 7465 7320 6c69 6b65 2060  ttributes like `
-00005dc0: 606d 6f64 656c 2e68 6964 6465 6e5f 6469  `model.hidden_di
-00005dd0: 6d73 6060 2e0a 2020 2020 2020 2020 2020  ms``..          
-00005de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005df0: 2020 2020 2020 2020 2020 2020 2020 0a20                . 
-00005e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005e10: 2020 2020 2020 2020 2020 2020 2020 200a                 .
-00005e20: 2323 2320 342e 204c 6f77 2d6c 6576 656c  ### 4. Low-level
-00005e30: 2050 7275 6e69 6e67 2046 756e 6374 696f   Pruning Functio
-00005e40: 6e73 0a0a 5768 696c 6520 6974 2069 7320  ns..While it is 
-00005e50: 706f 7373 6962 6c65 2074 6f20 6d61 6e75  possible to manu
-00005e60: 616c 6c79 2070 7275 6e65 2079 6f75 7220  ally prune your 
-00005e70: 6d6f 6465 6c20 7573 696e 6720 6c6f 772d  model using low-
-00005e80: 6c65 7665 6c20 6675 6e63 7469 6f6e 732c  level functions,
-00005e90: 2074 6869 7320 6170 7072 6f61 6368 2063   this approach c
-00005ea0: 616e 2062 6520 7175 6974 6520 6c61 626f  an be quite labo
-00005eb0: 7269 6f75 732c 2061 7320 6974 2072 6571  rious, as it req
-00005ec0: 7569 7265 7320 6361 7265 6675 6c20 6d61  uires careful ma
-00005ed0: 6e61 6765 6d65 6e74 206f 6620 7468 6520  nagement of the 
-00005ee0: 6173 736f 6369 6174 6564 2064 6570 656e  associated depen
-00005ef0: 6465 6e63 6965 732e 2041 7320 6120 7265  dencies. As a re
-00005f00: 7375 6c74 2c20 7765 2072 6563 6f6d 6d65  sult, we recomme
-00005f10: 6e64 2075 7469 6c69 7a69 6e67 2074 6865  nd utilizing the
-00005f20: 2061 666f 7265 6d65 6e74 696f 6e65 6420   aforementioned 
-00005f30: 6869 6768 2d6c 6576 656c 2070 7275 6e65  high-level prune
-00005f40: 7273 2074 6f20 7374 7265 616d 6c69 6e65  rs to streamline
-00005f50: 2074 6865 2070 7275 6e69 6e67 2070 726f   the pruning pro
-00005f60: 6365 7373 2e0a 0a60 6060 7079 7468 6f6e  cess...```python
-00005f70: 0a74 702e 7072 756e 655f 636f 6e76 5f6f  .tp.prune_conv_o
-00005f80: 7574 5f63 6861 6e6e 656c 7328 206d 6f64  ut_channels( mod
-00005f90: 656c 2e63 6f6e 7631 2c20 6964 7873 3d5b  el.conv1, idxs=[
-00005fa0: 322c 362c 395d 2029 0a0a 2320 6669 7820  2,6,9] )..# fix 
-00005fb0: 7468 6520 6272 6f6b 656e 2064 6570 656e  the broken depen
-00005fc0: 6465 6e63 6965 7320 6d61 6e75 616c 6c79  dencies manually
-00005fd0: 0a74 702e 7072 756e 655f 6261 7463 686e  .tp.prune_batchn
-00005fe0: 6f72 6d5f 6f75 745f 6368 616e 6e65 6c73  orm_out_channels
-00005ff0: 2820 6d6f 6465 6c2e 626e 312c 2069 6478  ( model.bn1, idx
-00006000: 733d 5b32 2c36 2c39 5d20 290a 7470 2e70  s=[2,6,9] ).tp.p
-00006010: 7275 6e65 5f63 6f6e 765f 696e 5f63 6861  rune_conv_in_cha
-00006020: 6e6e 656c 7328 206d 6f64 656c 2e6c 6179  nnels( model.lay
-00006030: 6572 325b 305d 2e63 6f6e 7631 2c20 6964  er2[0].conv1, id
-00006040: 7873 3d5b 322c 362c 395d 2029 0a2e 2e2e  xs=[2,6,9] )....
-00006050: 0a60 6060 0a0a 5468 6520 666f 6c6c 6f77  .```..The follow
-00006060: 696e 6720 7072 756e 696e 6720 6675 6e63  ing pruning func
-00006070: 7469 6f6e 7320 6172 6520 6176 6169 6c61  tions are availa
-00006080: 626c 653a 0a60 6060 7079 7468 6f6e 0a27  ble:.```python.'
-00006090: 7072 756e 655f 636f 6e76 5f6f 7574 5f63  prune_conv_out_c
-000060a0: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
-000060b0: 5f63 6f6e 765f 696e 5f63 6861 6e6e 656c  _conv_in_channel
-000060c0: 7327 2c0a 2770 7275 6e65 5f64 6570 7468  s',.'prune_depth
-000060d0: 7769 7365 5f63 6f6e 765f 6f75 745f 6368  wise_conv_out_ch
-000060e0: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-000060f0: 6465 7074 6877 6973 655f 636f 6e76 5f69  depthwise_conv_i
-00006100: 6e5f 6368 616e 6e65 6c73 272c 0a27 7072  n_channels',.'pr
-00006110: 756e 655f 6261 7463 686e 6f72 6d5f 6f75  une_batchnorm_ou
-00006120: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
-00006130: 756e 655f 6261 7463 686e 6f72 6d5f 696e  une_batchnorm_in
-00006140: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-00006150: 6e65 5f6c 696e 6561 725f 6f75 745f 6368  ne_linear_out_ch
-00006160: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-00006170: 6c69 6e65 6172 5f69 6e5f 6368 616e 6e65  linear_in_channe
-00006180: 6c73 272c 0a27 7072 756e 655f 7072 656c  ls',.'prune_prel
-00006190: 755f 6f75 745f 6368 616e 6e65 6c73 272c  u_out_channels',
-000061a0: 0a27 7072 756e 655f 7072 656c 755f 696e  .'prune_prelu_in
-000061b0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-000061c0: 6e65 5f6c 6179 6572 6e6f 726d 5f6f 7574  ne_layernorm_out
-000061d0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-000061e0: 6e65 5f6c 6179 6572 6e6f 726d 5f69 6e5f  ne_layernorm_in_
-000061f0: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
-00006200: 655f 656d 6265 6464 696e 675f 6f75 745f  e_embedding_out_
-00006210: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
-00006220: 655f 656d 6265 6464 696e 675f 696e 5f63  e_embedding_in_c
-00006230: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
-00006240: 5f70 6172 616d 6574 6572 5f6f 7574 5f63  _parameter_out_c
-00006250: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
-00006260: 5f70 6172 616d 6574 6572 5f69 6e5f 6368  _parameter_in_ch
-00006270: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-00006280: 6d75 6c74 6968 6561 645f 6174 7465 6e74  multihead_attent
-00006290: 696f 6e5f 6f75 745f 6368 616e 6e65 6c73  ion_out_channels
-000062a0: 272c 0a27 7072 756e 655f 6d75 6c74 6968  ',.'prune_multih
-000062b0: 6561 645f 6174 7465 6e74 696f 6e5f 696e  ead_attention_in
-000062c0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-000062d0: 6e65 5f67 726f 7570 6e6f 726d 5f6f 7574  ne_groupnorm_out
-000062e0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-000062f0: 6e65 5f67 726f 7570 6e6f 726d 5f69 6e5f  ne_groupnorm_in_
-00006300: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
-00006310: 655f 696e 7374 616e 6365 6e6f 726d 5f6f  e_instancenorm_o
-00006320: 7574 5f63 6861 6e6e 656c 7327 2c0a 2770  ut_channels',.'p
-00006330: 7275 6e65 5f69 6e73 7461 6e63 656e 6f72  rune_instancenor
-00006340: 6d5f 696e 5f63 6861 6e6e 656c 7327 2c0a  m_in_channels',.
-00006350: 6060 600a 0a0a 0a23 2323 2035 2e20 4375  ```....### 5. Cu
-00006360: 7374 6f6d 697a 6564 204c 6179 6572 730a  stomized Layers.
-00006370: 0a50 6c65 6173 6520 7265 6665 7220 746f  .Please refer to
-00006380: 205b 7465 7374 732f 7465 7374 5f63 7573   [tests/test_cus
-00006390: 746f 6d69 7a65 645f 6c61 7965 722e 7079  tomized_layer.py
-000063a0: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
-000063b0: 2e63 6f6d 2f56 6169 6e46 2f54 6f72 6368  .com/VainF/Torch
-000063c0: 2d50 7275 6e69 6e67 2f62 6c6f 622f 6d61  -Pruning/blob/ma
-000063d0: 7374 6572 2f74 6573 7473 2f74 6573 745f  ster/tests/test_
-000063e0: 6375 7374 6f6d 697a 6564 5f6c 6179 6572  customized_layer
-000063f0: 2e70 7929 2e0a 0a23 2323 2036 2e20 4265  .py)...### 6. Be
-00006400: 6e63 686d 6172 6b73 0a0a 4f75 7220 7265  nchmarks..Our re
-00006410: 7375 6c74 7320 6f6e 207b 5265 734e 6574  sults on {ResNet
-00006420: 2d35 3620 2f20 4349 4641 522d 3130 202f  -56 / CIFAR-10 /
-00006430: 2032 2e30 3078 7d0a 0a7c 204d 6574 686f   2.00x}..| Metho
-00006440: 6420 7c20 4261 7365 2028 2529 207c 2050  d | Base (%) | P
-00006450: 7275 6e65 6420 2825 2920 7c20 245c 4465  runed (%) | $\De
-00006460: 6c74 6124 2041 6363 2028 2529 207c 2053  lta$ Acc (%) | S
-00006470: 7065 6564 2055 7020 7c0a 7c3a 2d2d 2020  peed Up |.|:--  
-00006480: 2020 7c3a 2d2d 3a20 207c 3a2d 2d3a 2020    |:--:  |:--:  
-00006490: 2020 7c3a 2d2d 3a20 7c3a 2d2d 3a20 2020    |:--: |:--:   
-000064a0: 2020 207c 0a7c 204e 4950 5320 5b5b 315d     |.| NIPS [[1]
-000064b0: 5d28 2331 2920 207c 202d 2020 2020 7c20  ](#1)  | -    | 
-000064c0: 2d20 2020 2020 207c 2d30 2e30 3320 7c20  -      |-0.03 | 
-000064d0: 312e 3736 7820 2020 207c 0a7c 2047 656f  1.76x    |.| Geo
-000064e0: 6d65 7472 6963 205b 5b32 5d5d 2823 3229  metric [[2]](#2)
-000064f0: 207c 2039 332e 3539 207c 2039 332e 3236   | 93.59 | 93.26
-00006500: 207c 202d 302e 3333 207c 2031 2e37 3078   | -0.33 | 1.70x
-00006510: 207c 0a7c 2050 6f6c 6172 205b 5b33 5d5d   |.| Polar [[3]]
-00006520: 2823 3329 2020 7c20 3933 2e38 3020 7c20  (#3)  | 93.80 | 
-00006530: 3933 2e38 3320 7c20 2b30 2e30 3320 7c31  93.83 | +0.03 |1
-00006540: 2e38 3878 207c 0a7c 2043 5020 205b 5b34  .88x |.| CP  [[4
-00006550: 5d5d 2823 3429 2020 207c 2039 322e 3830  ]](#4)   | 92.80
-00006560: 207c 2039 312e 3830 207c 202d 312e 3030   | 91.80 | -1.00
-00006570: 207c 322e 3030 7820 7c0a 7c20 414d 4320   |2.00x |.| AMC 
-00006580: 5b5b 355d 5d28 2335 2920 2020 7c20 3932  [[5]](#5)   | 92
-00006590: 2e38 3020 7c20 3931 2e39 3020 7c20 2d30  .80 | 91.90 | -0
-000065a0: 2e39 3020 7c32 2e30 3078 207c 0a7c 2048  .90 |2.00x |.| H
-000065b0: 5261 6e6b 205b 5b36 5d5d 2823 3629 207c  Rank [[6]](#6) |
-000065c0: 2039 332e 3236 207c 2039 322e 3137 207c   93.26 | 92.17 |
-000065d0: 202d 302e 3039 207c 322e 3030 7820 7c0a   -0.09 |2.00x |.
-000065e0: 7c20 5346 5020 205b 5b37 5d5d 2823 3729  | SFP  [[7]](#7)
-000065f0: 2020 7c20 3933 2e35 3920 7c20 3933 2e33    | 93.59 | 93.3
-00006600: 3620 7c20 2b30 2e32 3320 7c32 2e31 3178  6 | +0.23 |2.11x
-00006610: 207c 0a7c 2052 6573 5265 7020 5b5b 385d   |.| ResRep [[8]
-00006620: 5d28 2338 2920 7c20 3933 2e37 3120 7c20  ](#8) | 93.71 | 
-00006630: 3933 2e37 3120 7c20 2b30 2e30 3020 7c32  93.71 | +0.00 |2
-00006640: 2e31 3278 207c 0a7c 7c0a 7c20 4f75 7273  .12x |.||.| Ours
-00006650: 2d4c 3120 7c20 3933 2e35 3320 7c20 3932  -L1 | 93.53 | 92
-00006660: 2e39 3320 7c20 2d30 2e36 3020 7c20 322e  .93 | -0.60 | 2.
-00006670: 3132 7820 7c0a 7c20 4f75 7273 2d42 4e20  12x |.| Ours-BN 
-00006680: 7c20 3933 2e35 3320 7c20 3933 2e32 3920  | 93.53 | 93.29 
-00006690: 7c20 2d30 2e32 3420 7c20 322e 3132 7820  | -0.24 | 2.12x 
-000066a0: 7c0a 7c20 4f75 7273 2d47 726f 7570 207c  |.| Ours-Group |
-000066b0: 2039 332e 3533 207c 2039 332e 3737 207c   93.53 | 93.77 |
-000066c0: 202b 302e 3338 207c 2032 2e31 3378 207c   +0.38 | 2.13x |
-000066d0: 0a0a 506c 6561 7365 2072 6566 6572 2074  ..Please refer t
-000066e0: 6f20 5b62 656e 6368 6d61 726b 735d 2862  o [benchmarks](b
-000066f0: 656e 6368 6d61 726b 7329 2066 6f72 206d  enchmarks) for m
-00006700: 6f72 6520 6465 7461 696c 732e 0a0a 2323  ore details...##
-00006710: 2320 372e 2053 6572 6965 7320 6f66 2057  # 7. Series of W
-00006720: 6f72 6b73 0a3e 202a 2a4c 4c4d 2d50 7275  orks.> **LLM-Pru
-00006730: 6e65 723a 204f 6e20 7468 6520 5374 7275  ner: On the Stru
-00006740: 6374 7572 616c 2050 7275 6e69 6e67 206f  ctural Pruning o
-00006750: 6620 4c61 7267 6520 4c61 6e67 7561 6765  f Large Language
-00006760: 204d 6f64 656c 732a 2a20 5b5b 5072 6f6a   Models** [[Proj
-00006770: 6563 745d 5d28 6874 7470 733a 2f2f 6769  ect]](https://gi
-00006780: 7468 7562 2e63 6f6d 2f68 6f72 7365 6565  thub.com/horseee
-00006790: 2f4c 4c4d 2d50 7275 6e65 7229 205b 5b61  /LLM-Pruner) [[a
-000067a0: 7258 6976 5d5d 2868 7474 7073 3a2f 2f61  rXiv]](https://a
-000067b0: 7278 6976 2e6f 7267 2f61 6273 2f32 3330  rxiv.org/abs/230
-000067c0: 352e 3131 3632 3729 2020 200a 3e20 2a58  5.11627)   .> *X
-000067d0: 696e 7969 6e20 4d61 2c20 476f 6e67 6661  inyin Ma, Gongfa
-000067e0: 6e20 4661 6e67 2c20 5869 6e63 6861 6f20  n Fang, Xinchao 
-000067f0: 5761 6e67 2a20 2020 0a0a 3e20 2a2a 5374  Wang*   ..> **St
-00006800: 7275 6374 7572 616c 2050 7275 6e69 6e67  ructural Pruning
-00006810: 2066 6f72 2044 6966 6675 7369 6f6e 204d   for Diffusion M
-00006820: 6f64 656c 732a 2a20 5b5b 5072 6f6a 6563  odels** [[Projec
-00006830: 745d 5d28 6874 7470 733a 2f2f 6769 7468  t]](https://gith
-00006840: 7562 2e63 6f6d 2f56 6169 6e46 2f44 6966  ub.com/VainF/Dif
-00006850: 662d 5072 756e 696e 6729 205b 5b61 7278  f-Pruning) [[arx
-00006860: 6976 5d5d 2868 7474 7073 3a2f 2f61 7278  iv]](https://arx
-00006870: 6976 2e6f 7267 2f61 6273 2f32 3330 352e  iv.org/abs/2305.
-00006880: 3130 3932 3429 2020 0a3e 202a 476f 6e67  10924)  .> *Gong
-00006890: 6661 6e20 4661 6e67 2c20 5869 6e79 696e  fan Fang, Xinyin
-000068a0: 204d 612c 2058 696e 6368 616f 2057 616e   Ma, Xinchao Wan
-000068b0: 672a 2020 2020 0a0a 0a23 2320 4369 7461  g*    ...## Cita
-000068c0: 7469 6f6e 0a60 6060 0a40 696e 7072 6f63  tion.```.@inproc
-000068d0: 6565 6469 6e67 737b 6661 6e67 3230 3233  eedings{fang2023
-000068e0: 6465 7067 7261 7068 2c0a 2020 7469 746c  depgraph,.  titl
-000068f0: 653d 7b44 6570 6772 6170 683a 2054 6f77  e={Depgraph: Tow
-00006900: 6172 6473 2061 6e79 2073 7472 7563 7475  ards any structu
-00006910: 7261 6c20 7072 756e 696e 677d 2c0a 2020  ral pruning},.  
-00006920: 6175 7468 6f72 3d7b 4661 6e67 2c20 476f  author={Fang, Go
-00006930: 6e67 6661 6e20 616e 6420 4d61 2c20 5869  ngfan and Ma, Xi
-00006940: 6e79 696e 2061 6e64 2053 6f6e 672c 204d  nyin and Song, M
-00006950: 696e 676c 6920 616e 6420 4d69 2c20 4d69  ingli and Mi, Mi
-00006960: 6368 6165 6c20 4269 2061 6e64 2057 616e  chael Bi and Wan
-00006970: 672c 2058 696e 6368 616f 7d2c 0a20 2062  g, Xinchao},.  b
-00006980: 6f6f 6b74 6974 6c65 3d7b 5072 6f63 6565  ooktitle={Procee
-00006990: 6469 6e67 7320 6f66 2074 6865 2049 4545  dings of the IEE
-000069a0: 452f 4356 4620 436f 6e66 6572 656e 6365  E/CVF Conference
-000069b0: 206f 6e20 436f 6d70 7574 6572 2056 6973   on Computer Vis
-000069c0: 696f 6e20 616e 6420 5061 7474 6572 6e20  ion and Pattern 
-000069d0: 5265 636f 676e 6974 696f 6e7d 2c0a 2020  Recognition},.  
-000069e0: 7061 6765 733d 7b31 3630 3931 2d2d 3136  pages={16091--16
-000069f0: 3130 317d 2c0a 2020 7965 6172 3d7b 3230  101},.  year={20
-00006a00: 3233 7d0a 7d0a 6060 600a 0a00 0000 0000  23}.}.```.......
-00006a10: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00006a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00006a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+00003940: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2020  ------.         
+00003950: 2050 7275 6e69 6e67 2047 726f 7570 0a2d   Pruning Group.-
+00003960: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00003970: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
+00003980: 5b30 5d20 7072 756e 655f 6f75 745f 6368  [0] prune_out_ch
+00003990: 616e 6e65 6c73 206f 6e20 636f 6e76 3120  annels on conv1 
+000039a0: 2843 6f6e 7632 6428 332c 2036 342c 206b  (Conv2d(3, 64, k
+000039b0: 6572 6e65 6c5f 7369 7a65 3d28 372c 2037  ernel_size=(7, 7
+000039c0: 292c 2073 7472 6964 653d 2832 2c20 3229  ), stride=(2, 2)
+000039d0: 2c20 7061 6464 696e 673d 2833 2c20 3329  , padding=(3, 3)
+000039e0: 2c20 6269 6173 3d46 616c 7365 2929 203d  , bias=False)) =
+000039f0: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
+00003a00: 6e65 6c73 206f 6e20 636f 6e76 3120 2843  nels on conv1 (C
+00003a10: 6f6e 7632 6428 332c 2036 342c 206b 6572  onv2d(3, 64, ker
+00003a20: 6e65 6c5f 7369 7a65 3d28 372c 2037 292c  nel_size=(7, 7),
+00003a30: 2073 7472 6964 653d 2832 2c20 3229 2c20   stride=(2, 2), 
+00003a40: 7061 6464 696e 673d 2833 2c20 3329 2c20  padding=(3, 3), 
+00003a50: 6269 6173 3d46 616c 7365 2929 2c20 6964  bias=False)), id
+00003a60: 7873 3d5b 322c 2036 2c20 395d 2028 5072  xs=[2, 6, 9] (Pr
+00003a70: 756e 696e 6720 526f 6f74 290a 5b31 5d20  uning Root).[1] 
+00003a80: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
+00003a90: 6c73 206f 6e20 636f 6e76 3120 2843 6f6e  ls on conv1 (Con
+00003aa0: 7632 6428 332c 2036 342c 206b 6572 6e65  v2d(3, 64, kerne
+00003ab0: 6c5f 7369 7a65 3d28 372c 2037 292c 2073  l_size=(7, 7), s
+00003ac0: 7472 6964 653d 2832 2c20 3229 2c20 7061  tride=(2, 2), pa
+00003ad0: 6464 696e 673d 2833 2c20 3329 2c20 6269  dding=(3, 3), bi
+00003ae0: 6173 3d46 616c 7365 2929 203d 3e20 7072  as=False)) => pr
+00003af0: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
+00003b00: 206f 6e20 626e 3120 2842 6174 6368 4e6f   on bn1 (BatchNo
+00003b10: 726d 3264 2836 342c 2065 7073 3d31 652d  rm2d(64, eps=1e-
+00003b20: 3035 2c20 6d6f 6d65 6e74 756d 3d30 2e31  05, momentum=0.1
+00003b30: 2c20 6166 6669 6e65 3d54 7275 652c 2074  , affine=True, t
+00003b40: 7261 636b 5f72 756e 6e69 6e67 5f73 7461  rack_running_sta
+00003b50: 7473 3d54 7275 6529 292c 2069 6478 733d  ts=True)), idxs=
+00003b60: 5b32 2c20 362c 2039 5d0a 5b32 5d20 7072  [2, 6, 9].[2] pr
+00003b70: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
+00003b80: 206f 6e20 626e 3120 2842 6174 6368 4e6f   on bn1 (BatchNo
+00003b90: 726d 3264 2836 342c 2065 7073 3d31 652d  rm2d(64, eps=1e-
+00003ba0: 3035 2c20 6d6f 6d65 6e74 756d 3d30 2e31  05, momentum=0.1
+00003bb0: 2c20 6166 6669 6e65 3d54 7275 652c 2074  , affine=True, t
+00003bc0: 7261 636b 5f72 756e 6e69 6e67 5f73 7461  rack_running_sta
+00003bd0: 7473 3d54 7275 6529 2920 3d3e 2070 7275  ts=True)) => pru
+00003be0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+00003bf0: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+00003c00: 705f 3230 2852 656c 7542 6163 6b77 6172  p_20(ReluBackwar
+00003c10: 6430 292c 2069 6478 733d 5b32 2c20 362c  d0), idxs=[2, 6,
+00003c20: 2039 5d0a 5b33 5d20 7072 756e 655f 6f75   9].[3] prune_ou
+00003c30: 745f 6368 616e 6e65 6c73 206f 6e20 5f45  t_channels on _E
+00003c40: 6c65 6d65 6e74 5769 7365 4f70 5f32 3028  lementWiseOp_20(
+00003c50: 5265 6c75 4261 636b 7761 7264 3029 203d  ReluBackward0) =
+00003c60: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
+00003c70: 6e65 6c73 206f 6e20 5f45 6c65 6d65 6e74  nels on _Element
+00003c80: 5769 7365 4f70 5f31 3928 4d61 7850 6f6f  WiseOp_19(MaxPoo
+00003c90: 6c32 4457 6974 6849 6e64 6963 6573 4261  l2DWithIndicesBa
+00003ca0: 636b 7761 7264 3029 2c20 6964 7873 3d5b  ckward0), idxs=[
+00003cb0: 322c 2036 2c20 395d 0a5b 345d 2070 7275  2, 6, 9].[4] pru
+00003cc0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+00003cd0: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+00003ce0: 705f 3139 284d 6178 506f 6f6c 3244 5769  p_19(MaxPool2DWi
+00003cf0: 7468 496e 6469 6365 7342 6163 6b77 6172  thIndicesBackwar
+00003d00: 6430 2920 3d3e 2070 7275 6e65 5f6f 7574  d0) => prune_out
+00003d10: 5f63 6861 6e6e 656c 7320 6f6e 205f 456c  _channels on _El
+00003d20: 656d 656e 7457 6973 654f 705f 3138 2841  ementWiseOp_18(A
+00003d30: 6464 4261 636b 7761 7264 3029 2c20 6964  ddBackward0), id
+00003d40: 7873 3d5b 322c 2036 2c20 395d 0a5b 355d  xs=[2, 6, 9].[5]
+00003d50: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+00003d60: 656c 7320 6f6e 205f 456c 656d 656e 7457  els on _ElementW
+00003d70: 6973 654f 705f 3139 284d 6178 506f 6f6c  iseOp_19(MaxPool
+00003d80: 3244 5769 7468 496e 6469 6365 7342 6163  2DWithIndicesBac
+00003d90: 6b77 6172 6430 2920 3d3e 2070 7275 6e65  kward0) => prune
+00003da0: 5f69 6e5f 6368 616e 6e65 6c73 206f 6e20  _in_channels on 
+00003db0: 6c61 7965 7231 2e30 2e63 6f6e 7631 2028  layer1.0.conv1 (
+00003dc0: 436f 6e76 3264 2836 342c 2036 342c 206b  Conv2d(64, 64, k
+00003dd0: 6572 6e65 6c5f 7369 7a65 3d28 332c 2033  ernel_size=(3, 3
+00003de0: 292c 2073 7472 6964 653d 2831 2c20 3129  ), stride=(1, 1)
+00003df0: 2c20 7061 6464 696e 673d 2831 2c20 3129  , padding=(1, 1)
+00003e00: 2c20 6269 6173 3d46 616c 7365 2929 2c20  , bias=False)), 
+00003e10: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
+00003e20: 365d 2070 7275 6e65 5f6f 7574 5f63 6861  6] prune_out_cha
+00003e30: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+00003e40: 7457 6973 654f 705f 3138 2841 6464 4261  tWiseOp_18(AddBa
+00003e50: 636b 7761 7264 3029 203d 3e20 7072 756e  ckward0) => prun
+00003e60: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
+00003e70: 6e20 6c61 7965 7231 2e30 2e62 6e32 2028  n layer1.0.bn2 (
+00003e80: 4261 7463 684e 6f72 6d32 6428 3634 2c20  BatchNorm2d(64, 
+00003e90: 6570 733d 3165 2d30 352c 206d 6f6d 656e  eps=1e-05, momen
+00003ea0: 7475 6d3d 302e 312c 2061 6666 696e 653d  tum=0.1, affine=
+00003eb0: 5472 7565 2c20 7472 6163 6b5f 7275 6e6e  True, track_runn
+00003ec0: 696e 675f 7374 6174 733d 5472 7565 2929  ing_stats=True))
+00003ed0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
+00003ee0: 0a5b 375d 2070 7275 6e65 5f6f 7574 5f63  .[7] prune_out_c
+00003ef0: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
+00003f00: 656e 7457 6973 654f 705f 3138 2841 6464  entWiseOp_18(Add
+00003f10: 4261 636b 7761 7264 3029 203d 3e20 7072  Backward0) => pr
+00003f20: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
+00003f30: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
+00003f40: 4f70 5f31 3728 5265 6c75 4261 636b 7761  Op_17(ReluBackwa
+00003f50: 7264 3029 2c20 6964 7873 3d5b 322c 2036  rd0), idxs=[2, 6
+00003f60: 2c20 395d 0a5b 385d 2070 7275 6e65 5f6f  , 9].[8] prune_o
+00003f70: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
+00003f80: 456c 656d 656e 7457 6973 654f 705f 3137  ElementWiseOp_17
+00003f90: 2852 656c 7542 6163 6b77 6172 6430 2920  (ReluBackward0) 
+00003fa0: 3d3e 2070 7275 6e65 5f6f 7574 5f63 6861  => prune_out_cha
+00003fb0: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+00003fc0: 7457 6973 654f 705f 3136 2841 6464 4261  tWiseOp_16(AddBa
+00003fd0: 636b 7761 7264 3029 2c20 6964 7873 3d5b  ckward0), idxs=[
+00003fe0: 322c 2036 2c20 395d 0a5b 395d 2070 7275  2, 6, 9].[9] pru
+00003ff0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+00004000: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+00004010: 705f 3137 2852 656c 7542 6163 6b77 6172  p_17(ReluBackwar
+00004020: 6430 2920 3d3e 2070 7275 6e65 5f69 6e5f  d0) => prune_in_
+00004030: 6368 616e 6e65 6c73 206f 6e20 6c61 7965  channels on laye
+00004040: 7231 2e31 2e63 6f6e 7631 2028 436f 6e76  r1.1.conv1 (Conv
+00004050: 3264 2836 342c 2036 342c 206b 6572 6e65  2d(64, 64, kerne
+00004060: 6c5f 7369 7a65 3d28 332c 2033 292c 2073  l_size=(3, 3), s
+00004070: 7472 6964 653d 2831 2c20 3129 2c20 7061  tride=(1, 1), pa
+00004080: 6464 696e 673d 2831 2c20 3129 2c20 6269  dding=(1, 1), bi
+00004090: 6173 3d46 616c 7365 2929 2c20 6964 7873  as=False)), idxs
+000040a0: 3d5b 322c 2036 2c20 395d 0a5b 3130 5d20  =[2, 6, 9].[10] 
+000040b0: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
+000040c0: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
+000040d0: 7365 4f70 5f31 3628 4164 6442 6163 6b77  seOp_16(AddBackw
+000040e0: 6172 6430 2920 3d3e 2070 7275 6e65 5f6f  ard0) => prune_o
+000040f0: 7574 5f63 6861 6e6e 656c 7320 6f6e 206c  ut_channels on l
+00004100: 6179 6572 312e 312e 626e 3220 2842 6174  ayer1.1.bn2 (Bat
+00004110: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
+00004120: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
+00004130: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
+00004140: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
+00004150: 5f73 7461 7473 3d54 7275 6529 292c 2069  _stats=True)), i
+00004160: 6478 733d 5b32 2c20 362c 2039 5d0a 5b31  dxs=[2, 6, 9].[1
+00004170: 315d 2070 7275 6e65 5f6f 7574 5f63 6861  1] prune_out_cha
+00004180: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+00004190: 7457 6973 654f 705f 3136 2841 6464 4261  tWiseOp_16(AddBa
+000041a0: 636b 7761 7264 3029 203d 3e20 7072 756e  ckward0) => prun
+000041b0: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
+000041c0: 6e20 5f45 6c65 6d65 6e74 5769 7365 4f70  n _ElementWiseOp
+000041d0: 5f31 3528 5265 6c75 4261 636b 7761 7264  _15(ReluBackward
+000041e0: 3029 2c20 6964 7873 3d5b 322c 2036 2c20  0), idxs=[2, 6, 
+000041f0: 395d 0a5b 3132 5d20 7072 756e 655f 6f75  9].[12] prune_ou
+00004200: 745f 6368 616e 6e65 6c73 206f 6e20 5f45  t_channels on _E
+00004210: 6c65 6d65 6e74 5769 7365 4f70 5f31 3528  lementWiseOp_15(
+00004220: 5265 6c75 4261 636b 7761 7264 3029 203d  ReluBackward0) =
+00004230: 3e20 7072 756e 655f 696e 5f63 6861 6e6e  > prune_in_chann
+00004240: 656c 7320 6f6e 206c 6179 6572 322e 302e  els on layer2.0.
+00004250: 646f 776e 7361 6d70 6c65 2e30 2028 436f  downsample.0 (Co
+00004260: 6e76 3264 2836 342c 2031 3238 2c20 6b65  nv2d(64, 128, ke
+00004270: 726e 656c 5f73 697a 653d 2831 2c20 3129  rnel_size=(1, 1)
+00004280: 2c20 7374 7269 6465 3d28 322c 2032 292c  , stride=(2, 2),
+00004290: 2062 6961 733d 4661 6c73 6529 292c 2069   bias=False)), i
+000042a0: 6478 733d 5b32 2c20 362c 2039 5d0a 5b31  dxs=[2, 6, 9].[1
+000042b0: 335d 2070 7275 6e65 5f6f 7574 5f63 6861  3] prune_out_cha
+000042c0: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+000042d0: 7457 6973 654f 705f 3135 2852 656c 7542  tWiseOp_15(ReluB
+000042e0: 6163 6b77 6172 6430 2920 3d3e 2070 7275  ackward0) => pru
+000042f0: 6e65 5f69 6e5f 6368 616e 6e65 6c73 206f  ne_in_channels o
+00004300: 6e20 6c61 7965 7232 2e30 2e63 6f6e 7631  n layer2.0.conv1
+00004310: 2028 436f 6e76 3264 2836 342c 2031 3238   (Conv2d(64, 128
+00004320: 2c20 6b65 726e 656c 5f73 697a 653d 2833  , kernel_size=(3
+00004330: 2c20 3329 2c20 7374 7269 6465 3d28 322c  , 3), stride=(2,
+00004340: 2032 292c 2070 6164 6469 6e67 3d28 312c   2), padding=(1,
+00004350: 2031 292c 2062 6961 733d 4661 6c73 6529   1), bias=False)
+00004360: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
+00004370: 5d0a 5b31 345d 2070 7275 6e65 5f6f 7574  ].[14] prune_out
+00004380: 5f63 6861 6e6e 656c 7320 6f6e 206c 6179  _channels on lay
+00004390: 6572 312e 312e 626e 3220 2842 6174 6368  er1.1.bn2 (Batch
+000043a0: 4e6f 726d 3264 2836 342c 2065 7073 3d31  Norm2d(64, eps=1
+000043b0: 652d 3035 2c20 6d6f 6d65 6e74 756d 3d30  e-05, momentum=0
+000043c0: 2e31 2c20 6166 6669 6e65 3d54 7275 652c  .1, affine=True,
+000043d0: 2074 7261 636b 5f72 756e 6e69 6e67 5f73   track_running_s
+000043e0: 7461 7473 3d54 7275 6529 2920 3d3e 2070  tats=True)) => p
+000043f0: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
+00004400: 7320 6f6e 206c 6179 6572 312e 312e 636f  s on layer1.1.co
+00004410: 6e76 3220 2843 6f6e 7632 6428 3634 2c20  nv2 (Conv2d(64, 
+00004420: 3634 2c20 6b65 726e 656c 5f73 697a 653d  64, kernel_size=
+00004430: 2833 2c20 3329 2c20 7374 7269 6465 3d28  (3, 3), stride=(
+00004440: 312c 2031 292c 2070 6164 6469 6e67 3d28  1, 1), padding=(
+00004450: 312c 2031 292c 2062 6961 733d 4661 6c73  1, 1), bias=Fals
+00004460: 6529 292c 2069 6478 733d 5b32 2c20 362c  e)), idxs=[2, 6,
+00004470: 2039 5d0a 5b31 355d 2070 7275 6e65 5f6f   9].[15] prune_o
+00004480: 7574 5f63 6861 6e6e 656c 7320 6f6e 206c  ut_channels on l
+00004490: 6179 6572 312e 302e 626e 3220 2842 6174  ayer1.0.bn2 (Bat
+000044a0: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
+000044b0: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
+000044c0: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
+000044d0: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
+000044e0: 5f73 7461 7473 3d54 7275 6529 2920 3d3e  _stats=True)) =>
+000044f0: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+00004500: 656c 7320 6f6e 206c 6179 6572 312e 302e  els on layer1.0.
+00004510: 636f 6e76 3220 2843 6f6e 7632 6428 3634  conv2 (Conv2d(64
+00004520: 2c20 3634 2c20 6b65 726e 656c 5f73 697a  , 64, kernel_siz
+00004530: 653d 2833 2c20 3329 2c20 7374 7269 6465  e=(3, 3), stride
+00004540: 3d28 312c 2031 292c 2070 6164 6469 6e67  =(1, 1), padding
+00004550: 3d28 312c 2031 292c 2062 6961 733d 4661  =(1, 1), bias=Fa
+00004560: 6c73 6529 292c 2069 6478 733d 5b32 2c20  lse)), idxs=[2, 
+00004570: 362c 2039 5d0a 2d2d 2d2d 2d2d 2d2d 2d2d  6, 9].----------
+00004580: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00004590: 2d2d 2d2d 2d2d 0a60 6060 0a46 6f72 206d  ------.```.For m
+000045a0: 6f72 6520 6465 7461 696c 7320 6162 6f75  ore details abou
+000045b0: 7420 6772 6f75 7069 6e67 2c20 706c 6561  t grouping, plea
+000045c0: 7365 2072 6566 6572 2074 6f20 5b74 7574  se refer to [tut
+000045d0: 6f72 6961 6c73 2f32 202d 2045 7870 6c6f  orials/2 - Explo
+000045e0: 7269 6e67 2044 6570 656e 6465 6e63 7920  ring Dependency 
+000045f0: 4772 6f75 7073 5d28 6874 7470 733a 2f2f  Groups](https://
+00004600: 6769 7468 7562 2e63 6f6d 2f56 6169 6e46  github.com/VainF
+00004610: 2f54 6f72 6368 2d50 7275 6e69 6e67 2f62  /Torch-Pruning/b
+00004620: 6c6f 622f 6d61 7374 6572 2f74 7574 6f72  lob/master/tutor
+00004630: 6961 6c73 2f32 2532 302d 2532 3045 7870  ials/2%20-%20Exp
+00004640: 6c6f 7269 6e67 2532 3044 6570 656e 6465  loring%20Depende
+00004650: 6e63 7925 3230 4772 6f75 7073 2e69 7079  ncy%20Groups.ipy
+00004660: 6e62 290a 2020 0a23 2323 2320 486f 7720  nb).  .#### How 
+00004670: 746f 2073 6361 6e20 616c 6c20 6772 6f75  to scan all grou
+00004680: 7073 2028 4164 7661 6e63 6564 293a 0a57  ps (Advanced):.W
+00004690: 6520 6361 6e20 7573 6520 6060 4447 2e67  e can use ``DG.g
+000046a0: 6574 5f61 6c6c 5f67 726f 7570 7328 6967  et_all_groups(ig
+000046b0: 6e6f 7265 645f 6c61 7965 7273 2c20 726f  nored_layers, ro
+000046c0: 6f74 5f6d 6f64 756c 655f 7479 7065 7329  ot_module_types)
+000046d0: 6060 2074 6f20 7363 616e 2061 6c6c 2067  `` to scan all g
+000046e0: 726f 7570 7320 7365 7175 656e 7469 616c  roups sequential
+000046f0: 6c79 2e20 4561 6368 2067 726f 7570 2077  ly. Each group w
+00004700: 696c 6c20 6265 6769 6e20 7769 7468 2061  ill begin with a
+00004710: 206c 6179 6572 2074 6861 7420 6d61 7463   layer that matc
+00004720: 6865 7320 6120 7479 7065 2069 6e20 7468  hes a type in th
+00004730: 6520 2272 6f6f 745f 6d6f 6475 6c65 5f74  e "root_module_t
+00004740: 7970 6573 2220 7061 7261 6d65 7465 722e  ypes" parameter.
+00004750: 204e 6f74 6520 7468 6174 2044 472e 6765   Note that DG.ge
+00004760: 745f 616c 6c5f 6772 6f75 7073 2069 7320  t_all_groups is 
+00004770: 6f6e 6c79 2072 6573 706f 6e73 6962 6c65  only responsible
+00004780: 2066 6f72 2067 726f 7570 696e 6720 616e   for grouping an
+00004790: 6420 646f 6573 206e 6f74 2068 6176 6520  d does not have 
+000047a0: 616e 7920 6b6e 6f77 6c65 6467 6520 6f72  any knowledge or
+000047b0: 2075 6e64 6572 7374 616e 6469 6e67 206f   understanding o
+000047c0: 6620 7768 6963 6820 7061 7261 6d65 7465  f which paramete
+000047d0: 7273 2073 686f 756c 6420 6265 2070 7275  rs should be pru
+000047e0: 6e65 642e 2054 6865 7265 666f 7265 2c20  ned. Therefore, 
+000047f0: 6974 2069 7320 6e65 6365 7373 6172 7920  it is necessary 
+00004800: 746f 2073 7065 6369 6679 2074 6865 2070  to specify the p
+00004810: 7275 6e69 6e67 2069 6478 7320 7573 696e  runing idxs usin
+00004820: 6720 2060 6067 726f 7570 2e70 7275 6e65  g  ``group.prune
+00004830: 2869 6478 733d 6964 7873 2960 602e 0a0a  (idxs=idxs)``...
+00004840: 6060 6070 7974 686f 6e0a 666f 7220 6772  ```python.for gr
+00004850: 6f75 7020 696e 2044 472e 6765 745f 616c  oup in DG.get_al
+00004860: 6c5f 6772 6f75 7073 2869 676e 6f72 6564  l_groups(ignored
+00004870: 5f6c 6179 6572 733d 5b6d 6f64 656c 2e63  _layers=[model.c
+00004880: 6f6e 7631 5d2c 2072 6f6f 745f 6d6f 6475  onv1], root_modu
+00004890: 6c65 5f74 7970 6573 3d5b 6e6e 2e43 6f6e  le_types=[nn.Con
+000048a0: 7632 642c 206e 6e2e 4c69 6e65 6172 5d29  v2d, nn.Linear])
+000048b0: 3a0a 2020 2020 2320 6861 6e64 6c65 2067  :.    # handle g
+000048c0: 726f 7570 7320 696e 2073 6571 7565 6e74  roups in sequent
+000048d0: 6961 6c20 6f72 6465 720a 2020 2020 6964  ial order.    id
+000048e0: 7873 203d 205b 322c 342c 365d 2023 2079  xs = [2,4,6] # y
+000048f0: 6f75 7220 7072 756e 696e 6720 696e 6469  our pruning indi
+00004900: 6365 730a 2020 2020 6772 6f75 702e 7072  ces.    group.pr
+00004910: 756e 6528 6964 7873 3d69 6478 7329 0a20  une(idxs=idxs). 
+00004920: 2020 2070 7269 6e74 2867 726f 7570 290a     print(group).
+00004930: 6060 600a 0a23 2323 2032 2e20 4869 6768  ```..### 2. High
+00004940: 2d6c 6576 656c 2050 7275 6e65 7273 0a0a  -level Pruners..
+00004950: 4c65 7665 7261 6769 6e67 2074 6865 2044  Leveraging the D
+00004960: 6570 656e 6465 6e63 7947 7261 7068 2c20  ependencyGraph, 
+00004970: 7765 2064 6576 656c 6f70 6564 2073 6576  we developed sev
+00004980: 6572 616c 2068 6967 682d 6c65 7665 6c20  eral high-level 
+00004990: 7072 756e 6572 7320 696e 2074 6869 7320  pruners in this 
+000049a0: 7265 706f 7369 746f 7279 2074 6f20 6661  repository to fa
+000049b0: 6369 6c69 7461 7465 2065 6666 6f72 746c  cilitate effortl
+000049c0: 6573 7320 7072 756e 696e 672e 2042 7920  ess pruning. By 
+000049d0: 7370 6563 6966 7969 6e67 2074 6865 2064  specifying the d
+000049e0: 6573 6972 6564 2063 6861 6e6e 656c 2073  esired channel s
+000049f0: 7061 7273 6974 792c 2079 6f75 2063 616e  parsity, you can
+00004a00: 2070 7275 6e65 2074 6865 2065 6e74 6972   prune the entir
+00004a10: 6520 6d6f 6465 6c20 616e 6420 6669 6e65  e model and fine
+00004a20: 2d74 756e 6520 6974 2075 7369 6e67 2079  -tune it using y
+00004a30: 6f75 7220 6f77 6e20 7472 6169 6e69 6e67  our own training
+00004a40: 2063 6f64 652e 2046 6f72 2064 6574 6169   code. For detai
+00004a50: 6c65 6420 696e 666f 726d 6174 696f 6e20  led information 
+00004a60: 6f6e 2074 6869 7320 7072 6f63 6573 732c  on this process,
+00004a70: 2070 6c65 6173 6520 7265 6665 7220 746f   please refer to
+00004a80: 205b 7468 6973 2074 7574 6f72 6961 6c5d   [this tutorial]
+00004a90: 2868 7474 7073 3a2f 2f67 6974 6875 622e  (https://github.
+00004aa0: 636f 6d2f 5661 696e 462f 546f 7263 682d  com/VainF/Torch-
+00004ab0: 5072 756e 696e 672f 626c 6f62 2f6d 6173  Pruning/blob/mas
+00004ac0: 7465 722f 7475 746f 7269 616c 732f 3125  ter/tutorials/1%
+00004ad0: 3230 2d25 3230 4375 7374 6f6d 697a 6525  20-%20Customize%
+00004ae0: 3230 596f 7572 2532 304f 776e 2532 3050  20Your%20Own%20P
+00004af0: 7275 6e65 7273 2e69 7079 6e62 292c 2077  runers.ipynb), w
+00004b00: 6869 6368 2073 686f 7773 2068 6f77 2074  hich shows how t
+00004b10: 6f20 696d 706c 656d 656e 7420 6120 5b73  o implement a [s
+00004b20: 6c69 6d6d 696e 675d 2868 7474 7073 3a2f  limming](https:/
+00004b30: 2f61 7278 6976 2e6f 7267 2f61 6273 2f31  /arxiv.org/abs/1
+00004b40: 3730 382e 3036 3531 3929 2070 7275 6e65  708.06519) prune
+00004b50: 7220 6672 6f6d 2073 6372 6174 6368 2e20  r from scratch. 
+00004b60: 4164 6469 7469 6f6e 616c 6c79 2c20 796f  Additionally, yo
+00004b70: 7520 6361 6e20 6669 6e64 206d 6f72 6520  u can find more 
+00004b80: 7072 6163 7469 6361 6c20 6578 616d 706c  practical exampl
+00004b90: 6573 2069 6e20 5b62 656e 6368 6d61 726b  es in [benchmark
+00004ba0: 732f 6d61 696e 2e70 795d 2862 656e 6368  s/main.py](bench
+00004bb0: 6d61 726b 732f 6d61 696e 2e70 7929 2e0a  marks/main.py)..
+00004bc0: 0a60 6060 7079 7468 6f6e 0a69 6d70 6f72  .```python.impor
+00004bd0: 7420 746f 7263 680a 6672 6f6d 2074 6f72  t torch.from tor
+00004be0: 6368 7669 7369 6f6e 2e6d 6f64 656c 7320  chvision.models 
+00004bf0: 696d 706f 7274 2072 6573 6e65 7431 380a  import resnet18.
+00004c00: 696d 706f 7274 2074 6f72 6368 5f70 7275  import torch_pru
+00004c10: 6e69 6e67 2061 7320 7470 0a0a 6d6f 6465  ning as tp..mode
+00004c20: 6c20 3d20 7265 736e 6574 3138 2870 7265  l = resnet18(pre
+00004c30: 7472 6169 6e65 643d 5472 7565 290a 0a23  trained=True)..#
+00004c40: 2049 6d70 6f72 7461 6e63 6520 6372 6974   Importance crit
+00004c50: 6572 6961 0a65 7861 6d70 6c65 5f69 6e70  eria.example_inp
+00004c60: 7574 7320 3d20 746f 7263 682e 7261 6e64  uts = torch.rand
+00004c70: 6e28 312c 2033 2c20 3232 342c 2032 3234  n(1, 3, 224, 224
+00004c80: 290a 696d 7020 3d20 7470 2e69 6d70 6f72  ).imp = tp.impor
+00004c90: 7461 6e63 652e 5461 796c 6f72 496d 706f  tance.TaylorImpo
+00004ca0: 7274 616e 6365 2829 0a0a 6967 6e6f 7265  rtance()..ignore
+00004cb0: 645f 6c61 7965 7273 203d 205b 5d0a 666f  d_layers = [].fo
+00004cc0: 7220 6d20 696e 206d 6f64 656c 2e6d 6f64  r m in model.mod
+00004cd0: 756c 6573 2829 3a0a 2020 2020 6966 2069  ules():.    if i
+00004ce0: 7369 6e73 7461 6e63 6528 6d2c 2074 6f72  sinstance(m, tor
+00004cf0: 6368 2e6e 6e2e 4c69 6e65 6172 2920 616e  ch.nn.Linear) an
+00004d00: 6420 6d2e 6f75 745f 6665 6174 7572 6573  d m.out_features
+00004d10: 203d 3d20 3130 3030 3a0a 2020 2020 2020   == 1000:.      
+00004d20: 2020 6967 6e6f 7265 645f 6c61 7965 7273    ignored_layers
+00004d30: 2e61 7070 656e 6428 6d29 2023 2044 4f20  .append(m) # DO 
+00004d40: 4e4f 5420 7072 756e 6520 7468 6520 6669  NOT prune the fi
+00004d50: 6e61 6c20 636c 6173 7369 6669 6572 210a  nal classifier!.
+00004d60: 0a69 7465 7261 7469 7665 5f73 7465 7073  .iterative_steps
+00004d70: 203d 2035 2023 2070 726f 6772 6573 7369   = 5 # progressi
+00004d80: 7665 2070 7275 6e69 6e67 0a70 7275 6e65  ve pruning.prune
+00004d90: 7220 3d20 7470 2e70 7275 6e65 722e 4d61  r = tp.pruner.Ma
+00004da0: 676e 6974 7564 6550 7275 6e65 7228 0a20  gnitudePruner(. 
+00004db0: 2020 206d 6f64 656c 2c0a 2020 2020 6578     model,.    ex
+00004dc0: 616d 706c 655f 696e 7075 7473 2c0a 2020  ample_inputs,.  
+00004dd0: 2020 696d 706f 7274 616e 6365 3d69 6d70    importance=imp
+00004de0: 2c0a 2020 2020 6974 6572 6174 6976 655f  ,.    iterative_
+00004df0: 7374 6570 733d 6974 6572 6174 6976 655f  steps=iterative_
+00004e00: 7374 6570 732c 0a20 2020 2063 685f 7370  steps,.    ch_sp
+00004e10: 6172 7369 7479 3d30 2e35 2c20 2320 7265  arsity=0.5, # re
+00004e20: 6d6f 7665 2035 3025 2063 6861 6e6e 656c  move 50% channel
+00004e30: 732c 2052 6573 4e65 7431 3820 3d20 7b36  s, ResNet18 = {6
+00004e40: 342c 2031 3238 2c20 3235 362c 2035 3132  4, 128, 256, 512
+00004e50: 7d20 3d3e 2052 6573 4e65 7431 385f 4861  } => ResNet18_Ha
+00004e60: 6c66 203d 207b 3332 2c20 3634 2c20 3132  lf = {32, 64, 12
+00004e70: 382c 2032 3536 7d0a 2020 2020 6967 6e6f  8, 256}.    igno
+00004e80: 7265 645f 6c61 7965 7273 3d69 676e 6f72  red_layers=ignor
+00004e90: 6564 5f6c 6179 6572 732c 0a29 0a0a 6261  ed_layers,.)..ba
+00004ea0: 7365 5f6d 6163 732c 2062 6173 655f 6e70  se_macs, base_np
+00004eb0: 6172 616d 7320 3d20 7470 2e75 7469 6c73  arams = tp.utils
+00004ec0: 2e63 6f75 6e74 5f6f 7073 5f61 6e64 5f70  .count_ops_and_p
+00004ed0: 6172 616d 7328 6d6f 6465 6c2c 2065 7861  arams(model, exa
+00004ee0: 6d70 6c65 5f69 6e70 7574 7329 0a66 6f72  mple_inputs).for
+00004ef0: 2069 2069 6e20 7261 6e67 6528 6974 6572   i in range(iter
+00004f00: 6174 6976 655f 7374 6570 7329 3a0a 2020  ative_steps):.  
+00004f10: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
+00004f20: 696d 702c 2074 702e 696d 706f 7274 616e  imp, tp.importan
+00004f30: 6365 2e54 6179 6c6f 7249 6d70 6f72 7461  ce.TaylorImporta
+00004f40: 6e63 6529 3a0a 2020 2020 2020 2020 2320  nce):.        # 
+00004f50: 5461 796c 6f72 2065 7870 616e 7369 6f6e  Taylor expansion
+00004f60: 2072 6571 7569 7265 7320 6772 6164 6965   requires gradie
+00004f70: 6e74 7320 666f 7220 696d 706f 7274 616e  nts for importan
+00004f80: 6365 2065 7374 696d 6174 696f 6e0a 2020  ce estimation.  
+00004f90: 2020 2020 2020 6c6f 7373 203d 206d 6f64        loss = mod
+00004fa0: 656c 2865 7861 6d70 6c65 5f69 6e70 7574  el(example_input
+00004fb0: 7329 2e73 756d 2829 2023 2061 2064 756d  s).sum() # a dum
+00004fc0: 6d79 206c 6f73 7320 666f 7220 5461 796c  my loss for Tayl
+00004fd0: 6f72 496d 706f 7274 616e 6365 0a20 2020  orImportance.   
+00004fe0: 2020 2020 206c 6f73 732e 6261 636b 7761       loss.backwa
+00004ff0: 7264 2829 2023 2062 6566 6f72 6520 7072  rd() # before pr
+00005000: 756e 6572 2e73 7465 7028 290a 2020 2020  uner.step().    
+00005010: 7072 756e 6572 2e73 7465 7028 290a 2020  pruner.step().  
+00005020: 2020 6d61 6373 2c20 6e70 6172 616d 7320    macs, nparams 
+00005030: 3d20 7470 2e75 7469 6c73 2e63 6f75 6e74  = tp.utils.count
+00005040: 5f6f 7073 5f61 6e64 5f70 6172 616d 7328  _ops_and_params(
+00005050: 6d6f 6465 6c2c 2065 7861 6d70 6c65 5f69  model, example_i
+00005060: 6e70 7574 7329 0a20 2020 2023 2066 696e  nputs).    # fin
+00005070: 6574 756e 6520 796f 7572 206d 6f64 656c  etune your model
+00005080: 2068 6572 650a 2020 2020 2320 6669 6e65   here.    # fine
+00005090: 7475 6e65 286d 6f64 656c 290a 2020 2020  tune(model).    
+000050a0: 2320 2e2e 2e0a 6060 600a 0a23 2323 2320  # ....```..#### 
+000050b0: 5370 6172 7365 2054 7261 696e 696e 670a  Sparse Training.
+000050c0: 536f 6d65 2070 7275 6e65 7273 206c 696b  Some pruners lik
+000050d0: 6520 5b42 4e53 6361 6c65 5072 756e 6572  e [BNScalePruner
+000050e0: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
+000050f0: 2e63 6f6d 2f56 6169 6e46 2f54 6f72 6368  .com/VainF/Torch
+00005100: 2d50 7275 6e69 6e67 2f62 6c6f 622f 6464  -Pruning/blob/dd
+00005110: 3539 3932 3133 3635 6437 3261 6362 3238  59921365d72acb28
+00005120: 3537 6433 6437 3466 3735 6330 3365 3437  57d3d74f75c03e47
+00005130: 3730 3630 6662 2f74 6f72 6368 5f70 7275  7060fb/torch_pru
+00005140: 6e69 6e67 2f70 7275 6e65 722f 616c 676f  ning/pruner/algo
+00005150: 7269 7468 6d73 2f62 6174 6368 6e6f 726d  rithms/batchnorm
+00005160: 5f73 6361 6c65 5f70 7275 6e65 722e 7079  _scale_pruner.py
+00005170: 234c 3435 2920 616e 6420 5b47 726f 7570  #L45) and [Group
+00005180: 4e6f 726d 5072 756e 6572 5d28 6874 7470  NormPruner](http
+00005190: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
+000051a0: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
+000051b0: 6e67 2f62 6c6f 622f 6464 3539 3932 3133  ng/blob/dd599213
+000051c0: 3635 6437 3261 6362 3238 3537 6433 6437  65d72acb2857d3d7
+000051d0: 3466 3735 6330 3365 3437 3730 3630 6662  4f75c03e477060fb
+000051e0: 2f74 6f72 6368 5f70 7275 6e69 6e67 2f70  /torch_pruning/p
+000051f0: 7275 6e65 722f 616c 676f 7269 7468 6d73  runer/algorithms
+00005200: 2f67 726f 7570 5f6e 6f72 6d5f 7072 756e  /group_norm_prun
+00005210: 6572 2e70 7923 4c35 3329 2072 6571 7569  er.py#L53) requi
+00005220: 7265 2073 7061 7273 6520 7472 6169 6e69  re sparse traini
+00005230: 6e67 2062 6566 6f72 6520 7072 756e 696e  ng before prunin
+00005240: 672e 2054 6869 7320 6361 6e20 6265 2065  g. This can be e
+00005250: 6173 696c 7920 6163 6869 6576 6564 2062  asily achieved b
+00005260: 7920 696e 7365 7274 696e 6720 6a75 7374  y inserting just
+00005270: 206f 6e65 206c 696e 6520 6f66 2063 6f64   one line of cod
+00005280: 6520 6060 7072 756e 6572 2e72 6567 756c  e ``pruner.regul
+00005290: 6172 697a 6528 6d6f 6465 6c29 6060 2069  arize(model)`` i
+000052a0: 6e20 796f 7572 2074 7261 696e 696e 6720  n your training 
+000052b0: 7363 7269 7074 2e20 5468 6520 7072 756e  script. The prun
+000052c0: 6572 2077 696c 6c20 7570 6461 7465 2074  er will update t
+000052d0: 6865 2067 7261 6469 656e 7420 6f66 2074  he gradient of t
+000052e0: 7261 696e 6162 6c65 2070 6172 616d 6574  rainable paramet
+000052f0: 6572 732e 0a60 6060 7079 7468 6f6e 0a66  ers..```python.f
+00005300: 6f72 2065 706f 6368 2069 6e20 7261 6e67  or epoch in rang
+00005310: 6528 6570 6f63 6873 293a 0a20 2020 206d  e(epochs):.    m
+00005320: 6f64 656c 2e74 7261 696e 2829 0a20 2020  odel.train().   
+00005330: 2066 6f72 2069 2c20 2864 6174 612c 2074   for i, (data, t
+00005340: 6172 6765 7429 2069 6e20 656e 756d 6572  arget) in enumer
+00005350: 6174 6528 7472 6169 6e5f 6c6f 6164 6572  ate(train_loader
+00005360: 293a 0a20 2020 2020 2020 2064 6174 612c  ):.        data,
+00005370: 2074 6172 6765 7420 3d20 6461 7461 2e74   target = data.t
+00005380: 6f28 6465 7669 6365 292c 2074 6172 6765  o(device), targe
+00005390: 742e 746f 2864 6576 6963 6529 0a20 2020  t.to(device).   
+000053a0: 2020 2020 206f 7074 696d 697a 6572 2e7a       optimizer.z
+000053b0: 6572 6f5f 6772 6164 2829 0a20 2020 2020  ero_grad().     
+000053c0: 2020 206f 7574 203d 206d 6f64 656c 2864     out = model(d
+000053d0: 6174 6129 0a20 2020 2020 2020 206c 6f73  ata).        los
+000053e0: 7320 3d20 462e 6372 6f73 735f 656e 7472  s = F.cross_entr
+000053f0: 6f70 7928 6f75 742c 2074 6172 6765 7429  opy(out, target)
+00005400: 0a20 2020 2020 2020 206c 6f73 732e 6261  .        loss.ba
+00005410: 636b 7761 7264 2829 0a20 2020 2020 2020  ckward().       
+00005420: 2070 7275 6e65 722e 7265 6775 6c61 7269   pruner.regulari
+00005430: 7a65 286d 6f64 656c 2920 2320 3c3d 3d20  ze(model) # <== 
+00005440: 666f 7220 7370 6172 7365 206c 6561 726e  for sparse learn
+00005450: 696e 670a 2020 2020 2020 2020 6f70 7469  ing.        opti
+00005460: 6d69 7a65 722e 7374 6570 2829 0a60 6060  mizer.step().```
+00005470: 0a0a 2323 2323 2049 6e74 6572 6163 7469  ..#### Interacti
+00005480: 7665 2050 7275 6e69 6e67 2028 4164 7661  ve Pruning (Adva
+00005490: 6e63 6564 290a 416c 6c20 6869 6768 2d6c  nced).All high-l
+000054a0: 6576 656c 2070 7275 6e65 7273 2073 7570  evel pruners sup
+000054b0: 706f 7274 2069 6e74 6572 6163 7469 7665  port interactive
+000054c0: 2070 7275 6e69 6e67 2e20 5573 6520 6060   pruning. Use ``
+000054d0: 7072 756e 6572 2e73 7465 7028 696e 7465  pruner.step(inte
+000054e0: 7261 6374 6976 653d 5472 7565 2960 6020  ractive=True)`` 
+000054f0: 746f 2067 6574 2061 6c6c 2067 726f 7570  to get all group
+00005500: 7320 616e 6420 696e 7465 7261 6374 6976  s and interactiv
+00005510: 656c 7920 7072 756e 6520 7468 656d 2062  ely prune them b
+00005520: 7920 6361 6c6c 696e 6720 6060 6772 6f75  y calling ``grou
+00005530: 702e 7072 756e 6528 2960 602e 2054 6869  p.prune()``. Thi
+00005540: 7320 6665 6174 7572 6520 6973 2075 7365  s feature is use
+00005550: 6675 6c20 6966 2079 6f75 2077 616e 7420  ful if you want 
+00005560: 746f 2063 6f6e 7472 6f6c 2f6d 6f6e 6974  to control/monit
+00005570: 6f72 2074 6865 2070 7275 6e69 6e67 2070  or the pruning p
+00005580: 726f 6365 7373 2e0a 0a60 6060 7079 7468  rocess...```pyth
+00005590: 6f6e 0a66 6f72 2069 2069 6e20 7261 6e67  on.for i in rang
+000055a0: 6528 6974 6572 6174 6976 655f 7374 6570  e(iterative_step
+000055b0: 7329 3a0a 2020 2020 666f 7220 6772 6f75  s):.    for grou
+000055c0: 7020 696e 2070 7275 6e65 722e 7374 6570  p in pruner.step
+000055d0: 2869 6e74 6572 6163 7469 7665 3d54 7275  (interactive=Tru
+000055e0: 6529 3a20 2320 5761 726e 696e 673a 2067  e): # Warning: g
+000055f0: 726f 7570 7320 6d75 7374 2062 6520 6861  roups must be ha
+00005600: 6e64 6c65 6420 7365 7175 656e 7469 616c  ndled sequential
+00005610: 6c79 2e20 446f 206e 6f74 206b 6565 7020  ly. Do not keep 
+00005620: 7468 656d 2061 7320 6120 6c69 7374 2e0a  them as a list..
+00005630: 2020 2020 2020 2020 7072 696e 7428 6772          print(gr
+00005640: 6f75 7029 200a 2020 2020 2020 2020 2320  oup) .        # 
+00005650: 646f 2077 6861 7465 7665 7220 796f 7520  do whatever you 
+00005660: 6c69 6b65 2077 6974 6820 7468 6520 6772  like with the gr
+00005670: 6f75 7020 0a20 2020 2020 2020 2064 6570  oup .        dep
+00005680: 2c20 6964 7873 203d 2067 726f 7570 5b30  , idxs = group[0
+00005690: 5d20 2320 6765 7420 7468 6520 6964 7873  ] # get the idxs
+000056a0: 0a20 2020 2020 2020 2074 6172 6765 745f  .        target_
+000056b0: 6d6f 6475 6c65 203d 2064 6570 2e74 6172  module = dep.tar
+000056c0: 6765 742e 6d6f 6475 6c65 2023 2067 6574  get.module # get
+000056d0: 2074 6865 2072 6f6f 7420 6d6f 6475 6c65   the root module
+000056e0: 0a20 2020 2020 2020 2070 7275 6e69 6e67  .        pruning
+000056f0: 5f66 6e20 3d20 6465 702e 6861 6e64 6c65  _fn = dep.handle
+00005700: 7220 2320 6765 7420 7468 6520 7072 756e  r # get the prun
+00005710: 696e 6720 6675 6e63 7469 6f6e 0a20 2020  ing function.   
+00005720: 2020 2020 0a20 2020 2020 2020 2023 2044      .        # D
+00005730: 6f6e 2774 2066 6f72 6765 7420 746f 2070  on't forget to p
+00005740: 7275 6e65 2074 6865 2067 726f 7570 0a20  rune the group. 
+00005750: 2020 2020 2020 2067 726f 7570 2e70 7275         group.pru
+00005760: 6e65 2829 0a20 2020 2020 2020 2020 200a  ne().          .
+00005770: 2020 2020 2020 2020 2320 6772 6f75 702e          # group.
+00005780: 7072 756e 6528 6964 7873 3d5b 302c 2032  prune(idxs=[0, 2
+00005790: 2c20 365d 2920 2320 4974 2069 7320 6576  , 6]) # It is ev
+000057a0: 656e 2070 6f73 7369 626c 6520 746f 2063  en possible to c
+000057b0: 6861 6e67 6520 7468 6520 7072 756e 696e  hange the prunin
+000057c0: 6720 6265 6861 7669 6f75 7220 7769 7468  g behaviour with
+000057d0: 2074 6865 2069 6478 7320 7061 7261 6d65   the idxs parame
+000057e0: 7465 720a 2020 2020 6d61 6373 2c20 6e70  ter.    macs, np
+000057f0: 6172 616d 7320 3d20 7470 2e75 7469 6c73  arams = tp.utils
+00005800: 2e63 6f75 6e74 5f6f 7073 5f61 6e64 5f70  .count_ops_and_p
+00005810: 6172 616d 7328 6d6f 6465 6c2c 2065 7861  arams(model, exa
+00005820: 6d70 6c65 5f69 6e70 7574 7329 0a20 2020  mple_inputs).   
+00005830: 2023 2066 696e 6574 756e 6520 796f 7572   # finetune your
+00005840: 206d 6f64 656c 2068 6572 650a 2020 2020   model here.    
+00005850: 2320 6669 6e65 7475 6e65 286d 6f64 656c  # finetune(model
+00005860: 290a 2020 2020 2320 2e2e 2e0a 6060 600a  ).    # ....```.
+00005870: 0a23 2323 2320 4772 6f75 702d 6c65 7665  .#### Group-leve
+00005880: 6c20 5072 756e 696e 670a 0a57 6974 6820  l Pruning..With 
+00005890: 4465 7047 7261 7068 2c20 6974 2069 7320  DepGraph, it is 
+000058a0: 6561 7379 2074 6f20 6465 7369 676e 2073  easy to design s
+000058b0: 6f6d 6520 2267 726f 7570 2d6c 6576 656c  ome "group-level
+000058c0: 2220 6372 6974 6572 6961 2074 6f20 6573  " criteria to es
+000058d0: 7469 6d61 7465 2074 6865 2069 6d70 6f72  timate the impor
+000058e0: 7461 6e63 6520 6f66 2061 2077 686f 6c65  tance of a whole
+000058f0: 2067 726f 7570 2072 6174 6865 7220 7468   group rather th
+00005900: 616e 2061 2073 696e 676c 6520 6c61 7965  an a single laye
+00005910: 722e 2049 6e20 546f 7263 682d 7072 756e  r. In Torch-prun
+00005920: 696e 672c 2061 6c6c 2070 7275 6e65 7273  ing, all pruners
+00005930: 2077 6f72 6b20 696e 2074 6865 2067 726f   work in the gro
+00005940: 7570 206c 6576 656c 2e0a 0a3c 6469 7620  up level...<div 
+00005950: 616c 6967 6e3d 2263 656e 7465 7222 3e0a  align="center">.
+00005960: 3c69 6d67 2073 7263 3d22 6173 7365 7473  <img src="assets
+00005970: 2f67 726f 7570 5f73 7061 7273 6974 792e  /group_sparsity.
+00005980: 706e 6722 2077 6964 7468 3d22 3830 2522  png" width="80%"
+00005990: 3e0a 3c2f 6469 763e 0a0a 2323 2320 332e  >.</div>..### 3.
+000059a0: 2053 6176 6520 2620 4c6f 6164 0a20 2020   Save & Load.   
+000059b0: 2020 2020 2020 200a 5468 6520 666f 6c6c         .The foll
+000059c0: 6f77 696e 6720 7363 7269 7074 2073 6176  owing script sav
+000059d0: 6573 2074 6865 2077 686f 6c65 206d 6f64  es the whole mod
+000059e0: 656c 206f 626a 6563 7420 2873 7472 7563  el object (struc
+000059f0: 7475 7265 2b77 6569 6768 7473 2920 6173  ture+weights) as
+00005a00: 2061 2027 6d6f 6465 6c2e 7074 6827 2e20   a 'model.pth'. 
+00005a10: 0a60 6060 7079 7468 6f6e 0a6d 6f64 656c  .```python.model
+00005a20: 2e7a 6572 6f5f 6772 6164 2829 2023 2057  .zero_grad() # W
+00005a30: 6520 646f 6e27 7420 7761 6e74 2074 6f20  e don't want to 
+00005a40: 7374 6f72 6520 6772 6164 6965 6e74 2069  store gradient i
+00005a50: 6e66 6f72 6d61 7469 6f6e 0a74 6f72 6368  nformation.torch
+00005a60: 2e73 6176 6528 6d6f 6465 6c2c 2027 6d6f  .save(model, 'mo
+00005a70: 6465 6c2e 7074 6827 2920 2320 7769 7468  del.pth') # with
+00005a80: 6f75 7420 2e73 7461 7465 5f64 6963 740a  out .state_dict.
+00005a90: 6d6f 6465 6c20 3d20 746f 7263 682e 6c6f  model = torch.lo
+00005aa0: 6164 2827 6d6f 6465 6c2e 7074 6827 2920  ad('model.pth') 
+00005ab0: 2320 6c6f 6164 2074 6865 2070 7275 6e65  # load the prune
+00005ac0: 6420 6d6f 6465 6c0a 6060 600a 0a2a 2a45  d model.```..**E
+00005ad0: 7870 6572 696d 656e 7461 6c20 4665 6174  xperimental Feat
+00005ae0: 7572 6573 2a2a 3a20 5265 2d63 7265 6174  ures**: Re-creat
+00005af0: 6520 7072 756e 6564 206d 6f64 656c 7320  e pruned models 
+00005b00: 6672 6f6d 2075 6e70 7275 6e65 6420 6f6e  from unpruned on
+00005b10: 6573 2075 7369 6e67 2060 6074 702e 7374  es using ``tp.st
+00005b20: 6174 655f 6469 6374 6060 2061 6e64 2060  ate_dict`` and `
+00005b30: 6074 702e 6c6f 6164 5f73 7461 7465 5f64  `tp.load_state_d
+00005b40: 6963 7460 602e 0a60 6060 7079 7468 6f6e  ict``..```python
+00005b50: 0a23 2073 6176 6520 7468 6520 7072 756e  .# save the prun
+00005b60: 6564 2073 7461 7465 5f64 6963 742c 2077  ed state_dict, w
+00005b70: 6869 6368 2069 6e63 6c75 6465 7320 626f  hich includes bo
+00005b80: 7468 2070 7275 6e65 6420 7061 7261 6d65  th pruned parame
+00005b90: 7465 7273 2061 6e64 206d 6f64 6966 6965  ters and modifie
+00005ba0: 6420 6174 7472 6962 7574 6573 0a73 7461  d attributes.sta
+00005bb0: 7465 5f64 6963 7420 3d20 7470 2e73 7461  te_dict = tp.sta
+00005bc0: 7465 5f64 6963 7428 7072 756e 6564 5f6d  te_dict(pruned_m
+00005bd0: 6f64 656c 2920 2320 7468 6520 7072 756e  odel) # the prun
+00005be0: 6564 206d 6f64 656c 2c20 652e 672e 2c20  ed model, e.g., 
+00005bf0: 6120 7265 736e 6574 2d31 382d 6861 6c66  a resnet-18-half
+00005c00: 0a74 6f72 6368 2e73 6176 6528 7374 6174  .torch.save(stat
+00005c10: 655f 6469 6374 2c20 2770 7275 6e65 642e  e_dict, 'pruned.
+00005c20: 7074 6827 290a 0a23 2063 7265 6174 6520  pth')..# create 
+00005c30: 6120 6e65 7720 6d6f 6465 6c2c 2065 2e67  a new model, e.g
+00005c40: 2e20 7265 736e 6574 3138 0a6e 6577 5f6d  . resnet18.new_m
+00005c50: 6f64 656c 203d 2072 6573 6e65 7431 3828  odel = resnet18(
+00005c60: 292e 6576 616c 2829 0a0a 2320 6c6f 6164  ).eval()..# load
+00005c70: 2074 6865 2070 7275 6e65 6420 7374 6174   the pruned stat
+00005c80: 655f 6469 6374 2069 6e74 6f20 7468 6520  e_dict into the 
+00005c90: 756e 7072 756e 6564 206d 6f64 656c 2e0a  unpruned model..
+00005ca0: 6c6f 6164 6564 5f73 7461 7465 5f64 6963  loaded_state_dic
+00005cb0: 7420 3d20 746f 7263 682e 6c6f 6164 2827  t = torch.load('
+00005cc0: 7072 756e 6564 2e70 7468 272c 206d 6170  pruned.pth', map
+00005cd0: 5f6c 6f63 6174 696f 6e3d 2763 7075 2729  _location='cpu')
+00005ce0: 0a74 702e 6c6f 6164 5f73 7461 7465 5f64  .tp.load_state_d
+00005cf0: 6963 7428 6e65 775f 6d6f 6465 6c2c 2073  ict(new_model, s
+00005d00: 7461 7465 5f64 6963 743d 6c6f 6164 6564  tate_dict=loaded
+00005d10: 5f73 7461 7465 5f64 6963 7429 0a70 7269  _state_dict).pri
+00005d20: 6e74 286e 6577 5f6d 6f64 656c 2920 2320  nt(new_model) # 
+00005d30: 5468 6973 2077 696c 6c20 6265 2061 2070  This will be a p
+00005d40: 7275 6e65 6420 6d6f 6465 6c2e 0a60 6060  runed model..```
+00005d50: 0a52 6566 6572 2074 6f20 5b74 6573 7473  .Refer to [tests
+00005d60: 2f74 6573 745f 7365 7269 616c 697a 6174  /test_serializat
+00005d70: 696f 6e2e 7079 5d28 7465 7374 732f 7465  ion.py](tests/te
+00005d80: 7374 5f73 6572 6961 6c69 7a61 7469 6f6e  st_serialization
+00005d90: 2e70 7929 2066 6f72 2061 6e20 5669 5420  .py) for an ViT 
+00005da0: 6578 616d 706c 652e 2049 6e20 7468 6973  example. In this
+00005db0: 2065 7861 6d70 6c65 2c20 7765 2077 696c   example, we wil
+00005dc0: 6c20 7072 756e 6520 7468 6520 6d6f 6465  l prune the mode
+00005dd0: 6c20 616e 6420 6d6f 6469 6679 2073 6f6d  l and modify som
+00005de0: 6520 6174 7472 6962 7574 6573 206c 696b  e attributes lik
+00005df0: 6520 6060 6d6f 6465 6c2e 6869 6464 656e  e ``model.hidden
+00005e00: 5f64 696d 7360 602e 0a20 2020 2020 2020  _dims``..       
+00005e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005e30: 200a 2020 2020 2020 2020 2020 2020 2020   .              
+00005e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005e50: 2020 0a23 2323 2034 2e20 4c6f 772d 6c65    .### 4. Low-le
+00005e60: 7665 6c20 5072 756e 696e 6720 4675 6e63  vel Pruning Func
+00005e70: 7469 6f6e 730a 0a57 6869 6c65 2069 7420  tions..While it 
+00005e80: 6973 2070 6f73 7369 626c 6520 746f 206d  is possible to m
+00005e90: 616e 7561 6c6c 7920 7072 756e 6520 796f  anually prune yo
+00005ea0: 7572 206d 6f64 656c 2075 7369 6e67 206c  ur model using l
+00005eb0: 6f77 2d6c 6576 656c 2066 756e 6374 696f  ow-level functio
+00005ec0: 6e73 2c20 7468 6973 2061 7070 726f 6163  ns, this approac
+00005ed0: 6820 6361 6e20 6265 2071 7569 7465 206c  h can be quite l
+00005ee0: 6162 6f72 696f 7573 2c20 6173 2069 7420  aborious, as it 
+00005ef0: 7265 7175 6972 6573 2063 6172 6566 756c  requires careful
+00005f00: 206d 616e 6167 656d 656e 7420 6f66 2074   management of t
+00005f10: 6865 2061 7373 6f63 6961 7465 6420 6465  he associated de
+00005f20: 7065 6e64 656e 6369 6573 2e20 4173 2061  pendencies. As a
+00005f30: 2072 6573 756c 742c 2077 6520 7265 636f   result, we reco
+00005f40: 6d6d 656e 6420 7574 696c 697a 696e 6720  mmend utilizing 
+00005f50: 7468 6520 6166 6f72 656d 656e 7469 6f6e  the aforemention
+00005f60: 6564 2068 6967 682d 6c65 7665 6c20 7072  ed high-level pr
+00005f70: 756e 6572 7320 746f 2073 7472 6561 6d6c  uners to streaml
+00005f80: 696e 6520 7468 6520 7072 756e 696e 6720  ine the pruning 
+00005f90: 7072 6f63 6573 732e 0a0a 6060 6070 7974  process...```pyt
+00005fa0: 686f 6e0a 7470 2e70 7275 6e65 5f63 6f6e  hon.tp.prune_con
+00005fb0: 765f 6f75 745f 6368 616e 6e65 6c73 2820  v_out_channels( 
+00005fc0: 6d6f 6465 6c2e 636f 6e76 312c 2069 6478  model.conv1, idx
+00005fd0: 733d 5b32 2c36 2c39 5d20 290a 0a23 2066  s=[2,6,9] )..# f
+00005fe0: 6978 2074 6865 2062 726f 6b65 6e20 6465  ix the broken de
+00005ff0: 7065 6e64 656e 6369 6573 206d 616e 7561  pendencies manua
+00006000: 6c6c 790a 7470 2e70 7275 6e65 5f62 6174  lly.tp.prune_bat
+00006010: 6368 6e6f 726d 5f6f 7574 5f63 6861 6e6e  chnorm_out_chann
+00006020: 656c 7328 206d 6f64 656c 2e62 6e31 2c20  els( model.bn1, 
+00006030: 6964 7873 3d5b 322c 362c 395d 2029 0a74  idxs=[2,6,9] ).t
+00006040: 702e 7072 756e 655f 636f 6e76 5f69 6e5f  p.prune_conv_in_
+00006050: 6368 616e 6e65 6c73 2820 6d6f 6465 6c2e  channels( model.
+00006060: 6c61 7965 7232 5b30 5d2e 636f 6e76 312c  layer2[0].conv1,
+00006070: 2069 6478 733d 5b32 2c36 2c39 5d20 290a   idxs=[2,6,9] ).
+00006080: 2e2e 2e0a 6060 600a 0a54 6865 2066 6f6c  ....```..The fol
+00006090: 6c6f 7769 6e67 2070 7275 6e69 6e67 2066  lowing pruning f
+000060a0: 756e 6374 696f 6e73 2061 7265 2061 7661  unctions are ava
+000060b0: 696c 6162 6c65 3a0a 6060 6070 7974 686f  ilable:.```pytho
+000060c0: 6e0a 2770 7275 6e65 5f63 6f6e 765f 6f75  n.'prune_conv_ou
+000060d0: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
+000060e0: 756e 655f 636f 6e76 5f69 6e5f 6368 616e  une_conv_in_chan
+000060f0: 6e65 6c73 272c 0a27 7072 756e 655f 6465  nels',.'prune_de
+00006100: 7074 6877 6973 655f 636f 6e76 5f6f 7574  pthwise_conv_out
+00006110: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+00006120: 6e65 5f64 6570 7468 7769 7365 5f63 6f6e  ne_depthwise_con
+00006130: 765f 696e 5f63 6861 6e6e 656c 7327 2c0a  v_in_channels',.
+00006140: 2770 7275 6e65 5f62 6174 6368 6e6f 726d  'prune_batchnorm
+00006150: 5f6f 7574 5f63 6861 6e6e 656c 7327 2c0a  _out_channels',.
+00006160: 2770 7275 6e65 5f62 6174 6368 6e6f 726d  'prune_batchnorm
+00006170: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
+00006180: 7072 756e 655f 6c69 6e65 6172 5f6f 7574  prune_linear_out
+00006190: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+000061a0: 6e65 5f6c 696e 6561 725f 696e 5f63 6861  ne_linear_in_cha
+000061b0: 6e6e 656c 7327 2c0a 2770 7275 6e65 5f70  nnels',.'prune_p
+000061c0: 7265 6c75 5f6f 7574 5f63 6861 6e6e 656c  relu_out_channel
+000061d0: 7327 2c0a 2770 7275 6e65 5f70 7265 6c75  s',.'prune_prelu
+000061e0: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
+000061f0: 7072 756e 655f 6c61 7965 726e 6f72 6d5f  prune_layernorm_
+00006200: 6f75 745f 6368 616e 6e65 6c73 272c 0a27  out_channels',.'
+00006210: 7072 756e 655f 6c61 7965 726e 6f72 6d5f  prune_layernorm_
+00006220: 696e 5f63 6861 6e6e 656c 7327 2c0a 2770  in_channels',.'p
+00006230: 7275 6e65 5f65 6d62 6564 6469 6e67 5f6f  rune_embedding_o
+00006240: 7574 5f63 6861 6e6e 656c 7327 2c0a 2770  ut_channels',.'p
+00006250: 7275 6e65 5f65 6d62 6564 6469 6e67 5f69  rune_embedding_i
+00006260: 6e5f 6368 616e 6e65 6c73 272c 0a27 7072  n_channels',.'pr
+00006270: 756e 655f 7061 7261 6d65 7465 725f 6f75  une_parameter_ou
+00006280: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
+00006290: 756e 655f 7061 7261 6d65 7465 725f 696e  une_parameter_in
+000062a0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+000062b0: 6e65 5f6d 756c 7469 6865 6164 5f61 7474  ne_multihead_att
+000062c0: 656e 7469 6f6e 5f6f 7574 5f63 6861 6e6e  ention_out_chann
+000062d0: 656c 7327 2c0a 2770 7275 6e65 5f6d 756c  els',.'prune_mul
+000062e0: 7469 6865 6164 5f61 7474 656e 7469 6f6e  tihead_attention
+000062f0: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
+00006300: 7072 756e 655f 6772 6f75 706e 6f72 6d5f  prune_groupnorm_
+00006310: 6f75 745f 6368 616e 6e65 6c73 272c 0a27  out_channels',.'
+00006320: 7072 756e 655f 6772 6f75 706e 6f72 6d5f  prune_groupnorm_
+00006330: 696e 5f63 6861 6e6e 656c 7327 2c0a 2770  in_channels',.'p
+00006340: 7275 6e65 5f69 6e73 7461 6e63 656e 6f72  rune_instancenor
+00006350: 6d5f 6f75 745f 6368 616e 6e65 6c73 272c  m_out_channels',
+00006360: 0a27 7072 756e 655f 696e 7374 616e 6365  .'prune_instance
+00006370: 6e6f 726d 5f69 6e5f 6368 616e 6e65 6c73  norm_in_channels
+00006380: 272c 0a60 6060 0a0a 0a0a 2323 2320 352e  ',.```....### 5.
+00006390: 2043 7573 746f 6d69 7a65 6420 4c61 7965   Customized Laye
+000063a0: 7273 0a0a 506c 6561 7365 2072 6566 6572  rs..Please refer
+000063b0: 2074 6f20 5b74 6573 7473 2f74 6573 745f   to [tests/test_
+000063c0: 6375 7374 6f6d 697a 6564 5f6c 6179 6572  customized_layer
+000063d0: 2e70 795d 2868 7474 7073 3a2f 2f67 6974  .py](https://git
+000063e0: 6875 622e 636f 6d2f 5661 696e 462f 546f  hub.com/VainF/To
+000063f0: 7263 682d 5072 756e 696e 672f 626c 6f62  rch-Pruning/blob
+00006400: 2f6d 6173 7465 722f 7465 7374 732f 7465  /master/tests/te
+00006410: 7374 5f63 7573 746f 6d69 7a65 645f 6c61  st_customized_la
+00006420: 7965 722e 7079 292e 0a0a 2323 2320 362e  yer.py)...### 6.
+00006430: 2042 656e 6368 6d61 726b 730a 0a4f 7572   Benchmarks..Our
+00006440: 2072 6573 756c 7473 206f 6e20 7b52 6573   results on {Res
+00006450: 4e65 742d 3536 202f 2043 4946 4152 2d31  Net-56 / CIFAR-1
+00006460: 3020 2f20 322e 3030 787d 0a0a 7c20 4d65  0 / 2.00x}..| Me
+00006470: 7468 6f64 207c 2042 6173 6520 2825 2920  thod | Base (%) 
+00006480: 7c20 5072 756e 6564 2028 2529 207c 2024  | Pruned (%) | $
+00006490: 5c44 656c 7461 2420 4163 6320 2825 2920  \Delta$ Acc (%) 
+000064a0: 7c20 5370 6565 6420 5570 207c 0a7c 3a2d  | Speed Up |.|:-
+000064b0: 2d20 2020 207c 3a2d 2d3a 2020 7c3a 2d2d  -    |:--:  |:--
+000064c0: 3a20 2020 207c 3a2d 2d3a 207c 3a2d 2d3a  :    |:--: |:--:
+000064d0: 2020 2020 2020 7c0a 7c20 4e49 5053 205b        |.| NIPS [
+000064e0: 5b31 5d5d 2823 3129 2020 7c20 2d20 2020  [1]](#1)  | -   
+000064f0: 207c 202d 2020 2020 2020 7c2d 302e 3033   | -      |-0.03
+00006500: 207c 2031 2e37 3678 2020 2020 7c0a 7c20   | 1.76x    |.| 
+00006510: 4765 6f6d 6574 7269 6320 5b5b 325d 5d28  Geometric [[2]](
+00006520: 2332 2920 7c20 3933 2e35 3920 7c20 3933  #2) | 93.59 | 93
+00006530: 2e32 3620 7c20 2d30 2e33 3320 7c20 312e  .26 | -0.33 | 1.
+00006540: 3730 7820 7c0a 7c20 506f 6c61 7220 5b5b  70x |.| Polar [[
+00006550: 335d 5d28 2333 2920 207c 2039 332e 3830  3]](#3)  | 93.80
+00006560: 207c 2039 332e 3833 207c 202b 302e 3033   | 93.83 | +0.03
+00006570: 207c 312e 3838 7820 7c0a 7c20 4350 2020   |1.88x |.| CP  
+00006580: 5b5b 345d 5d28 2334 2920 2020 7c20 3932  [[4]](#4)   | 92
+00006590: 2e38 3020 7c20 3931 2e38 3020 7c20 2d31  .80 | 91.80 | -1
+000065a0: 2e30 3020 7c32 2e30 3078 207c 0a7c 2041  .00 |2.00x |.| A
+000065b0: 4d43 205b 5b35 5d5d 2823 3529 2020 207c  MC [[5]](#5)   |
+000065c0: 2039 322e 3830 207c 2039 312e 3930 207c   92.80 | 91.90 |
+000065d0: 202d 302e 3930 207c 322e 3030 7820 7c0a   -0.90 |2.00x |.
+000065e0: 7c20 4852 616e 6b20 5b5b 365d 5d28 2336  | HRank [[6]](#6
+000065f0: 2920 7c20 3933 2e32 3620 7c20 3932 2e31  ) | 93.26 | 92.1
+00006600: 3720 7c20 2d30 2e30 3920 7c32 2e30 3078  7 | -0.09 |2.00x
+00006610: 207c 0a7c 2053 4650 2020 5b5b 375d 5d28   |.| SFP  [[7]](
+00006620: 2337 2920 207c 2039 332e 3539 207c 2039  #7)  | 93.59 | 9
+00006630: 332e 3336 207c 202b 302e 3233 207c 322e  3.36 | +0.23 |2.
+00006640: 3131 7820 7c0a 7c20 5265 7352 6570 205b  11x |.| ResRep [
+00006650: 5b38 5d5d 2823 3829 207c 2039 332e 3731  [8]](#8) | 93.71
+00006660: 207c 2039 332e 3731 207c 202b 302e 3030   | 93.71 | +0.00
+00006670: 207c 322e 3132 7820 7c0a 7c7c 0a7c 204f   |2.12x |.||.| O
+00006680: 7572 732d 4c31 207c 2039 332e 3533 207c  urs-L1 | 93.53 |
+00006690: 2039 322e 3933 207c 202d 302e 3630 207c   92.93 | -0.60 |
+000066a0: 2032 2e31 3278 207c 0a7c 204f 7572 732d   2.12x |.| Ours-
+000066b0: 424e 207c 2039 332e 3533 207c 2039 332e  BN | 93.53 | 93.
+000066c0: 3239 207c 202d 302e 3234 207c 2032 2e31  29 | -0.24 | 2.1
+000066d0: 3278 207c 0a7c 204f 7572 732d 4772 6f75  2x |.| Ours-Grou
+000066e0: 7020 7c20 3933 2e35 3320 7c20 3933 2e37  p | 93.53 | 93.7
+000066f0: 3720 7c20 2b30 2e33 3820 7c20 322e 3133  7 | +0.38 | 2.13
+00006700: 7820 7c0a 0a50 6c65 6173 6520 7265 6665  x |..Please refe
+00006710: 7220 746f 205b 6265 6e63 686d 6172 6b73  r to [benchmarks
+00006720: 5d28 6265 6e63 686d 6172 6b73 2920 666f  ](benchmarks) fo
+00006730: 7220 6d6f 7265 2064 6574 6169 6c73 2e0a  r more details..
+00006740: 0a23 2323 2037 2e20 5365 7269 6573 206f  .### 7. Series o
+00006750: 6620 576f 726b 730a 3e20 2a2a 4c4c 4d2d  f Works.> **LLM-
+00006760: 5072 756e 6572 3a20 4f6e 2074 6865 2053  Pruner: On the S
+00006770: 7472 7563 7475 7261 6c20 5072 756e 696e  tructural Prunin
+00006780: 6720 6f66 204c 6172 6765 204c 616e 6775  g of Large Langu
+00006790: 6167 6520 4d6f 6465 6c73 2a2a 205b 5b50  age Models** [[P
+000067a0: 726f 6a65 6374 5d5d 2868 7474 7073 3a2f  roject]](https:/
+000067b0: 2f67 6974 6875 622e 636f 6d2f 686f 7273  /github.com/hors
+000067c0: 6565 652f 4c4c 4d2d 5072 756e 6572 2920  eee/LLM-Pruner) 
+000067d0: 5b5b 6172 5869 765d 5d28 6874 7470 733a  [[arXiv]](https:
+000067e0: 2f2f 6172 7869 762e 6f72 672f 6162 732f  //arxiv.org/abs/
+000067f0: 3233 3035 2e31 3136 3237 2920 2020 0a3e  2305.11627)   .>
+00006800: 202a 5869 6e79 696e 204d 612c 2047 6f6e   *Xinyin Ma, Gon
+00006810: 6766 616e 2046 616e 672c 2058 696e 6368  gfan Fang, Xinch
+00006820: 616f 2057 616e 672a 2020 200a 0a3e 202a  ao Wang*   ..> *
+00006830: 2a53 7472 7563 7475 7261 6c20 5072 756e  *Structural Prun
+00006840: 696e 6720 666f 7220 4469 6666 7573 696f  ing for Diffusio
+00006850: 6e20 4d6f 6465 6c73 2a2a 205b 5b50 726f  n Models** [[Pro
+00006860: 6a65 6374 5d5d 2868 7474 7073 3a2f 2f67  ject]](https://g
+00006870: 6974 6875 622e 636f 6d2f 5661 696e 462f  ithub.com/VainF/
+00006880: 4469 6666 2d50 7275 6e69 6e67 2920 5b5b  Diff-Pruning) [[
+00006890: 6172 7869 765d 5d28 6874 7470 733a 2f2f  arxiv]](https://
+000068a0: 6172 7869 762e 6f72 672f 6162 732f 3233  arxiv.org/abs/23
+000068b0: 3035 2e31 3039 3234 2920 200a 3e20 2a47  05.10924)  .> *G
+000068c0: 6f6e 6766 616e 2046 616e 672c 2058 696e  ongfan Fang, Xin
+000068d0: 7969 6e20 4d61 2c20 5869 6e63 6861 6f20  yin Ma, Xinchao 
+000068e0: 5761 6e67 2a20 2020 200a 0a0a 2323 2043  Wang*    ...## C
+000068f0: 6974 6174 696f 6e0a 6060 600a 4069 6e70  itation.```.@inp
+00006900: 726f 6365 6564 696e 6773 7b66 616e 6732  roceedings{fang2
+00006910: 3032 3364 6570 6772 6170 682c 0a20 2074  023depgraph,.  t
+00006920: 6974 6c65 3d7b 4465 7067 7261 7068 3a20  itle={Depgraph: 
+00006930: 546f 7761 7264 7320 616e 7920 7374 7275  Towards any stru
+00006940: 6374 7572 616c 2070 7275 6e69 6e67 7d2c  ctural pruning},
+00006950: 0a20 2061 7574 686f 723d 7b46 616e 672c  .  author={Fang,
+00006960: 2047 6f6e 6766 616e 2061 6e64 204d 612c   Gongfan and Ma,
+00006970: 2058 696e 7969 6e20 616e 6420 536f 6e67   Xinyin and Song
+00006980: 2c20 4d69 6e67 6c69 2061 6e64 204d 692c  , Mingli and Mi,
+00006990: 204d 6963 6861 656c 2042 6920 616e 6420   Michael Bi and 
+000069a0: 5761 6e67 2c20 5869 6e63 6861 6f7d 2c0a  Wang, Xinchao},.
+000069b0: 2020 626f 6f6b 7469 746c 653d 7b50 726f    booktitle={Pro
+000069c0: 6365 6564 696e 6773 206f 6620 7468 6520  ceedings of the 
+000069d0: 4945 4545 2f43 5646 2043 6f6e 6665 7265  IEEE/CVF Confere
+000069e0: 6e63 6520 6f6e 2043 6f6d 7075 7465 7220  nce on Computer 
+000069f0: 5669 7369 6f6e 2061 6e64 2050 6174 7465  Vision and Patte
+00006a00: 726e 2052 6563 6f67 6e69 7469 6f6e 7d2c  rn Recognition},
+00006a10: 0a20 2070 6167 6573 3d7b 3136 3039 312d  .  pages={16091-
+00006a20: 2d31 3631 3031 7d2c 0a20 2079 6561 723d  -16101},.  year=
+00006a30: 7b32 3032 337d 0a7d 0a60 6060 0a0a 0000  {2023}.}.```....
 00006a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006a90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006aa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -1754,16 +1754,16 @@
 00006d90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00006e00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00006e10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00006e00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00006e10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00006e20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006e60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006e70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006e80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -1787,23 +1787,23 @@
 00006fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00006ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00007000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00007010: 312e 382f 5245 4144 4d45 2e6d 6400 0000  1.8/README.md...
+00007010: 312e 392f 5245 4144 4d45 2e6d 6400 0000  1.9/README.md...
 00007020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00007030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00007040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00007050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00007060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00007070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00007080: 3030 3530 3131 3700 3134 3433 3431 3434  0050117.14434144
-00007090: 3037 3100 3031 3532 3036 0020 3000 0000  071.015206. 0...
+00007080: 3030 3530 3230 3200 3134 3434 3632 3331  0050202.14446231
+00007090: 3335 3300 3031 3532 3035 0020 3000 0000  353.015205. 0...
 000070a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000070b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000070c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000070d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000070e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000070f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00007100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -1878,15 +1878,15 @@
 00007550: 2020 3c61 2068 7265 663d 2268 7474 7073    <a href="https
 00007560: 3a2f 2f67 6974 6875 622e 636f 6d2f 5661  ://github.com/Va
 00007570: 696e 462f 546f 7263 682d 5072 756e 696e  inF/Torch-Prunin
 00007580: 672f 7265 6c65 6173 6573 2f6c 6174 6573  g/releases/lates
 00007590: 7422 3e3c 696d 6720 7372 633d 2268 7474  t"><img src="htt
 000075a0: 7073 3a2f 2f69 6d67 2e73 6869 656c 6473  ps://img.shields
 000075b0: 2e69 6f2f 6261 6467 652f 4c61 7465 7374  .io/badge/Latest
-000075c0: 2532 3056 6572 7369 6f6e 2d31 2e31 2e38  %20Version-1.1.8
+000075c0: 2532 3056 6572 7369 6f6e 2d31 2e31 2e39  %20Version-1.1.9
 000075d0: 2d33 6635 3162 352e 7376 6722 2061 6c74  -3f51b5.svg" alt
 000075e0: 3d22 4c61 7465 7374 2056 6572 7369 6f6e  ="Latest Version
 000075f0: 223e 3c2f 613e 0a20 203c 6120 6872 6566  "></a>.  <a href
 00007600: 3d22 6874 7470 733a 2f2f 636f 6c61 622e  ="https://colab.
 00007610: 7265 7365 6172 6368 2e67 6f6f 676c 652e  research.google.
 00007620: 636f 6d2f 6472 6976 652f 3154 5276 454c  com/drive/1TRvEL
 00007630: 5144 4e6a 3950 774d 2d45 4552 5762 4633  QDNj9PwM-EERWbF3
@@ -2046,1075 +2046,1075 @@
 00007fd0: 290a 2a20 3230 3233 2e30 342e 3231 204a  ).* 2023.04.21 J
 00007fe0: 6f69 6e20 6f75 7220 5465 6c65 6772 616d  oin our Telegram
 00007ff0: 206f 7220 5765 6368 6174 2067 726f 7570   or Wechat group
 00008000: 2066 6f72 2063 6173 7561 6c20 6469 7363   for casual disc
 00008010: 7573 7369 6f6e 733a 0a20 202a 2054 656c  ussions:.  * Tel
 00008020: 6567 7261 6d3a 2068 7474 7073 3a2f 2f74  egram: https://t
 00008030: 2e6d 652f 2b4e 776a 6242 444e 3261 6f31  .me/+NwjbBDN2ao1
-00008040: 6c5a 6a5a 6c0a 2020 2a20 5765 6368 6174  lZjZl.  * Wechat
+00008040: 6c5a 6a5a 6c0a 2020 2a20 5765 4368 6174  lZjZl.  * WeChat
 00008050: 3a20 3c69 6d67 2077 6964 7468 3d22 3130  : <img width="10
 00008060: 3022 2061 6c74 3d22 696d 6167 6522 2073  0" alt="image" s
 00008070: 7263 3d22 6874 7470 733a 2f2f 6769 7468  rc="https://gith
 00008080: 7562 2e63 6f6d 2f56 6169 6e46 2f54 6f72  ub.com/VainF/Tor
 00008090: 6368 2d50 7275 6e69 6e67 2f61 7373 6574  ch-Pruning/asset
-000080a0: 732f 3138 3539 3232 3131 2f35 3932 6161  s/18592211/592aa
-000080b0: 3035 342d 3762 3737 2d34 3334 632d 3935  054-7b77-434c-95
-000080c0: 3931 2d32 3762 3633 3538 3439 6535 3322  91-27b635849e53"
-000080d0: 3e0a 0a50 6c65 6173 6520 646f 206e 6f74  >..Please do not
-000080e0: 2068 6573 6974 6174 6520 746f 206f 7065   hesitate to ope
-000080f0: 6e20 6120 5b64 6973 6375 7373 696f 6e5d  n a [discussion]
-00008100: 2868 7474 7073 3a2f 2f67 6974 6875 622e  (https://github.
-00008110: 636f 6d2f 5661 696e 462f 546f 7263 682d  com/VainF/Torch-
-00008120: 5072 756e 696e 672f 6469 7363 7573 7369  Pruning/discussi
-00008130: 6f6e 7329 206f 7220 5b69 7373 7565 5d28  ons) or [issue](
-00008140: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
-00008150: 6f6d 2f56 6169 6e46 2f54 6f72 6368 2d50  om/VainF/Torch-P
-00008160: 7275 6e69 6e67 2f69 7373 7565 7329 2069  runing/issues) i
-00008170: 6620 796f 7520 656e 636f 756e 7465 7220  f you encounter 
-00008180: 616e 7920 7072 6f62 6c65 6d73 2077 6974  any problems wit
-00008190: 6820 7468 6520 6c69 6272 6172 7920 6f72  h the library or
-000081a0: 2074 6865 2070 6170 6572 2e0a 0a0a 2323   the paper....##
-000081b0: 2320 2a2a 4665 6174 7572 6573 3a2a 2a0a  # **Features:**.
-000081c0: 2d20 5b78 5d20 5374 7275 6374 7572 616c  - [x] Structural
-000081d0: 2070 7275 6e69 6e67 2066 6f72 2043 4e4e   pruning for CNN
-000081e0: 732c 2054 7261 6e73 666f 726d 6572 732c  s, Transformers,
-000081f0: 2044 6574 6563 746f 7273 2c20 4c61 6e67   Detectors, Lang
-00008200: 7561 6765 204d 6f64 656c 7320 616e 6420  uage Models and 
-00008210: 4469 6666 7573 696f 6e20 4d6f 6465 6c73  Diffusion Models
-00008220: 2e20 506c 6561 7365 2072 6566 6572 2074  . Please refer t
-00008230: 6f20 7468 6520 5b50 7275 6e61 6269 6c69  o the [Prunabili
-00008240: 7479 2042 656e 6368 6d61 726b 5d28 6265  ty Benchmark](be
-00008250: 6e63 686d 6172 6b73 2f70 7275 6e61 6269  nchmarks/prunabi
-00008260: 6c69 7479 292e 0a2d 205b 785d 2048 6967  lity)..- [x] Hig
-00008270: 682d 6c65 7665 6c20 7072 756e 6572 733a  h-level pruners:
-00008280: 205b 4d61 676e 6974 7564 6550 7275 6e65   [MagnitudePrune
-00008290: 725d 2868 7474 7073 3a2f 2f61 7278 6976  r](https://arxiv
-000082a0: 2e6f 7267 2f61 6273 2f31 3630 382e 3038  .org/abs/1608.08
-000082b0: 3731 3029 2c20 5b42 4e53 6361 6c65 5072  710), [BNScalePr
-000082c0: 756e 6572 5d28 6874 7470 733a 2f2f 6172  uner](https://ar
-000082d0: 7869 762e 6f72 672f 6162 732f 3137 3038  xiv.org/abs/1708
-000082e0: 2e30 3635 3139 292c 205b 4772 6f75 704e  .06519), [GroupN
-000082f0: 6f72 6d50 7275 6e65 725d 2868 7474 7073  ormPruner](https
-00008300: 3a2f 2f61 7278 6976 2e6f 7267 2f61 6273  ://arxiv.org/abs
-00008310: 2f32 3330 312e 3132 3930 3029 2c20 5261  /2301.12900), Ra
-00008320: 6e64 6f6d 5072 756e 6572 2c20 6574 632e  ndomPruner, etc.
-00008330: 0a2d 205b 785d 2049 6d70 6f72 7461 6e63  .- [x] Importanc
-00008340: 6520 4372 6974 6572 6961 3a20 4c2d 7020  e Criteria: L-p 
-00008350: 4e6f 726d 2c20 5461 796c 6f72 2c20 5261  Norm, Taylor, Ra
-00008360: 6e64 6f6d 2c20 424e 5363 616c 696e 672c  ndom, BNScaling,
-00008370: 2065 7463 2e0a 2d20 5b78 5d20 4465 7065   etc..- [x] Depe
-00008380: 6e64 656e 6379 2047 7261 7068 2066 6f72  ndency Graph for
-00008390: 2064 6570 656e 6465 6e63 7920 6d6f 6465   dependency mode
-000083a0: 6c69 6e67 2e0a 2d20 5b78 5d20 5375 7070  ling..- [x] Supp
-000083b0: 6f72 7465 6420 6d6f 6475 6c65 733a 204c  orted modules: L
-000083c0: 696e 6561 722c 2028 5472 616e 7370 6f73  inear, (Transpos
-000083d0: 6564 2920 436f 6e76 2c20 4e6f 726d 616c  ed) Conv, Normal
-000083e0: 697a 6174 696f 6e2c 2050 5265 4c55 2c20  ization, PReLU, 
-000083f0: 456d 6265 6464 696e 672c 204d 756c 7469  Embedding, Multi
-00008400: 6865 6164 4174 7465 6e74 696f 6e2c 206e  headAttention, n
-00008410: 6e2e 5061 7261 6d65 7465 7273 2061 6e64  n.Parameters and
-00008420: 205b 6375 7374 6f6d 697a 6564 206d 6f64   [customized mod
-00008430: 756c 6573 5d28 7465 7374 732f 7465 7374  ules](tests/test
-00008440: 5f63 7573 746f 6d69 7a65 645f 6c61 7965  _customized_laye
-00008450: 722e 7079 292e 0a2d 205b 785d 2053 7570  r.py)..- [x] Sup
-00008460: 706f 7274 6564 206f 7065 7261 746f 7273  ported operators
-00008470: 3a20 7370 6c69 742c 2063 6f6e 6361 7465  : split, concate
-00008480: 6e61 7469 6f6e 2c20 736b 6970 2063 6f6e  nation, skip con
-00008490: 6e65 6374 696f 6e2c 2066 6c61 7474 656e  nection, flatten
-000084a0: 2c20 7265 7368 6170 652c 2076 6965 772c  , reshape, view,
-000084b0: 2061 6c6c 2065 6c65 6d65 6e74 2d77 6973   all element-wis
-000084c0: 6520 6f70 732c 2065 7463 2e0a 2d20 5b78  e ops, etc..- [x
-000084d0: 5d20 5b4c 6f77 2d6c 6576 656c 2070 7275  ] [Low-level pru
-000084e0: 6e69 6e67 2066 756e 6374 696f 6e73 5d28  ning functions](
-000084f0: 746f 7263 685f 7072 756e 696e 672f 7072  torch_pruning/pr
-00008500: 756e 6572 2f66 756e 6374 696f 6e2e 7079  uner/function.py
-00008510: 290a 2d20 5b78 5d20 5b42 656e 6368 6d61  ).- [x] [Benchma
-00008520: 726b 735d 2862 656e 6368 6d61 726b 7329  rks](benchmarks)
-00008530: 2061 6e64 205b 7475 746f 7269 616c 735d   and [tutorials]
-00008540: 2874 7574 6f72 6961 6c73 290a 2d20 5b78  (tutorials).- [x
-00008550: 5d20 4120 5b72 6573 6f75 7263 6520 6c69  ] A [resource li
-00008560: 7374 5d28 7072 6163 7469 6361 6c5f 7374  st](practical_st
-00008570: 7275 6374 7572 616c 5f70 7275 6e69 6e67  ructural_pruning
-00008580: 2e6d 6429 2066 6f72 2070 7261 6374 6963  .md) for practic
-00008590: 616c 2073 7472 7563 7472 7561 6c20 7072  al structrual pr
-000085a0: 756e 696e 672e 0a20 200a 2323 2320 2a2a  uning..  .### **
-000085b0: 544f 444f 204c 6973 743a 2a2a 0a2d 205b  TODO List:**.- [
-000085c0: 205d 2041 2073 7472 6f6e 6720 6261 7365   ] A strong base
-000085d0: 6c69 6e65 2077 6974 6820 6261 6773 206f  line with bags o
-000085e0: 6620 7472 6963 6b73 2066 726f 6d20 6578  f tricks from ex
-000085f0: 6973 7469 6e67 206d 6574 686f 6473 2e0a  isting methods..
-00008600: 2d20 5b20 5d20 4120 6265 6e63 686d 6172  - [ ] A benchmar
-00008610: 6b20 666f 7220 5b54 6f72 6368 7669 7369  k for [Torchvisi
-00008620: 6f6e 5d28 6874 7470 733a 2f2f 7079 746f  on](https://pyto
-00008630: 7263 682e 6f72 672f 7669 7369 6f6e 2f73  rch.org/vision/s
-00008640: 7461 626c 652f 6d6f 6465 6c73 2e68 746d  table/models.htm
-00008650: 6c29 2063 6f6d 7061 7469 6269 6c69 7479  l) compatibility
-00008660: 2028 2a2a 3831 2f38 353d 3935 2e33 252a   (**81/85=95.3%*
-00008670: 2a2c 203a 6865 6176 795f 6368 6563 6b5f  *, :heavy_check_
-00008680: 6d61 726b 3a29 2061 6e64 205b 7469 6d6d  mark:) and [timm
-00008690: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
-000086a0: 2e63 6f6d 2f68 7567 6769 6e67 6661 6365  .com/huggingface
-000086b0: 2f70 7974 6f72 6368 2d69 6d61 6765 2d6d  /pytorch-image-m
-000086c0: 6f64 656c 7329 2063 6f6d 7061 7469 6269  odels) compatibi
-000086d0: 6c69 7479 2e0a 2d20 5b20 5d20 5072 756e  lity..- [ ] Prun
-000086e0: 696e 6720 6672 6f6d 2053 6372 6174 6368  ing from Scratch
-000086f0: 202f 2061 7420 496e 6974 6961 6c69 7a61   / at Initializa
-00008700: 7469 6f6e 2e0a 2d20 5b20 5d20 4d6f 7265  tion..- [ ] More
-00008710: 2068 6967 682d 6c65 7665 6c20 7072 756e   high-level prun
-00008720: 6572 7320 6c69 6b65 205b 4669 7368 6572  ers like [Fisher
-00008730: 5072 756e 6572 5d28 6874 7470 733a 2f2f  Pruner](https://
-00008740: 6172 7869 762e 6f72 672f 6162 732f 3231  arxiv.org/abs/21
-00008750: 3038 2e30 3037 3038 292c 205b 4772 6f77  08.00708), [Grow
-00008760: 696e 6752 6567 5d28 6874 7470 733a 2f2f  ingReg](https://
-00008770: 6172 7869 762e 6f72 672f 6162 732f 3230  arxiv.org/abs/20
-00008780: 3132 2e30 3932 3433 292c 2065 7463 2e0a  12.09243), etc..
-00008790: 2d20 5b20 5d20 4d6f 7265 2054 7261 6e73  - [ ] More Trans
-000087a0: 666f 726d 6572 7320 6c69 6b65 2056 6973  formers like Vis
-000087b0: 696f 6e20 5472 616e 7366 6f72 6d65 7273  ion Transformers
-000087c0: 2028 3a68 6561 7679 5f63 6865 636b 5f6d   (:heavy_check_m
-000087d0: 6172 6b3a 292c 2053 7769 6e20 5472 616e  ark:), Swin Tran
-000087e0: 7366 6f72 6d65 7273 2c20 506f 6f6c 466f  sformers, PoolFo
-000087f0: 726d 6572 732e 0a2d 205b 205d 2042 6c6f  rmers..- [ ] Blo
-00008800: 636b 2f4c 6179 6572 2f44 6570 7468 2050  ck/Layer/Depth P
-00008810: 7275 6e69 6e67 0a2d 205b 205d 2050 7275  runing.- [ ] Pru
-00008820: 6e69 6e67 2062 656e 6368 6d61 726b 7320  ning benchmarks 
-00008830: 666f 7220 4349 4641 522c 2049 6d61 6765  for CIFAR, Image
-00008840: 4e65 7420 616e 6420 434f 434f 2e0a 0a23  Net and COCO...#
-00008850: 2320 496e 7374 616c 6c61 7469 6f6e 0a0a  # Installation..
-00008860: 546f 7263 682d 5072 756e 696e 6720 6973  Torch-Pruning is
-00008870: 2063 6f6d 7061 7469 626c 6520 7769 7468   compatible with
-00008880: 2050 7954 6f72 6368 2031 2e78 2061 6e64   PyTorch 1.x and
-00008890: 2032 2e78 2e20 2a2a 5079 546f 7263 6820   2.x. **PyTorch 
-000088a0: 312e 3132 2e31 2069 7320 7265 636f 6d6d  1.12.1 is recomm
-000088b0: 656e 6465 6421 2a2a 0a0a 6060 6062 6173  ended!**..```bas
-000088c0: 680a 7069 7020 696e 7374 616c 6c20 746f  h.pip install to
-000088d0: 7263 682d 7072 756e 696e 6720 2320 7631  rch-pruning # v1
-000088e0: 2e31 2e38 0a60 6060 0a6f 720a 6060 6062  .1.8.```.or.```b
-000088f0: 6173 680a 6769 7420 636c 6f6e 6520 6874  ash.git clone ht
-00008900: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-00008910: 2f56 6169 6e46 2f54 6f72 6368 2d50 7275  /VainF/Torch-Pru
-00008920: 6e69 6e67 2e67 6974 0a60 6060 0a0a 2323  ning.git.```..##
-00008930: 2051 7569 636b 7374 6172 740a 2020 0a48   Quickstart.  .H
-00008940: 6572 6520 7765 2070 726f 7669 6465 2061  ere we provide a
-00008950: 2071 7569 636b 2073 7461 7274 2066 6f72   quick start for
-00008960: 2054 6f72 6368 2d50 7275 6e69 6e67 2e20   Torch-Pruning. 
-00008970: 4d6f 7265 2065 7870 6c61 696e 6564 2064  More explained d
-00008980: 6574 6169 6c73 2063 616e 2062 6520 666f  etails can be fo
-00008990: 756e 6420 696e 205b 7475 746f 7261 6c73  und in [tutorals
-000089a0: 5d28 2e2f 7475 746f 7269 616c 732f 290a  ](./tutorials/).
-000089b0: 0a23 2323 2030 2e20 486f 7720 4974 2057  .### 0. How It W
-000089c0: 6f72 6b73 0a0a 496e 2073 7472 7563 7475  orks..In structu
-000089d0: 7261 6c20 7072 756e 696e 672c 202a 2a60  ral pruning, **`
-000089e0: 6047 726f 7570 6060 2069 7320 7468 6520  `Group`` is the 
-000089f0: 6d69 6e69 6d61 6c20 7265 6d6f 7661 626c  minimal removabl
-00008a00: 6520 756e 6974 2077 6974 6869 6e20 6465  e unit within de
-00008a10: 6570 206e 6574 776f 726b 732a 2a2e 2045  ep networks**. E
-00008a20: 6163 6820 6772 6f75 7020 636f 6e74 6169  ach group contai
-00008a30: 6e73 2073 6576 6572 616c 2069 6e74 6572  ns several inter
-00008a40: 6465 7065 6e64 656e 7420 6c61 7965 7273  dependent layers
-00008a50: 2074 6861 7420 6d75 7374 2062 6520 7072   that must be pr
-00008a60: 756e 6564 2073 696d 756c 7461 6e65 6f75  uned simultaneou
-00008a70: 736c 7920 746f 206d 6169 6e74 6169 6e20  sly to maintain 
-00008a80: 7468 6520 696e 7465 6772 6974 7920 6f66  the integrity of
-00008a90: 2074 6865 2072 6573 756c 7469 6e67 2073   the resulting s
-00008aa0: 7472 7563 7475 7265 732e 2048 6f77 6576  tructures. Howev
-00008ab0: 6572 2c20 6465 6570 206e 6574 776f 726b  er, deep network
-00008ac0: 7320 6f66 7465 6e20 7072 6573 656e 7420  s often present 
-00008ad0: 636f 6d70 6c65 7820 6465 7065 6e64 656e  complex dependen
-00008ae0: 6369 6573 2061 6d6f 6e67 206c 6179 6572  cies among layer
-00008af0: 732c 206d 616b 696e 6720 7374 7275 6374  s, making struct
-00008b00: 7572 616c 2070 7275 6e69 6e67 2061 2063  ural pruning a c
-00008b10: 6861 6c6c 656e 6769 6e67 2065 6e64 6561  hallenging endea
-00008b20: 766f 722e 2054 6869 7320 776f 726b 2061  vor. This work a
-00008b30: 6464 7265 7373 6573 2074 6869 7320 6368  ddresses this ch
-00008b40: 616c 6c65 6e67 6520 6279 206f 6666 6572  allenge by offer
-00008b50: 696e 6720 616e 2061 7574 6f6d 6174 6564  ing an automated
-00008b60: 206d 6563 6861 6e69 736d 2c20 6060 4465   mechanism, ``De
-00008b70: 7047 7261 7068 6060 2c20 666f 7220 7061  pGraph``, for pa
-00008b80: 7261 6d65 7465 7220 6772 6f75 7069 6e67  rameter grouping
-00008b90: 2c20 7768 6963 6820 6661 6369 6c69 7461  , which facilita
-00008ba0: 7465 7320 6566 666f 7274 6c65 7373 2070  tes effortless p
-00008bb0: 7275 6e69 6e67 2066 6f72 2061 2077 6964  runing for a wid
-00008bc0: 6520 7261 6e67 6520 6f66 2064 6565 7020  e range of deep 
-00008bd0: 6e65 7477 6f72 6b73 2e0a 0a3c 6469 7620  networks...<div 
-00008be0: 616c 6967 6e3d 2263 656e 7465 7222 3e0a  align="center">.
-00008bf0: 3c69 6d67 2073 7263 3d22 6173 7365 7473  <img src="assets
-00008c00: 2f64 6570 2e70 6e67 2220 7769 6474 683d  /dep.png" width=
-00008c10: 2231 3030 2522 3e0a 3c2f 6469 763e 0a0a  "100%">.</div>..
-00008c20: 2323 2320 312e 2041 204d 696e 696d 616c  ### 1. A Minimal
-00008c30: 2045 7861 6d70 6c65 0a0a 6060 6070 7974   Example..```pyt
-00008c40: 686f 6e0a 696d 706f 7274 2074 6f72 6368  hon.import torch
-00008c50: 0a66 726f 6d20 746f 7263 6876 6973 696f  .from torchvisio
-00008c60: 6e2e 6d6f 6465 6c73 2069 6d70 6f72 7420  n.models import 
-00008c70: 7265 736e 6574 3138 0a69 6d70 6f72 7420  resnet18.import 
-00008c80: 746f 7263 685f 7072 756e 696e 6720 6173  torch_pruning as
-00008c90: 2074 700a 0a6d 6f64 656c 203d 2072 6573   tp..model = res
-00008ca0: 6e65 7431 3828 7072 6574 7261 696e 6564  net18(pretrained
-00008cb0: 3d54 7275 6529 2e65 7661 6c28 290a 0a23  =True).eval()..#
-00008cc0: 2031 2e20 6275 696c 6420 6465 7065 6e64   1. build depend
-00008cd0: 656e 6379 2067 7261 7068 2066 6f72 2072  ency graph for r
-00008ce0: 6573 6e65 7431 380a 4447 203d 2074 702e  esnet18.DG = tp.
-00008cf0: 4465 7065 6e64 656e 6379 4772 6170 6828  DependencyGraph(
-00008d00: 292e 6275 696c 645f 6465 7065 6e64 656e  ).build_dependen
-00008d10: 6379 286d 6f64 656c 2c20 6578 616d 706c  cy(model, exampl
-00008d20: 655f 696e 7075 7473 3d74 6f72 6368 2e72  e_inputs=torch.r
-00008d30: 616e 646e 2831 2c33 2c32 3234 2c32 3234  andn(1,3,224,224
-00008d40: 2929 0a0a 2320 322e 2053 7065 6369 6679  ))..# 2. Specify
-00008d50: 2074 6865 2074 6f2d 6265 2d70 7275 6e65   the to-be-prune
-00008d60: 6420 6368 616e 6e65 6c73 2e20 4865 7265  d channels. Here
-00008d70: 2077 6520 7072 756e 6520 7468 6f73 6520   we prune those 
-00008d80: 6368 616e 6e65 6c73 2069 6e64 6578 6564  channels indexed
-00008d90: 2062 7920 5b32 2c20 362c 2039 5d2e 0a67   by [2, 6, 9]..g
-00008da0: 726f 7570 203d 2044 472e 6765 745f 7072  roup = DG.get_pr
-00008db0: 756e 696e 675f 6772 6f75 7028 206d 6f64  uning_group( mod
-00008dc0: 656c 2e63 6f6e 7631 2c20 7470 2e70 7275  el.conv1, tp.pru
-00008dd0: 6e65 5f63 6f6e 765f 6f75 745f 6368 616e  ne_conv_out_chan
-00008de0: 6e65 6c73 2c20 6964 7873 3d5b 322c 2036  nels, idxs=[2, 6
-00008df0: 2c20 395d 2029 0a0a 2320 332e 2070 7275  , 9] )..# 3. pru
-00008e00: 6e65 2061 6c6c 2067 726f 7570 6564 206c  ne all grouped l
-00008e10: 6179 6572 7320 7468 6174 2061 7265 2063  ayers that are c
-00008e20: 6f75 706c 6564 2077 6974 6820 6d6f 6465  oupled with mode
-00008e30: 6c2e 636f 6e76 3120 2869 6e63 6c75 6465  l.conv1 (include
-00008e40: 6429 2e0a 6966 2044 472e 6368 6563 6b5f  d)..if DG.check_
-00008e50: 7072 756e 696e 675f 6772 6f75 7028 6772  pruning_group(gr
-00008e60: 6f75 7029 3a20 2320 6176 6f69 6420 6675  oup): # avoid fu
-00008e70: 6c6c 2070 7275 6e69 6e67 2c20 692e 652e  ll pruning, i.e.
-00008e80: 2c20 6368 616e 6e65 6c73 3d30 2e0a 2020  , channels=0..  
-00008e90: 2020 6772 6f75 702e 7072 756e 6528 290a    group.prune().
-00008ea0: 2020 2020 0a23 2034 2e20 5361 7665 2026      .# 4. Save &
-00008eb0: 204c 6f61 640a 6d6f 6465 6c2e 7a65 726f   Load.model.zero
-00008ec0: 5f67 7261 6428 2920 2320 5765 2064 6f6e  _grad() # We don
-00008ed0: 2774 2077 616e 7420 746f 2073 746f 7265  't want to store
-00008ee0: 2067 7261 6469 656e 7420 696e 666f 726d   gradient inform
-00008ef0: 6174 696f 6e0a 746f 7263 682e 7361 7665  ation.torch.save
-00008f00: 286d 6f64 656c 2c20 276d 6f64 656c 2e70  (model, 'model.p
-00008f10: 7468 2729 2023 2077 6974 686f 7574 202e  th') # without .
-00008f20: 7374 6174 655f 6469 6374 0a6d 6f64 656c  state_dict.model
-00008f30: 203d 2074 6f72 6368 2e6c 6f61 6428 276d   = torch.load('m
-00008f40: 6f64 656c 2e70 7468 2729 2023 206c 6f61  odel.pth') # loa
-00008f50: 6420 7468 6520 6d6f 6465 6c20 6f62 6a65  d the model obje
-00008f60: 6374 0a60 6060 0a20 200a 5468 6520 6162  ct.```.  .The ab
-00008f70: 6f76 6520 6578 616d 706c 6520 6465 6d6f  ove example demo
-00008f80: 6e73 7472 6174 6573 2074 6865 2066 756e  nstrates the fun
-00008f90: 6461 6d65 6e74 616c 2070 7275 6e69 6e67  damental pruning
-00008fa0: 2070 6970 656c 696e 6520 7573 696e 6720   pipeline using 
-00008fb0: 4465 7047 7261 7068 2e20 5468 6520 7461  DepGraph. The ta
-00008fc0: 7267 6574 206c 6179 6572 2072 6573 6e65  rget layer resne
-00008fd0: 742e 636f 6e76 3120 6973 2063 6f75 706c  t.conv1 is coupl
-00008fe0: 6564 2077 6974 6820 7365 7665 7261 6c20  ed with several 
-00008ff0: 6c61 7965 7273 2c20 7768 6963 6820 7265  layers, which re
-00009000: 7175 6972 6573 2073 696d 756c 7461 6e65  quires simultane
-00009010: 6f75 7320 7265 6d6f 7661 6c20 696e 2073  ous removal in s
-00009020: 7472 7563 7475 7261 6c20 7072 756e 696e  tructural prunin
-00009030: 672e 204c 6574 2773 2070 7269 6e74 2074  g. Let's print t
-00009040: 6865 2067 726f 7570 2061 6e64 206f 6273  he group and obs
-00009050: 6572 7665 2068 6f77 2061 2070 7275 6e69  erve how a pruni
-00009060: 6e67 206f 7065 7261 7469 6f6e 2022 7472  ng operation "tr
-00009070: 6967 6765 7273 2220 6f74 6865 7220 6f6e  iggers" other on
-00009080: 6573 2e20 496e 2074 6865 2066 6f6c 6c6f  es. In the follo
-00009090: 7769 6e67 206f 7574 7075 7473 2c20 6060  wing outputs, ``
-000090a0: 4120 3d3e 2042 6060 206d 6561 6e73 2074  A => B`` means t
-000090b0: 6865 2070 7275 6e69 6e67 206f 7065 7261  he pruning opera
-000090c0: 7469 6f6e 2060 6041 6060 2074 7269 6767  tion ``A`` trigg
-000090d0: 6572 7320 7468 6520 7072 756e 696e 6720  ers the pruning 
-000090e0: 6f70 6572 6174 696f 6e20 6060 4260 602e  operation ``B``.
-000090f0: 2067 726f 7570 5b30 5d20 7265 6665 7273   group[0] refers
-00009100: 2074 6f20 7468 6520 7072 756e 696e 6720   to the pruning 
-00009110: 726f 6f74 2069 6e20 6060 4447 2e67 6574  root in ``DG.get
-00009120: 5f70 7275 6e69 6e67 5f67 726f 7570 6060  _pruning_group``
-00009130: 2e0a 0a60 6060 0a2d 2d2d 2d2d 2d2d 2d2d  ...```.---------
-00009140: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00009150: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
-00009160: 2020 5072 756e 696e 6720 4772 6f75 700a    Pruning Group.
+000080a0: 732f 3138 3539 3232 3131 2f31 6131 3261  s/18592211/1a12a
+000080b0: 3634 652d 6139 3465 2d34 6137 342d 3839  64e-a94e-4a74-89
+000080c0: 3039 2d38 6663 3038 3836 6636 3366 3222  09-8fc0886f63f2"
+000080d0: 3e0a 0a0a 0a50 6c65 6173 6520 646f 206e  >....Please do n
+000080e0: 6f74 2068 6573 6974 6174 6520 746f 206f  ot hesitate to o
+000080f0: 7065 6e20 6120 5b64 6973 6375 7373 696f  pen a [discussio
+00008100: 6e5d 2868 7474 7073 3a2f 2f67 6974 6875  n](https://githu
+00008110: 622e 636f 6d2f 5661 696e 462f 546f 7263  b.com/VainF/Torc
+00008120: 682d 5072 756e 696e 672f 6469 7363 7573  h-Pruning/discus
+00008130: 7369 6f6e 7329 206f 7220 5b69 7373 7565  sions) or [issue
+00008140: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
+00008150: 2e63 6f6d 2f56 6169 6e46 2f54 6f72 6368  .com/VainF/Torch
+00008160: 2d50 7275 6e69 6e67 2f69 7373 7565 7329  -Pruning/issues)
+00008170: 2069 6620 796f 7520 656e 636f 756e 7465   if you encounte
+00008180: 7220 616e 7920 7072 6f62 6c65 6d73 2077  r any problems w
+00008190: 6974 6820 7468 6520 6c69 6272 6172 7920  ith the library 
+000081a0: 6f72 2074 6865 2070 6170 6572 2e0a 0a0a  or the paper....
+000081b0: 2323 2320 2a2a 4665 6174 7572 6573 3a2a  ### **Features:*
+000081c0: 2a0a 2d20 5b78 5d20 5374 7275 6374 7572  *.- [x] Structur
+000081d0: 616c 2070 7275 6e69 6e67 2066 6f72 2043  al pruning for C
+000081e0: 4e4e 732c 2054 7261 6e73 666f 726d 6572  NNs, Transformer
+000081f0: 732c 2044 6574 6563 746f 7273 2c20 4c61  s, Detectors, La
+00008200: 6e67 7561 6765 204d 6f64 656c 7320 616e  nguage Models an
+00008210: 6420 4469 6666 7573 696f 6e20 4d6f 6465  d Diffusion Mode
+00008220: 6c73 2e20 506c 6561 7365 2072 6566 6572  ls. Please refer
+00008230: 2074 6f20 7468 6520 5b50 7275 6e61 6269   to the [Prunabi
+00008240: 6c69 7479 2042 656e 6368 6d61 726b 5d28  lity Benchmark](
+00008250: 6265 6e63 686d 6172 6b73 2f70 7275 6e61  benchmarks/pruna
+00008260: 6269 6c69 7479 292e 0a2d 205b 785d 2048  bility)..- [x] H
+00008270: 6967 682d 6c65 7665 6c20 7072 756e 6572  igh-level pruner
+00008280: 733a 205b 4d61 676e 6974 7564 6550 7275  s: [MagnitudePru
+00008290: 6e65 725d 2868 7474 7073 3a2f 2f61 7278  ner](https://arx
+000082a0: 6976 2e6f 7267 2f61 6273 2f31 3630 382e  iv.org/abs/1608.
+000082b0: 3038 3731 3029 2c20 5b42 4e53 6361 6c65  08710), [BNScale
+000082c0: 5072 756e 6572 5d28 6874 7470 733a 2f2f  Pruner](https://
+000082d0: 6172 7869 762e 6f72 672f 6162 732f 3137  arxiv.org/abs/17
+000082e0: 3038 2e30 3635 3139 292c 205b 4772 6f75  08.06519), [Grou
+000082f0: 704e 6f72 6d50 7275 6e65 725d 2868 7474  pNormPruner](htt
+00008300: 7073 3a2f 2f61 7278 6976 2e6f 7267 2f61  ps://arxiv.org/a
+00008310: 6273 2f32 3330 312e 3132 3930 3029 2c20  bs/2301.12900), 
+00008320: 5261 6e64 6f6d 5072 756e 6572 2c20 6574  RandomPruner, et
+00008330: 632e 0a2d 205b 785d 2049 6d70 6f72 7461  c..- [x] Importa
+00008340: 6e63 6520 4372 6974 6572 6961 3a20 4c2d  nce Criteria: L-
+00008350: 7020 4e6f 726d 2c20 5461 796c 6f72 2c20  p Norm, Taylor, 
+00008360: 5261 6e64 6f6d 2c20 424e 5363 616c 696e  Random, BNScalin
+00008370: 672c 2065 7463 2e0a 2d20 5b78 5d20 4465  g, etc..- [x] De
+00008380: 7065 6e64 656e 6379 2047 7261 7068 2066  pendency Graph f
+00008390: 6f72 2064 6570 656e 6465 6e63 7920 6d6f  or dependency mo
+000083a0: 6465 6c69 6e67 2e0a 2d20 5b78 5d20 5375  deling..- [x] Su
+000083b0: 7070 6f72 7465 6420 6d6f 6475 6c65 733a  pported modules:
+000083c0: 204c 696e 6561 722c 2028 5472 616e 7370   Linear, (Transp
+000083d0: 6f73 6564 2920 436f 6e76 2c20 4e6f 726d  osed) Conv, Norm
+000083e0: 616c 697a 6174 696f 6e2c 2050 5265 4c55  alization, PReLU
+000083f0: 2c20 456d 6265 6464 696e 672c 204d 756c  , Embedding, Mul
+00008400: 7469 6865 6164 4174 7465 6e74 696f 6e2c  tiheadAttention,
+00008410: 206e 6e2e 5061 7261 6d65 7465 7273 2061   nn.Parameters a
+00008420: 6e64 205b 6375 7374 6f6d 697a 6564 206d  nd [customized m
+00008430: 6f64 756c 6573 5d28 7465 7374 732f 7465  odules](tests/te
+00008440: 7374 5f63 7573 746f 6d69 7a65 645f 6c61  st_customized_la
+00008450: 7965 722e 7079 292e 0a2d 205b 785d 2053  yer.py)..- [x] S
+00008460: 7570 706f 7274 6564 206f 7065 7261 746f  upported operato
+00008470: 7273 3a20 7370 6c69 742c 2063 6f6e 6361  rs: split, conca
+00008480: 7465 6e61 7469 6f6e 2c20 736b 6970 2063  tenation, skip c
+00008490: 6f6e 6e65 6374 696f 6e2c 2066 6c61 7474  onnection, flatt
+000084a0: 656e 2c20 7265 7368 6170 652c 2076 6965  en, reshape, vie
+000084b0: 772c 2061 6c6c 2065 6c65 6d65 6e74 2d77  w, all element-w
+000084c0: 6973 6520 6f70 732c 2065 7463 2e0a 2d20  ise ops, etc..- 
+000084d0: 5b78 5d20 5b4c 6f77 2d6c 6576 656c 2070  [x] [Low-level p
+000084e0: 7275 6e69 6e67 2066 756e 6374 696f 6e73  runing functions
+000084f0: 5d28 746f 7263 685f 7072 756e 696e 672f  ](torch_pruning/
+00008500: 7072 756e 6572 2f66 756e 6374 696f 6e2e  pruner/function.
+00008510: 7079 290a 2d20 5b78 5d20 5b42 656e 6368  py).- [x] [Bench
+00008520: 6d61 726b 735d 2862 656e 6368 6d61 726b  marks](benchmark
+00008530: 7329 2061 6e64 205b 7475 746f 7269 616c  s) and [tutorial
+00008540: 735d 2874 7574 6f72 6961 6c73 290a 2d20  s](tutorials).- 
+00008550: 5b78 5d20 4120 5b72 6573 6f75 7263 6520  [x] A [resource 
+00008560: 6c69 7374 5d28 7072 6163 7469 6361 6c5f  list](practical_
+00008570: 7374 7275 6374 7572 616c 5f70 7275 6e69  structural_pruni
+00008580: 6e67 2e6d 6429 2066 6f72 2070 7261 6374  ng.md) for pract
+00008590: 6963 616c 2073 7472 7563 7472 7561 6c20  ical structrual 
+000085a0: 7072 756e 696e 672e 0a20 200a 2323 2320  pruning..  .### 
+000085b0: 2a2a 544f 444f 204c 6973 743a 2a2a 0a2d  **TODO List:**.-
+000085c0: 205b 205d 2041 2073 7472 6f6e 6720 6261   [ ] A strong ba
+000085d0: 7365 6c69 6e65 2077 6974 6820 6261 6773  seline with bags
+000085e0: 206f 6620 7472 6963 6b73 2066 726f 6d20   of tricks from 
+000085f0: 6578 6973 7469 6e67 206d 6574 686f 6473  existing methods
+00008600: 2e0a 2d20 5b20 5d20 4120 6265 6e63 686d  ..- [ ] A benchm
+00008610: 6172 6b20 666f 7220 5b54 6f72 6368 7669  ark for [Torchvi
+00008620: 7369 6f6e 5d28 6874 7470 733a 2f2f 7079  sion](https://py
+00008630: 746f 7263 682e 6f72 672f 7669 7369 6f6e  torch.org/vision
+00008640: 2f73 7461 626c 652f 6d6f 6465 6c73 2e68  /stable/models.h
+00008650: 746d 6c29 2063 6f6d 7061 7469 6269 6c69  tml) compatibili
+00008660: 7479 2028 2a2a 3831 2f38 353d 3935 2e33  ty (**81/85=95.3
+00008670: 252a 2a2c 203a 6865 6176 795f 6368 6563  %**, :heavy_chec
+00008680: 6b5f 6d61 726b 3a29 2061 6e64 205b 7469  k_mark:) and [ti
+00008690: 6d6d 5d28 6874 7470 733a 2f2f 6769 7468  mm](https://gith
+000086a0: 7562 2e63 6f6d 2f68 7567 6769 6e67 6661  ub.com/huggingfa
+000086b0: 6365 2f70 7974 6f72 6368 2d69 6d61 6765  ce/pytorch-image
+000086c0: 2d6d 6f64 656c 7329 2063 6f6d 7061 7469  -models) compati
+000086d0: 6269 6c69 7479 2e0a 2d20 5b20 5d20 5072  bility..- [ ] Pr
+000086e0: 756e 696e 6720 6672 6f6d 2053 6372 6174  uning from Scrat
+000086f0: 6368 202f 2061 7420 496e 6974 6961 6c69  ch / at Initiali
+00008700: 7a61 7469 6f6e 2e0a 2d20 5b20 5d20 4d6f  zation..- [ ] Mo
+00008710: 7265 2068 6967 682d 6c65 7665 6c20 7072  re high-level pr
+00008720: 756e 6572 7320 6c69 6b65 205b 4669 7368  uners like [Fish
+00008730: 6572 5072 756e 6572 5d28 6874 7470 733a  erPruner](https:
+00008740: 2f2f 6172 7869 762e 6f72 672f 6162 732f  //arxiv.org/abs/
+00008750: 3231 3038 2e30 3037 3038 292c 205b 4772  2108.00708), [Gr
+00008760: 6f77 696e 6752 6567 5d28 6874 7470 733a  owingReg](https:
+00008770: 2f2f 6172 7869 762e 6f72 672f 6162 732f  //arxiv.org/abs/
+00008780: 3230 3132 2e30 3932 3433 292c 2065 7463  2012.09243), etc
+00008790: 2e0a 2d20 5b20 5d20 4d6f 7265 2054 7261  ..- [ ] More Tra
+000087a0: 6e73 666f 726d 6572 7320 6c69 6b65 2056  nsformers like V
+000087b0: 6973 696f 6e20 5472 616e 7366 6f72 6d65  ision Transforme
+000087c0: 7273 2028 3a68 6561 7679 5f63 6865 636b  rs (:heavy_check
+000087d0: 5f6d 6172 6b3a 292c 2053 7769 6e20 5472  _mark:), Swin Tr
+000087e0: 616e 7366 6f72 6d65 7273 2c20 506f 6f6c  ansformers, Pool
+000087f0: 466f 726d 6572 732e 0a2d 205b 205d 2042  Formers..- [ ] B
+00008800: 6c6f 636b 2f4c 6179 6572 2f44 6570 7468  lock/Layer/Depth
+00008810: 2050 7275 6e69 6e67 0a2d 205b 205d 2050   Pruning.- [ ] P
+00008820: 7275 6e69 6e67 2062 656e 6368 6d61 726b  runing benchmark
+00008830: 7320 666f 7220 4349 4641 522c 2049 6d61  s for CIFAR, Ima
+00008840: 6765 4e65 7420 616e 6420 434f 434f 2e0a  geNet and COCO..
+00008850: 0a23 2320 496e 7374 616c 6c61 7469 6f6e  .## Installation
+00008860: 0a0a 546f 7263 682d 5072 756e 696e 6720  ..Torch-Pruning 
+00008870: 6973 2063 6f6d 7061 7469 626c 6520 7769  is compatible wi
+00008880: 7468 2050 7954 6f72 6368 2031 2e78 2061  th PyTorch 1.x a
+00008890: 6e64 2032 2e78 2e20 2a2a 5079 546f 7263  nd 2.x. **PyTorc
+000088a0: 6820 312e 3132 2e31 2069 7320 7265 636f  h 1.12.1 is reco
+000088b0: 6d6d 656e 6465 6421 2a2a 0a0a 6060 6062  mmended!**..```b
+000088c0: 6173 680a 7069 7020 696e 7374 616c 6c20  ash.pip install 
+000088d0: 746f 7263 682d 7072 756e 696e 6720 2320  torch-pruning # 
+000088e0: 7631 2e31 2e39 0a60 6060 0a6f 720a 6060  v1.1.9.```.or.``
+000088f0: 6062 6173 680a 6769 7420 636c 6f6e 6520  `bash.git clone 
+00008900: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
+00008910: 6f6d 2f56 6169 6e46 2f54 6f72 6368 2d50  om/VainF/Torch-P
+00008920: 7275 6e69 6e67 2e67 6974 0a60 6060 0a0a  runing.git.```..
+00008930: 2323 2051 7569 636b 7374 6172 740a 2020  ## Quickstart.  
+00008940: 0a48 6572 6520 7765 2070 726f 7669 6465  .Here we provide
+00008950: 2061 2071 7569 636b 2073 7461 7274 2066   a quick start f
+00008960: 6f72 2054 6f72 6368 2d50 7275 6e69 6e67  or Torch-Pruning
+00008970: 2e20 4d6f 7265 2065 7870 6c61 696e 6564  . More explained
+00008980: 2064 6574 6169 6c73 2063 616e 2062 6520   details can be 
+00008990: 666f 756e 6420 696e 205b 7475 746f 7261  found in [tutora
+000089a0: 6c73 5d28 2e2f 7475 746f 7269 616c 732f  ls](./tutorials/
+000089b0: 290a 0a23 2323 2030 2e20 486f 7720 4974  )..### 0. How It
+000089c0: 2057 6f72 6b73 0a0a 496e 2073 7472 7563   Works..In struc
+000089d0: 7475 7261 6c20 7072 756e 696e 672c 2061  tural pruning, a
+000089e0: 2060 6047 726f 7570 6060 2069 7320 6465   ``Group`` is de
+000089f0: 6669 6e65 6420 6173 2074 6865 206d 696e  fined as the min
+00008a00: 696d 616c 2072 656d 6f76 6162 6c65 2075  imal removable u
+00008a10: 6e69 7420 7769 7468 696e 2064 6565 7020  nit within deep 
+00008a20: 6e65 7477 6f72 6b73 2e20 4561 6368 2067  networks. Each g
+00008a30: 726f 7570 2063 6f6e 7369 7374 7320 6f66  roup consists of
+00008a40: 206d 756c 7469 706c 6520 696e 7465 7264   multiple interd
+00008a50: 6570 656e 6465 6e74 206c 6179 6572 7320  ependent layers 
+00008a60: 7468 6174 206e 6565 6420 746f 2062 6520  that need to be 
+00008a70: 7072 756e 6564 2073 696d 756c 7461 6e65  pruned simultane
+00008a80: 6f75 736c 7920 696e 206f 7264 6572 2074  ously in order t
+00008a90: 6f20 7072 6573 6572 7665 2074 6865 2069  o preserve the i
+00008aa0: 6e74 6567 7269 7479 206f 6620 7468 6520  ntegrity of the 
+00008ab0: 7265 7375 6c74 696e 6720 7374 7275 6374  resulting struct
+00008ac0: 7572 6573 2e20 486f 7765 7665 722c 2064  ures. However, d
+00008ad0: 6565 7020 6e65 7477 6f72 6b73 206f 6674  eep networks oft
+00008ae0: 656e 2065 7868 6962 6974 2069 6e74 7269  en exhibit intri
+00008af0: 6361 7465 2064 6570 656e 6465 6e63 6965  cate dependencie
+00008b00: 7320 616d 6f6e 6720 6c61 7965 7273 2c20  s among layers, 
+00008b10: 706f 7369 6e67 2061 2073 6967 6e69 6669  posing a signifi
+00008b20: 6361 6e74 2063 6861 6c6c 656e 6765 2066  cant challenge f
+00008b30: 6f72 2073 7472 7563 7475 7261 6c20 7072  or structural pr
+00008b40: 756e 696e 672e 2054 6869 7320 776f 726b  uning. This work
+00008b50: 2074 6163 6b6c 6573 2074 6869 7320 6368   tackles this ch
+00008b60: 616c 6c65 6e67 6520 6279 2069 6e74 726f  allenge by intro
+00008b70: 6475 6369 6e67 2061 6e20 6175 746f 6d61  ducing an automa
+00008b80: 7465 6420 6d65 6368 616e 6973 6d20 6361  ted mechanism ca
+00008b90: 6c6c 6564 2060 6044 6570 4772 6170 6860  lled ``DepGraph`
+00008ba0: 602c 2077 6869 6368 2065 6e61 626c 6573  `, which enables
+00008bb0: 2065 6666 6f72 746c 6573 7320 7061 7261   effortless para
+00008bc0: 6d65 7465 7220 6772 6f75 7069 6e67 2061  meter grouping a
+00008bd0: 6e64 2066 6163 696c 6974 6174 6573 2070  nd facilitates p
+00008be0: 7275 6e69 6e67 2066 6f72 2061 2064 6976  runing for a div
+00008bf0: 6572 7365 2072 616e 6765 206f 6620 6465  erse range of de
+00008c00: 6570 206e 6574 776f 726b 732e 0a0a 3c64  ep networks...<d
+00008c10: 6976 2061 6c69 676e 3d22 6365 6e74 6572  iv align="center
+00008c20: 223e 0a3c 696d 6720 7372 633d 2261 7373  ">.<img src="ass
+00008c30: 6574 732f 6465 702e 706e 6722 2077 6964  ets/dep.png" wid
+00008c40: 7468 3d22 3130 3025 223e 0a3c 2f64 6976  th="100%">.</div
+00008c50: 3e0a 0a23 2323 2031 2e20 4120 4d69 6e69  >..### 1. A Mini
+00008c60: 6d61 6c20 4578 616d 706c 650a 0a60 6060  mal Example..```
+00008c70: 7079 7468 6f6e 0a69 6d70 6f72 7420 746f  python.import to
+00008c80: 7263 680a 6672 6f6d 2074 6f72 6368 7669  rch.from torchvi
+00008c90: 7369 6f6e 2e6d 6f64 656c 7320 696d 706f  sion.models impo
+00008ca0: 7274 2072 6573 6e65 7431 380a 696d 706f  rt resnet18.impo
+00008cb0: 7274 2074 6f72 6368 5f70 7275 6e69 6e67  rt torch_pruning
+00008cc0: 2061 7320 7470 0a0a 6d6f 6465 6c20 3d20   as tp..model = 
+00008cd0: 7265 736e 6574 3138 2870 7265 7472 6169  resnet18(pretrai
+00008ce0: 6e65 643d 5472 7565 292e 6576 616c 2829  ned=True).eval()
+00008cf0: 0a0a 2320 312e 2062 7569 6c64 2064 6570  ..# 1. build dep
+00008d00: 656e 6465 6e63 7920 6772 6170 6820 666f  endency graph fo
+00008d10: 7220 7265 736e 6574 3138 0a44 4720 3d20  r resnet18.DG = 
+00008d20: 7470 2e44 6570 656e 6465 6e63 7947 7261  tp.DependencyGra
+00008d30: 7068 2829 2e62 7569 6c64 5f64 6570 656e  ph().build_depen
+00008d40: 6465 6e63 7928 6d6f 6465 6c2c 2065 7861  dency(model, exa
+00008d50: 6d70 6c65 5f69 6e70 7574 733d 746f 7263  mple_inputs=torc
+00008d60: 682e 7261 6e64 6e28 312c 332c 3232 342c  h.randn(1,3,224,
+00008d70: 3232 3429 290a 0a23 2032 2e20 5370 6563  224))..# 2. Spec
+00008d80: 6966 7920 7468 6520 746f 2d62 652d 7072  ify the to-be-pr
+00008d90: 756e 6564 2063 6861 6e6e 656c 732e 2048  uned channels. H
+00008da0: 6572 6520 7765 2070 7275 6e65 2074 686f  ere we prune tho
+00008db0: 7365 2063 6861 6e6e 656c 7320 696e 6465  se channels inde
+00008dc0: 7865 6420 6279 205b 322c 2036 2c20 395d  xed by [2, 6, 9]
+00008dd0: 2e0a 6772 6f75 7020 3d20 4447 2e67 6574  ..group = DG.get
+00008de0: 5f70 7275 6e69 6e67 5f67 726f 7570 2820  _pruning_group( 
+00008df0: 6d6f 6465 6c2e 636f 6e76 312c 2074 702e  model.conv1, tp.
+00008e00: 7072 756e 655f 636f 6e76 5f6f 7574 5f63  prune_conv_out_c
+00008e10: 6861 6e6e 656c 732c 2069 6478 733d 5b32  hannels, idxs=[2
+00008e20: 2c20 362c 2039 5d20 290a 0a23 2033 2e20  , 6, 9] )..# 3. 
+00008e30: 7072 756e 6520 616c 6c20 6772 6f75 7065  prune all groupe
+00008e40: 6420 6c61 7965 7273 2074 6861 7420 6172  d layers that ar
+00008e50: 6520 636f 7570 6c65 6420 7769 7468 206d  e coupled with m
+00008e60: 6f64 656c 2e63 6f6e 7631 2028 696e 636c  odel.conv1 (incl
+00008e70: 7564 6564 292e 0a69 6620 4447 2e63 6865  uded)..if DG.che
+00008e80: 636b 5f70 7275 6e69 6e67 5f67 726f 7570  ck_pruning_group
+00008e90: 2867 726f 7570 293a 2023 2061 766f 6964  (group): # avoid
+00008ea0: 2066 756c 6c20 7072 756e 696e 672c 2069   full pruning, i
+00008eb0: 2e65 2e2c 2063 6861 6e6e 656c 733d 302e  .e., channels=0.
+00008ec0: 0a20 2020 2067 726f 7570 2e70 7275 6e65  .    group.prune
+00008ed0: 2829 0a20 2020 200a 2320 342e 2053 6176  ().    .# 4. Sav
+00008ee0: 6520 2620 4c6f 6164 0a6d 6f64 656c 2e7a  e & Load.model.z
+00008ef0: 6572 6f5f 6772 6164 2829 2023 2057 6520  ero_grad() # We 
+00008f00: 646f 6e27 7420 7761 6e74 2074 6f20 7374  don't want to st
+00008f10: 6f72 6520 6772 6164 6965 6e74 2069 6e66  ore gradient inf
+00008f20: 6f72 6d61 7469 6f6e 0a74 6f72 6368 2e73  ormation.torch.s
+00008f30: 6176 6528 6d6f 6465 6c2c 2027 6d6f 6465  ave(model, 'mode
+00008f40: 6c2e 7074 6827 2920 2320 7769 7468 6f75  l.pth') # withou
+00008f50: 7420 2e73 7461 7465 5f64 6963 740a 6d6f  t .state_dict.mo
+00008f60: 6465 6c20 3d20 746f 7263 682e 6c6f 6164  del = torch.load
+00008f70: 2827 6d6f 6465 6c2e 7074 6827 2920 2320  ('model.pth') # 
+00008f80: 6c6f 6164 2074 6865 206d 6f64 656c 206f  load the model o
+00008f90: 626a 6563 740a 6060 600a 2020 0a54 6865  bject.```.  .The
+00008fa0: 2061 626f 7665 2065 7861 6d70 6c65 2064   above example d
+00008fb0: 656d 6f6e 7374 7261 7465 7320 7468 6520  emonstrates the 
+00008fc0: 6675 6e64 616d 656e 7461 6c20 7072 756e  fundamental prun
+00008fd0: 696e 6720 7069 7065 6c69 6e65 2075 7369  ing pipeline usi
+00008fe0: 6e67 2044 6570 4772 6170 682e 2054 6865  ng DepGraph. The
+00008ff0: 2074 6172 6765 7420 6c61 7965 7220 7265   target layer re
+00009000: 736e 6574 2e63 6f6e 7631 2069 7320 636f  snet.conv1 is co
+00009010: 7570 6c65 6420 7769 7468 2073 6576 6572  upled with sever
+00009020: 616c 206c 6179 6572 732c 2077 6869 6368  al layers, which
+00009030: 2072 6571 7569 7265 7320 7369 6d75 6c74   requires simult
+00009040: 616e 656f 7573 2072 656d 6f76 616c 2069  aneous removal i
+00009050: 6e20 7374 7275 6374 7572 616c 2070 7275  n structural pru
+00009060: 6e69 6e67 2e20 4c65 7427 7320 7072 696e  ning. Let's prin
+00009070: 7420 7468 6520 6772 6f75 7020 616e 6420  t the group and 
+00009080: 6f62 7365 7276 6520 686f 7720 6120 7072  observe how a pr
+00009090: 756e 696e 6720 6f70 6572 6174 696f 6e20  uning operation 
+000090a0: 2274 7269 6767 6572 7322 206f 7468 6572  "triggers" other
+000090b0: 206f 6e65 732e 2049 6e20 7468 6520 666f   ones. In the fo
+000090c0: 6c6c 6f77 696e 6720 6f75 7470 7574 732c  llowing outputs,
+000090d0: 2060 6041 203d 3e20 4260 6020 6d65 616e   ``A => B`` mean
+000090e0: 7320 7468 6520 7072 756e 696e 6720 6f70  s the pruning op
+000090f0: 6572 6174 696f 6e20 6060 4160 6020 7472  eration ``A`` tr
+00009100: 6967 6765 7273 2074 6865 2070 7275 6e69  iggers the pruni
+00009110: 6e67 206f 7065 7261 7469 6f6e 2060 6042  ng operation ``B
+00009120: 6060 2e20 6772 6f75 705b 305d 2072 6566  ``. group[0] ref
+00009130: 6572 7320 746f 2074 6865 2070 7275 6e69  ers to the pruni
+00009140: 6e67 2072 6f6f 7420 696e 2060 6044 472e  ng root in ``DG.
+00009150: 6765 745f 7072 756e 696e 675f 6772 6f75  get_pruning_grou
+00009160: 7060 602e 0a0a 6060 600a 2d2d 2d2d 2d2d  p``...```.------
 00009170: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00009180: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00009190: 0a5b 305d 2070 7275 6e65 5f6f 7574 5f63  .[0] prune_out_c
-000091a0: 6861 6e6e 656c 7320 6f6e 2063 6f6e 7631  hannels on conv1
-000091b0: 2028 436f 6e76 3264 2833 2c20 3634 2c20   (Conv2d(3, 64, 
-000091c0: 6b65 726e 656c 5f73 697a 653d 2837 2c20  kernel_size=(7, 
-000091d0: 3729 2c20 7374 7269 6465 3d28 322c 2032  7), stride=(2, 2
-000091e0: 292c 2070 6164 6469 6e67 3d28 332c 2033  ), padding=(3, 3
-000091f0: 292c 2062 6961 733d 4661 6c73 6529 2920  ), bias=False)) 
-00009200: 3d3e 2070 7275 6e65 5f6f 7574 5f63 6861  => prune_out_cha
-00009210: 6e6e 656c 7320 6f6e 2063 6f6e 7631 2028  nnels on conv1 (
-00009220: 436f 6e76 3264 2833 2c20 3634 2c20 6b65  Conv2d(3, 64, ke
-00009230: 726e 656c 5f73 697a 653d 2837 2c20 3729  rnel_size=(7, 7)
-00009240: 2c20 7374 7269 6465 3d28 322c 2032 292c  , stride=(2, 2),
-00009250: 2070 6164 6469 6e67 3d28 332c 2033 292c   padding=(3, 3),
-00009260: 2062 6961 733d 4661 6c73 6529 292c 2069   bias=False)), i
-00009270: 6478 733d 5b32 2c20 362c 2039 5d20 2850  dxs=[2, 6, 9] (P
-00009280: 7275 6e69 6e67 2052 6f6f 7429 0a5b 315d  runing Root).[1]
-00009290: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
-000092a0: 656c 7320 6f6e 2063 6f6e 7631 2028 436f  els on conv1 (Co
-000092b0: 6e76 3264 2833 2c20 3634 2c20 6b65 726e  nv2d(3, 64, kern
-000092c0: 656c 5f73 697a 653d 2837 2c20 3729 2c20  el_size=(7, 7), 
-000092d0: 7374 7269 6465 3d28 322c 2032 292c 2070  stride=(2, 2), p
-000092e0: 6164 6469 6e67 3d28 332c 2033 292c 2062  adding=(3, 3), b
-000092f0: 6961 733d 4661 6c73 6529 2920 3d3e 2070  ias=False)) => p
-00009300: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-00009310: 7320 6f6e 2062 6e31 2028 4261 7463 684e  s on bn1 (BatchN
-00009320: 6f72 6d32 6428 3634 2c20 6570 733d 3165  orm2d(64, eps=1e
-00009330: 2d30 352c 206d 6f6d 656e 7475 6d3d 302e  -05, momentum=0.
-00009340: 312c 2061 6666 696e 653d 5472 7565 2c20  1, affine=True, 
-00009350: 7472 6163 6b5f 7275 6e6e 696e 675f 7374  track_running_st
-00009360: 6174 733d 5472 7565 2929 2c20 6964 7873  ats=True)), idxs
-00009370: 3d5b 322c 2036 2c20 395d 0a5b 325d 2070  =[2, 6, 9].[2] p
-00009380: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-00009390: 7320 6f6e 2062 6e31 2028 4261 7463 684e  s on bn1 (BatchN
-000093a0: 6f72 6d32 6428 3634 2c20 6570 733d 3165  orm2d(64, eps=1e
-000093b0: 2d30 352c 206d 6f6d 656e 7475 6d3d 302e  -05, momentum=0.
-000093c0: 312c 2061 6666 696e 653d 5472 7565 2c20  1, affine=True, 
-000093d0: 7472 6163 6b5f 7275 6e6e 696e 675f 7374  track_running_st
-000093e0: 6174 733d 5472 7565 2929 203d 3e20 7072  ats=True)) => pr
-000093f0: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-00009400: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
-00009410: 4f70 5f32 3028 5265 6c75 4261 636b 7761  Op_20(ReluBackwa
-00009420: 7264 3029 2c20 6964 7873 3d5b 322c 2036  rd0), idxs=[2, 6
-00009430: 2c20 395d 0a5b 335d 2070 7275 6e65 5f6f  , 9].[3] prune_o
-00009440: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
-00009450: 456c 656d 656e 7457 6973 654f 705f 3230  ElementWiseOp_20
-00009460: 2852 656c 7542 6163 6b77 6172 6430 2920  (ReluBackward0) 
-00009470: 3d3e 2070 7275 6e65 5f6f 7574 5f63 6861  => prune_out_cha
-00009480: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
-00009490: 7457 6973 654f 705f 3139 284d 6178 506f  tWiseOp_19(MaxPo
-000094a0: 6f6c 3244 5769 7468 496e 6469 6365 7342  ol2DWithIndicesB
-000094b0: 6163 6b77 6172 6430 292c 2069 6478 733d  ackward0), idxs=
-000094c0: 5b32 2c20 362c 2039 5d0a 5b34 5d20 7072  [2, 6, 9].[4] pr
-000094d0: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-000094e0: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
-000094f0: 4f70 5f31 3928 4d61 7850 6f6f 6c32 4457  Op_19(MaxPool2DW
-00009500: 6974 6849 6e64 6963 6573 4261 636b 7761  ithIndicesBackwa
-00009510: 7264 3029 203d 3e20 7072 756e 655f 6f75  rd0) => prune_ou
-00009520: 745f 6368 616e 6e65 6c73 206f 6e20 5f45  t_channels on _E
-00009530: 6c65 6d65 6e74 5769 7365 4f70 5f31 3828  lementWiseOp_18(
-00009540: 4164 6442 6163 6b77 6172 6430 292c 2069  AddBackward0), i
-00009550: 6478 733d 5b32 2c20 362c 2039 5d0a 5b35  dxs=[2, 6, 9].[5
-00009560: 5d20 7072 756e 655f 6f75 745f 6368 616e  ] prune_out_chan
-00009570: 6e65 6c73 206f 6e20 5f45 6c65 6d65 6e74  nels on _Element
-00009580: 5769 7365 4f70 5f31 3928 4d61 7850 6f6f  WiseOp_19(MaxPoo
-00009590: 6c32 4457 6974 6849 6e64 6963 6573 4261  l2DWithIndicesBa
-000095a0: 636b 7761 7264 3029 203d 3e20 7072 756e  ckward0) => prun
-000095b0: 655f 696e 5f63 6861 6e6e 656c 7320 6f6e  e_in_channels on
-000095c0: 206c 6179 6572 312e 302e 636f 6e76 3120   layer1.0.conv1 
-000095d0: 2843 6f6e 7632 6428 3634 2c20 3634 2c20  (Conv2d(64, 64, 
-000095e0: 6b65 726e 656c 5f73 697a 653d 2833 2c20  kernel_size=(3, 
-000095f0: 3329 2c20 7374 7269 6465 3d28 312c 2031  3), stride=(1, 1
-00009600: 292c 2070 6164 6469 6e67 3d28 312c 2031  ), padding=(1, 1
-00009610: 292c 2062 6961 733d 4661 6c73 6529 292c  ), bias=False)),
-00009620: 2069 6478 733d 5b32 2c20 362c 2039 5d0a   idxs=[2, 6, 9].
-00009630: 5b36 5d20 7072 756e 655f 6f75 745f 6368  [6] prune_out_ch
-00009640: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
-00009650: 6e74 5769 7365 4f70 5f31 3828 4164 6442  ntWiseOp_18(AddB
-00009660: 6163 6b77 6172 6430 2920 3d3e 2070 7275  ackward0) => pru
-00009670: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
-00009680: 6f6e 206c 6179 6572 312e 302e 626e 3220  on layer1.0.bn2 
-00009690: 2842 6174 6368 4e6f 726d 3264 2836 342c  (BatchNorm2d(64,
-000096a0: 2065 7073 3d31 652d 3035 2c20 6d6f 6d65   eps=1e-05, mome
-000096b0: 6e74 756d 3d30 2e31 2c20 6166 6669 6e65  ntum=0.1, affine
-000096c0: 3d54 7275 652c 2074 7261 636b 5f72 756e  =True, track_run
-000096d0: 6e69 6e67 5f73 7461 7473 3d54 7275 6529  ning_stats=True)
-000096e0: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
-000096f0: 5d0a 5b37 5d20 7072 756e 655f 6f75 745f  ].[7] prune_out_
-00009700: 6368 616e 6e65 6c73 206f 6e20 5f45 6c65  channels on _Ele
-00009710: 6d65 6e74 5769 7365 4f70 5f31 3828 4164  mentWiseOp_18(Ad
-00009720: 6442 6163 6b77 6172 6430 2920 3d3e 2070  dBackward0) => p
-00009730: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-00009740: 7320 6f6e 205f 456c 656d 656e 7457 6973  s on _ElementWis
-00009750: 654f 705f 3137 2852 656c 7542 6163 6b77  eOp_17(ReluBackw
-00009760: 6172 6430 292c 2069 6478 733d 5b32 2c20  ard0), idxs=[2, 
-00009770: 362c 2039 5d0a 5b38 5d20 7072 756e 655f  6, 9].[8] prune_
-00009780: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-00009790: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f31  _ElementWiseOp_1
-000097a0: 3728 5265 6c75 4261 636b 7761 7264 3029  7(ReluBackward0)
-000097b0: 203d 3e20 7072 756e 655f 6f75 745f 6368   => prune_out_ch
-000097c0: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
-000097d0: 6e74 5769 7365 4f70 5f31 3628 4164 6442  ntWiseOp_16(AddB
-000097e0: 6163 6b77 6172 6430 292c 2069 6478 733d  ackward0), idxs=
-000097f0: 5b32 2c20 362c 2039 5d0a 5b39 5d20 7072  [2, 6, 9].[9] pr
-00009800: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-00009810: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
-00009820: 4f70 5f31 3728 5265 6c75 4261 636b 7761  Op_17(ReluBackwa
-00009830: 7264 3029 203d 3e20 7072 756e 655f 696e  rd0) => prune_in
-00009840: 5f63 6861 6e6e 656c 7320 6f6e 206c 6179  _channels on lay
-00009850: 6572 312e 312e 636f 6e76 3120 2843 6f6e  er1.1.conv1 (Con
-00009860: 7632 6428 3634 2c20 3634 2c20 6b65 726e  v2d(64, 64, kern
-00009870: 656c 5f73 697a 653d 2833 2c20 3329 2c20  el_size=(3, 3), 
-00009880: 7374 7269 6465 3d28 312c 2031 292c 2070  stride=(1, 1), p
-00009890: 6164 6469 6e67 3d28 312c 2031 292c 2062  adding=(1, 1), b
-000098a0: 6961 733d 4661 6c73 6529 292c 2069 6478  ias=False)), idx
-000098b0: 733d 5b32 2c20 362c 2039 5d0a 5b31 305d  s=[2, 6, 9].[10]
-000098c0: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
-000098d0: 656c 7320 6f6e 205f 456c 656d 656e 7457  els on _ElementW
-000098e0: 6973 654f 705f 3136 2841 6464 4261 636b  iseOp_16(AddBack
-000098f0: 7761 7264 3029 203d 3e20 7072 756e 655f  ward0) => prune_
-00009900: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-00009910: 6c61 7965 7231 2e31 2e62 6e32 2028 4261  layer1.1.bn2 (Ba
-00009920: 7463 684e 6f72 6d32 6428 3634 2c20 6570  tchNorm2d(64, ep
-00009930: 733d 3165 2d30 352c 206d 6f6d 656e 7475  s=1e-05, momentu
-00009940: 6d3d 302e 312c 2061 6666 696e 653d 5472  m=0.1, affine=Tr
-00009950: 7565 2c20 7472 6163 6b5f 7275 6e6e 696e  ue, track_runnin
-00009960: 675f 7374 6174 733d 5472 7565 2929 2c20  g_stats=True)), 
-00009970: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
-00009980: 3131 5d20 7072 756e 655f 6f75 745f 6368  11] prune_out_ch
-00009990: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
-000099a0: 6e74 5769 7365 4f70 5f31 3628 4164 6442  ntWiseOp_16(AddB
-000099b0: 6163 6b77 6172 6430 2920 3d3e 2070 7275  ackward0) => pru
-000099c0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
-000099d0: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
-000099e0: 705f 3135 2852 656c 7542 6163 6b77 6172  p_15(ReluBackwar
-000099f0: 6430 292c 2069 6478 733d 5b32 2c20 362c  d0), idxs=[2, 6,
-00009a00: 2039 5d0a 5b31 325d 2070 7275 6e65 5f6f   9].[12] prune_o
-00009a10: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
-00009a20: 456c 656d 656e 7457 6973 654f 705f 3135  ElementWiseOp_15
-00009a30: 2852 656c 7542 6163 6b77 6172 6430 2920  (ReluBackward0) 
-00009a40: 3d3e 2070 7275 6e65 5f69 6e5f 6368 616e  => prune_in_chan
-00009a50: 6e65 6c73 206f 6e20 6c61 7965 7232 2e30  nels on layer2.0
-00009a60: 2e64 6f77 6e73 616d 706c 652e 3020 2843  .downsample.0 (C
-00009a70: 6f6e 7632 6428 3634 2c20 3132 382c 206b  onv2d(64, 128, k
-00009a80: 6572 6e65 6c5f 7369 7a65 3d28 312c 2031  ernel_size=(1, 1
-00009a90: 292c 2073 7472 6964 653d 2832 2c20 3229  ), stride=(2, 2)
-00009aa0: 2c20 6269 6173 3d46 616c 7365 2929 2c20  , bias=False)), 
-00009ab0: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
-00009ac0: 3133 5d20 7072 756e 655f 6f75 745f 6368  13] prune_out_ch
-00009ad0: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
-00009ae0: 6e74 5769 7365 4f70 5f31 3528 5265 6c75  ntWiseOp_15(Relu
-00009af0: 4261 636b 7761 7264 3029 203d 3e20 7072  Backward0) => pr
-00009b00: 756e 655f 696e 5f63 6861 6e6e 656c 7320  une_in_channels 
-00009b10: 6f6e 206c 6179 6572 322e 302e 636f 6e76  on layer2.0.conv
-00009b20: 3120 2843 6f6e 7632 6428 3634 2c20 3132  1 (Conv2d(64, 12
-00009b30: 382c 206b 6572 6e65 6c5f 7369 7a65 3d28  8, kernel_size=(
-00009b40: 332c 2033 292c 2073 7472 6964 653d 2832  3, 3), stride=(2
-00009b50: 2c20 3229 2c20 7061 6464 696e 673d 2831  , 2), padding=(1
-00009b60: 2c20 3129 2c20 6269 6173 3d46 616c 7365  , 1), bias=False
-00009b70: 2929 2c20 6964 7873 3d5b 322c 2036 2c20  )), idxs=[2, 6, 
-00009b80: 395d 0a5b 3134 5d20 7072 756e 655f 6f75  9].[14] prune_ou
-00009b90: 745f 6368 616e 6e65 6c73 206f 6e20 6c61  t_channels on la
-00009ba0: 7965 7231 2e31 2e62 6e32 2028 4261 7463  yer1.1.bn2 (Batc
-00009bb0: 684e 6f72 6d32 6428 3634 2c20 6570 733d  hNorm2d(64, eps=
-00009bc0: 3165 2d30 352c 206d 6f6d 656e 7475 6d3d  1e-05, momentum=
-00009bd0: 302e 312c 2061 6666 696e 653d 5472 7565  0.1, affine=True
-00009be0: 2c20 7472 6163 6b5f 7275 6e6e 696e 675f  , track_running_
-00009bf0: 7374 6174 733d 5472 7565 2929 203d 3e20  stats=True)) => 
-00009c00: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-00009c10: 6c73 206f 6e20 6c61 7965 7231 2e31 2e63  ls on layer1.1.c
-00009c20: 6f6e 7632 2028 436f 6e76 3264 2836 342c  onv2 (Conv2d(64,
-00009c30: 2036 342c 206b 6572 6e65 6c5f 7369 7a65   64, kernel_size
-00009c40: 3d28 332c 2033 292c 2073 7472 6964 653d  =(3, 3), stride=
-00009c50: 2831 2c20 3129 2c20 7061 6464 696e 673d  (1, 1), padding=
-00009c60: 2831 2c20 3129 2c20 6269 6173 3d46 616c  (1, 1), bias=Fal
-00009c70: 7365 2929 2c20 6964 7873 3d5b 322c 2036  se)), idxs=[2, 6
-00009c80: 2c20 395d 0a5b 3135 5d20 7072 756e 655f  , 9].[15] prune_
-00009c90: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-00009ca0: 6c61 7965 7231 2e30 2e62 6e32 2028 4261  layer1.0.bn2 (Ba
-00009cb0: 7463 684e 6f72 6d32 6428 3634 2c20 6570  tchNorm2d(64, ep
-00009cc0: 733d 3165 2d30 352c 206d 6f6d 656e 7475  s=1e-05, momentu
-00009cd0: 6d3d 302e 312c 2061 6666 696e 653d 5472  m=0.1, affine=Tr
-00009ce0: 7565 2c20 7472 6163 6b5f 7275 6e6e 696e  ue, track_runnin
-00009cf0: 675f 7374 6174 733d 5472 7565 2929 203d  g_stats=True)) =
-00009d00: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
-00009d10: 6e65 6c73 206f 6e20 6c61 7965 7231 2e30  nels on layer1.0
-00009d20: 2e63 6f6e 7632 2028 436f 6e76 3264 2836  .conv2 (Conv2d(6
-00009d30: 342c 2036 342c 206b 6572 6e65 6c5f 7369  4, 64, kernel_si
-00009d40: 7a65 3d28 332c 2033 292c 2073 7472 6964  ze=(3, 3), strid
-00009d50: 653d 2831 2c20 3129 2c20 7061 6464 696e  e=(1, 1), paddin
-00009d60: 673d 2831 2c20 3129 2c20 6269 6173 3d46  g=(1, 1), bias=F
-00009d70: 616c 7365 2929 2c20 6964 7873 3d5b 322c  alse)), idxs=[2,
-00009d80: 2036 2c20 395d 0a2d 2d2d 2d2d 2d2d 2d2d   6, 9].---------
-00009d90: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00009da0: 2d2d 2d2d 2d2d 2d0a 6060 600a 466f 7220  -------.```.For 
-00009db0: 6d6f 7265 2064 6574 6169 6c73 2061 626f  more details abo
-00009dc0: 7574 2067 726f 7570 696e 672c 2070 6c65  ut grouping, ple
-00009dd0: 6173 6520 7265 6665 7220 746f 205b 7475  ase refer to [tu
-00009de0: 746f 7269 616c 732f 3220 2d20 4578 706c  torials/2 - Expl
-00009df0: 6f72 696e 6720 4465 7065 6e64 656e 6379  oring Dependency
-00009e00: 2047 726f 7570 735d 2868 7474 7073 3a2f   Groups](https:/
-00009e10: 2f67 6974 6875 622e 636f 6d2f 5661 696e  /github.com/Vain
-00009e20: 462f 546f 7263 682d 5072 756e 696e 672f  F/Torch-Pruning/
-00009e30: 626c 6f62 2f6d 6173 7465 722f 7475 746f  blob/master/tuto
-00009e40: 7269 616c 732f 3225 3230 2d25 3230 4578  rials/2%20-%20Ex
-00009e50: 706c 6f72 696e 6725 3230 4465 7065 6e64  ploring%20Depend
-00009e60: 656e 6379 2532 3047 726f 7570 732e 6970  ency%20Groups.ip
-00009e70: 796e 6229 0a20 200a 2323 2323 2048 6f77  ynb).  .#### How
-00009e80: 2074 6f20 7363 616e 2061 6c6c 2067 726f   to scan all gro
-00009e90: 7570 7320 2841 6476 616e 6365 6429 3a0a  ups (Advanced):.
-00009ea0: 5765 2063 616e 2075 7365 2060 6044 472e  We can use ``DG.
-00009eb0: 6765 745f 616c 6c5f 6772 6f75 7073 2869  get_all_groups(i
-00009ec0: 676e 6f72 6564 5f6c 6179 6572 732c 2072  gnored_layers, r
-00009ed0: 6f6f 745f 6d6f 6475 6c65 5f74 7970 6573  oot_module_types
-00009ee0: 2960 6020 746f 2073 6361 6e20 616c 6c20  )`` to scan all 
-00009ef0: 6772 6f75 7073 2073 6571 7565 6e74 6961  groups sequentia
-00009f00: 6c6c 792e 2045 6163 6820 6772 6f75 7020  lly. Each group 
-00009f10: 7769 6c6c 2062 6567 696e 2077 6974 6820  will begin with 
-00009f20: 6120 6c61 7965 7220 7468 6174 206d 6174  a layer that mat
-00009f30: 6368 6573 2061 2074 7970 6520 696e 2074  ches a type in t
-00009f40: 6865 2022 726f 6f74 5f6d 6f64 756c 655f  he "root_module_
-00009f50: 7479 7065 7322 2070 6172 616d 6574 6572  types" parameter
-00009f60: 2e20 4e6f 7465 2074 6861 7420 4447 2e67  . Note that DG.g
-00009f70: 6574 5f61 6c6c 5f67 726f 7570 7320 6973  et_all_groups is
-00009f80: 206f 6e6c 7920 7265 7370 6f6e 7369 626c   only responsibl
-00009f90: 6520 666f 7220 6772 6f75 7069 6e67 2061  e for grouping a
-00009fa0: 6e64 2064 6f65 7320 6e6f 7420 6861 7665  nd does not have
-00009fb0: 2061 6e79 206b 6e6f 776c 6564 6765 206f   any knowledge o
-00009fc0: 7220 756e 6465 7273 7461 6e64 696e 6720  r understanding 
-00009fd0: 6f66 2077 6869 6368 2070 6172 616d 6574  of which paramet
-00009fe0: 6572 7320 7368 6f75 6c64 2062 6520 7072  ers should be pr
-00009ff0: 756e 6564 2e20 5468 6572 6566 6f72 652c  uned. Therefore,
-0000a000: 2069 7420 6973 206e 6563 6573 7361 7279   it is necessary
-0000a010: 2074 6f20 7370 6563 6966 7920 7468 6520   to specify the 
-0000a020: 7072 756e 696e 6720 6964 7873 2075 7369  pruning idxs usi
-0000a030: 6e67 2020 6060 6772 6f75 702e 7072 756e  ng  ``group.prun
-0000a040: 6528 6964 7873 3d69 6478 7329 6060 2e0a  e(idxs=idxs)``..
-0000a050: 0a60 6060 7079 7468 6f6e 0a66 6f72 2067  .```python.for g
-0000a060: 726f 7570 2069 6e20 4447 2e67 6574 5f61  roup in DG.get_a
-0000a070: 6c6c 5f67 726f 7570 7328 6967 6e6f 7265  ll_groups(ignore
-0000a080: 645f 6c61 7965 7273 3d5b 6d6f 6465 6c2e  d_layers=[model.
-0000a090: 636f 6e76 315d 2c20 726f 6f74 5f6d 6f64  conv1], root_mod
-0000a0a0: 756c 655f 7479 7065 733d 5b6e 6e2e 436f  ule_types=[nn.Co
-0000a0b0: 6e76 3264 2c20 6e6e 2e4c 696e 6561 725d  nv2d, nn.Linear]
-0000a0c0: 293a 0a20 2020 2023 2068 616e 646c 6520  ):.    # handle 
-0000a0d0: 6772 6f75 7073 2069 6e20 7365 7175 656e  groups in sequen
-0000a0e0: 7469 616c 206f 7264 6572 0a20 2020 2069  tial order.    i
-0000a0f0: 6478 7320 3d20 5b32 2c34 2c36 5d20 2320  dxs = [2,4,6] # 
-0000a100: 796f 7572 2070 7275 6e69 6e67 2069 6e64  your pruning ind
-0000a110: 6963 6573 0a20 2020 2067 726f 7570 2e70  ices.    group.p
-0000a120: 7275 6e65 2869 6478 733d 6964 7873 290a  rune(idxs=idxs).
-0000a130: 2020 2020 7072 696e 7428 6772 6f75 7029      print(group)
-0000a140: 0a60 6060 0a0a 2323 2320 322e 2048 6967  .```..### 2. Hig
-0000a150: 682d 6c65 7665 6c20 5072 756e 6572 730a  h-level Pruners.
-0000a160: 0a4c 6576 6572 6167 696e 6720 7468 6520  .Leveraging the 
-0000a170: 4465 7065 6e64 656e 6379 4772 6170 682c  DependencyGraph,
-0000a180: 2077 6520 6465 7665 6c6f 7065 6420 7365   we developed se
-0000a190: 7665 7261 6c20 6869 6768 2d6c 6576 656c  veral high-level
-0000a1a0: 2070 7275 6e65 7273 2069 6e20 7468 6973   pruners in this
-0000a1b0: 2072 6570 6f73 6974 6f72 7920 746f 2066   repository to f
-0000a1c0: 6163 696c 6974 6174 6520 6566 666f 7274  acilitate effort
-0000a1d0: 6c65 7373 2070 7275 6e69 6e67 2e20 4279  less pruning. By
-0000a1e0: 2073 7065 6369 6679 696e 6720 7468 6520   specifying the 
-0000a1f0: 6465 7369 7265 6420 6368 616e 6e65 6c20  desired channel 
-0000a200: 7370 6172 7369 7479 2c20 796f 7520 6361  sparsity, you ca
-0000a210: 6e20 7072 756e 6520 7468 6520 656e 7469  n prune the enti
-0000a220: 7265 206d 6f64 656c 2061 6e64 2066 696e  re model and fin
-0000a230: 652d 7475 6e65 2069 7420 7573 696e 6720  e-tune it using 
-0000a240: 796f 7572 206f 776e 2074 7261 696e 696e  your own trainin
-0000a250: 6720 636f 6465 2e20 466f 7220 6465 7461  g code. For deta
-0000a260: 696c 6564 2069 6e66 6f72 6d61 7469 6f6e  iled information
-0000a270: 206f 6e20 7468 6973 2070 726f 6365 7373   on this process
-0000a280: 2c20 706c 6561 7365 2072 6566 6572 2074  , please refer t
-0000a290: 6f20 5b74 6869 7320 7475 746f 7269 616c  o [this tutorial
-0000a2a0: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
-0000a2b0: 2e63 6f6d 2f56 6169 6e46 2f54 6f72 6368  .com/VainF/Torch
-0000a2c0: 2d50 7275 6e69 6e67 2f62 6c6f 622f 6d61  -Pruning/blob/ma
-0000a2d0: 7374 6572 2f74 7574 6f72 6961 6c73 2f31  ster/tutorials/1
-0000a2e0: 2532 302d 2532 3043 7573 746f 6d69 7a65  %20-%20Customize
-0000a2f0: 2532 3059 6f75 7225 3230 4f77 6e25 3230  %20Your%20Own%20
-0000a300: 5072 756e 6572 732e 6970 796e 6229 2c20  Pruners.ipynb), 
-0000a310: 7768 6963 6820 7368 6f77 7320 686f 7720  which shows how 
-0000a320: 746f 2069 6d70 6c65 6d65 6e74 2061 205b  to implement a [
-0000a330: 736c 696d 6d69 6e67 5d28 6874 7470 733a  slimming](https:
-0000a340: 2f2f 6172 7869 762e 6f72 672f 6162 732f  //arxiv.org/abs/
-0000a350: 3137 3038 2e30 3635 3139 2920 7072 756e  1708.06519) prun
-0000a360: 6572 2066 726f 6d20 7363 7261 7463 682e  er from scratch.
-0000a370: 2041 6464 6974 696f 6e61 6c6c 792c 2079   Additionally, y
-0000a380: 6f75 2063 616e 2066 696e 6420 6d6f 7265  ou can find more
-0000a390: 2070 7261 6374 6963 616c 2065 7861 6d70   practical examp
-0000a3a0: 6c65 7320 696e 205b 6265 6e63 686d 6172  les in [benchmar
-0000a3b0: 6b73 2f6d 6169 6e2e 7079 5d28 6265 6e63  ks/main.py](benc
-0000a3c0: 686d 6172 6b73 2f6d 6169 6e2e 7079 292e  hmarks/main.py).
-0000a3d0: 0a0a 6060 6070 7974 686f 6e0a 696d 706f  ..```python.impo
-0000a3e0: 7274 2074 6f72 6368 0a66 726f 6d20 746f  rt torch.from to
-0000a3f0: 7263 6876 6973 696f 6e2e 6d6f 6465 6c73  rchvision.models
-0000a400: 2069 6d70 6f72 7420 7265 736e 6574 3138   import resnet18
-0000a410: 0a69 6d70 6f72 7420 746f 7263 685f 7072  .import torch_pr
-0000a420: 756e 696e 6720 6173 2074 700a 0a6d 6f64  uning as tp..mod
-0000a430: 656c 203d 2072 6573 6e65 7431 3828 7072  el = resnet18(pr
-0000a440: 6574 7261 696e 6564 3d54 7275 6529 0a0a  etrained=True)..
-0000a450: 2320 496d 706f 7274 616e 6365 2063 7269  # Importance cri
-0000a460: 7465 7269 610a 6578 616d 706c 655f 696e  teria.example_in
-0000a470: 7075 7473 203d 2074 6f72 6368 2e72 616e  puts = torch.ran
-0000a480: 646e 2831 2c20 332c 2032 3234 2c20 3232  dn(1, 3, 224, 22
-0000a490: 3429 0a69 6d70 203d 2074 702e 696d 706f  4).imp = tp.impo
-0000a4a0: 7274 616e 6365 2e54 6179 6c6f 7249 6d70  rtance.TaylorImp
-0000a4b0: 6f72 7461 6e63 6528 290a 0a69 676e 6f72  ortance()..ignor
-0000a4c0: 6564 5f6c 6179 6572 7320 3d20 5b5d 0a66  ed_layers = [].f
-0000a4d0: 6f72 206d 2069 6e20 6d6f 6465 6c2e 6d6f  or m in model.mo
-0000a4e0: 6475 6c65 7328 293a 0a20 2020 2069 6620  dules():.    if 
-0000a4f0: 6973 696e 7374 616e 6365 286d 2c20 746f  isinstance(m, to
-0000a500: 7263 682e 6e6e 2e4c 696e 6561 7229 2061  rch.nn.Linear) a
-0000a510: 6e64 206d 2e6f 7574 5f66 6561 7475 7265  nd m.out_feature
-0000a520: 7320 3d3d 2031 3030 303a 0a20 2020 2020  s == 1000:.     
-0000a530: 2020 2069 676e 6f72 6564 5f6c 6179 6572     ignored_layer
-0000a540: 732e 6170 7065 6e64 286d 2920 2320 444f  s.append(m) # DO
-0000a550: 204e 4f54 2070 7275 6e65 2074 6865 2066   NOT prune the f
-0000a560: 696e 616c 2063 6c61 7373 6966 6965 7221  inal classifier!
-0000a570: 0a0a 6974 6572 6174 6976 655f 7374 6570  ..iterative_step
-0000a580: 7320 3d20 3520 2320 7072 6f67 7265 7373  s = 5 # progress
-0000a590: 6976 6520 7072 756e 696e 670a 7072 756e  ive pruning.prun
-0000a5a0: 6572 203d 2074 702e 7072 756e 6572 2e4d  er = tp.pruner.M
-0000a5b0: 6167 6e69 7475 6465 5072 756e 6572 280a  agnitudePruner(.
-0000a5c0: 2020 2020 6d6f 6465 6c2c 0a20 2020 2065      model,.    e
-0000a5d0: 7861 6d70 6c65 5f69 6e70 7574 732c 0a20  xample_inputs,. 
-0000a5e0: 2020 2069 6d70 6f72 7461 6e63 653d 696d     importance=im
-0000a5f0: 702c 0a20 2020 2069 7465 7261 7469 7665  p,.    iterative
-0000a600: 5f73 7465 7073 3d69 7465 7261 7469 7665  _steps=iterative
-0000a610: 5f73 7465 7073 2c0a 2020 2020 6368 5f73  _steps,.    ch_s
-0000a620: 7061 7273 6974 793d 302e 352c 2023 2072  parsity=0.5, # r
-0000a630: 656d 6f76 6520 3530 2520 6368 616e 6e65  emove 50% channe
-0000a640: 6c73 2c20 5265 734e 6574 3138 203d 207b  ls, ResNet18 = {
-0000a650: 3634 2c20 3132 382c 2032 3536 2c20 3531  64, 128, 256, 51
-0000a660: 327d 203d 3e20 5265 734e 6574 3138 5f48  2} => ResNet18_H
-0000a670: 616c 6620 3d20 7b33 322c 2036 342c 2031  alf = {32, 64, 1
-0000a680: 3238 2c20 3235 367d 0a20 2020 2069 676e  28, 256}.    ign
-0000a690: 6f72 6564 5f6c 6179 6572 733d 6967 6e6f  ored_layers=igno
-0000a6a0: 7265 645f 6c61 7965 7273 2c0a 290a 0a62  red_layers,.)..b
-0000a6b0: 6173 655f 6d61 6373 2c20 6261 7365 5f6e  ase_macs, base_n
-0000a6c0: 7061 7261 6d73 203d 2074 702e 7574 696c  params = tp.util
-0000a6d0: 732e 636f 756e 745f 6f70 735f 616e 645f  s.count_ops_and_
-0000a6e0: 7061 7261 6d73 286d 6f64 656c 2c20 6578  params(model, ex
-0000a6f0: 616d 706c 655f 696e 7075 7473 290a 666f  ample_inputs).fo
-0000a700: 7220 6920 696e 2072 616e 6765 2869 7465  r i in range(ite
-0000a710: 7261 7469 7665 5f73 7465 7073 293a 0a20  rative_steps):. 
-0000a720: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
-0000a730: 2869 6d70 2c20 7470 2e69 6d70 6f72 7461  (imp, tp.importa
-0000a740: 6e63 652e 5461 796c 6f72 496d 706f 7274  nce.TaylorImport
-0000a750: 616e 6365 293a 0a20 2020 2020 2020 2023  ance):.        #
-0000a760: 2054 6179 6c6f 7220 6578 7061 6e73 696f   Taylor expansio
-0000a770: 6e20 7265 7175 6972 6573 2067 7261 6469  n requires gradi
-0000a780: 656e 7473 2066 6f72 2069 6d70 6f72 7461  ents for importa
-0000a790: 6e63 6520 6573 7469 6d61 7469 6f6e 0a20  nce estimation. 
-0000a7a0: 2020 2020 2020 206c 6f73 7320 3d20 6d6f         loss = mo
-0000a7b0: 6465 6c28 6578 616d 706c 655f 696e 7075  del(example_inpu
-0000a7c0: 7473 292e 7375 6d28 2920 2320 6120 6475  ts).sum() # a du
-0000a7d0: 6d6d 7920 6c6f 7373 2066 6f72 2054 6179  mmy loss for Tay
-0000a7e0: 6c6f 7249 6d70 6f72 7461 6e63 650a 2020  lorImportance.  
-0000a7f0: 2020 2020 2020 6c6f 7373 2e62 6163 6b77        loss.backw
-0000a800: 6172 6428 2920 2320 6265 666f 7265 2070  ard() # before p
-0000a810: 7275 6e65 722e 7374 6570 2829 0a20 2020  runer.step().   
-0000a820: 2070 7275 6e65 722e 7374 6570 2829 0a20   pruner.step(). 
-0000a830: 2020 206d 6163 732c 206e 7061 7261 6d73     macs, nparams
-0000a840: 203d 2074 702e 7574 696c 732e 636f 756e   = tp.utils.coun
-0000a850: 745f 6f70 735f 616e 645f 7061 7261 6d73  t_ops_and_params
-0000a860: 286d 6f64 656c 2c20 6578 616d 706c 655f  (model, example_
-0000a870: 696e 7075 7473 290a 2020 2020 2320 6669  inputs).    # fi
-0000a880: 6e65 7475 6e65 2079 6f75 7220 6d6f 6465  netune your mode
-0000a890: 6c20 6865 7265 0a20 2020 2023 2066 696e  l here.    # fin
-0000a8a0: 6574 756e 6528 6d6f 6465 6c29 0a20 2020  etune(model).   
-0000a8b0: 2023 202e 2e2e 0a60 6060 0a0a 2323 2323   # ....```..####
-0000a8c0: 2053 7061 7273 6520 5472 6169 6e69 6e67   Sparse Training
-0000a8d0: 0a53 6f6d 6520 7072 756e 6572 7320 6c69  .Some pruners li
-0000a8e0: 6b65 205b 424e 5363 616c 6550 7275 6e65  ke [BNScalePrune
-0000a8f0: 725d 2868 7474 7073 3a2f 2f67 6974 6875  r](https://githu
-0000a900: 622e 636f 6d2f 5661 696e 462f 546f 7263  b.com/VainF/Torc
-0000a910: 682d 5072 756e 696e 672f 626c 6f62 2f64  h-Pruning/blob/d
-0000a920: 6435 3939 3231 3336 3564 3732 6163 6232  d59921365d72acb2
-0000a930: 3835 3764 3364 3734 6637 3563 3033 6534  857d3d74f75c03e4
-0000a940: 3737 3036 3066 622f 746f 7263 685f 7072  77060fb/torch_pr
-0000a950: 756e 696e 672f 7072 756e 6572 2f61 6c67  uning/pruner/alg
-0000a960: 6f72 6974 686d 732f 6261 7463 686e 6f72  orithms/batchnor
-0000a970: 6d5f 7363 616c 655f 7072 756e 6572 2e70  m_scale_pruner.p
-0000a980: 7923 4c34 3529 2061 6e64 205b 4772 6f75  y#L45) and [Grou
-0000a990: 704e 6f72 6d50 7275 6e65 725d 2868 7474  pNormPruner](htt
-0000a9a0: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-0000a9b0: 5661 696e 462f 546f 7263 682d 5072 756e  VainF/Torch-Prun
-0000a9c0: 696e 672f 626c 6f62 2f64 6435 3939 3231  ing/blob/dd59921
-0000a9d0: 3336 3564 3732 6163 6232 3835 3764 3364  365d72acb2857d3d
-0000a9e0: 3734 6637 3563 3033 6534 3737 3036 3066  74f75c03e477060f
-0000a9f0: 622f 746f 7263 685f 7072 756e 696e 672f  b/torch_pruning/
-0000aa00: 7072 756e 6572 2f61 6c67 6f72 6974 686d  pruner/algorithm
-0000aa10: 732f 6772 6f75 705f 6e6f 726d 5f70 7275  s/group_norm_pru
-0000aa20: 6e65 722e 7079 234c 3533 2920 7265 7175  ner.py#L53) requ
-0000aa30: 6972 6520 7370 6172 7365 2074 7261 696e  ire sparse train
-0000aa40: 696e 6720 6265 666f 7265 2070 7275 6e69  ing before pruni
-0000aa50: 6e67 2e20 5468 6973 2063 616e 2062 6520  ng. This can be 
-0000aa60: 6561 7369 6c79 2061 6368 6965 7665 6420  easily achieved 
-0000aa70: 6279 2069 6e73 6572 7469 6e67 206a 7573  by inserting jus
-0000aa80: 7420 6f6e 6520 6c69 6e65 206f 6620 636f  t one line of co
-0000aa90: 6465 2060 6070 7275 6e65 722e 7265 6775  de ``pruner.regu
-0000aaa0: 6c61 7269 7a65 286d 6f64 656c 2960 6020  larize(model)`` 
-0000aab0: 696e 2079 6f75 7220 7472 6169 6e69 6e67  in your training
-0000aac0: 2073 6372 6970 742e 2054 6865 2070 7275   script. The pru
-0000aad0: 6e65 7220 7769 6c6c 2075 7064 6174 6520  ner will update 
-0000aae0: 7468 6520 6772 6164 6965 6e74 206f 6620  the gradient of 
-0000aaf0: 7472 6169 6e61 626c 6520 7061 7261 6d65  trainable parame
-0000ab00: 7465 7273 2e0a 6060 6070 7974 686f 6e0a  ters..```python.
-0000ab10: 666f 7220 6570 6f63 6820 696e 2072 616e  for epoch in ran
-0000ab20: 6765 2865 706f 6368 7329 3a0a 2020 2020  ge(epochs):.    
-0000ab30: 6d6f 6465 6c2e 7472 6169 6e28 290a 2020  model.train().  
-0000ab40: 2020 666f 7220 692c 2028 6461 7461 2c20    for i, (data, 
-0000ab50: 7461 7267 6574 2920 696e 2065 6e75 6d65  target) in enume
-0000ab60: 7261 7465 2874 7261 696e 5f6c 6f61 6465  rate(train_loade
-0000ab70: 7229 3a0a 2020 2020 2020 2020 6461 7461  r):.        data
-0000ab80: 2c20 7461 7267 6574 203d 2064 6174 612e  , target = data.
-0000ab90: 746f 2864 6576 6963 6529 2c20 7461 7267  to(device), targ
-0000aba0: 6574 2e74 6f28 6465 7669 6365 290a 2020  et.to(device).  
-0000abb0: 2020 2020 2020 6f70 7469 6d69 7a65 722e        optimizer.
-0000abc0: 7a65 726f 5f67 7261 6428 290a 2020 2020  zero_grad().    
-0000abd0: 2020 2020 6f75 7420 3d20 6d6f 6465 6c28      out = model(
-0000abe0: 6461 7461 290a 2020 2020 2020 2020 6c6f  data).        lo
-0000abf0: 7373 203d 2046 2e63 726f 7373 5f65 6e74  ss = F.cross_ent
-0000ac00: 726f 7079 286f 7574 2c20 7461 7267 6574  ropy(out, target
-0000ac10: 290a 2020 2020 2020 2020 6c6f 7373 2e62  ).        loss.b
-0000ac20: 6163 6b77 6172 6428 290a 2020 2020 2020  ackward().      
-0000ac30: 2020 7072 756e 6572 2e72 6567 756c 6172    pruner.regular
-0000ac40: 697a 6528 6d6f 6465 6c29 2023 203c 3d3d  ize(model) # <==
-0000ac50: 2066 6f72 2073 7061 7273 6520 6c65 6172   for sparse lear
-0000ac60: 6e69 6e67 0a20 2020 2020 2020 206f 7074  ning.        opt
-0000ac70: 696d 697a 6572 2e73 7465 7028 290a 6060  imizer.step().``
-0000ac80: 600a 0a23 2323 2320 496e 7465 7261 6374  `..#### Interact
-0000ac90: 6976 6520 5072 756e 696e 6720 2841 6476  ive Pruning (Adv
-0000aca0: 616e 6365 6429 0a41 6c6c 2068 6967 682d  anced).All high-
-0000acb0: 6c65 7665 6c20 7072 756e 6572 7320 7375  level pruners su
-0000acc0: 7070 6f72 7420 696e 7465 7261 6374 6976  pport interactiv
-0000acd0: 6520 7072 756e 696e 672e 2055 7365 2060  e pruning. Use `
-0000ace0: 6070 7275 6e65 722e 7374 6570 2869 6e74  `pruner.step(int
-0000acf0: 6572 6163 7469 7665 3d54 7275 6529 6060  eractive=True)``
-0000ad00: 2074 6f20 6765 7420 616c 6c20 6772 6f75   to get all grou
-0000ad10: 7073 2061 6e64 2069 6e74 6572 6163 7469  ps and interacti
-0000ad20: 7665 6c79 2070 7275 6e65 2074 6865 6d20  vely prune them 
-0000ad30: 6279 2063 616c 6c69 6e67 2060 6067 726f  by calling ``gro
-0000ad40: 7570 2e70 7275 6e65 2829 6060 2e20 5468  up.prune()``. Th
-0000ad50: 6973 2066 6561 7475 7265 2069 7320 7573  is feature is us
-0000ad60: 6566 756c 2069 6620 796f 7520 7761 6e74  eful if you want
-0000ad70: 2074 6f20 636f 6e74 726f 6c2f 6d6f 6e69   to control/moni
-0000ad80: 746f 7220 7468 6520 7072 756e 696e 6720  tor the pruning 
-0000ad90: 7072 6f63 6573 732e 0a0a 6060 6070 7974  process...```pyt
-0000ada0: 686f 6e0a 666f 7220 6920 696e 2072 616e  hon.for i in ran
-0000adb0: 6765 2869 7465 7261 7469 7665 5f73 7465  ge(iterative_ste
-0000adc0: 7073 293a 0a20 2020 2066 6f72 2067 726f  ps):.    for gro
-0000add0: 7570 2069 6e20 7072 756e 6572 2e73 7465  up in pruner.ste
-0000ade0: 7028 696e 7465 7261 6374 6976 653d 5472  p(interactive=Tr
-0000adf0: 7565 293a 2023 2057 6172 6e69 6e67 3a20  ue): # Warning: 
-0000ae00: 6772 6f75 7073 206d 7573 7420 6265 2068  groups must be h
-0000ae10: 616e 646c 6564 2073 6571 7565 6e74 6961  andled sequentia
-0000ae20: 6c6c 792e 2044 6f20 6e6f 7420 6b65 6570  lly. Do not keep
-0000ae30: 2074 6865 6d20 6173 2061 206c 6973 742e   them as a list.
-0000ae40: 0a20 2020 2020 2020 2070 7269 6e74 2867  .        print(g
-0000ae50: 726f 7570 2920 0a20 2020 2020 2020 2023  roup) .        #
-0000ae60: 2064 6f20 7768 6174 6576 6572 2079 6f75   do whatever you
-0000ae70: 206c 696b 6520 7769 7468 2074 6865 2067   like with the g
-0000ae80: 726f 7570 200a 2020 2020 2020 2020 6465  roup .        de
-0000ae90: 702c 2069 6478 7320 3d20 6772 6f75 705b  p, idxs = group[
-0000aea0: 305d 2023 2067 6574 2074 6865 2069 6478  0] # get the idx
-0000aeb0: 730a 2020 2020 2020 2020 7461 7267 6574  s.        target
-0000aec0: 5f6d 6f64 756c 6520 3d20 6465 702e 7461  _module = dep.ta
-0000aed0: 7267 6574 2e6d 6f64 756c 6520 2320 6765  rget.module # ge
-0000aee0: 7420 7468 6520 726f 6f74 206d 6f64 756c  t the root modul
-0000aef0: 650a 2020 2020 2020 2020 7072 756e 696e  e.        prunin
-0000af00: 675f 666e 203d 2064 6570 2e68 616e 646c  g_fn = dep.handl
-0000af10: 6572 2023 2067 6574 2074 6865 2070 7275  er # get the pru
-0000af20: 6e69 6e67 2066 756e 6374 696f 6e0a 2020  ning function.  
-0000af30: 2020 2020 200a 2020 2020 2020 2020 2320       .        # 
-0000af40: 446f 6e27 7420 666f 7267 6574 2074 6f20  Don't forget to 
-0000af50: 7072 756e 6520 7468 6520 6772 6f75 700a  prune the group.
-0000af60: 2020 2020 2020 2020 6772 6f75 702e 7072          group.pr
-0000af70: 756e 6528 290a 2020 2020 2020 2020 2020  une().          
-0000af80: 0a20 2020 2020 2020 2023 2067 726f 7570  .        # group
-0000af90: 2e70 7275 6e65 2869 6478 733d 5b30 2c20  .prune(idxs=[0, 
-0000afa0: 322c 2036 5d29 2023 2049 7420 6973 2065  2, 6]) # It is e
-0000afb0: 7665 6e20 706f 7373 6962 6c65 2074 6f20  ven possible to 
-0000afc0: 6368 616e 6765 2074 6865 2070 7275 6e69  change the pruni
-0000afd0: 6e67 2062 6568 6176 696f 7572 2077 6974  ng behaviour wit
-0000afe0: 6820 7468 6520 6964 7873 2070 6172 616d  h the idxs param
-0000aff0: 6574 6572 0a20 2020 206d 6163 732c 206e  eter.    macs, n
-0000b000: 7061 7261 6d73 203d 2074 702e 7574 696c  params = tp.util
-0000b010: 732e 636f 756e 745f 6f70 735f 616e 645f  s.count_ops_and_
-0000b020: 7061 7261 6d73 286d 6f64 656c 2c20 6578  params(model, ex
-0000b030: 616d 706c 655f 696e 7075 7473 290a 2020  ample_inputs).  
-0000b040: 2020 2320 6669 6e65 7475 6e65 2079 6f75    # finetune you
-0000b050: 7220 6d6f 6465 6c20 6865 7265 0a20 2020  r model here.   
-0000b060: 2023 2066 696e 6574 756e 6528 6d6f 6465   # finetune(mode
-0000b070: 6c29 0a20 2020 2023 202e 2e2e 0a60 6060  l).    # ....```
-0000b080: 0a0a 2323 2323 2047 726f 7570 2d6c 6576  ..#### Group-lev
-0000b090: 656c 2050 7275 6e69 6e67 0a0a 5769 7468  el Pruning..With
-0000b0a0: 2044 6570 4772 6170 682c 2069 7420 6973   DepGraph, it is
-0000b0b0: 2065 6173 7920 746f 2064 6573 6967 6e20   easy to design 
-0000b0c0: 736f 6d65 2022 6772 6f75 702d 6c65 7665  some "group-leve
-0000b0d0: 6c22 2063 7269 7465 7269 6120 746f 2065  l" criteria to e
-0000b0e0: 7374 696d 6174 6520 7468 6520 696d 706f  stimate the impo
-0000b0f0: 7274 616e 6365 206f 6620 6120 7768 6f6c  rtance of a whol
-0000b100: 6520 6772 6f75 7020 7261 7468 6572 2074  e group rather t
-0000b110: 6861 6e20 6120 7369 6e67 6c65 206c 6179  han a single lay
-0000b120: 6572 2e20 496e 2054 6f72 6368 2d70 7275  er. In Torch-pru
-0000b130: 6e69 6e67 2c20 616c 6c20 7072 756e 6572  ning, all pruner
-0000b140: 7320 776f 726b 2069 6e20 7468 6520 6772  s work in the gr
-0000b150: 6f75 7020 6c65 7665 6c2e 0a0a 3c64 6976  oup level...<div
-0000b160: 2061 6c69 676e 3d22 6365 6e74 6572 223e   align="center">
-0000b170: 0a3c 696d 6720 7372 633d 2261 7373 6574  .<img src="asset
-0000b180: 732f 6772 6f75 705f 7370 6172 7369 7479  s/group_sparsity
-0000b190: 2e70 6e67 2220 7769 6474 683d 2238 3025  .png" width="80%
-0000b1a0: 223e 0a3c 2f64 6976 3e0a 0a23 2323 2033  ">.</div>..### 3
-0000b1b0: 2e20 5361 7665 2026 204c 6f61 640a 2020  . Save & Load.  
-0000b1c0: 2020 2020 2020 2020 0a54 6865 2066 6f6c          .The fol
-0000b1d0: 6c6f 7769 6e67 2073 6372 6970 7420 7361  lowing script sa
-0000b1e0: 7665 7320 7468 6520 7768 6f6c 6520 6d6f  ves the whole mo
-0000b1f0: 6465 6c20 6f62 6a65 6374 2028 7374 7275  del object (stru
-0000b200: 6374 7572 652b 7765 6967 6874 7329 2061  cture+weights) a
-0000b210: 7320 6120 276d 6f64 656c 2e70 7468 272e  s a 'model.pth'.
-0000b220: 200a 6060 6070 7974 686f 6e0a 6d6f 6465   .```python.mode
-0000b230: 6c2e 7a65 726f 5f67 7261 6428 2920 2320  l.zero_grad() # 
-0000b240: 5765 2064 6f6e 2774 2077 616e 7420 746f  We don't want to
-0000b250: 2073 746f 7265 2067 7261 6469 656e 7420   store gradient 
-0000b260: 696e 666f 726d 6174 696f 6e0a 746f 7263  information.torc
-0000b270: 682e 7361 7665 286d 6f64 656c 2c20 276d  h.save(model, 'm
-0000b280: 6f64 656c 2e70 7468 2729 2023 2077 6974  odel.pth') # wit
-0000b290: 686f 7574 202e 7374 6174 655f 6469 6374  hout .state_dict
-0000b2a0: 0a6d 6f64 656c 203d 2074 6f72 6368 2e6c  .model = torch.l
-0000b2b0: 6f61 6428 276d 6f64 656c 2e70 7468 2729  oad('model.pth')
-0000b2c0: 2023 206c 6f61 6420 7468 6520 7072 756e   # load the prun
-0000b2d0: 6564 206d 6f64 656c 0a60 6060 0a0a 2a2a  ed model.```..**
-0000b2e0: 4578 7065 7269 6d65 6e74 616c 2046 6561  Experimental Fea
-0000b2f0: 7475 7265 732a 2a3a 2052 652d 6372 6561  tures**: Re-crea
-0000b300: 7465 2070 7275 6e65 6420 6d6f 6465 6c73  te pruned models
-0000b310: 2066 726f 6d20 756e 7072 756e 6564 206f   from unpruned o
-0000b320: 6e65 7320 7573 696e 6720 6060 7470 2e73  nes using ``tp.s
-0000b330: 7461 7465 5f64 6963 7460 6020 616e 6420  tate_dict`` and 
-0000b340: 6060 7470 2e6c 6f61 645f 7374 6174 655f  ``tp.load_state_
-0000b350: 6469 6374 6060 2e0a 6060 6070 7974 686f  dict``..```pytho
-0000b360: 6e0a 2320 7361 7665 2074 6865 2070 7275  n.# save the pru
-0000b370: 6e65 6420 7374 6174 655f 6469 6374 2c20  ned state_dict, 
-0000b380: 7768 6963 6820 696e 636c 7564 6573 2062  which includes b
-0000b390: 6f74 6820 7072 756e 6564 2070 6172 616d  oth pruned param
-0000b3a0: 6574 6572 7320 616e 6420 6d6f 6469 6669  eters and modifi
-0000b3b0: 6564 2061 7474 7269 6275 7465 730a 7374  ed attributes.st
-0000b3c0: 6174 655f 6469 6374 203d 2074 702e 7374  ate_dict = tp.st
-0000b3d0: 6174 655f 6469 6374 2870 7275 6e65 645f  ate_dict(pruned_
-0000b3e0: 6d6f 6465 6c29 2023 2074 6865 2070 7275  model) # the pru
-0000b3f0: 6e65 6420 6d6f 6465 6c2c 2065 2e67 2e2c  ned model, e.g.,
-0000b400: 2061 2072 6573 6e65 742d 3138 2d68 616c   a resnet-18-hal
-0000b410: 660a 746f 7263 682e 7361 7665 2873 7461  f.torch.save(sta
-0000b420: 7465 5f64 6963 742c 2027 7072 756e 6564  te_dict, 'pruned
-0000b430: 2e70 7468 2729 0a0a 2320 6372 6561 7465  .pth')..# create
-0000b440: 2061 206e 6577 206d 6f64 656c 2c20 652e   a new model, e.
-0000b450: 672e 2072 6573 6e65 7431 380a 6e65 775f  g. resnet18.new_
-0000b460: 6d6f 6465 6c20 3d20 7265 736e 6574 3138  model = resnet18
-0000b470: 2829 2e65 7661 6c28 290a 0a23 206c 6f61  ().eval()..# loa
-0000b480: 6420 7468 6520 7072 756e 6564 2073 7461  d the pruned sta
-0000b490: 7465 5f64 6963 7420 696e 746f 2074 6865  te_dict into the
-0000b4a0: 2075 6e70 7275 6e65 6420 6d6f 6465 6c2e   unpruned model.
-0000b4b0: 0a6c 6f61 6465 645f 7374 6174 655f 6469  .loaded_state_di
-0000b4c0: 6374 203d 2074 6f72 6368 2e6c 6f61 6428  ct = torch.load(
-0000b4d0: 2770 7275 6e65 642e 7074 6827 2c20 6d61  'pruned.pth', ma
-0000b4e0: 705f 6c6f 6361 7469 6f6e 3d27 6370 7527  p_location='cpu'
-0000b4f0: 290a 7470 2e6c 6f61 645f 7374 6174 655f  ).tp.load_state_
-0000b500: 6469 6374 286e 6577 5f6d 6f64 656c 2c20  dict(new_model, 
-0000b510: 7374 6174 655f 6469 6374 3d6c 6f61 6465  state_dict=loade
-0000b520: 645f 7374 6174 655f 6469 6374 290a 7072  d_state_dict).pr
-0000b530: 696e 7428 6e65 775f 6d6f 6465 6c29 2023  int(new_model) #
-0000b540: 2054 6869 7320 7769 6c6c 2062 6520 6120   This will be a 
-0000b550: 7072 756e 6564 206d 6f64 656c 2e0a 6060  pruned model..``
-0000b560: 600a 5265 6665 7220 746f 205b 7465 7374  `.Refer to [test
-0000b570: 732f 7465 7374 5f73 6572 6961 6c69 7a61  s/test_serializa
-0000b580: 7469 6f6e 2e70 795d 2874 6573 7473 2f74  tion.py](tests/t
-0000b590: 6573 745f 7365 7269 616c 697a 6174 696f  est_serializatio
-0000b5a0: 6e2e 7079 2920 666f 7220 616e 2056 6954  n.py) for an ViT
-0000b5b0: 2065 7861 6d70 6c65 2e20 496e 2074 6869   example. In thi
-0000b5c0: 7320 6578 616d 706c 652c 2077 6520 7769  s example, we wi
-0000b5d0: 6c6c 2070 7275 6e65 2074 6865 206d 6f64  ll prune the mod
-0000b5e0: 656c 2061 6e64 206d 6f64 6966 7920 736f  el and modify so
-0000b5f0: 6d65 2061 7474 7269 6275 7465 7320 6c69  me attributes li
-0000b600: 6b65 2060 606d 6f64 656c 2e68 6964 6465  ke ``model.hidde
-0000b610: 6e5f 6469 6d73 6060 2e0a 2020 2020 2020  n_dims``..      
-0000b620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000b630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000b640: 2020 0a20 2020 2020 2020 2020 2020 2020    .             
+00009180: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
+00009190: 2020 2020 2050 7275 6e69 6e67 2047 726f       Pruning Gro
+000091a0: 7570 0a2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  up.-------------
+000091b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000091c0: 2d2d 2d0a 5b30 5d20 7072 756e 655f 6f75  ---.[0] prune_ou
+000091d0: 745f 6368 616e 6e65 6c73 206f 6e20 636f  t_channels on co
+000091e0: 6e76 3120 2843 6f6e 7632 6428 332c 2036  nv1 (Conv2d(3, 6
+000091f0: 342c 206b 6572 6e65 6c5f 7369 7a65 3d28  4, kernel_size=(
+00009200: 372c 2037 292c 2073 7472 6964 653d 2832  7, 7), stride=(2
+00009210: 2c20 3229 2c20 7061 6464 696e 673d 2833  , 2), padding=(3
+00009220: 2c20 3329 2c20 6269 6173 3d46 616c 7365  , 3), bias=False
+00009230: 2929 203d 3e20 7072 756e 655f 6f75 745f  )) => prune_out_
+00009240: 6368 616e 6e65 6c73 206f 6e20 636f 6e76  channels on conv
+00009250: 3120 2843 6f6e 7632 6428 332c 2036 342c  1 (Conv2d(3, 64,
+00009260: 206b 6572 6e65 6c5f 7369 7a65 3d28 372c   kernel_size=(7,
+00009270: 2037 292c 2073 7472 6964 653d 2832 2c20   7), stride=(2, 
+00009280: 3229 2c20 7061 6464 696e 673d 2833 2c20  2), padding=(3, 
+00009290: 3329 2c20 6269 6173 3d46 616c 7365 2929  3), bias=False))
+000092a0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
+000092b0: 2028 5072 756e 696e 6720 526f 6f74 290a   (Pruning Root).
+000092c0: 5b31 5d20 7072 756e 655f 6f75 745f 6368  [1] prune_out_ch
+000092d0: 616e 6e65 6c73 206f 6e20 636f 6e76 3120  annels on conv1 
+000092e0: 2843 6f6e 7632 6428 332c 2036 342c 206b  (Conv2d(3, 64, k
+000092f0: 6572 6e65 6c5f 7369 7a65 3d28 372c 2037  ernel_size=(7, 7
+00009300: 292c 2073 7472 6964 653d 2832 2c20 3229  ), stride=(2, 2)
+00009310: 2c20 7061 6464 696e 673d 2833 2c20 3329  , padding=(3, 3)
+00009320: 2c20 6269 6173 3d46 616c 7365 2929 203d  , bias=False)) =
+00009330: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
+00009340: 6e65 6c73 206f 6e20 626e 3120 2842 6174  nels on bn1 (Bat
+00009350: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
+00009360: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
+00009370: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
+00009380: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
+00009390: 5f73 7461 7473 3d54 7275 6529 292c 2069  _stats=True)), i
+000093a0: 6478 733d 5b32 2c20 362c 2039 5d0a 5b32  dxs=[2, 6, 9].[2
+000093b0: 5d20 7072 756e 655f 6f75 745f 6368 616e  ] prune_out_chan
+000093c0: 6e65 6c73 206f 6e20 626e 3120 2842 6174  nels on bn1 (Bat
+000093d0: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
+000093e0: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
+000093f0: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
+00009400: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
+00009410: 5f73 7461 7473 3d54 7275 6529 2920 3d3e  _stats=True)) =>
+00009420: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+00009430: 656c 7320 6f6e 205f 456c 656d 656e 7457  els on _ElementW
+00009440: 6973 654f 705f 3230 2852 656c 7542 6163  iseOp_20(ReluBac
+00009450: 6b77 6172 6430 292c 2069 6478 733d 5b32  kward0), idxs=[2
+00009460: 2c20 362c 2039 5d0a 5b33 5d20 7072 756e  , 6, 9].[3] prun
+00009470: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
+00009480: 6e20 5f45 6c65 6d65 6e74 5769 7365 4f70  n _ElementWiseOp
+00009490: 5f32 3028 5265 6c75 4261 636b 7761 7264  _20(ReluBackward
+000094a0: 3029 203d 3e20 7072 756e 655f 6f75 745f  0) => prune_out_
+000094b0: 6368 616e 6e65 6c73 206f 6e20 5f45 6c65  channels on _Ele
+000094c0: 6d65 6e74 5769 7365 4f70 5f31 3928 4d61  mentWiseOp_19(Ma
+000094d0: 7850 6f6f 6c32 4457 6974 6849 6e64 6963  xPool2DWithIndic
+000094e0: 6573 4261 636b 7761 7264 3029 2c20 6964  esBackward0), id
+000094f0: 7873 3d5b 322c 2036 2c20 395d 0a5b 345d  xs=[2, 6, 9].[4]
+00009500: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+00009510: 656c 7320 6f6e 205f 456c 656d 656e 7457  els on _ElementW
+00009520: 6973 654f 705f 3139 284d 6178 506f 6f6c  iseOp_19(MaxPool
+00009530: 3244 5769 7468 496e 6469 6365 7342 6163  2DWithIndicesBac
+00009540: 6b77 6172 6430 2920 3d3e 2070 7275 6e65  kward0) => prune
+00009550: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
+00009560: 205f 456c 656d 656e 7457 6973 654f 705f   _ElementWiseOp_
+00009570: 3138 2841 6464 4261 636b 7761 7264 3029  18(AddBackward0)
+00009580: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
+00009590: 0a5b 355d 2070 7275 6e65 5f6f 7574 5f63  .[5] prune_out_c
+000095a0: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
+000095b0: 656e 7457 6973 654f 705f 3139 284d 6178  entWiseOp_19(Max
+000095c0: 506f 6f6c 3244 5769 7468 496e 6469 6365  Pool2DWithIndice
+000095d0: 7342 6163 6b77 6172 6430 2920 3d3e 2070  sBackward0) => p
+000095e0: 7275 6e65 5f69 6e5f 6368 616e 6e65 6c73  rune_in_channels
+000095f0: 206f 6e20 6c61 7965 7231 2e30 2e63 6f6e   on layer1.0.con
+00009600: 7631 2028 436f 6e76 3264 2836 342c 2036  v1 (Conv2d(64, 6
+00009610: 342c 206b 6572 6e65 6c5f 7369 7a65 3d28  4, kernel_size=(
+00009620: 332c 2033 292c 2073 7472 6964 653d 2831  3, 3), stride=(1
+00009630: 2c20 3129 2c20 7061 6464 696e 673d 2831  , 1), padding=(1
+00009640: 2c20 3129 2c20 6269 6173 3d46 616c 7365  , 1), bias=False
+00009650: 2929 2c20 6964 7873 3d5b 322c 2036 2c20  )), idxs=[2, 6, 
+00009660: 395d 0a5b 365d 2070 7275 6e65 5f6f 7574  9].[6] prune_out
+00009670: 5f63 6861 6e6e 656c 7320 6f6e 205f 456c  _channels on _El
+00009680: 656d 656e 7457 6973 654f 705f 3138 2841  ementWiseOp_18(A
+00009690: 6464 4261 636b 7761 7264 3029 203d 3e20  ddBackward0) => 
+000096a0: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
+000096b0: 6c73 206f 6e20 6c61 7965 7231 2e30 2e62  ls on layer1.0.b
+000096c0: 6e32 2028 4261 7463 684e 6f72 6d32 6428  n2 (BatchNorm2d(
+000096d0: 3634 2c20 6570 733d 3165 2d30 352c 206d  64, eps=1e-05, m
+000096e0: 6f6d 656e 7475 6d3d 302e 312c 2061 6666  omentum=0.1, aff
+000096f0: 696e 653d 5472 7565 2c20 7472 6163 6b5f  ine=True, track_
+00009700: 7275 6e6e 696e 675f 7374 6174 733d 5472  running_stats=Tr
+00009710: 7565 2929 2c20 6964 7873 3d5b 322c 2036  ue)), idxs=[2, 6
+00009720: 2c20 395d 0a5b 375d 2070 7275 6e65 5f6f  , 9].[7] prune_o
+00009730: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
+00009740: 456c 656d 656e 7457 6973 654f 705f 3138  ElementWiseOp_18
+00009750: 2841 6464 4261 636b 7761 7264 3029 203d  (AddBackward0) =
+00009760: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
+00009770: 6e65 6c73 206f 6e20 5f45 6c65 6d65 6e74  nels on _Element
+00009780: 5769 7365 4f70 5f31 3728 5265 6c75 4261  WiseOp_17(ReluBa
+00009790: 636b 7761 7264 3029 2c20 6964 7873 3d5b  ckward0), idxs=[
+000097a0: 322c 2036 2c20 395d 0a5b 385d 2070 7275  2, 6, 9].[8] pru
+000097b0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+000097c0: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+000097d0: 705f 3137 2852 656c 7542 6163 6b77 6172  p_17(ReluBackwar
+000097e0: 6430 2920 3d3e 2070 7275 6e65 5f6f 7574  d0) => prune_out
+000097f0: 5f63 6861 6e6e 656c 7320 6f6e 205f 456c  _channels on _El
+00009800: 656d 656e 7457 6973 654f 705f 3136 2841  ementWiseOp_16(A
+00009810: 6464 4261 636b 7761 7264 3029 2c20 6964  ddBackward0), id
+00009820: 7873 3d5b 322c 2036 2c20 395d 0a5b 395d  xs=[2, 6, 9].[9]
+00009830: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+00009840: 656c 7320 6f6e 205f 456c 656d 656e 7457  els on _ElementW
+00009850: 6973 654f 705f 3137 2852 656c 7542 6163  iseOp_17(ReluBac
+00009860: 6b77 6172 6430 2920 3d3e 2070 7275 6e65  kward0) => prune
+00009870: 5f69 6e5f 6368 616e 6e65 6c73 206f 6e20  _in_channels on 
+00009880: 6c61 7965 7231 2e31 2e63 6f6e 7631 2028  layer1.1.conv1 (
+00009890: 436f 6e76 3264 2836 342c 2036 342c 206b  Conv2d(64, 64, k
+000098a0: 6572 6e65 6c5f 7369 7a65 3d28 332c 2033  ernel_size=(3, 3
+000098b0: 292c 2073 7472 6964 653d 2831 2c20 3129  ), stride=(1, 1)
+000098c0: 2c20 7061 6464 696e 673d 2831 2c20 3129  , padding=(1, 1)
+000098d0: 2c20 6269 6173 3d46 616c 7365 2929 2c20  , bias=False)), 
+000098e0: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
+000098f0: 3130 5d20 7072 756e 655f 6f75 745f 6368  10] prune_out_ch
+00009900: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
+00009910: 6e74 5769 7365 4f70 5f31 3628 4164 6442  ntWiseOp_16(AddB
+00009920: 6163 6b77 6172 6430 2920 3d3e 2070 7275  ackward0) => pru
+00009930: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+00009940: 6f6e 206c 6179 6572 312e 312e 626e 3220  on layer1.1.bn2 
+00009950: 2842 6174 6368 4e6f 726d 3264 2836 342c  (BatchNorm2d(64,
+00009960: 2065 7073 3d31 652d 3035 2c20 6d6f 6d65   eps=1e-05, mome
+00009970: 6e74 756d 3d30 2e31 2c20 6166 6669 6e65  ntum=0.1, affine
+00009980: 3d54 7275 652c 2074 7261 636b 5f72 756e  =True, track_run
+00009990: 6e69 6e67 5f73 7461 7473 3d54 7275 6529  ning_stats=True)
+000099a0: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
+000099b0: 5d0a 5b31 315d 2070 7275 6e65 5f6f 7574  ].[11] prune_out
+000099c0: 5f63 6861 6e6e 656c 7320 6f6e 205f 456c  _channels on _El
+000099d0: 656d 656e 7457 6973 654f 705f 3136 2841  ementWiseOp_16(A
+000099e0: 6464 4261 636b 7761 7264 3029 203d 3e20  ddBackward0) => 
+000099f0: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
+00009a00: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
+00009a10: 7365 4f70 5f31 3528 5265 6c75 4261 636b  seOp_15(ReluBack
+00009a20: 7761 7264 3029 2c20 6964 7873 3d5b 322c  ward0), idxs=[2,
+00009a30: 2036 2c20 395d 0a5b 3132 5d20 7072 756e   6, 9].[12] prun
+00009a40: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
+00009a50: 6e20 5f45 6c65 6d65 6e74 5769 7365 4f70  n _ElementWiseOp
+00009a60: 5f31 3528 5265 6c75 4261 636b 7761 7264  _15(ReluBackward
+00009a70: 3029 203d 3e20 7072 756e 655f 696e 5f63  0) => prune_in_c
+00009a80: 6861 6e6e 656c 7320 6f6e 206c 6179 6572  hannels on layer
+00009a90: 322e 302e 646f 776e 7361 6d70 6c65 2e30  2.0.downsample.0
+00009aa0: 2028 436f 6e76 3264 2836 342c 2031 3238   (Conv2d(64, 128
+00009ab0: 2c20 6b65 726e 656c 5f73 697a 653d 2831  , kernel_size=(1
+00009ac0: 2c20 3129 2c20 7374 7269 6465 3d28 322c  , 1), stride=(2,
+00009ad0: 2032 292c 2062 6961 733d 4661 6c73 6529   2), bias=False)
+00009ae0: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
+00009af0: 5d0a 5b31 335d 2070 7275 6e65 5f6f 7574  ].[13] prune_out
+00009b00: 5f63 6861 6e6e 656c 7320 6f6e 205f 456c  _channels on _El
+00009b10: 656d 656e 7457 6973 654f 705f 3135 2852  ementWiseOp_15(R
+00009b20: 656c 7542 6163 6b77 6172 6430 2920 3d3e  eluBackward0) =>
+00009b30: 2070 7275 6e65 5f69 6e5f 6368 616e 6e65   prune_in_channe
+00009b40: 6c73 206f 6e20 6c61 7965 7232 2e30 2e63  ls on layer2.0.c
+00009b50: 6f6e 7631 2028 436f 6e76 3264 2836 342c  onv1 (Conv2d(64,
+00009b60: 2031 3238 2c20 6b65 726e 656c 5f73 697a   128, kernel_siz
+00009b70: 653d 2833 2c20 3329 2c20 7374 7269 6465  e=(3, 3), stride
+00009b80: 3d28 322c 2032 292c 2070 6164 6469 6e67  =(2, 2), padding
+00009b90: 3d28 312c 2031 292c 2062 6961 733d 4661  =(1, 1), bias=Fa
+00009ba0: 6c73 6529 292c 2069 6478 733d 5b32 2c20  lse)), idxs=[2, 
+00009bb0: 362c 2039 5d0a 5b31 345d 2070 7275 6e65  6, 9].[14] prune
+00009bc0: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
+00009bd0: 206c 6179 6572 312e 312e 626e 3220 2842   layer1.1.bn2 (B
+00009be0: 6174 6368 4e6f 726d 3264 2836 342c 2065  atchNorm2d(64, e
+00009bf0: 7073 3d31 652d 3035 2c20 6d6f 6d65 6e74  ps=1e-05, moment
+00009c00: 756d 3d30 2e31 2c20 6166 6669 6e65 3d54  um=0.1, affine=T
+00009c10: 7275 652c 2074 7261 636b 5f72 756e 6e69  rue, track_runni
+00009c20: 6e67 5f73 7461 7473 3d54 7275 6529 2920  ng_stats=True)) 
+00009c30: 3d3e 2070 7275 6e65 5f6f 7574 5f63 6861  => prune_out_cha
+00009c40: 6e6e 656c 7320 6f6e 206c 6179 6572 312e  nnels on layer1.
+00009c50: 312e 636f 6e76 3220 2843 6f6e 7632 6428  1.conv2 (Conv2d(
+00009c60: 3634 2c20 3634 2c20 6b65 726e 656c 5f73  64, 64, kernel_s
+00009c70: 697a 653d 2833 2c20 3329 2c20 7374 7269  ize=(3, 3), stri
+00009c80: 6465 3d28 312c 2031 292c 2070 6164 6469  de=(1, 1), paddi
+00009c90: 6e67 3d28 312c 2031 292c 2062 6961 733d  ng=(1, 1), bias=
+00009ca0: 4661 6c73 6529 292c 2069 6478 733d 5b32  False)), idxs=[2
+00009cb0: 2c20 362c 2039 5d0a 5b31 355d 2070 7275  , 6, 9].[15] pru
+00009cc0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+00009cd0: 6f6e 206c 6179 6572 312e 302e 626e 3220  on layer1.0.bn2 
+00009ce0: 2842 6174 6368 4e6f 726d 3264 2836 342c  (BatchNorm2d(64,
+00009cf0: 2065 7073 3d31 652d 3035 2c20 6d6f 6d65   eps=1e-05, mome
+00009d00: 6e74 756d 3d30 2e31 2c20 6166 6669 6e65  ntum=0.1, affine
+00009d10: 3d54 7275 652c 2074 7261 636b 5f72 756e  =True, track_run
+00009d20: 6e69 6e67 5f73 7461 7473 3d54 7275 6529  ning_stats=True)
+00009d30: 2920 3d3e 2070 7275 6e65 5f6f 7574 5f63  ) => prune_out_c
+00009d40: 6861 6e6e 656c 7320 6f6e 206c 6179 6572  hannels on layer
+00009d50: 312e 302e 636f 6e76 3220 2843 6f6e 7632  1.0.conv2 (Conv2
+00009d60: 6428 3634 2c20 3634 2c20 6b65 726e 656c  d(64, 64, kernel
+00009d70: 5f73 697a 653d 2833 2c20 3329 2c20 7374  _size=(3, 3), st
+00009d80: 7269 6465 3d28 312c 2031 292c 2070 6164  ride=(1, 1), pad
+00009d90: 6469 6e67 3d28 312c 2031 292c 2062 6961  ding=(1, 1), bia
+00009da0: 733d 4661 6c73 6529 292c 2069 6478 733d  s=False)), idxs=
+00009db0: 5b32 2c20 362c 2039 5d0a 2d2d 2d2d 2d2d  [2, 6, 9].------
+00009dc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00009dd0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a60 6060 0a46  ----------.```.F
+00009de0: 6f72 206d 6f72 6520 6465 7461 696c 7320  or more details 
+00009df0: 6162 6f75 7420 6772 6f75 7069 6e67 2c20  about grouping, 
+00009e00: 706c 6561 7365 2072 6566 6572 2074 6f20  please refer to 
+00009e10: 5b74 7574 6f72 6961 6c73 2f32 202d 2045  [tutorials/2 - E
+00009e20: 7870 6c6f 7269 6e67 2044 6570 656e 6465  xploring Depende
+00009e30: 6e63 7920 4772 6f75 7073 5d28 6874 7470  ncy Groups](http
+00009e40: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
+00009e50: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
+00009e60: 6e67 2f62 6c6f 622f 6d61 7374 6572 2f74  ng/blob/master/t
+00009e70: 7574 6f72 6961 6c73 2f32 2532 302d 2532  utorials/2%20-%2
+00009e80: 3045 7870 6c6f 7269 6e67 2532 3044 6570  0Exploring%20Dep
+00009e90: 656e 6465 6e63 7925 3230 4772 6f75 7073  endency%20Groups
+00009ea0: 2e69 7079 6e62 290a 2020 0a23 2323 2320  .ipynb).  .#### 
+00009eb0: 486f 7720 746f 2073 6361 6e20 616c 6c20  How to scan all 
+00009ec0: 6772 6f75 7073 2028 4164 7661 6e63 6564  groups (Advanced
+00009ed0: 293a 0a57 6520 6361 6e20 7573 6520 6060  ):.We can use ``
+00009ee0: 4447 2e67 6574 5f61 6c6c 5f67 726f 7570  DG.get_all_group
+00009ef0: 7328 6967 6e6f 7265 645f 6c61 7965 7273  s(ignored_layers
+00009f00: 2c20 726f 6f74 5f6d 6f64 756c 655f 7479  , root_module_ty
+00009f10: 7065 7329 6060 2074 6f20 7363 616e 2061  pes)`` to scan a
+00009f20: 6c6c 2067 726f 7570 7320 7365 7175 656e  ll groups sequen
+00009f30: 7469 616c 6c79 2e20 4561 6368 2067 726f  tially. Each gro
+00009f40: 7570 2077 696c 6c20 6265 6769 6e20 7769  up will begin wi
+00009f50: 7468 2061 206c 6179 6572 2074 6861 7420  th a layer that 
+00009f60: 6d61 7463 6865 7320 6120 7479 7065 2069  matches a type i
+00009f70: 6e20 7468 6520 2272 6f6f 745f 6d6f 6475  n the "root_modu
+00009f80: 6c65 5f74 7970 6573 2220 7061 7261 6d65  le_types" parame
+00009f90: 7465 722e 204e 6f74 6520 7468 6174 2044  ter. Note that D
+00009fa0: 472e 6765 745f 616c 6c5f 6772 6f75 7073  G.get_all_groups
+00009fb0: 2069 7320 6f6e 6c79 2072 6573 706f 6e73   is only respons
+00009fc0: 6962 6c65 2066 6f72 2067 726f 7570 696e  ible for groupin
+00009fd0: 6720 616e 6420 646f 6573 206e 6f74 2068  g and does not h
+00009fe0: 6176 6520 616e 7920 6b6e 6f77 6c65 6467  ave any knowledg
+00009ff0: 6520 6f72 2075 6e64 6572 7374 616e 6469  e or understandi
+0000a000: 6e67 206f 6620 7768 6963 6820 7061 7261  ng of which para
+0000a010: 6d65 7465 7273 2073 686f 756c 6420 6265  meters should be
+0000a020: 2070 7275 6e65 642e 2054 6865 7265 666f   pruned. Therefo
+0000a030: 7265 2c20 6974 2069 7320 6e65 6365 7373  re, it is necess
+0000a040: 6172 7920 746f 2073 7065 6369 6679 2074  ary to specify t
+0000a050: 6865 2070 7275 6e69 6e67 2069 6478 7320  he pruning idxs 
+0000a060: 7573 696e 6720 2060 6067 726f 7570 2e70  using  ``group.p
+0000a070: 7275 6e65 2869 6478 733d 6964 7873 2960  rune(idxs=idxs)`
+0000a080: 602e 0a0a 6060 6070 7974 686f 6e0a 666f  `...```python.fo
+0000a090: 7220 6772 6f75 7020 696e 2044 472e 6765  r group in DG.ge
+0000a0a0: 745f 616c 6c5f 6772 6f75 7073 2869 676e  t_all_groups(ign
+0000a0b0: 6f72 6564 5f6c 6179 6572 733d 5b6d 6f64  ored_layers=[mod
+0000a0c0: 656c 2e63 6f6e 7631 5d2c 2072 6f6f 745f  el.conv1], root_
+0000a0d0: 6d6f 6475 6c65 5f74 7970 6573 3d5b 6e6e  module_types=[nn
+0000a0e0: 2e43 6f6e 7632 642c 206e 6e2e 4c69 6e65  .Conv2d, nn.Line
+0000a0f0: 6172 5d29 3a0a 2020 2020 2320 6861 6e64  ar]):.    # hand
+0000a100: 6c65 2067 726f 7570 7320 696e 2073 6571  le groups in seq
+0000a110: 7565 6e74 6961 6c20 6f72 6465 720a 2020  uential order.  
+0000a120: 2020 6964 7873 203d 205b 322c 342c 365d    idxs = [2,4,6]
+0000a130: 2023 2079 6f75 7220 7072 756e 696e 6720   # your pruning 
+0000a140: 696e 6469 6365 730a 2020 2020 6772 6f75  indices.    grou
+0000a150: 702e 7072 756e 6528 6964 7873 3d69 6478  p.prune(idxs=idx
+0000a160: 7329 0a20 2020 2070 7269 6e74 2867 726f  s).    print(gro
+0000a170: 7570 290a 6060 600a 0a23 2323 2032 2e20  up).```..### 2. 
+0000a180: 4869 6768 2d6c 6576 656c 2050 7275 6e65  High-level Prune
+0000a190: 7273 0a0a 4c65 7665 7261 6769 6e67 2074  rs..Leveraging t
+0000a1a0: 6865 2044 6570 656e 6465 6e63 7947 7261  he DependencyGra
+0000a1b0: 7068 2c20 7765 2064 6576 656c 6f70 6564  ph, we developed
+0000a1c0: 2073 6576 6572 616c 2068 6967 682d 6c65   several high-le
+0000a1d0: 7665 6c20 7072 756e 6572 7320 696e 2074  vel pruners in t
+0000a1e0: 6869 7320 7265 706f 7369 746f 7279 2074  his repository t
+0000a1f0: 6f20 6661 6369 6c69 7461 7465 2065 6666  o facilitate eff
+0000a200: 6f72 746c 6573 7320 7072 756e 696e 672e  ortless pruning.
+0000a210: 2042 7920 7370 6563 6966 7969 6e67 2074   By specifying t
+0000a220: 6865 2064 6573 6972 6564 2063 6861 6e6e  he desired chann
+0000a230: 656c 2073 7061 7273 6974 792c 2079 6f75  el sparsity, you
+0000a240: 2063 616e 2070 7275 6e65 2074 6865 2065   can prune the e
+0000a250: 6e74 6972 6520 6d6f 6465 6c20 616e 6420  ntire model and 
+0000a260: 6669 6e65 2d74 756e 6520 6974 2075 7369  fine-tune it usi
+0000a270: 6e67 2079 6f75 7220 6f77 6e20 7472 6169  ng your own trai
+0000a280: 6e69 6e67 2063 6f64 652e 2046 6f72 2064  ning code. For d
+0000a290: 6574 6169 6c65 6420 696e 666f 726d 6174  etailed informat
+0000a2a0: 696f 6e20 6f6e 2074 6869 7320 7072 6f63  ion on this proc
+0000a2b0: 6573 732c 2070 6c65 6173 6520 7265 6665  ess, please refe
+0000a2c0: 7220 746f 205b 7468 6973 2074 7574 6f72  r to [this tutor
+0000a2d0: 6961 6c5d 2868 7474 7073 3a2f 2f67 6974  ial](https://git
+0000a2e0: 6875 622e 636f 6d2f 5661 696e 462f 546f  hub.com/VainF/To
+0000a2f0: 7263 682d 5072 756e 696e 672f 626c 6f62  rch-Pruning/blob
+0000a300: 2f6d 6173 7465 722f 7475 746f 7269 616c  /master/tutorial
+0000a310: 732f 3125 3230 2d25 3230 4375 7374 6f6d  s/1%20-%20Custom
+0000a320: 697a 6525 3230 596f 7572 2532 304f 776e  ize%20Your%20Own
+0000a330: 2532 3050 7275 6e65 7273 2e69 7079 6e62  %20Pruners.ipynb
+0000a340: 292c 2077 6869 6368 2073 686f 7773 2068  ), which shows h
+0000a350: 6f77 2074 6f20 696d 706c 656d 656e 7420  ow to implement 
+0000a360: 6120 5b73 6c69 6d6d 696e 675d 2868 7474  a [slimming](htt
+0000a370: 7073 3a2f 2f61 7278 6976 2e6f 7267 2f61  ps://arxiv.org/a
+0000a380: 6273 2f31 3730 382e 3036 3531 3929 2070  bs/1708.06519) p
+0000a390: 7275 6e65 7220 6672 6f6d 2073 6372 6174  runer from scrat
+0000a3a0: 6368 2e20 4164 6469 7469 6f6e 616c 6c79  ch. Additionally
+0000a3b0: 2c20 796f 7520 6361 6e20 6669 6e64 206d  , you can find m
+0000a3c0: 6f72 6520 7072 6163 7469 6361 6c20 6578  ore practical ex
+0000a3d0: 616d 706c 6573 2069 6e20 5b62 656e 6368  amples in [bench
+0000a3e0: 6d61 726b 732f 6d61 696e 2e70 795d 2862  marks/main.py](b
+0000a3f0: 656e 6368 6d61 726b 732f 6d61 696e 2e70  enchmarks/main.p
+0000a400: 7929 2e0a 0a60 6060 7079 7468 6f6e 0a69  y)...```python.i
+0000a410: 6d70 6f72 7420 746f 7263 680a 6672 6f6d  mport torch.from
+0000a420: 2074 6f72 6368 7669 7369 6f6e 2e6d 6f64   torchvision.mod
+0000a430: 656c 7320 696d 706f 7274 2072 6573 6e65  els import resne
+0000a440: 7431 380a 696d 706f 7274 2074 6f72 6368  t18.import torch
+0000a450: 5f70 7275 6e69 6e67 2061 7320 7470 0a0a  _pruning as tp..
+0000a460: 6d6f 6465 6c20 3d20 7265 736e 6574 3138  model = resnet18
+0000a470: 2870 7265 7472 6169 6e65 643d 5472 7565  (pretrained=True
+0000a480: 290a 0a23 2049 6d70 6f72 7461 6e63 6520  )..# Importance 
+0000a490: 6372 6974 6572 6961 0a65 7861 6d70 6c65  criteria.example
+0000a4a0: 5f69 6e70 7574 7320 3d20 746f 7263 682e  _inputs = torch.
+0000a4b0: 7261 6e64 6e28 312c 2033 2c20 3232 342c  randn(1, 3, 224,
+0000a4c0: 2032 3234 290a 696d 7020 3d20 7470 2e69   224).imp = tp.i
+0000a4d0: 6d70 6f72 7461 6e63 652e 5461 796c 6f72  mportance.Taylor
+0000a4e0: 496d 706f 7274 616e 6365 2829 0a0a 6967  Importance()..ig
+0000a4f0: 6e6f 7265 645f 6c61 7965 7273 203d 205b  nored_layers = [
+0000a500: 5d0a 666f 7220 6d20 696e 206d 6f64 656c  ].for m in model
+0000a510: 2e6d 6f64 756c 6573 2829 3a0a 2020 2020  .modules():.    
+0000a520: 6966 2069 7369 6e73 7461 6e63 6528 6d2c  if isinstance(m,
+0000a530: 2074 6f72 6368 2e6e 6e2e 4c69 6e65 6172   torch.nn.Linear
+0000a540: 2920 616e 6420 6d2e 6f75 745f 6665 6174  ) and m.out_feat
+0000a550: 7572 6573 203d 3d20 3130 3030 3a0a 2020  ures == 1000:.  
+0000a560: 2020 2020 2020 6967 6e6f 7265 645f 6c61        ignored_la
+0000a570: 7965 7273 2e61 7070 656e 6428 6d29 2023  yers.append(m) #
+0000a580: 2044 4f20 4e4f 5420 7072 756e 6520 7468   DO NOT prune th
+0000a590: 6520 6669 6e61 6c20 636c 6173 7369 6669  e final classifi
+0000a5a0: 6572 210a 0a69 7465 7261 7469 7665 5f73  er!..iterative_s
+0000a5b0: 7465 7073 203d 2035 2023 2070 726f 6772  teps = 5 # progr
+0000a5c0: 6573 7369 7665 2070 7275 6e69 6e67 0a70  essive pruning.p
+0000a5d0: 7275 6e65 7220 3d20 7470 2e70 7275 6e65  runer = tp.prune
+0000a5e0: 722e 4d61 676e 6974 7564 6550 7275 6e65  r.MagnitudePrune
+0000a5f0: 7228 0a20 2020 206d 6f64 656c 2c0a 2020  r(.    model,.  
+0000a600: 2020 6578 616d 706c 655f 696e 7075 7473    example_inputs
+0000a610: 2c0a 2020 2020 696d 706f 7274 616e 6365  ,.    importance
+0000a620: 3d69 6d70 2c0a 2020 2020 6974 6572 6174  =imp,.    iterat
+0000a630: 6976 655f 7374 6570 733d 6974 6572 6174  ive_steps=iterat
+0000a640: 6976 655f 7374 6570 732c 0a20 2020 2063  ive_steps,.    c
+0000a650: 685f 7370 6172 7369 7479 3d30 2e35 2c20  h_sparsity=0.5, 
+0000a660: 2320 7265 6d6f 7665 2035 3025 2063 6861  # remove 50% cha
+0000a670: 6e6e 656c 732c 2052 6573 4e65 7431 3820  nnels, ResNet18 
+0000a680: 3d20 7b36 342c 2031 3238 2c20 3235 362c  = {64, 128, 256,
+0000a690: 2035 3132 7d20 3d3e 2052 6573 4e65 7431   512} => ResNet1
+0000a6a0: 385f 4861 6c66 203d 207b 3332 2c20 3634  8_Half = {32, 64
+0000a6b0: 2c20 3132 382c 2032 3536 7d0a 2020 2020  , 128, 256}.    
+0000a6c0: 6967 6e6f 7265 645f 6c61 7965 7273 3d69  ignored_layers=i
+0000a6d0: 676e 6f72 6564 5f6c 6179 6572 732c 0a29  gnored_layers,.)
+0000a6e0: 0a0a 6261 7365 5f6d 6163 732c 2062 6173  ..base_macs, bas
+0000a6f0: 655f 6e70 6172 616d 7320 3d20 7470 2e75  e_nparams = tp.u
+0000a700: 7469 6c73 2e63 6f75 6e74 5f6f 7073 5f61  tils.count_ops_a
+0000a710: 6e64 5f70 6172 616d 7328 6d6f 6465 6c2c  nd_params(model,
+0000a720: 2065 7861 6d70 6c65 5f69 6e70 7574 7329   example_inputs)
+0000a730: 0a66 6f72 2069 2069 6e20 7261 6e67 6528  .for i in range(
+0000a740: 6974 6572 6174 6976 655f 7374 6570 7329  iterative_steps)
+0000a750: 3a0a 2020 2020 6966 2069 7369 6e73 7461  :.    if isinsta
+0000a760: 6e63 6528 696d 702c 2074 702e 696d 706f  nce(imp, tp.impo
+0000a770: 7274 616e 6365 2e54 6179 6c6f 7249 6d70  rtance.TaylorImp
+0000a780: 6f72 7461 6e63 6529 3a0a 2020 2020 2020  ortance):.      
+0000a790: 2020 2320 5461 796c 6f72 2065 7870 616e    # Taylor expan
+0000a7a0: 7369 6f6e 2072 6571 7569 7265 7320 6772  sion requires gr
+0000a7b0: 6164 6965 6e74 7320 666f 7220 696d 706f  adients for impo
+0000a7c0: 7274 616e 6365 2065 7374 696d 6174 696f  rtance estimatio
+0000a7d0: 6e0a 2020 2020 2020 2020 6c6f 7373 203d  n.        loss =
+0000a7e0: 206d 6f64 656c 2865 7861 6d70 6c65 5f69   model(example_i
+0000a7f0: 6e70 7574 7329 2e73 756d 2829 2023 2061  nputs).sum() # a
+0000a800: 2064 756d 6d79 206c 6f73 7320 666f 7220   dummy loss for 
+0000a810: 5461 796c 6f72 496d 706f 7274 616e 6365  TaylorImportance
+0000a820: 0a20 2020 2020 2020 206c 6f73 732e 6261  .        loss.ba
+0000a830: 636b 7761 7264 2829 2023 2062 6566 6f72  ckward() # befor
+0000a840: 6520 7072 756e 6572 2e73 7465 7028 290a  e pruner.step().
+0000a850: 2020 2020 7072 756e 6572 2e73 7465 7028      pruner.step(
+0000a860: 290a 2020 2020 6d61 6373 2c20 6e70 6172  ).    macs, npar
+0000a870: 616d 7320 3d20 7470 2e75 7469 6c73 2e63  ams = tp.utils.c
+0000a880: 6f75 6e74 5f6f 7073 5f61 6e64 5f70 6172  ount_ops_and_par
+0000a890: 616d 7328 6d6f 6465 6c2c 2065 7861 6d70  ams(model, examp
+0000a8a0: 6c65 5f69 6e70 7574 7329 0a20 2020 2023  le_inputs).    #
+0000a8b0: 2066 696e 6574 756e 6520 796f 7572 206d   finetune your m
+0000a8c0: 6f64 656c 2068 6572 650a 2020 2020 2320  odel here.    # 
+0000a8d0: 6669 6e65 7475 6e65 286d 6f64 656c 290a  finetune(model).
+0000a8e0: 2020 2020 2320 2e2e 2e0a 6060 600a 0a23      # ....```..#
+0000a8f0: 2323 2320 5370 6172 7365 2054 7261 696e  ### Sparse Train
+0000a900: 696e 670a 536f 6d65 2070 7275 6e65 7273  ing.Some pruners
+0000a910: 206c 696b 6520 5b42 4e53 6361 6c65 5072   like [BNScalePr
+0000a920: 756e 6572 5d28 6874 7470 733a 2f2f 6769  uner](https://gi
+0000a930: 7468 7562 2e63 6f6d 2f56 6169 6e46 2f54  thub.com/VainF/T
+0000a940: 6f72 6368 2d50 7275 6e69 6e67 2f62 6c6f  orch-Pruning/blo
+0000a950: 622f 6464 3539 3932 3133 3635 6437 3261  b/dd59921365d72a
+0000a960: 6362 3238 3537 6433 6437 3466 3735 6330  cb2857d3d74f75c0
+0000a970: 3365 3437 3730 3630 6662 2f74 6f72 6368  3e477060fb/torch
+0000a980: 5f70 7275 6e69 6e67 2f70 7275 6e65 722f  _pruning/pruner/
+0000a990: 616c 676f 7269 7468 6d73 2f62 6174 6368  algorithms/batch
+0000a9a0: 6e6f 726d 5f73 6361 6c65 5f70 7275 6e65  norm_scale_prune
+0000a9b0: 722e 7079 234c 3435 2920 616e 6420 5b47  r.py#L45) and [G
+0000a9c0: 726f 7570 4e6f 726d 5072 756e 6572 5d28  roupNormPruner](
+0000a9d0: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
+0000a9e0: 6f6d 2f56 6169 6e46 2f54 6f72 6368 2d50  om/VainF/Torch-P
+0000a9f0: 7275 6e69 6e67 2f62 6c6f 622f 6464 3539  runing/blob/dd59
+0000aa00: 3932 3133 3635 6437 3261 6362 3238 3537  921365d72acb2857
+0000aa10: 6433 6437 3466 3735 6330 3365 3437 3730  d3d74f75c03e4770
+0000aa20: 3630 6662 2f74 6f72 6368 5f70 7275 6e69  60fb/torch_pruni
+0000aa30: 6e67 2f70 7275 6e65 722f 616c 676f 7269  ng/pruner/algori
+0000aa40: 7468 6d73 2f67 726f 7570 5f6e 6f72 6d5f  thms/group_norm_
+0000aa50: 7072 756e 6572 2e70 7923 4c35 3329 2072  pruner.py#L53) r
+0000aa60: 6571 7569 7265 2073 7061 7273 6520 7472  equire sparse tr
+0000aa70: 6169 6e69 6e67 2062 6566 6f72 6520 7072  aining before pr
+0000aa80: 756e 696e 672e 2054 6869 7320 6361 6e20  uning. This can 
+0000aa90: 6265 2065 6173 696c 7920 6163 6869 6576  be easily achiev
+0000aaa0: 6564 2062 7920 696e 7365 7274 696e 6720  ed by inserting 
+0000aab0: 6a75 7374 206f 6e65 206c 696e 6520 6f66  just one line of
+0000aac0: 2063 6f64 6520 6060 7072 756e 6572 2e72   code ``pruner.r
+0000aad0: 6567 756c 6172 697a 6528 6d6f 6465 6c29  egularize(model)
+0000aae0: 6060 2069 6e20 796f 7572 2074 7261 696e  `` in your train
+0000aaf0: 696e 6720 7363 7269 7074 2e20 5468 6520  ing script. The 
+0000ab00: 7072 756e 6572 2077 696c 6c20 7570 6461  pruner will upda
+0000ab10: 7465 2074 6865 2067 7261 6469 656e 7420  te the gradient 
+0000ab20: 6f66 2074 7261 696e 6162 6c65 2070 6172  of trainable par
+0000ab30: 616d 6574 6572 732e 0a60 6060 7079 7468  ameters..```pyth
+0000ab40: 6f6e 0a66 6f72 2065 706f 6368 2069 6e20  on.for epoch in 
+0000ab50: 7261 6e67 6528 6570 6f63 6873 293a 0a20  range(epochs):. 
+0000ab60: 2020 206d 6f64 656c 2e74 7261 696e 2829     model.train()
+0000ab70: 0a20 2020 2066 6f72 2069 2c20 2864 6174  .    for i, (dat
+0000ab80: 612c 2074 6172 6765 7429 2069 6e20 656e  a, target) in en
+0000ab90: 756d 6572 6174 6528 7472 6169 6e5f 6c6f  umerate(train_lo
+0000aba0: 6164 6572 293a 0a20 2020 2020 2020 2064  ader):.        d
+0000abb0: 6174 612c 2074 6172 6765 7420 3d20 6461  ata, target = da
+0000abc0: 7461 2e74 6f28 6465 7669 6365 292c 2074  ta.to(device), t
+0000abd0: 6172 6765 742e 746f 2864 6576 6963 6529  arget.to(device)
+0000abe0: 0a20 2020 2020 2020 206f 7074 696d 697a  .        optimiz
+0000abf0: 6572 2e7a 6572 6f5f 6772 6164 2829 0a20  er.zero_grad(). 
+0000ac00: 2020 2020 2020 206f 7574 203d 206d 6f64         out = mod
+0000ac10: 656c 2864 6174 6129 0a20 2020 2020 2020  el(data).       
+0000ac20: 206c 6f73 7320 3d20 462e 6372 6f73 735f   loss = F.cross_
+0000ac30: 656e 7472 6f70 7928 6f75 742c 2074 6172  entropy(out, tar
+0000ac40: 6765 7429 0a20 2020 2020 2020 206c 6f73  get).        los
+0000ac50: 732e 6261 636b 7761 7264 2829 0a20 2020  s.backward().   
+0000ac60: 2020 2020 2070 7275 6e65 722e 7265 6775       pruner.regu
+0000ac70: 6c61 7269 7a65 286d 6f64 656c 2920 2320  larize(model) # 
+0000ac80: 3c3d 3d20 666f 7220 7370 6172 7365 206c  <== for sparse l
+0000ac90: 6561 726e 696e 670a 2020 2020 2020 2020  earning.        
+0000aca0: 6f70 7469 6d69 7a65 722e 7374 6570 2829  optimizer.step()
+0000acb0: 0a60 6060 0a0a 2323 2323 2049 6e74 6572  .```..#### Inter
+0000acc0: 6163 7469 7665 2050 7275 6e69 6e67 2028  active Pruning (
+0000acd0: 4164 7661 6e63 6564 290a 416c 6c20 6869  Advanced).All hi
+0000ace0: 6768 2d6c 6576 656c 2070 7275 6e65 7273  gh-level pruners
+0000acf0: 2073 7570 706f 7274 2069 6e74 6572 6163   support interac
+0000ad00: 7469 7665 2070 7275 6e69 6e67 2e20 5573  tive pruning. Us
+0000ad10: 6520 6060 7072 756e 6572 2e73 7465 7028  e ``pruner.step(
+0000ad20: 696e 7465 7261 6374 6976 653d 5472 7565  interactive=True
+0000ad30: 2960 6020 746f 2067 6574 2061 6c6c 2067  )`` to get all g
+0000ad40: 726f 7570 7320 616e 6420 696e 7465 7261  roups and intera
+0000ad50: 6374 6976 656c 7920 7072 756e 6520 7468  ctively prune th
+0000ad60: 656d 2062 7920 6361 6c6c 696e 6720 6060  em by calling ``
+0000ad70: 6772 6f75 702e 7072 756e 6528 2960 602e  group.prune()``.
+0000ad80: 2054 6869 7320 6665 6174 7572 6520 6973   This feature is
+0000ad90: 2075 7365 6675 6c20 6966 2079 6f75 2077   useful if you w
+0000ada0: 616e 7420 746f 2063 6f6e 7472 6f6c 2f6d  ant to control/m
+0000adb0: 6f6e 6974 6f72 2074 6865 2070 7275 6e69  onitor the pruni
+0000adc0: 6e67 2070 726f 6365 7373 2e0a 0a60 6060  ng process...```
+0000add0: 7079 7468 6f6e 0a66 6f72 2069 2069 6e20  python.for i in 
+0000ade0: 7261 6e67 6528 6974 6572 6174 6976 655f  range(iterative_
+0000adf0: 7374 6570 7329 3a0a 2020 2020 666f 7220  steps):.    for 
+0000ae00: 6772 6f75 7020 696e 2070 7275 6e65 722e  group in pruner.
+0000ae10: 7374 6570 2869 6e74 6572 6163 7469 7665  step(interactive
+0000ae20: 3d54 7275 6529 3a20 2320 5761 726e 696e  =True): # Warnin
+0000ae30: 673a 2067 726f 7570 7320 6d75 7374 2062  g: groups must b
+0000ae40: 6520 6861 6e64 6c65 6420 7365 7175 656e  e handled sequen
+0000ae50: 7469 616c 6c79 2e20 446f 206e 6f74 206b  tially. Do not k
+0000ae60: 6565 7020 7468 656d 2061 7320 6120 6c69  eep them as a li
+0000ae70: 7374 2e0a 2020 2020 2020 2020 7072 696e  st..        prin
+0000ae80: 7428 6772 6f75 7029 200a 2020 2020 2020  t(group) .      
+0000ae90: 2020 2320 646f 2077 6861 7465 7665 7220    # do whatever 
+0000aea0: 796f 7520 6c69 6b65 2077 6974 6820 7468  you like with th
+0000aeb0: 6520 6772 6f75 7020 0a20 2020 2020 2020  e group .       
+0000aec0: 2064 6570 2c20 6964 7873 203d 2067 726f   dep, idxs = gro
+0000aed0: 7570 5b30 5d20 2320 6765 7420 7468 6520  up[0] # get the 
+0000aee0: 6964 7873 0a20 2020 2020 2020 2074 6172  idxs.        tar
+0000aef0: 6765 745f 6d6f 6475 6c65 203d 2064 6570  get_module = dep
+0000af00: 2e74 6172 6765 742e 6d6f 6475 6c65 2023  .target.module #
+0000af10: 2067 6574 2074 6865 2072 6f6f 7420 6d6f   get the root mo
+0000af20: 6475 6c65 0a20 2020 2020 2020 2070 7275  dule.        pru
+0000af30: 6e69 6e67 5f66 6e20 3d20 6465 702e 6861  ning_fn = dep.ha
+0000af40: 6e64 6c65 7220 2320 6765 7420 7468 6520  ndler # get the 
+0000af50: 7072 756e 696e 6720 6675 6e63 7469 6f6e  pruning function
+0000af60: 0a20 2020 2020 2020 0a20 2020 2020 2020  .       .       
+0000af70: 2023 2044 6f6e 2774 2066 6f72 6765 7420   # Don't forget 
+0000af80: 746f 2070 7275 6e65 2074 6865 2067 726f  to prune the gro
+0000af90: 7570 0a20 2020 2020 2020 2067 726f 7570  up.        group
+0000afa0: 2e70 7275 6e65 2829 0a20 2020 2020 2020  .prune().       
+0000afb0: 2020 200a 2020 2020 2020 2020 2320 6772     .        # gr
+0000afc0: 6f75 702e 7072 756e 6528 6964 7873 3d5b  oup.prune(idxs=[
+0000afd0: 302c 2032 2c20 365d 2920 2320 4974 2069  0, 2, 6]) # It i
+0000afe0: 7320 6576 656e 2070 6f73 7369 626c 6520  s even possible 
+0000aff0: 746f 2063 6861 6e67 6520 7468 6520 7072  to change the pr
+0000b000: 756e 696e 6720 6265 6861 7669 6f75 7220  uning behaviour 
+0000b010: 7769 7468 2074 6865 2069 6478 7320 7061  with the idxs pa
+0000b020: 7261 6d65 7465 720a 2020 2020 6d61 6373  rameter.    macs
+0000b030: 2c20 6e70 6172 616d 7320 3d20 7470 2e75  , nparams = tp.u
+0000b040: 7469 6c73 2e63 6f75 6e74 5f6f 7073 5f61  tils.count_ops_a
+0000b050: 6e64 5f70 6172 616d 7328 6d6f 6465 6c2c  nd_params(model,
+0000b060: 2065 7861 6d70 6c65 5f69 6e70 7574 7329   example_inputs)
+0000b070: 0a20 2020 2023 2066 696e 6574 756e 6520  .    # finetune 
+0000b080: 796f 7572 206d 6f64 656c 2068 6572 650a  your model here.
+0000b090: 2020 2020 2320 6669 6e65 7475 6e65 286d      # finetune(m
+0000b0a0: 6f64 656c 290a 2020 2020 2320 2e2e 2e0a  odel).    # ....
+0000b0b0: 6060 600a 0a23 2323 2320 4772 6f75 702d  ```..#### Group-
+0000b0c0: 6c65 7665 6c20 5072 756e 696e 670a 0a57  level Pruning..W
+0000b0d0: 6974 6820 4465 7047 7261 7068 2c20 6974  ith DepGraph, it
+0000b0e0: 2069 7320 6561 7379 2074 6f20 6465 7369   is easy to desi
+0000b0f0: 676e 2073 6f6d 6520 2267 726f 7570 2d6c  gn some "group-l
+0000b100: 6576 656c 2220 6372 6974 6572 6961 2074  evel" criteria t
+0000b110: 6f20 6573 7469 6d61 7465 2074 6865 2069  o estimate the i
+0000b120: 6d70 6f72 7461 6e63 6520 6f66 2061 2077  mportance of a w
+0000b130: 686f 6c65 2067 726f 7570 2072 6174 6865  hole group rathe
+0000b140: 7220 7468 616e 2061 2073 696e 676c 6520  r than a single 
+0000b150: 6c61 7965 722e 2049 6e20 546f 7263 682d  layer. In Torch-
+0000b160: 7072 756e 696e 672c 2061 6c6c 2070 7275  pruning, all pru
+0000b170: 6e65 7273 2077 6f72 6b20 696e 2074 6865  ners work in the
+0000b180: 2067 726f 7570 206c 6576 656c 2e0a 0a3c   group level...<
+0000b190: 6469 7620 616c 6967 6e3d 2263 656e 7465  div align="cente
+0000b1a0: 7222 3e0a 3c69 6d67 2073 7263 3d22 6173  r">.<img src="as
+0000b1b0: 7365 7473 2f67 726f 7570 5f73 7061 7273  sets/group_spars
+0000b1c0: 6974 792e 706e 6722 2077 6964 7468 3d22  ity.png" width="
+0000b1d0: 3830 2522 3e0a 3c2f 6469 763e 0a0a 2323  80%">.</div>..##
+0000b1e0: 2320 332e 2053 6176 6520 2620 4c6f 6164  # 3. Save & Load
+0000b1f0: 0a20 2020 2020 2020 2020 200a 5468 6520  .          .The 
+0000b200: 666f 6c6c 6f77 696e 6720 7363 7269 7074  following script
+0000b210: 2073 6176 6573 2074 6865 2077 686f 6c65   saves the whole
+0000b220: 206d 6f64 656c 206f 626a 6563 7420 2873   model object (s
+0000b230: 7472 7563 7475 7265 2b77 6569 6768 7473  tructure+weights
+0000b240: 2920 6173 2061 2027 6d6f 6465 6c2e 7074  ) as a 'model.pt
+0000b250: 6827 2e20 0a60 6060 7079 7468 6f6e 0a6d  h'. .```python.m
+0000b260: 6f64 656c 2e7a 6572 6f5f 6772 6164 2829  odel.zero_grad()
+0000b270: 2023 2057 6520 646f 6e27 7420 7761 6e74   # We don't want
+0000b280: 2074 6f20 7374 6f72 6520 6772 6164 6965   to store gradie
+0000b290: 6e74 2069 6e66 6f72 6d61 7469 6f6e 0a74  nt information.t
+0000b2a0: 6f72 6368 2e73 6176 6528 6d6f 6465 6c2c  orch.save(model,
+0000b2b0: 2027 6d6f 6465 6c2e 7074 6827 2920 2320   'model.pth') # 
+0000b2c0: 7769 7468 6f75 7420 2e73 7461 7465 5f64  without .state_d
+0000b2d0: 6963 740a 6d6f 6465 6c20 3d20 746f 7263  ict.model = torc
+0000b2e0: 682e 6c6f 6164 2827 6d6f 6465 6c2e 7074  h.load('model.pt
+0000b2f0: 6827 2920 2320 6c6f 6164 2074 6865 2070  h') # load the p
+0000b300: 7275 6e65 6420 6d6f 6465 6c0a 6060 600a  runed model.```.
+0000b310: 0a2a 2a45 7870 6572 696d 656e 7461 6c20  .**Experimental 
+0000b320: 4665 6174 7572 6573 2a2a 3a20 5265 2d63  Features**: Re-c
+0000b330: 7265 6174 6520 7072 756e 6564 206d 6f64  reate pruned mod
+0000b340: 656c 7320 6672 6f6d 2075 6e70 7275 6e65  els from unprune
+0000b350: 6420 6f6e 6573 2075 7369 6e67 2060 6074  d ones using ``t
+0000b360: 702e 7374 6174 655f 6469 6374 6060 2061  p.state_dict`` a
+0000b370: 6e64 2060 6074 702e 6c6f 6164 5f73 7461  nd ``tp.load_sta
+0000b380: 7465 5f64 6963 7460 602e 0a60 6060 7079  te_dict``..```py
+0000b390: 7468 6f6e 0a23 2073 6176 6520 7468 6520  thon.# save the 
+0000b3a0: 7072 756e 6564 2073 7461 7465 5f64 6963  pruned state_dic
+0000b3b0: 742c 2077 6869 6368 2069 6e63 6c75 6465  t, which include
+0000b3c0: 7320 626f 7468 2070 7275 6e65 6420 7061  s both pruned pa
+0000b3d0: 7261 6d65 7465 7273 2061 6e64 206d 6f64  rameters and mod
+0000b3e0: 6966 6965 6420 6174 7472 6962 7574 6573  ified attributes
+0000b3f0: 0a73 7461 7465 5f64 6963 7420 3d20 7470  .state_dict = tp
+0000b400: 2e73 7461 7465 5f64 6963 7428 7072 756e  .state_dict(prun
+0000b410: 6564 5f6d 6f64 656c 2920 2320 7468 6520  ed_model) # the 
+0000b420: 7072 756e 6564 206d 6f64 656c 2c20 652e  pruned model, e.
+0000b430: 672e 2c20 6120 7265 736e 6574 2d31 382d  g., a resnet-18-
+0000b440: 6861 6c66 0a74 6f72 6368 2e73 6176 6528  half.torch.save(
+0000b450: 7374 6174 655f 6469 6374 2c20 2770 7275  state_dict, 'pru
+0000b460: 6e65 642e 7074 6827 290a 0a23 2063 7265  ned.pth')..# cre
+0000b470: 6174 6520 6120 6e65 7720 6d6f 6465 6c2c  ate a new model,
+0000b480: 2065 2e67 2e20 7265 736e 6574 3138 0a6e   e.g. resnet18.n
+0000b490: 6577 5f6d 6f64 656c 203d 2072 6573 6e65  ew_model = resne
+0000b4a0: 7431 3828 292e 6576 616c 2829 0a0a 2320  t18().eval()..# 
+0000b4b0: 6c6f 6164 2074 6865 2070 7275 6e65 6420  load the pruned 
+0000b4c0: 7374 6174 655f 6469 6374 2069 6e74 6f20  state_dict into 
+0000b4d0: 7468 6520 756e 7072 756e 6564 206d 6f64  the unpruned mod
+0000b4e0: 656c 2e0a 6c6f 6164 6564 5f73 7461 7465  el..loaded_state
+0000b4f0: 5f64 6963 7420 3d20 746f 7263 682e 6c6f  _dict = torch.lo
+0000b500: 6164 2827 7072 756e 6564 2e70 7468 272c  ad('pruned.pth',
+0000b510: 206d 6170 5f6c 6f63 6174 696f 6e3d 2763   map_location='c
+0000b520: 7075 2729 0a74 702e 6c6f 6164 5f73 7461  pu').tp.load_sta
+0000b530: 7465 5f64 6963 7428 6e65 775f 6d6f 6465  te_dict(new_mode
+0000b540: 6c2c 2073 7461 7465 5f64 6963 743d 6c6f  l, state_dict=lo
+0000b550: 6164 6564 5f73 7461 7465 5f64 6963 7429  aded_state_dict)
+0000b560: 0a70 7269 6e74 286e 6577 5f6d 6f64 656c  .print(new_model
+0000b570: 2920 2320 5468 6973 2077 696c 6c20 6265  ) # This will be
+0000b580: 2061 2070 7275 6e65 6420 6d6f 6465 6c2e   a pruned model.
+0000b590: 0a60 6060 0a52 6566 6572 2074 6f20 5b74  .```.Refer to [t
+0000b5a0: 6573 7473 2f74 6573 745f 7365 7269 616c  ests/test_serial
+0000b5b0: 697a 6174 696f 6e2e 7079 5d28 7465 7374  ization.py](test
+0000b5c0: 732f 7465 7374 5f73 6572 6961 6c69 7a61  s/test_serializa
+0000b5d0: 7469 6f6e 2e70 7929 2066 6f72 2061 6e20  tion.py) for an 
+0000b5e0: 5669 5420 6578 616d 706c 652e 2049 6e20  ViT example. In 
+0000b5f0: 7468 6973 2065 7861 6d70 6c65 2c20 7765  this example, we
+0000b600: 2077 696c 6c20 7072 756e 6520 7468 6520   will prune the 
+0000b610: 6d6f 6465 6c20 616e 6420 6d6f 6469 6679  model and modify
+0000b620: 2073 6f6d 6520 6174 7472 6962 7574 6573   some attributes
+0000b630: 206c 696b 6520 6060 6d6f 6465 6c2e 6869   like ``model.hi
+0000b640: 6464 656e 5f64 696d 7360 602e 0a20 2020  dden_dims``..   
 0000b650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000b660: 2020 200a 2323 2320 342e 204c 6f77 2d6c     .### 4. Low-l
-0000b670: 6576 656c 2050 7275 6e69 6e67 2046 756e  evel Pruning Fun
-0000b680: 6374 696f 6e73 0a0a 5768 696c 6520 6974  ctions..While it
-0000b690: 2069 7320 706f 7373 6962 6c65 2074 6f20   is possible to 
-0000b6a0: 6d61 6e75 616c 6c79 2070 7275 6e65 2079  manually prune y
-0000b6b0: 6f75 7220 6d6f 6465 6c20 7573 696e 6720  our model using 
-0000b6c0: 6c6f 772d 6c65 7665 6c20 6675 6e63 7469  low-level functi
-0000b6d0: 6f6e 732c 2074 6869 7320 6170 7072 6f61  ons, this approa
-0000b6e0: 6368 2063 616e 2062 6520 7175 6974 6520  ch can be quite 
-0000b6f0: 6c61 626f 7269 6f75 732c 2061 7320 6974  laborious, as it
-0000b700: 2072 6571 7569 7265 7320 6361 7265 6675   requires carefu
-0000b710: 6c20 6d61 6e61 6765 6d65 6e74 206f 6620  l management of 
-0000b720: 7468 6520 6173 736f 6369 6174 6564 2064  the associated d
-0000b730: 6570 656e 6465 6e63 6965 732e 2041 7320  ependencies. As 
-0000b740: 6120 7265 7375 6c74 2c20 7765 2072 6563  a result, we rec
-0000b750: 6f6d 6d65 6e64 2075 7469 6c69 7a69 6e67  ommend utilizing
-0000b760: 2074 6865 2061 666f 7265 6d65 6e74 696f   the aforementio
-0000b770: 6e65 6420 6869 6768 2d6c 6576 656c 2070  ned high-level p
-0000b780: 7275 6e65 7273 2074 6f20 7374 7265 616d  runers to stream
-0000b790: 6c69 6e65 2074 6865 2070 7275 6e69 6e67  line the pruning
-0000b7a0: 2070 726f 6365 7373 2e0a 0a60 6060 7079   process...```py
-0000b7b0: 7468 6f6e 0a74 702e 7072 756e 655f 636f  thon.tp.prune_co
-0000b7c0: 6e76 5f6f 7574 5f63 6861 6e6e 656c 7328  nv_out_channels(
-0000b7d0: 206d 6f64 656c 2e63 6f6e 7631 2c20 6964   model.conv1, id
-0000b7e0: 7873 3d5b 322c 362c 395d 2029 0a0a 2320  xs=[2,6,9] )..# 
-0000b7f0: 6669 7820 7468 6520 6272 6f6b 656e 2064  fix the broken d
-0000b800: 6570 656e 6465 6e63 6965 7320 6d61 6e75  ependencies manu
-0000b810: 616c 6c79 0a74 702e 7072 756e 655f 6261  ally.tp.prune_ba
-0000b820: 7463 686e 6f72 6d5f 6f75 745f 6368 616e  tchnorm_out_chan
-0000b830: 6e65 6c73 2820 6d6f 6465 6c2e 626e 312c  nels( model.bn1,
-0000b840: 2069 6478 733d 5b32 2c36 2c39 5d20 290a   idxs=[2,6,9] ).
-0000b850: 7470 2e70 7275 6e65 5f63 6f6e 765f 696e  tp.prune_conv_in
-0000b860: 5f63 6861 6e6e 656c 7328 206d 6f64 656c  _channels( model
-0000b870: 2e6c 6179 6572 325b 305d 2e63 6f6e 7631  .layer2[0].conv1
-0000b880: 2c20 6964 7873 3d5b 322c 362c 395d 2029  , idxs=[2,6,9] )
-0000b890: 0a2e 2e2e 0a60 6060 0a0a 5468 6520 666f  .....```..The fo
-0000b8a0: 6c6c 6f77 696e 6720 7072 756e 696e 6720  llowing pruning 
-0000b8b0: 6675 6e63 7469 6f6e 7320 6172 6520 6176  functions are av
-0000b8c0: 6169 6c61 626c 653a 0a60 6060 7079 7468  ailable:.```pyth
-0000b8d0: 6f6e 0a27 7072 756e 655f 636f 6e76 5f6f  on.'prune_conv_o
-0000b8e0: 7574 5f63 6861 6e6e 656c 7327 2c0a 2770  ut_channels',.'p
-0000b8f0: 7275 6e65 5f63 6f6e 765f 696e 5f63 6861  rune_conv_in_cha
-0000b900: 6e6e 656c 7327 2c0a 2770 7275 6e65 5f64  nnels',.'prune_d
-0000b910: 6570 7468 7769 7365 5f63 6f6e 765f 6f75  epthwise_conv_ou
-0000b920: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
-0000b930: 756e 655f 6465 7074 6877 6973 655f 636f  une_depthwise_co
-0000b940: 6e76 5f69 6e5f 6368 616e 6e65 6c73 272c  nv_in_channels',
-0000b950: 0a27 7072 756e 655f 6261 7463 686e 6f72  .'prune_batchnor
-0000b960: 6d5f 6f75 745f 6368 616e 6e65 6c73 272c  m_out_channels',
-0000b970: 0a27 7072 756e 655f 6261 7463 686e 6f72  .'prune_batchnor
-0000b980: 6d5f 696e 5f63 6861 6e6e 656c 7327 2c0a  m_in_channels',.
-0000b990: 2770 7275 6e65 5f6c 696e 6561 725f 6f75  'prune_linear_ou
-0000b9a0: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
-0000b9b0: 756e 655f 6c69 6e65 6172 5f69 6e5f 6368  une_linear_in_ch
-0000b9c0: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-0000b9d0: 7072 656c 755f 6f75 745f 6368 616e 6e65  prelu_out_channe
-0000b9e0: 6c73 272c 0a27 7072 756e 655f 7072 656c  ls',.'prune_prel
-0000b9f0: 755f 696e 5f63 6861 6e6e 656c 7327 2c0a  u_in_channels',.
-0000ba00: 2770 7275 6e65 5f6c 6179 6572 6e6f 726d  'prune_layernorm
-0000ba10: 5f6f 7574 5f63 6861 6e6e 656c 7327 2c0a  _out_channels',.
-0000ba20: 2770 7275 6e65 5f6c 6179 6572 6e6f 726d  'prune_layernorm
-0000ba30: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
-0000ba40: 7072 756e 655f 656d 6265 6464 696e 675f  prune_embedding_
-0000ba50: 6f75 745f 6368 616e 6e65 6c73 272c 0a27  out_channels',.'
-0000ba60: 7072 756e 655f 656d 6265 6464 696e 675f  prune_embedding_
-0000ba70: 696e 5f63 6861 6e6e 656c 7327 2c0a 2770  in_channels',.'p
-0000ba80: 7275 6e65 5f70 6172 616d 6574 6572 5f6f  rune_parameter_o
-0000ba90: 7574 5f63 6861 6e6e 656c 7327 2c0a 2770  ut_channels',.'p
-0000baa0: 7275 6e65 5f70 6172 616d 6574 6572 5f69  rune_parameter_i
-0000bab0: 6e5f 6368 616e 6e65 6c73 272c 0a27 7072  n_channels',.'pr
-0000bac0: 756e 655f 6d75 6c74 6968 6561 645f 6174  une_multihead_at
-0000bad0: 7465 6e74 696f 6e5f 6f75 745f 6368 616e  tention_out_chan
-0000bae0: 6e65 6c73 272c 0a27 7072 756e 655f 6d75  nels',.'prune_mu
-0000baf0: 6c74 6968 6561 645f 6174 7465 6e74 696f  ltihead_attentio
-0000bb00: 6e5f 696e 5f63 6861 6e6e 656c 7327 2c0a  n_in_channels',.
-0000bb10: 2770 7275 6e65 5f67 726f 7570 6e6f 726d  'prune_groupnorm
-0000bb20: 5f6f 7574 5f63 6861 6e6e 656c 7327 2c0a  _out_channels',.
-0000bb30: 2770 7275 6e65 5f67 726f 7570 6e6f 726d  'prune_groupnorm
-0000bb40: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
-0000bb50: 7072 756e 655f 696e 7374 616e 6365 6e6f  prune_instanceno
-0000bb60: 726d 5f6f 7574 5f63 6861 6e6e 656c 7327  rm_out_channels'
-0000bb70: 2c0a 2770 7275 6e65 5f69 6e73 7461 6e63  ,.'prune_instanc
-0000bb80: 656e 6f72 6d5f 696e 5f63 6861 6e6e 656c  enorm_in_channel
-0000bb90: 7327 2c0a 6060 600a 0a0a 0a23 2323 2035  s',.```....### 5
-0000bba0: 2e20 4375 7374 6f6d 697a 6564 204c 6179  . Customized Lay
-0000bbb0: 6572 730a 0a50 6c65 6173 6520 7265 6665  ers..Please refe
-0000bbc0: 7220 746f 205b 7465 7374 732f 7465 7374  r to [tests/test
-0000bbd0: 5f63 7573 746f 6d69 7a65 645f 6c61 7965  _customized_laye
-0000bbe0: 722e 7079 5d28 6874 7470 733a 2f2f 6769  r.py](https://gi
-0000bbf0: 7468 7562 2e63 6f6d 2f56 6169 6e46 2f54  thub.com/VainF/T
-0000bc00: 6f72 6368 2d50 7275 6e69 6e67 2f62 6c6f  orch-Pruning/blo
-0000bc10: 622f 6d61 7374 6572 2f74 6573 7473 2f74  b/master/tests/t
-0000bc20: 6573 745f 6375 7374 6f6d 697a 6564 5f6c  est_customized_l
-0000bc30: 6179 6572 2e70 7929 2e0a 0a23 2323 2036  ayer.py)...### 6
-0000bc40: 2e20 4265 6e63 686d 6172 6b73 0a0a 4f75  . Benchmarks..Ou
-0000bc50: 7220 7265 7375 6c74 7320 6f6e 207b 5265  r results on {Re
-0000bc60: 734e 6574 2d35 3620 2f20 4349 4641 522d  sNet-56 / CIFAR-
-0000bc70: 3130 202f 2032 2e30 3078 7d0a 0a7c 204d  10 / 2.00x}..| M
-0000bc80: 6574 686f 6420 7c20 4261 7365 2028 2529  ethod | Base (%)
-0000bc90: 207c 2050 7275 6e65 6420 2825 2920 7c20   | Pruned (%) | 
-0000bca0: 245c 4465 6c74 6124 2041 6363 2028 2529  $\Delta$ Acc (%)
-0000bcb0: 207c 2053 7065 6564 2055 7020 7c0a 7c3a   | Speed Up |.|:
-0000bcc0: 2d2d 2020 2020 7c3a 2d2d 3a20 207c 3a2d  --    |:--:  |:-
-0000bcd0: 2d3a 2020 2020 7c3a 2d2d 3a20 7c3a 2d2d  -:    |:--: |:--
-0000bce0: 3a20 2020 2020 207c 0a7c 204e 4950 5320  :      |.| NIPS 
-0000bcf0: 5b5b 315d 5d28 2331 2920 207c 202d 2020  [[1]](#1)  | -  
-0000bd00: 2020 7c20 2d20 2020 2020 207c 2d30 2e30    | -      |-0.0
-0000bd10: 3320 7c20 312e 3736 7820 2020 207c 0a7c  3 | 1.76x    |.|
-0000bd20: 2047 656f 6d65 7472 6963 205b 5b32 5d5d   Geometric [[2]]
-0000bd30: 2823 3229 207c 2039 332e 3539 207c 2039  (#2) | 93.59 | 9
-0000bd40: 332e 3236 207c 202d 302e 3333 207c 2031  3.26 | -0.33 | 1
-0000bd50: 2e37 3078 207c 0a7c 2050 6f6c 6172 205b  .70x |.| Polar [
-0000bd60: 5b33 5d5d 2823 3329 2020 7c20 3933 2e38  [3]](#3)  | 93.8
-0000bd70: 3020 7c20 3933 2e38 3320 7c20 2b30 2e30  0 | 93.83 | +0.0
-0000bd80: 3320 7c31 2e38 3878 207c 0a7c 2043 5020  3 |1.88x |.| CP 
-0000bd90: 205b 5b34 5d5d 2823 3429 2020 207c 2039   [[4]](#4)   | 9
-0000bda0: 322e 3830 207c 2039 312e 3830 207c 202d  2.80 | 91.80 | -
-0000bdb0: 312e 3030 207c 322e 3030 7820 7c0a 7c20  1.00 |2.00x |.| 
-0000bdc0: 414d 4320 5b5b 355d 5d28 2335 2920 2020  AMC [[5]](#5)   
-0000bdd0: 7c20 3932 2e38 3020 7c20 3931 2e39 3020  | 92.80 | 91.90 
-0000bde0: 7c20 2d30 2e39 3020 7c32 2e30 3078 207c  | -0.90 |2.00x |
-0000bdf0: 0a7c 2048 5261 6e6b 205b 5b36 5d5d 2823  .| HRank [[6]](#
-0000be00: 3629 207c 2039 332e 3236 207c 2039 322e  6) | 93.26 | 92.
-0000be10: 3137 207c 202d 302e 3039 207c 322e 3030  17 | -0.09 |2.00
-0000be20: 7820 7c0a 7c20 5346 5020 205b 5b37 5d5d  x |.| SFP  [[7]]
-0000be30: 2823 3729 2020 7c20 3933 2e35 3920 7c20  (#7)  | 93.59 | 
-0000be40: 3933 2e33 3620 7c20 2b30 2e32 3320 7c32  93.36 | +0.23 |2
-0000be50: 2e31 3178 207c 0a7c 2052 6573 5265 7020  .11x |.| ResRep 
-0000be60: 5b5b 385d 5d28 2338 2920 7c20 3933 2e37  [[8]](#8) | 93.7
-0000be70: 3120 7c20 3933 2e37 3120 7c20 2b30 2e30  1 | 93.71 | +0.0
-0000be80: 3020 7c32 2e31 3278 207c 0a7c 7c0a 7c20  0 |2.12x |.||.| 
-0000be90: 4f75 7273 2d4c 3120 7c20 3933 2e35 3320  Ours-L1 | 93.53 
-0000bea0: 7c20 3932 2e39 3320 7c20 2d30 2e36 3020  | 92.93 | -0.60 
-0000beb0: 7c20 322e 3132 7820 7c0a 7c20 4f75 7273  | 2.12x |.| Ours
-0000bec0: 2d42 4e20 7c20 3933 2e35 3320 7c20 3933  -BN | 93.53 | 93
-0000bed0: 2e32 3920 7c20 2d30 2e32 3420 7c20 322e  .29 | -0.24 | 2.
-0000bee0: 3132 7820 7c0a 7c20 4f75 7273 2d47 726f  12x |.| Ours-Gro
-0000bef0: 7570 207c 2039 332e 3533 207c 2039 332e  up | 93.53 | 93.
-0000bf00: 3737 207c 202b 302e 3338 207c 2032 2e31  77 | +0.38 | 2.1
-0000bf10: 3378 207c 0a0a 506c 6561 7365 2072 6566  3x |..Please ref
-0000bf20: 6572 2074 6f20 5b62 656e 6368 6d61 726b  er to [benchmark
-0000bf30: 735d 2862 656e 6368 6d61 726b 7329 2066  s](benchmarks) f
-0000bf40: 6f72 206d 6f72 6520 6465 7461 696c 732e  or more details.
-0000bf50: 0a0a 2323 2320 372e 2053 6572 6965 7320  ..### 7. Series 
-0000bf60: 6f66 2057 6f72 6b73 0a3e 202a 2a4c 4c4d  of Works.> **LLM
-0000bf70: 2d50 7275 6e65 723a 204f 6e20 7468 6520  -Pruner: On the 
-0000bf80: 5374 7275 6374 7572 616c 2050 7275 6e69  Structural Pruni
-0000bf90: 6e67 206f 6620 4c61 7267 6520 4c61 6e67  ng of Large Lang
-0000bfa0: 7561 6765 204d 6f64 656c 732a 2a20 5b5b  uage Models** [[
-0000bfb0: 5072 6f6a 6563 745d 5d28 6874 7470 733a  Project]](https:
-0000bfc0: 2f2f 6769 7468 7562 2e63 6f6d 2f68 6f72  //github.com/hor
-0000bfd0: 7365 6565 2f4c 4c4d 2d50 7275 6e65 7229  seee/LLM-Pruner)
-0000bfe0: 205b 5b61 7258 6976 5d5d 2868 7474 7073   [[arXiv]](https
-0000bff0: 3a2f 2f61 7278 6976 2e6f 7267 2f61 6273  ://arxiv.org/abs
-0000c000: 2f32 3330 352e 3131 3632 3729 2020 200a  /2305.11627)   .
-0000c010: 3e20 2a58 696e 7969 6e20 4d61 2c20 476f  > *Xinyin Ma, Go
-0000c020: 6e67 6661 6e20 4661 6e67 2c20 5869 6e63  ngfan Fang, Xinc
-0000c030: 6861 6f20 5761 6e67 2a20 2020 0a0a 3e20  hao Wang*   ..> 
-0000c040: 2a2a 5374 7275 6374 7572 616c 2050 7275  **Structural Pru
-0000c050: 6e69 6e67 2066 6f72 2044 6966 6675 7369  ning for Diffusi
-0000c060: 6f6e 204d 6f64 656c 732a 2a20 5b5b 5072  on Models** [[Pr
-0000c070: 6f6a 6563 745d 5d28 6874 7470 733a 2f2f  oject]](https://
-0000c080: 6769 7468 7562 2e63 6f6d 2f56 6169 6e46  github.com/VainF
-0000c090: 2f44 6966 662d 5072 756e 696e 6729 205b  /Diff-Pruning) [
-0000c0a0: 5b61 7278 6976 5d5d 2868 7474 7073 3a2f  [arxiv]](https:/
-0000c0b0: 2f61 7278 6976 2e6f 7267 2f61 6273 2f32  /arxiv.org/abs/2
-0000c0c0: 3330 352e 3130 3932 3429 2020 0a3e 202a  305.10924)  .> *
-0000c0d0: 476f 6e67 6661 6e20 4661 6e67 2c20 5869  Gongfan Fang, Xi
-0000c0e0: 6e79 696e 204d 612c 2058 696e 6368 616f  nyin Ma, Xinchao
-0000c0f0: 2057 616e 672a 2020 2020 0a0a 0a23 2320   Wang*    ...## 
-0000c100: 4369 7461 7469 6f6e 0a60 6060 0a40 696e  Citation.```.@in
-0000c110: 7072 6f63 6565 6469 6e67 737b 6661 6e67  proceedings{fang
-0000c120: 3230 3233 6465 7067 7261 7068 2c0a 2020  2023depgraph,.  
-0000c130: 7469 746c 653d 7b44 6570 6772 6170 683a  title={Depgraph:
-0000c140: 2054 6f77 6172 6473 2061 6e79 2073 7472   Towards any str
-0000c150: 7563 7475 7261 6c20 7072 756e 696e 677d  uctural pruning}
-0000c160: 2c0a 2020 6175 7468 6f72 3d7b 4661 6e67  ,.  author={Fang
-0000c170: 2c20 476f 6e67 6661 6e20 616e 6420 4d61  , Gongfan and Ma
-0000c180: 2c20 5869 6e79 696e 2061 6e64 2053 6f6e  , Xinyin and Son
-0000c190: 672c 204d 696e 676c 6920 616e 6420 4d69  g, Mingli and Mi
-0000c1a0: 2c20 4d69 6368 6165 6c20 4269 2061 6e64  , Michael Bi and
-0000c1b0: 2057 616e 672c 2058 696e 6368 616f 7d2c   Wang, Xinchao},
-0000c1c0: 0a20 2062 6f6f 6b74 6974 6c65 3d7b 5072  .  booktitle={Pr
-0000c1d0: 6f63 6565 6469 6e67 7320 6f66 2074 6865  oceedings of the
-0000c1e0: 2049 4545 452f 4356 4620 436f 6e66 6572   IEEE/CVF Confer
-0000c1f0: 656e 6365 206f 6e20 436f 6d70 7574 6572  ence on Computer
-0000c200: 2056 6973 696f 6e20 616e 6420 5061 7474   Vision and Patt
-0000c210: 6572 6e20 5265 636f 676e 6974 696f 6e7d  ern Recognition}
-0000c220: 2c0a 2020 7061 6765 733d 7b31 3630 3931  ,.  pages={16091
-0000c230: 2d2d 3136 3130 317d 2c0a 2020 7965 6172  --16101},.  year
-0000c240: 3d7b 3230 3233 7d0a 7d0a 6060 600a 0a00  ={2023}.}.```...
-0000c250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000c260: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000c270: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000c280: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+0000b660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b670: 2020 2020 200a 2020 2020 2020 2020 2020       .          
+0000b680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b690: 2020 2020 2020 0a23 2323 2034 2e20 4c6f        .### 4. Lo
+0000b6a0: 772d 6c65 7665 6c20 5072 756e 696e 6720  w-level Pruning 
+0000b6b0: 4675 6e63 7469 6f6e 730a 0a57 6869 6c65  Functions..While
+0000b6c0: 2069 7420 6973 2070 6f73 7369 626c 6520   it is possible 
+0000b6d0: 746f 206d 616e 7561 6c6c 7920 7072 756e  to manually prun
+0000b6e0: 6520 796f 7572 206d 6f64 656c 2075 7369  e your model usi
+0000b6f0: 6e67 206c 6f77 2d6c 6576 656c 2066 756e  ng low-level fun
+0000b700: 6374 696f 6e73 2c20 7468 6973 2061 7070  ctions, this app
+0000b710: 726f 6163 6820 6361 6e20 6265 2071 7569  roach can be qui
+0000b720: 7465 206c 6162 6f72 696f 7573 2c20 6173  te laborious, as
+0000b730: 2069 7420 7265 7175 6972 6573 2063 6172   it requires car
+0000b740: 6566 756c 206d 616e 6167 656d 656e 7420  eful management 
+0000b750: 6f66 2074 6865 2061 7373 6f63 6961 7465  of the associate
+0000b760: 6420 6465 7065 6e64 656e 6369 6573 2e20  d dependencies. 
+0000b770: 4173 2061 2072 6573 756c 742c 2077 6520  As a result, we 
+0000b780: 7265 636f 6d6d 656e 6420 7574 696c 697a  recommend utiliz
+0000b790: 696e 6720 7468 6520 6166 6f72 656d 656e  ing the aforemen
+0000b7a0: 7469 6f6e 6564 2068 6967 682d 6c65 7665  tioned high-leve
+0000b7b0: 6c20 7072 756e 6572 7320 746f 2073 7472  l pruners to str
+0000b7c0: 6561 6d6c 696e 6520 7468 6520 7072 756e  eamline the prun
+0000b7d0: 696e 6720 7072 6f63 6573 732e 0a0a 6060  ing process...``
+0000b7e0: 6070 7974 686f 6e0a 7470 2e70 7275 6e65  `python.tp.prune
+0000b7f0: 5f63 6f6e 765f 6f75 745f 6368 616e 6e65  _conv_out_channe
+0000b800: 6c73 2820 6d6f 6465 6c2e 636f 6e76 312c  ls( model.conv1,
+0000b810: 2069 6478 733d 5b32 2c36 2c39 5d20 290a   idxs=[2,6,9] ).
+0000b820: 0a23 2066 6978 2074 6865 2062 726f 6b65  .# fix the broke
+0000b830: 6e20 6465 7065 6e64 656e 6369 6573 206d  n dependencies m
+0000b840: 616e 7561 6c6c 790a 7470 2e70 7275 6e65  anually.tp.prune
+0000b850: 5f62 6174 6368 6e6f 726d 5f6f 7574 5f63  _batchnorm_out_c
+0000b860: 6861 6e6e 656c 7328 206d 6f64 656c 2e62  hannels( model.b
+0000b870: 6e31 2c20 6964 7873 3d5b 322c 362c 395d  n1, idxs=[2,6,9]
+0000b880: 2029 0a74 702e 7072 756e 655f 636f 6e76   ).tp.prune_conv
+0000b890: 5f69 6e5f 6368 616e 6e65 6c73 2820 6d6f  _in_channels( mo
+0000b8a0: 6465 6c2e 6c61 7965 7232 5b30 5d2e 636f  del.layer2[0].co
+0000b8b0: 6e76 312c 2069 6478 733d 5b32 2c36 2c39  nv1, idxs=[2,6,9
+0000b8c0: 5d20 290a 2e2e 2e0a 6060 600a 0a54 6865  ] ).....```..The
+0000b8d0: 2066 6f6c 6c6f 7769 6e67 2070 7275 6e69   following pruni
+0000b8e0: 6e67 2066 756e 6374 696f 6e73 2061 7265  ng functions are
+0000b8f0: 2061 7661 696c 6162 6c65 3a0a 6060 6070   available:.```p
+0000b900: 7974 686f 6e0a 2770 7275 6e65 5f63 6f6e  ython.'prune_con
+0000b910: 765f 6f75 745f 6368 616e 6e65 6c73 272c  v_out_channels',
+0000b920: 0a27 7072 756e 655f 636f 6e76 5f69 6e5f  .'prune_conv_in_
+0000b930: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
+0000b940: 655f 6465 7074 6877 6973 655f 636f 6e76  e_depthwise_conv
+0000b950: 5f6f 7574 5f63 6861 6e6e 656c 7327 2c0a  _out_channels',.
+0000b960: 2770 7275 6e65 5f64 6570 7468 7769 7365  'prune_depthwise
+0000b970: 5f63 6f6e 765f 696e 5f63 6861 6e6e 656c  _conv_in_channel
+0000b980: 7327 2c0a 2770 7275 6e65 5f62 6174 6368  s',.'prune_batch
+0000b990: 6e6f 726d 5f6f 7574 5f63 6861 6e6e 656c  norm_out_channel
+0000b9a0: 7327 2c0a 2770 7275 6e65 5f62 6174 6368  s',.'prune_batch
+0000b9b0: 6e6f 726d 5f69 6e5f 6368 616e 6e65 6c73  norm_in_channels
+0000b9c0: 272c 0a27 7072 756e 655f 6c69 6e65 6172  ',.'prune_linear
+0000b9d0: 5f6f 7574 5f63 6861 6e6e 656c 7327 2c0a  _out_channels',.
+0000b9e0: 2770 7275 6e65 5f6c 696e 6561 725f 696e  'prune_linear_in
+0000b9f0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+0000ba00: 6e65 5f70 7265 6c75 5f6f 7574 5f63 6861  ne_prelu_out_cha
+0000ba10: 6e6e 656c 7327 2c0a 2770 7275 6e65 5f70  nnels',.'prune_p
+0000ba20: 7265 6c75 5f69 6e5f 6368 616e 6e65 6c73  relu_in_channels
+0000ba30: 272c 0a27 7072 756e 655f 6c61 7965 726e  ',.'prune_layern
+0000ba40: 6f72 6d5f 6f75 745f 6368 616e 6e65 6c73  orm_out_channels
+0000ba50: 272c 0a27 7072 756e 655f 6c61 7965 726e  ',.'prune_layern
+0000ba60: 6f72 6d5f 696e 5f63 6861 6e6e 656c 7327  orm_in_channels'
+0000ba70: 2c0a 2770 7275 6e65 5f65 6d62 6564 6469  ,.'prune_embeddi
+0000ba80: 6e67 5f6f 7574 5f63 6861 6e6e 656c 7327  ng_out_channels'
+0000ba90: 2c0a 2770 7275 6e65 5f65 6d62 6564 6469  ,.'prune_embeddi
+0000baa0: 6e67 5f69 6e5f 6368 616e 6e65 6c73 272c  ng_in_channels',
+0000bab0: 0a27 7072 756e 655f 7061 7261 6d65 7465  .'prune_paramete
+0000bac0: 725f 6f75 745f 6368 616e 6e65 6c73 272c  r_out_channels',
+0000bad0: 0a27 7072 756e 655f 7061 7261 6d65 7465  .'prune_paramete
+0000bae0: 725f 696e 5f63 6861 6e6e 656c 7327 2c0a  r_in_channels',.
+0000baf0: 2770 7275 6e65 5f6d 756c 7469 6865 6164  'prune_multihead
+0000bb00: 5f61 7474 656e 7469 6f6e 5f6f 7574 5f63  _attention_out_c
+0000bb10: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
+0000bb20: 5f6d 756c 7469 6865 6164 5f61 7474 656e  _multihead_atten
+0000bb30: 7469 6f6e 5f69 6e5f 6368 616e 6e65 6c73  tion_in_channels
+0000bb40: 272c 0a27 7072 756e 655f 6772 6f75 706e  ',.'prune_groupn
+0000bb50: 6f72 6d5f 6f75 745f 6368 616e 6e65 6c73  orm_out_channels
+0000bb60: 272c 0a27 7072 756e 655f 6772 6f75 706e  ',.'prune_groupn
+0000bb70: 6f72 6d5f 696e 5f63 6861 6e6e 656c 7327  orm_in_channels'
+0000bb80: 2c0a 2770 7275 6e65 5f69 6e73 7461 6e63  ,.'prune_instanc
+0000bb90: 656e 6f72 6d5f 6f75 745f 6368 616e 6e65  enorm_out_channe
+0000bba0: 6c73 272c 0a27 7072 756e 655f 696e 7374  ls',.'prune_inst
+0000bbb0: 616e 6365 6e6f 726d 5f69 6e5f 6368 616e  ancenorm_in_chan
+0000bbc0: 6e65 6c73 272c 0a60 6060 0a0a 0a0a 2323  nels',.```....##
+0000bbd0: 2320 352e 2043 7573 746f 6d69 7a65 6420  # 5. Customized 
+0000bbe0: 4c61 7965 7273 0a0a 506c 6561 7365 2072  Layers..Please r
+0000bbf0: 6566 6572 2074 6f20 5b74 6573 7473 2f74  efer to [tests/t
+0000bc00: 6573 745f 6375 7374 6f6d 697a 6564 5f6c  est_customized_l
+0000bc10: 6179 6572 2e70 795d 2868 7474 7073 3a2f  ayer.py](https:/
+0000bc20: 2f67 6974 6875 622e 636f 6d2f 5661 696e  /github.com/Vain
+0000bc30: 462f 546f 7263 682d 5072 756e 696e 672f  F/Torch-Pruning/
+0000bc40: 626c 6f62 2f6d 6173 7465 722f 7465 7374  blob/master/test
+0000bc50: 732f 7465 7374 5f63 7573 746f 6d69 7a65  s/test_customize
+0000bc60: 645f 6c61 7965 722e 7079 292e 0a0a 2323  d_layer.py)...##
+0000bc70: 2320 362e 2042 656e 6368 6d61 726b 730a  # 6. Benchmarks.
+0000bc80: 0a4f 7572 2072 6573 756c 7473 206f 6e20  .Our results on 
+0000bc90: 7b52 6573 4e65 742d 3536 202f 2043 4946  {ResNet-56 / CIF
+0000bca0: 4152 2d31 3020 2f20 322e 3030 787d 0a0a  AR-10 / 2.00x}..
+0000bcb0: 7c20 4d65 7468 6f64 207c 2042 6173 6520  | Method | Base 
+0000bcc0: 2825 2920 7c20 5072 756e 6564 2028 2529  (%) | Pruned (%)
+0000bcd0: 207c 2024 5c44 656c 7461 2420 4163 6320   | $\Delta$ Acc 
+0000bce0: 2825 2920 7c20 5370 6565 6420 5570 207c  (%) | Speed Up |
+0000bcf0: 0a7c 3a2d 2d20 2020 207c 3a2d 2d3a 2020  .|:--    |:--:  
+0000bd00: 7c3a 2d2d 3a20 2020 207c 3a2d 2d3a 207c  |:--:    |:--: |
+0000bd10: 3a2d 2d3a 2020 2020 2020 7c0a 7c20 4e49  :--:      |.| NI
+0000bd20: 5053 205b 5b31 5d5d 2823 3129 2020 7c20  PS [[1]](#1)  | 
+0000bd30: 2d20 2020 207c 202d 2020 2020 2020 7c2d  -    | -      |-
+0000bd40: 302e 3033 207c 2031 2e37 3678 2020 2020  0.03 | 1.76x    
+0000bd50: 7c0a 7c20 4765 6f6d 6574 7269 6320 5b5b  |.| Geometric [[
+0000bd60: 325d 5d28 2332 2920 7c20 3933 2e35 3920  2]](#2) | 93.59 
+0000bd70: 7c20 3933 2e32 3620 7c20 2d30 2e33 3320  | 93.26 | -0.33 
+0000bd80: 7c20 312e 3730 7820 7c0a 7c20 506f 6c61  | 1.70x |.| Pola
+0000bd90: 7220 5b5b 335d 5d28 2333 2920 207c 2039  r [[3]](#3)  | 9
+0000bda0: 332e 3830 207c 2039 332e 3833 207c 202b  3.80 | 93.83 | +
+0000bdb0: 302e 3033 207c 312e 3838 7820 7c0a 7c20  0.03 |1.88x |.| 
+0000bdc0: 4350 2020 5b5b 345d 5d28 2334 2920 2020  CP  [[4]](#4)   
+0000bdd0: 7c20 3932 2e38 3020 7c20 3931 2e38 3020  | 92.80 | 91.80 
+0000bde0: 7c20 2d31 2e30 3020 7c32 2e30 3078 207c  | -1.00 |2.00x |
+0000bdf0: 0a7c 2041 4d43 205b 5b35 5d5d 2823 3529  .| AMC [[5]](#5)
+0000be00: 2020 207c 2039 322e 3830 207c 2039 312e     | 92.80 | 91.
+0000be10: 3930 207c 202d 302e 3930 207c 322e 3030  90 | -0.90 |2.00
+0000be20: 7820 7c0a 7c20 4852 616e 6b20 5b5b 365d  x |.| HRank [[6]
+0000be30: 5d28 2336 2920 7c20 3933 2e32 3620 7c20  ](#6) | 93.26 | 
+0000be40: 3932 2e31 3720 7c20 2d30 2e30 3920 7c32  92.17 | -0.09 |2
+0000be50: 2e30 3078 207c 0a7c 2053 4650 2020 5b5b  .00x |.| SFP  [[
+0000be60: 375d 5d28 2337 2920 207c 2039 332e 3539  7]](#7)  | 93.59
+0000be70: 207c 2039 332e 3336 207c 202b 302e 3233   | 93.36 | +0.23
+0000be80: 207c 322e 3131 7820 7c0a 7c20 5265 7352   |2.11x |.| ResR
+0000be90: 6570 205b 5b38 5d5d 2823 3829 207c 2039  ep [[8]](#8) | 9
+0000bea0: 332e 3731 207c 2039 332e 3731 207c 202b  3.71 | 93.71 | +
+0000beb0: 302e 3030 207c 322e 3132 7820 7c0a 7c7c  0.00 |2.12x |.||
+0000bec0: 0a7c 204f 7572 732d 4c31 207c 2039 332e  .| Ours-L1 | 93.
+0000bed0: 3533 207c 2039 322e 3933 207c 202d 302e  53 | 92.93 | -0.
+0000bee0: 3630 207c 2032 2e31 3278 207c 0a7c 204f  60 | 2.12x |.| O
+0000bef0: 7572 732d 424e 207c 2039 332e 3533 207c  urs-BN | 93.53 |
+0000bf00: 2039 332e 3239 207c 202d 302e 3234 207c   93.29 | -0.24 |
+0000bf10: 2032 2e31 3278 207c 0a7c 204f 7572 732d   2.12x |.| Ours-
+0000bf20: 4772 6f75 7020 7c20 3933 2e35 3320 7c20  Group | 93.53 | 
+0000bf30: 3933 2e37 3720 7c20 2b30 2e33 3820 7c20  93.77 | +0.38 | 
+0000bf40: 322e 3133 7820 7c0a 0a50 6c65 6173 6520  2.13x |..Please 
+0000bf50: 7265 6665 7220 746f 205b 6265 6e63 686d  refer to [benchm
+0000bf60: 6172 6b73 5d28 6265 6e63 686d 6172 6b73  arks](benchmarks
+0000bf70: 2920 666f 7220 6d6f 7265 2064 6574 6169  ) for more detai
+0000bf80: 6c73 2e0a 0a23 2323 2037 2e20 5365 7269  ls...### 7. Seri
+0000bf90: 6573 206f 6620 576f 726b 730a 3e20 2a2a  es of Works.> **
+0000bfa0: 4c4c 4d2d 5072 756e 6572 3a20 4f6e 2074  LLM-Pruner: On t
+0000bfb0: 6865 2053 7472 7563 7475 7261 6c20 5072  he Structural Pr
+0000bfc0: 756e 696e 6720 6f66 204c 6172 6765 204c  uning of Large L
+0000bfd0: 616e 6775 6167 6520 4d6f 6465 6c73 2a2a  anguage Models**
+0000bfe0: 205b 5b50 726f 6a65 6374 5d5d 2868 7474   [[Project]](htt
+0000bff0: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
+0000c000: 686f 7273 6565 652f 4c4c 4d2d 5072 756e  horseee/LLM-Prun
+0000c010: 6572 2920 5b5b 6172 5869 765d 5d28 6874  er) [[arXiv]](ht
+0000c020: 7470 733a 2f2f 6172 7869 762e 6f72 672f  tps://arxiv.org/
+0000c030: 6162 732f 3233 3035 2e31 3136 3237 2920  abs/2305.11627) 
+0000c040: 2020 0a3e 202a 5869 6e79 696e 204d 612c    .> *Xinyin Ma,
+0000c050: 2047 6f6e 6766 616e 2046 616e 672c 2058   Gongfan Fang, X
+0000c060: 696e 6368 616f 2057 616e 672a 2020 200a  inchao Wang*   .
+0000c070: 0a3e 202a 2a53 7472 7563 7475 7261 6c20  .> **Structural 
+0000c080: 5072 756e 696e 6720 666f 7220 4469 6666  Pruning for Diff
+0000c090: 7573 696f 6e20 4d6f 6465 6c73 2a2a 205b  usion Models** [
+0000c0a0: 5b50 726f 6a65 6374 5d5d 2868 7474 7073  [Project]](https
+0000c0b0: 3a2f 2f67 6974 6875 622e 636f 6d2f 5661  ://github.com/Va
+0000c0c0: 696e 462f 4469 6666 2d50 7275 6e69 6e67  inF/Diff-Pruning
+0000c0d0: 2920 5b5b 6172 7869 765d 5d28 6874 7470  ) [[arxiv]](http
+0000c0e0: 733a 2f2f 6172 7869 762e 6f72 672f 6162  s://arxiv.org/ab
+0000c0f0: 732f 3233 3035 2e31 3039 3234 2920 200a  s/2305.10924)  .
+0000c100: 3e20 2a47 6f6e 6766 616e 2046 616e 672c  > *Gongfan Fang,
+0000c110: 2058 696e 7969 6e20 4d61 2c20 5869 6e63   Xinyin Ma, Xinc
+0000c120: 6861 6f20 5761 6e67 2a20 2020 200a 0a0a  hao Wang*    ...
+0000c130: 2323 2043 6974 6174 696f 6e0a 6060 600a  ## Citation.```.
+0000c140: 4069 6e70 726f 6365 6564 696e 6773 7b66  @inproceedings{f
+0000c150: 616e 6732 3032 3364 6570 6772 6170 682c  ang2023depgraph,
+0000c160: 0a20 2074 6974 6c65 3d7b 4465 7067 7261  .  title={Depgra
+0000c170: 7068 3a20 546f 7761 7264 7320 616e 7920  ph: Towards any 
+0000c180: 7374 7275 6374 7572 616c 2070 7275 6e69  structural pruni
+0000c190: 6e67 7d2c 0a20 2061 7574 686f 723d 7b46  ng},.  author={F
+0000c1a0: 616e 672c 2047 6f6e 6766 616e 2061 6e64  ang, Gongfan and
+0000c1b0: 204d 612c 2058 696e 7969 6e20 616e 6420   Ma, Xinyin and 
+0000c1c0: 536f 6e67 2c20 4d69 6e67 6c69 2061 6e64  Song, Mingli and
+0000c1d0: 204d 692c 204d 6963 6861 656c 2042 6920   Mi, Michael Bi 
+0000c1e0: 616e 6420 5761 6e67 2c20 5869 6e63 6861  and Wang, Xincha
+0000c1f0: 6f7d 2c0a 2020 626f 6f6b 7469 746c 653d  o},.  booktitle=
+0000c200: 7b50 726f 6365 6564 696e 6773 206f 6620  {Proceedings of 
+0000c210: 7468 6520 4945 4545 2f43 5646 2043 6f6e  the IEEE/CVF Con
+0000c220: 6665 7265 6e63 6520 6f6e 2043 6f6d 7075  ference on Compu
+0000c230: 7465 7220 5669 7369 6f6e 2061 6e64 2050  ter Vision and P
+0000c240: 6174 7465 726e 2052 6563 6f67 6e69 7469  attern Recogniti
+0000c250: 6f6e 7d2c 0a20 2070 6167 6573 3d7b 3136  on},.  pages={16
+0000c260: 3039 312d 2d31 3631 3031 7d2c 0a20 2079  091--16101},.  y
+0000c270: 6561 723d 7b32 3032 337d 0a7d 0a60 6060  ear={2023}.}.```
+0000c280: 0a0a 0000 0000 0000 0000 0000 0000 0000  ................
 0000c290: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c2a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c2b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c2c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c2d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c2e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c2f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -3162,16 +3162,16 @@
 0000c590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c5a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c5b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c5c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c5d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c5e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c5f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000c600: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-0000c610: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+0000c600: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+0000c610: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 0000c620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -3195,23 +3195,23 @@
 0000c7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0000c810: 312e 382f 7365 7475 702e 6366 6700 0000  1.8/setup.cfg...
+0000c810: 312e 392f 7365 7475 702e 6366 6700 0000  1.9/setup.cfg...
 0000c820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0000c870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0000c880: 3030 3030 3034 3600 3134 3433 3431 3434  0000046.14434144
-0000c890: 3131 3000 3031 3535 3336 0020 3000 0000  110.015536. 0...
+0000c880: 3030 3030 3034 3600 3134 3434 3632 3331  0000046.14446231
+0000c890: 3336 3700 3031 3535 3535 0020 3000 0000  367.015555. 0...
 0000c8a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c8b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c8c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c8d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c8e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c8f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000c900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -3290,16 +3290,16 @@
 0000cd90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cda0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cdb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cdc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cdd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cde0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cdf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000ce00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0000ce10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0000ce00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0000ce10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0000ce20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ce30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ce40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ce50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ce60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ce70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ce80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -3323,23 +3323,23 @@
 0000cfa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cfb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cfc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cfd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cfe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000cff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0000d010: 312e 382f 7365 7475 702e 7079 0000 0000  1.8/setup.py....
+0000d010: 312e 392f 7365 7475 702e 7079 0000 0000  1.9/setup.py....
 0000d020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0000d070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0000d080: 3030 3031 3237 3600 3134 3433 3431 3434  0001276.14434144
-0000d090: 3037 3100 3031 3534 3433 0020 3000 0000  071.015443. 0...
+0000d080: 3030 3031 3237 3600 3134 3434 3632 3331  0001276.14446231
+0000d090: 3335 3300 3031 3534 3437 0020 3000 0000  353.015447. 0...
 0000d0a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d0b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d0c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d0d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d0e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d0f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -3363,15 +3363,15 @@
 0000d220: 4144 4d45 2e6d 6422 2c20 2272 2229 2061  ADME.md", "r") a
 0000d230: 7320 6668 3a0a 2020 2020 6c6f 6e67 5f64  s fh:.    long_d
 0000d240: 6573 6372 6970 7469 6f6e 203d 2066 682e  escription = fh.
 0000d250: 7265 6164 2829 0a0a 7365 7475 7074 6f6f  read()..setuptoo
 0000d260: 6c73 2e73 6574 7570 280a 2020 2020 6e61  ls.setup(.    na
 0000d270: 6d65 3d22 746f 7263 682d 7072 756e 696e  me="torch-prunin
 0000d280: 6722 2c0a 2020 2020 7665 7273 696f 6e3d  g",.    version=
-0000d290: 2276 312e 312e 3822 2c0a 2020 2020 6175  "v1.1.8",.    au
+0000d290: 2276 312e 312e 3922 2c0a 2020 2020 6175  "v1.1.9",.    au
 0000d2a0: 7468 6f72 3d22 476f 6e67 6661 6e20 4661  thor="Gongfan Fa
 0000d2b0: 6e67 222c 0a20 2020 2061 7574 686f 725f  ng",.    author_
 0000d2c0: 656d 6169 6c3d 2267 6f6e 6766 616e 4075  email="gongfan@u
 0000d2d0: 2e6e 7573 2e65 6475 222c 0a20 2020 2064  .nus.edu",.    d
 0000d2e0: 6573 6372 6970 7469 6f6e 3d22 5374 7275  escription="Stru
 0000d2f0: 6374 7572 616c 2050 7275 6e69 6e67 2066  ctural Pruning f
 0000d300: 6f72 204d 6f64 656c 2041 6363 656c 6572  or Model Acceler
@@ -3450,16 +3450,16 @@
 0000d790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000d800: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-0000d810: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+0000d800: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+0000d810: 3635 352e 3239 3034 3231 320a 0000 0000  655.2904212.....
 0000d820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -3483,23 +3483,23 @@
 0000d9a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d9b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d9c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d9d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d9e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000d9f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000da00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0000da10: 312e 382f 7465 7374 732f 0000 0000 0000  1.8/tests/......
+0000da10: 312e 392f 7465 7374 732f 0000 0000 0000  1.9/tests/......
 0000da20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000da30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000da40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000da50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000da60: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 0000da70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0000da80: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-0000da90: 3131 3000 3031 3530 3537 0020 3500 0000  110.015057. 5...
+0000da80: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+0000da90: 3336 3700 3031 3530 3736 0020 3500 0000  367.015076. 5...
 0000daa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000daf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000db00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -3546,16 +3546,16 @@
 0000dd90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dda0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ddb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ddc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ddd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dde0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000ddf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0000de00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0000de10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0000de00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0000de10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0000de20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000de30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000de40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000de50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000de60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000de70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000de80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -3579,23 +3579,23 @@
 0000dfa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dfb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dfc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dfd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dfe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000dff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0000e010: 312e 382f 7465 7374 732f 7465 7374 5f62  1.8/tests/test_b
+0000e010: 312e 392f 7465 7374 732f 7465 7374 5f62  1.9/tests/test_b
 0000e020: 6163 6b77 6172 642e 7079 0000 0000 0000  ackward.py......
 0000e030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0000e070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0000e080: 3030 3137 3631 3700 3134 3433 3431 3434  0017617.14434144
-0000e090: 3037 3100 3032 3032 3730 0020 3000 0000  071.020270. 0...
+0000e080: 3030 3137 3631 3700 3134 3434 3632 3331  0017617.14446231
+0000e090: 3335 3300 3032 3032 3734 0020 3000 0000  353.020274. 0...
 0000e0a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e0b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e0c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e0d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e0e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e0f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0000e100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -4154,16 +4154,16 @@
 00010390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000103a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000103b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000103c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000103d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000103e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000103f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00010400: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00010410: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00010400: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00010410: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00010420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010460: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010470: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010480: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -4187,23 +4187,23 @@
 000105a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000105b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000105c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000105d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000105e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000105f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010600: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00010610: 312e 382f 7465 7374 732f 7465 7374 5f63  1.8/tests/test_c
+00010610: 312e 392f 7465 7374 732f 7465 7374 5f63  1.9/tests/test_c
 00010620: 6f6e 6361 742e 7079 0000 0000 0000 0000  oncat.py........
 00010630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010660: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00010670: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00010680: 3030 3034 3534 3100 3134 3433 3431 3434  0004541.14434144
-00010690: 3037 3100 3031 3737 3531 0020 3000 0000  071.017751. 0...
+00010680: 3030 3034 3534 3100 3134 3434 3632 3331  0004541.14446231
+00010690: 3335 3300 3031 3737 3535 0020 3000 0000  353.017755. 0...
 000106a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000106b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000106c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000106d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000106e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000106f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00010700: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -4410,16 +4410,16 @@
 00011390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000113a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000113b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000113c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000113d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000113e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000113f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00011400: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00011410: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00011400: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00011410: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00011420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011460: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011470: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011480: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -4443,23 +4443,23 @@
 000115a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000115b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000115c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000115d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000115e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000115f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011600: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00011610: 312e 382f 7465 7374 732f 7465 7374 5f63  1.8/tests/test_c
+00011610: 312e 392f 7465 7374 732f 7465 7374 5f63  1.9/tests/test_c
 00011620: 6f6e 6361 745f 7370 6c69 742e 7079 0000  oncat_split.py..
 00011630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011660: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00011670: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00011680: 3030 3035 3135 3200 3134 3433 3431 3434  0005152.14434144
-00011690: 3037 3100 3032 3131 3633 0020 3000 0000  071.021163. 0...
+00011680: 3030 3035 3135 3200 3134 3434 3632 3331  0005152.14446231
+00011690: 3335 3300 3032 3131 3637 0020 3000 0000  353.021167. 0...
 000116a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000116b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000116c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000116d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000116e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000116f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00011700: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -4698,16 +4698,16 @@
 00012590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000125a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000125b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000125c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000125d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000125e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000125f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00012600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00012610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00012600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00012610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00012620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -4731,23 +4731,23 @@
 000127a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000127b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000127c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000127d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000127e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000127f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00012810: 312e 382f 7465 7374 732f 7465 7374 5f63  1.8/tests/test_c
+00012810: 312e 392f 7465 7374 732f 7465 7374 5f63  1.9/tests/test_c
 00012820: 7573 746f 6d69 7a65 645f 6c61 7965 722e  ustomized_layer.
 00012830: 7079 0000 0000 0000 0000 0000 0000 0000  py..............
 00012840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00012870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00012880: 3030 3036 3135 3300 3134 3433 3431 3434  0006153.14434144
-00012890: 3037 3100 3032 3230 3635 0020 3000 0000  071.022065. 0...
+00012880: 3030 3036 3135 3300 3134 3434 3632 3331  0006153.14446231
+00012890: 3335 3300 3032 3230 3731 0020 3000 0000  353.022071. 0...
 000128a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000128b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000128c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000128d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000128e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000128f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00012900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -5018,16 +5018,16 @@
 00013990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000139a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000139b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000139c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000139d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000139e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000139f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00013a00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00013a10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00013a00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00013a10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00013a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -5051,23 +5051,23 @@
 00013ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013c00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00013c10: 312e 382f 7465 7374 732f 7465 7374 5f64  1.8/tests/test_d
+00013c10: 312e 392f 7465 7374 732f 7465 7374 5f64  1.9/tests/test_d
 00013c20: 6570 656e 6465 6e63 795f 6772 6170 682e  ependency_graph.
 00013c30: 7079 0000 0000 0000 0000 0000 0000 0000  py..............
 00013c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013c60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00013c70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00013c80: 3030 3032 3331 3700 3134 3433 3431 3434  0002317.14434144
-00013c90: 3037 3100 3032 3230 3030 0020 3000 0000  071.022000. 0...
+00013c80: 3030 3032 3331 3700 3134 3434 3632 3331  0002317.14446231
+00013c90: 3335 3300 3032 3230 3034 0020 3000 0000  353.022004. 0...
 00013ca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013cb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013cc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013cd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013ce0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013cf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00013d00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -5210,16 +5210,16 @@
 00014590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000145a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000145b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000145c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000145d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000145e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000145f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00014600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00014610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00014600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00014610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00014620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -5243,23 +5243,23 @@
 000147a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000147b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000147c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000147d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000147e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000147f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00014810: 312e 382f 7465 7374 732f 7465 7374 5f64  1.8/tests/test_d
+00014810: 312e 392f 7465 7374 732f 7465 7374 5f64  1.9/tests/test_d
 00014820: 6570 656e 6465 6e63 795f 6c65 6e65 742e  ependency_lenet.
 00014830: 7079 0000 0000 0000 0000 0000 0000 0000  py..............
 00014840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00014870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00014880: 3030 3033 3332 3500 3134 3433 3431 3434  0003325.14434144
-00014890: 3037 3100 3032 3230 3036 0020 3000 0000  071.022006. 0...
+00014880: 3030 3033 3332 3500 3134 3434 3632 3331  0003325.14446231
+00014890: 3335 3300 3032 3230 3132 0020 3000 0000  353.022012. 0...
 000148a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000148b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000148c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000148d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000148e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000148f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00014900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -5434,16 +5434,16 @@
 00015390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000153a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000153b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000153c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000153d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000153e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000153f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00015400: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00015410: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00015400: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00015410: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00015420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015460: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015470: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015480: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -5467,23 +5467,23 @@
 000155a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000155b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000155c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000155d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000155e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000155f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015600: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00015610: 312e 382f 7465 7374 732f 7465 7374 5f66  1.8/tests/test_f
+00015610: 312e 392f 7465 7374 732f 7465 7374 5f66  1.9/tests/test_f
 00015620: 756c 6c79 5f63 6f6e 6e65 6374 6564 5f6c  ully_connected_l
 00015630: 6179 6572 732e 7079 0000 0000 0000 0000  ayers.py........
 00015640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015660: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00015670: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00015680: 3030 3032 3433 3000 3134 3433 3431 3434  0002430.14434144
-00015690: 3037 3100 3032 3332 3331 0020 3000 0000  071.023231. 0...
+00015680: 3030 3032 3433 3000 3134 3434 3632 3331  0002430.14446231
+00015690: 3335 3300 3032 3332 3335 0020 3000 0000  353.023235. 0...
 000156a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000156b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000156c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000156d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000156e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000156f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015700: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -5626,16 +5626,16 @@
 00015f90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00015ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00016000: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00016010: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00016000: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00016010: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00016020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -5659,23 +5659,23 @@
 000161a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000161b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000161c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000161d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000161e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000161f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016200: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00016210: 312e 382f 7465 7374 732f 7465 7374 5f69  1.8/tests/test_i
+00016210: 312e 392f 7465 7374 732f 7465 7374 5f69  1.9/tests/test_i
 00016220: 6d70 6f72 7461 6e63 652e 7079 0000 0000  mportance.py....
 00016230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016260: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00016270: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00016280: 3030 3033 3233 3500 3134 3433 3431 3434  0003235.14434144
-00016290: 3037 3100 3032 3036 3432 0020 3000 0000  071.020642. 0...
+00016280: 3030 3033 3233 3500 3134 3434 3632 3331  0003235.14446231
+00016290: 3335 3300 3032 3036 3436 0020 3000 0000  353.020646. 0...
 000162a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000162b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000162c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000162d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000162e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000162f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016300: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -5850,16 +5850,16 @@
 00016d90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00016e00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00016e10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00016e00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00016e10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00016e20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016e60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016e70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016e80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -5883,23 +5883,23 @@
 00016fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00016ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00017010: 312e 382f 7465 7374 732f 7465 7374 5f69  1.8/tests/test_i
+00017010: 312e 392f 7465 7374 732f 7465 7374 5f69  1.9/tests/test_i
 00017020: 6e74 6572 6163 7469 7665 5f70 7275 6e65  nteractive_prune
 00017030: 722e 7079 0000 0000 0000 0000 0000 0000  r.py............
 00017040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00017070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00017080: 3030 3033 3236 3300 3134 3433 3431 3434  0003263.14434144
-00017090: 3037 3100 3032 3234 3132 0020 3000 0000  071.022412. 0...
+00017080: 3030 3033 3236 3300 3134 3434 3632 3331  0003263.14446231
+00017090: 3335 3300 3032 3234 3136 0020 3000 0000  353.022416. 0...
 000170a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000170b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000170c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000170d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000170e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000170f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -6074,16 +6074,16 @@
 00017b90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00017c00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00017c10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00017c00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00017c10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00017c20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017c30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017c60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017c70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017c80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -6107,23 +6107,23 @@
 00017da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017e00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00017e10: 312e 382f 7465 7374 732f 7465 7374 5f6c  1.8/tests/test_l
+00017e10: 312e 392f 7465 7374 732f 7465 7374 5f6c  1.9/tests/test_l
 00017e20: 6f61 642e 7079 0000 0000 0000 0000 0000  oad.py..........
 00017e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017e60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00017e70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00017e80: 3030 3033 3635 3100 3134 3433 3431 3434  0003651.14434144
-00017e90: 3037 3100 3031 3734 3232 0020 3000 0000  071.017422. 0...
+00017e80: 3030 3033 3635 3100 3134 3434 3632 3331  0003651.14446231
+00017e90: 3335 3300 3031 3734 3236 0020 3000 0000  353.017426. 0...
 00017ea0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017eb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017ec0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017ed0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017ee0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017ef0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00017f00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -6298,16 +6298,16 @@
 00018990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000189a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000189b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000189c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000189d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000189e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000189f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00018a00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00018a10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00018a00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00018a10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00018a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -6331,23 +6331,23 @@
 00018ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018c00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00018c10: 312e 382f 7465 7374 732f 7465 7374 5f6d  1.8/tests/test_m
+00018c10: 312e 392f 7465 7374 732f 7465 7374 5f6d  1.9/tests/test_m
 00018c20: 756c 7469 706c 655f 696e 7075 7473 5f61  ultiple_inputs_a
 00018c30: 6e64 5f6f 7574 7075 7473 2e70 7900 0000  nd_outputs.py...
 00018c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018c60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00018c70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00018c80: 3030 3032 3637 3400 3134 3433 3431 3434  0002674.14434144
-00018c90: 3037 3100 3032 3433 3731 0020 3000 0000  071.024371. 0...
+00018c80: 3030 3032 3637 3400 3134 3434 3632 3331  0002674.14446231
+00018c90: 3335 3300 3032 3433 3735 0020 3000 0000  353.024375. 0...
 00018ca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018cb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018cc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018cd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018ce0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018cf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00018d00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -6490,16 +6490,16 @@
 00019590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000195a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000195b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000195c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000195d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000195e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000195f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00019600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00019610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00019600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00019610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00019620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -6523,23 +6523,23 @@
 000197a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000197b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000197c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000197d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000197e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000197f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00019810: 312e 382f 7465 7374 732f 7465 7374 5f70  1.8/tests/test_p
+00019810: 312e 392f 7465 7374 732f 7465 7374 5f70  1.9/tests/test_p
 00019820: 7275 6e65 722e 7079 0000 0000 0000 0000  runer.py........
 00019830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00019870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00019880: 3030 3130 3334 3400 3134 3433 3431 3434  0010344.14434144
-00019890: 3037 3100 3032 3030 3133 0020 3000 0000  071.020013. 0...
+00019880: 3030 3130 3334 3400 3134 3434 3632 3331  0010344.14446231
+00019890: 3335 3300 3032 3030 3137 0020 3000 0000  353.020017. 0...
 000198a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000198b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000198c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000198d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000198e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000198f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00019900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -6874,16 +6874,16 @@
 0001ad90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ada0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001adb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001adc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001add0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ade0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001adf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0001ae00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0001ae10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0001ae00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0001ae10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0001ae20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ae30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ae40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ae50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ae60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ae70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ae80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -6907,23 +6907,23 @@
 0001afa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001afb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001afc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001afd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001afe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001aff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0001b010: 312e 382f 7465 7374 732f 7465 7374 5f70  1.8/tests/test_p
+0001b010: 312e 392f 7465 7374 732f 7465 7374 5f70  1.9/tests/test_p
 0001b020: 7275 6e69 6e67 5f66 6e2e 7079 0000 0000  runing_fn.py....
 0001b030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0001b070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0001b080: 3030 3031 3235 3100 3134 3433 3431 3434  0001251.14434144
-0001b090: 3037 3100 3032 3036 3432 0020 3000 0000  071.020642. 0...
+0001b080: 3030 3031 3235 3100 3134 3434 3632 3331  0001251.14446231
+0001b090: 3335 3300 3032 3036 3436 0020 3000 0000  353.020646. 0...
 0001b0a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b0b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b0c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b0d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b0e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b0f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -7034,16 +7034,16 @@
 0001b790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0001b800: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0001b810: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0001b800: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0001b810: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0001b820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -7067,23 +7067,23 @@
 0001b9a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b9b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b9c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b9d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b9e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001b9f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ba00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0001ba10: 312e 382f 7465 7374 732f 7465 7374 5f72  1.8/tests/test_r
+0001ba10: 312e 392f 7465 7374 732f 7465 7374 5f72  1.9/tests/test_r
 0001ba20: 6573 6861 7065 2e70 7900 0000 0000 0000  eshape.py.......
 0001ba30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ba40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ba50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ba60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0001ba70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0001ba80: 3030 3034 3033 3100 3134 3433 3431 3434  0004031.14434144
-0001ba90: 3037 3100 3032 3031 3233 0020 3000 0000  071.020123. 0...
+0001ba80: 3030 3034 3033 3100 3134 3434 3632 3331  0004031.14446231
+0001ba90: 3335 3300 3032 3031 3237 0020 3000 0000  353.020127. 0...
 0001baa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001bab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001bac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001bad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001bae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001baf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001bb00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -7290,16 +7290,16 @@
 0001c790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0001c800: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0001c810: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0001c800: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0001c810: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0001c820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -7323,23 +7323,23 @@
 0001c9a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c9b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c9c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c9d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c9e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001c9f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ca00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0001ca10: 312e 382f 7465 7374 732f 7465 7374 5f73  1.8/tests/test_s
+0001ca10: 312e 392f 7465 7374 732f 7465 7374 5f73  1.9/tests/test_s
 0001ca20: 636f 7265 5f6e 6f72 6d61 6c69 7a61 7469  core_normalizati
 0001ca30: 6f6e 2e70 7900 0000 0000 0000 0000 0000  on.py...........
 0001ca40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ca50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ca60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0001ca70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0001ca80: 3030 3033 3630 3300 3134 3433 3431 3434  0003603.14434144
-0001ca90: 3037 3100 3032 3235 3631 0020 3000 0000  071.022561. 0...
+0001ca80: 3030 3033 3630 3300 3134 3434 3632 3331  0003603.14446231
+0001ca90: 3335 3300 3032 3235 3635 0020 3000 0000  353.022565. 0...
 0001caa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001cab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001cac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001cad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001cae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001caf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001cb00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -7514,16 +7514,16 @@
 0001d590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d5a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d5b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d5c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d5d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d5e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d5f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0001d600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0001d610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0001d600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0001d610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0001d620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -7547,23 +7547,23 @@
 0001d7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0001d810: 312e 382f 7465 7374 732f 7465 7374 5f73  1.8/tests/test_s
+0001d810: 312e 392f 7465 7374 732f 7465 7374 5f73  1.9/tests/test_s
 0001d820: 6572 6961 6c69 7a61 7469 6f6e 2e70 7900  erialization.py.
 0001d830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0001d870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0001d880: 3030 3033 3433 3300 3134 3433 3431 3434  0003433.14434144
-0001d890: 3037 3100 3032 3133 3536 0020 3000 0000  071.021356. 0...
+0001d880: 3030 3033 3433 3300 3134 3434 3632 3331  0003433.14446231
+0001d890: 3335 3300 3032 3133 3632 0020 3000 0000  353.021362. 0...
 0001d8a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d8b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d8c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d8d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d8e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d8f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001d900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -7738,16 +7738,16 @@
 0001e390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e3a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e3b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e3c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e3d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e3e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e3f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0001e400: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0001e410: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0001e400: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0001e410: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0001e420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e460: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e470: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e480: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -7771,23 +7771,23 @@
 0001e5a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e5b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e5c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e5d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e5e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e5f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e600: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0001e610: 312e 382f 7465 7374 732f 7465 7374 5f73  1.8/tests/test_s
+0001e610: 312e 392f 7465 7374 732f 7465 7374 5f73  1.9/tests/test_s
 0001e620: 696e 676c 655f 6368 616e 6e65 6c5f 6f75  ingle_channel_ou
 0001e630: 7470 7574 2e70 7900 0000 0000 0000 0000  tput.py.........
 0001e640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e660: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0001e670: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0001e680: 3030 3031 3732 3600 3134 3433 3431 3434  0001726.14434144
-0001e690: 3037 3100 3032 3330 3735 0020 3000 0000  071.023075. 0...
+0001e680: 3030 3031 3732 3600 3134 3434 3632 3331  0001726.14446231
+0001e690: 3335 3300 3032 3331 3031 0020 3000 0000  353.023101. 0...
 0001e6a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e6b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e6c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e6d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e6e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e6f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001e700: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -7898,16 +7898,16 @@
 0001ed90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001eda0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001edb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001edc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001edd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ede0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001edf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0001ee00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0001ee10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0001ee00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0001ee10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0001ee20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ee30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ee40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ee50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ee60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ee70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ee80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -7931,23 +7931,23 @@
 0001efa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001efb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001efc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001efd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001efe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001eff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0001f010: 312e 382f 7465 7374 732f 7465 7374 5f73  1.8/tests/test_s
+0001f010: 312e 392f 7465 7374 732f 7465 7374 5f73  1.9/tests/test_s
 0001f020: 706c 6974 2e70 7900 0000 0000 0000 0000  plit.py.........
 0001f030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0001f070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0001f080: 3030 3035 3034 3300 3134 3433 3431 3434  0005043.14434144
-0001f090: 3037 3100 3031 3736 3333 0020 3000 0000  071.017633. 0...
+0001f080: 3030 3035 3034 3300 3134 3434 3632 3331  0005043.14446231
+0001f090: 3335 3300 3031 3736 3337 0020 3000 0000  353.017637. 0...
 0001f0a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f0b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f0c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f0d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f0e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f0f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001f100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -8186,16 +8186,16 @@
 0001ff90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ffa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ffb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ffc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ffd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001ffe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0001fff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00020000: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00020010: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00020000: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00020010: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00020020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -8219,23 +8219,23 @@
 000201a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000201b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000201c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000201d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000201e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000201f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020200: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00020210: 312e 382f 7465 7374 732f 7465 7374 5f74  1.8/tests/test_t
+00020210: 312e 392f 7465 7374 732f 7465 7374 5f74  1.9/tests/test_t
 00020220: 6179 6c6f 725f 696d 706f 7274 616e 6365  aylor_importance
 00020230: 2e70 7900 0000 0000 0000 0000 0000 0000  .py.............
 00020240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020260: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00020270: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00020280: 3030 3032 3534 3300 3134 3433 3431 3434  0002543.14434144
-00020290: 3037 3100 3032 3232 3335 0020 3000 0000  071.022235. 0...
+00020280: 3030 3032 3534 3300 3134 3434 3632 3331  0002543.14446231
+00020290: 3335 3300 3032 3232 3431 0020 3000 0000  353.022241. 0...
 000202a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000202b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000202c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000202d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000202e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000202f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020300: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -8378,16 +8378,16 @@
 00020b90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00020c00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00020c10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00020c00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00020c10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00020c20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020c30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020c60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020c70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020c80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -8411,23 +8411,23 @@
 00020da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020e00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00020e10: 312e 382f 7465 7374 732f 7465 7374 5f75  1.8/tests/test_u
+00020e10: 312e 392f 7465 7374 732f 7465 7374 5f75  1.9/tests/test_u
 00020e20: 6e77 7261 7070 6564 5f70 6172 616d 6574  nwrapped_paramet
 00020e30: 6572 732e 7079 0000 0000 0000 0000 0000  ers.py..........
 00020e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020e60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00020e70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00020e80: 3030 3033 3134 3200 3134 3433 3431 3434  0003142.14434144
-00020e90: 3037 3100 3032 3237 3236 0020 3000 0000  071.022726. 0...
+00020e80: 3030 3033 3134 3200 3134 3434 3632 3331  0003142.14446231
+00020e90: 3335 3300 3032 3237 3332 0020 3000 0000  353.022732. 0...
 00020ea0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020eb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020ec0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020ed0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020ee0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020ef0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00020f00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -8602,16 +8602,16 @@
 00021990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000219a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000219b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000219c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000219d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000219e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000219f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00021a00: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-00021a10: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+00021a00: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+00021a10: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 00021a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -8635,23 +8635,23 @@
 00021ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021c00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00021c10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00021c10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00021c20: 672f 0000 0000 0000 0000 0000 0000 0000  g/..............
 00021c30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021c60: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 00021c70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00021c80: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-00021c90: 3131 3000 3031 3635 3736 0020 3500 0000  110.016576. 5...
+00021c80: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+00021c90: 3336 3700 3031 3636 3135 0020 3500 0000  367.016615. 5...
 00021ca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021cb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021cc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021cd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021ce0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021cf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021d00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -8698,16 +8698,16 @@
 00021f90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00021ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00022000: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00022010: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00022000: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00022010: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00022020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -8731,23 +8731,23 @@
 000221a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000221b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000221c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000221d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000221e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000221f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022200: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00022210: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00022210: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00022220: 672f 5f5f 696e 6974 5f5f 2e70 7900 0000  g/__init__.py...
 00022230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022260: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00022270: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00022280: 3030 3030 3233 3500 3134 3433 3431 3434  0000235.14434144
-00022290: 3037 3100 3032 3037 3135 0020 3000 0000  071.020715. 0...
+00022280: 3030 3030 3233 3500 3134 3434 3632 3331  0000235.14446231
+00022290: 3335 3300 3032 3037 3231 0020 3000 0000  353.020721. 0...
 000222a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000222b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000222c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000222d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000222e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000222f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022300: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -8826,16 +8826,16 @@
 00022790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000227a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000227b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000227c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000227d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000227e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000227f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00022800: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00022810: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00022800: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00022810: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00022820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -8859,23 +8859,23 @@
 000229a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000229b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000229c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000229d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000229e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000229f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022a00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00022a10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00022a10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00022a20: 672f 5f68 656c 7065 7273 2e70 7900 0000  g/_helpers.py...
 00022a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022a60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00022a70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00022a80: 3030 3036 3433 3400 3134 3433 3431 3434  0006434.14434144
-00022a90: 3037 3100 3032 3037 3636 0020 3000 0000  071.020766. 0...
+00022a80: 3030 3036 3433 3400 3134 3434 3632 3331  0006434.14446231
+00022a90: 3335 3300 3032 3037 3732 0020 3000 0000  353.020772. 0...
 00022aa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022ab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022ac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022ad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022ae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022af0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00022b00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -9146,16 +9146,16 @@
 00023b90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00023c00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00023c10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00023c00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00023c10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00023c20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023c30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023c60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023c70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023c80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -9179,23 +9179,23 @@
 00023da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023e00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00023e10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00023e10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00023e20: 672f 6465 7065 6e64 656e 6379 2e70 7900  g/dependency.py.
 00023e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023e60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00023e70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00023e80: 3031 3233 3635 3000 3134 3433 3431 3434  0123650.14434144
-00023e90: 3037 3100 3032 3133 3033 0020 3000 0000  071.021303. 0...
+00023e80: 3031 3233 3635 3000 3134 3434 3632 3331  0123650.14446231
+00023e90: 3335 3300 3032 3133 3037 0020 3000 0000  353.021307. 0...
 00023ea0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023eb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023ec0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023ed0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023ee0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023ef0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00023f00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -11930,16 +11930,16 @@
 0002e990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002e9a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002e9b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002e9c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002e9d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002e9e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002e9f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0002ea00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0002ea10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0002ea00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0002ea10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0002ea20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ea30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ea40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ea50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ea60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ea70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ea80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -11963,23 +11963,23 @@
 0002eba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ebb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ebc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ebd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ebe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ebf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ec00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0002ec10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0002ec10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0002ec20: 672f 696d 706f 7274 616e 6365 2e70 7900  g/importance.py.
 0002ec30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ec40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ec50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ec60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0002ec70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0002ec80: 3030 3436 3430 3400 3134 3433 3431 3434  0046404.14434144
-0002ec90: 3037 3100 3032 3133 3237 0020 3000 0000  071.021327. 0...
+0002ec80: 3030 3436 3430 3400 3134 3434 3632 3331  0046404.14446231
+0002ec90: 3335 3300 3032 3133 3333 0020 3000 0000  353.021333. 0...
 0002eca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ecb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ecc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ecd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ece0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ecf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0002ed00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -13274,16 +13274,16 @@
 00033d90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00033e00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00033e10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00033e00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00033e10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00033e20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033e60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033e70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033e80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -13307,23 +13307,23 @@
 00033fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00033ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00034000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00034010: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00034010: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00034020: 672f 6f70 732e 7079 0000 0000 0000 0000  g/ops.py........
 00034030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00034040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00034050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00034060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00034070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00034080: 3030 3135 3332 3000 3134 3433 3431 3434  0015320.14434144
-00034090: 3037 3100 3031 3737 3630 0020 3000 0000  071.017760. 0...
+00034080: 3030 3135 3332 3000 3134 3434 3632 3331  0015320.14446231
+00034090: 3335 3300 3031 3737 3634 0020 3000 0000  353.017764. 0...
 000340a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000340b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000340c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000340d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000340e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000340f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00034100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -13818,16 +13818,16 @@
 00035f90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00035fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00035fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00035fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00035fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00035fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00035ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00036000: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-00036010: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+00036000: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+00036010: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 00036020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -13851,23 +13851,23 @@
 000361a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000361b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000361c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000361d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000361e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000361f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036200: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00036210: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00036210: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00036220: 672f 7072 756e 6572 2f00 0000 0000 0000  g/pruner/.......
 00036230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036260: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 00036270: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00036280: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-00036290: 3131 3000 3032 3031 3131 0020 3500 0000  110.020111. 5...
+00036280: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+00036290: 3336 3700 3032 3031 3330 0020 3500 0000  367.020130. 5...
 000362a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000362b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000362c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000362d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000362e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000362f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036300: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -13914,16 +13914,16 @@
 00036590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000365a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000365b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000365c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000365d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000365e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000365f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00036600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00036610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00036600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00036610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00036620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -13947,23 +13947,23 @@
 000367a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000367b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000367c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000367d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000367e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000367f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00036810: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00036810: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00036820: 672f 7072 756e 6572 2f5f 5f69 6e69 745f  g/pruner/__init_
 00036830: 5f2e 7079 0000 0000 0000 0000 0000 0000  _.py............
 00036840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00036870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00036880: 3030 3030 3036 3100 3134 3433 3431 3434  0000061.14434144
-00036890: 3037 3100 3032 3232 3235 0020 3000 0000  071.022225. 0...
+00036880: 3030 3030 3036 3100 3134 3434 3632 3331  0000061.14446231
+00036890: 3335 3300 3032 3232 3331 0020 3000 0000  353.022231. 0...
 000368a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000368b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000368c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000368d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000368e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000368f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -14042,16 +14042,16 @@
 00036d90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00036e00: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-00036e10: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+00036e00: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+00036e10: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 00036e20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036e60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036e70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036e80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -14075,23 +14075,23 @@
 00036fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00036ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00037010: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00037010: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00037020: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 00037030: 686d 732f 0000 0000 0000 0000 0000 0000  hms/............
 00037040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037060: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 00037070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00037080: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-00037090: 3131 3000 3032 3232 3632 0020 3500 0000  110.022262. 5...
+00037080: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+00037090: 3336 3700 3032 3233 3031 0020 3500 0000  367.022301. 5...
 000370a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000370b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000370c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000370d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000370e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000370f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -14138,16 +14138,16 @@
 00037390: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000373a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000373b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000373c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000373d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000373e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000373f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00037400: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00037410: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00037400: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00037410: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00037420: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037430: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037460: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037470: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037480: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -14171,23 +14171,23 @@
 000375a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000375b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000375c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000375d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000375e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000375f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037600: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00037610: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00037610: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00037620: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 00037630: 686d 732f 5f5f 696e 6974 5f5f 2e70 7900  hms/__init__.py.
 00037640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037660: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00037670: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00037680: 3030 3030 3236 3700 3134 3433 3431 3434  0000267.14434144
-00037690: 3037 3100 3032 3434 3036 0020 3000 0000  071.024406. 0...
+00037680: 3030 3030 3236 3700 3134 3434 3632 3331  0000267.14446231
+00037690: 3335 3300 3032 3434 3132 0020 3000 0000  353.024412. 0...
 000376a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000376b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000376c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000376d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000376e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000376f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037700: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -14266,16 +14266,16 @@
 00037b90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00037c00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00037c10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00037c00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00037c10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00037c20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037c30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037c60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037c70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037c80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -14299,23 +14299,23 @@
 00037da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037e00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00037e10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00037e10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00037e20: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 00037e30: 686d 732f 6261 7463 686e 6f72 6d5f 7363  hms/batchnorm_sc
 00037e40: 616c 655f 7072 756e 6572 2e70 7900 0000  ale_pruner.py...
 00037e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037e60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00037e70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00037e80: 3030 3033 3135 3700 3134 3433 3431 3434  0003157.14434144
-00037e90: 3037 3100 3032 3733 3637 0020 3000 0000  071.027367. 0...
+00037e80: 3030 3033 3135 3700 3134 3434 3632 3331  0003157.14446231
+00037e90: 3335 3300 3032 3733 3733 0020 3000 0000  353.027373. 0...
 00037ea0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037eb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037ec0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037ed0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037ee0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037ef0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00037f00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -14490,16 +14490,16 @@
 00038990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000389a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000389b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000389c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000389d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000389e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000389f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00038a00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00038a10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00038a00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00038a10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00038a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -14523,23 +14523,23 @@
 00038ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038c00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00038c10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00038c10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00038c20: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 00038c30: 686d 732f 6772 6f75 705f 6e6f 726d 5f70  hms/group_norm_p
 00038c40: 7275 6e65 722e 7079 0000 0000 0000 0000  runer.py........
 00038c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038c60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00038c70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00038c80: 3030 3137 3537 3400 3134 3433 3431 3434  0017574.14434144
-00038c90: 3037 3100 3032 3634 3432 0020 3000 0000  071.026442. 0...
+00038c80: 3030 3137 3537 3400 3134 3434 3632 3331  0017574.14446231
+00038c90: 3335 3300 3032 3634 3436 0020 3000 0000  353.026446. 0...
 00038ca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038cb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038cc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038cd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038ce0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038cf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00038d00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -15098,16 +15098,16 @@
 0003af90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003afa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003afb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003afc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003afd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003afe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003aff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003b000: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0003b010: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0003b000: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0003b010: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0003b020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b030: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b060: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b070: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b080: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -15131,23 +15131,23 @@
 0003b1a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b1b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b1c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b1d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b1e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b1f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b200: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0003b210: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0003b210: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0003b220: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 0003b230: 686d 732f 6d61 676e 6974 7564 655f 6261  hms/magnitude_ba
 0003b240: 7365 645f 7072 756e 6572 2e70 7900 0000  sed_pruner.py...
 0003b250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b260: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0003b270: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0003b280: 3030 3030 3132 3400 3134 3433 3431 3434  0000124.14434144
-0003b290: 3037 3100 3032 3733 3435 0020 3000 0000  071.027345. 0...
+0003b280: 3030 3030 3132 3400 3134 3434 3632 3331  0000124.14446231
+0003b290: 3335 3300 3032 3733 3531 0020 3000 0000  353.027351. 0...
 0003b2a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b2b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b2c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b2d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b2e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b2f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b300: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -15226,16 +15226,16 @@
 0003b790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003b800: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0003b810: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0003b800: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0003b810: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0003b820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -15259,23 +15259,23 @@
 0003b9a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b9b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b9c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b9d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b9e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003b9f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ba00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0003ba10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0003ba10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0003ba20: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 0003ba30: 686d 732f 6d65 7461 7072 756e 6572 2e70  hms/metapruner.p
 0003ba40: 7900 0000 0000 0000 0000 0000 0000 0000  y...............
 0003ba50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ba60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0003ba70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0003ba80: 3030 3237 3133 3500 3134 3433 3431 3434  0027135.14434144
-0003ba90: 3037 3100 3032 3530 3334 0020 3000 0000  071.025034. 0...
+0003ba80: 3030 3237 3235 3100 3134 3434 3632 3331  0027251.14446231
+0003ba90: 3335 3300 3032 3530 3337 0020 3000 0000  353.025037. 0...
 0003baa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003bab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003bac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003bad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003bae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003baf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003bb00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -15949,102 +15949,102 @@
 0003e4c0: 2020 2020 2020 2020 2020 2020 696d 7020              imp 
 0003e4d0: 3d20 696d 705b 3a6c 656e 2869 6d70 292f  = imp[:len(imp)/
 0003e4e0: 2f63 685f 6772 6f75 7073 5d0a 2020 2020  /ch_groups].    
 0003e4f0: 2020 2020 2020 2020 2020 2020 676c 6f62              glob
 0003e500: 616c 5f69 6d70 6f72 7461 6e63 652e 6170  al_importance.ap
 0003e510: 7065 6e64 2828 6772 6f75 702c 2063 685f  pend((group, ch_
 0003e520: 6772 6f75 7073 2c20 696d 7029 290a 0a20  groups, imp)).. 
-0003e530: 2020 2020 2020 2069 6d70 203d 2074 6f72         imp = tor
-0003e540: 6368 2e63 6174 285b 6c6f 6361 6c5f 696d  ch.cat([local_im
-0003e550: 705b 2d31 5d0a 2020 2020 2020 2020 2020  p[-1].          
-0003e560: 2020 2020 2020 2020 2020 2020 2020 666f                fo
-0003e570: 7220 6c6f 6361 6c5f 696d 7020 696e 2067  r local_imp in g
-0003e580: 6c6f 6261 6c5f 696d 706f 7274 616e 6365  lobal_importance
-0003e590: 5d2c 2064 696d 3d30 290a 2020 2020 2020  ], dim=0).      
-0003e5a0: 2020 7461 7267 6574 5f73 7061 7273 6974    target_sparsit
-0003e5b0: 7920 3d20 7365 6c66 2e70 6572 5f73 7465  y = self.per_ste
-0003e5c0: 705f 6368 5f73 7061 7273 6974 795b 7365  p_ch_sparsity[se
-0003e5d0: 6c66 2e63 7572 7265 6e74 5f73 7465 705d  lf.current_step]
-0003e5e0: 0a20 2020 2020 2020 206e 5f70 7275 6e65  .        n_prune
-0003e5f0: 6420 3d20 6c65 6e28 696d 7029 202d 2069  d = len(imp) - i
-0003e600: 6e74 280a 2020 2020 2020 2020 2020 2020  nt(.            
-0003e610: 7365 6c66 2e69 6e69 7469 616c 5f74 6f74  self.initial_tot
-0003e620: 616c 5f63 6861 6e6e 656c 7320 2a0a 2020  al_channels *.  
-0003e630: 2020 2020 2020 2020 2020 2831 202d 2074            (1 - t
-0003e640: 6172 6765 745f 7370 6172 7369 7479 290a  arget_sparsity).
-0003e650: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-0003e660: 2020 6966 206e 5f70 7275 6e65 6420 3c3d    if n_pruned <=
-0003e670: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
-0003e680: 7265 7475 726e 0a20 2020 2020 2020 2074  return.        t
-0003e690: 6f70 6b5f 696d 702c 205f 203d 2074 6f72  opk_imp, _ = tor
-0003e6a0: 6368 2e74 6f70 6b28 696d 702c 206b 3d6e  ch.topk(imp, k=n
-0003e6b0: 5f70 7275 6e65 642c 206c 6172 6765 7374  _pruned, largest
-0003e6c0: 3d46 616c 7365 290a 2020 2020 2020 2020  =False).        
-0003e6d0: 0a20 2020 2020 2020 2023 2067 6c6f 6261  .        # globa
-0003e6e0: 6c20 7072 756e 696e 6720 7468 726f 7567  l pruning throug
-0003e6f0: 6820 7468 7265 7368 6f6c 6469 6e67 0a20  h thresholding. 
-0003e700: 2020 2020 2020 2074 6872 6573 203d 2074         thres = t
-0003e710: 6f70 6b5f 696d 705b 2d31 5d0a 2020 2020  opk_imp[-1].    
-0003e720: 2020 2020 666f 7220 6772 6f75 702c 2063      for group, c
-0003e730: 685f 6772 6f75 7073 2c20 696d 7020 696e  h_groups, imp in
-0003e740: 2067 6c6f 6261 6c5f 696d 706f 7274 616e   global_importan
-0003e750: 6365 3a0a 2020 2020 2020 2020 2020 2020  ce:.            
-0003e760: 6d6f 6475 6c65 203d 2067 726f 7570 5b30  module = group[0
-0003e770: 5d5b 305d 2e74 6172 6765 742e 6d6f 6475  ][0].target.modu
-0003e780: 6c65 0a20 2020 2020 2020 2020 2020 2070  le.            p
-0003e790: 7275 6e69 6e67 5f66 6e20 3d20 6772 6f75  runing_fn = grou
-0003e7a0: 705b 305d 5b30 5d2e 6861 6e64 6c65 720a  p[0][0].handler.
-0003e7b0: 2020 2020 2020 2020 2020 2020 7072 756e              prun
-0003e7c0: 696e 675f 696e 6469 6365 7320 3d20 2869  ing_indices = (i
-0003e7d0: 6d70 203c 3d20 7468 7265 7329 2e6e 6f6e  mp <= thres).non
-0003e7e0: 7a65 726f 2829 2e76 6965 7728 2d31 290a  zero().view(-1).
-0003e7f0: 2020 2020 2020 2020 2020 2020 6966 2063              if c
-0003e800: 685f 6772 6f75 7073 203e 2031 3a0a 2020  h_groups > 1:.  
-0003e810: 2020 2020 2020 2020 2020 2020 2020 6772                gr
-0003e820: 6f75 705f 7369 7a65 203d 2073 656c 662e  oup_size = self.
-0003e830: 4447 2e67 6574 5f6f 7574 5f63 6861 6e6e  DG.get_out_chann
-0003e840: 656c 7328 6d6f 6475 6c65 292f 2f63 685f  els(module)//ch_
-0003e850: 6772 6f75 7073 0a20 2020 2020 2020 2020  groups.         
-0003e860: 2020 2020 2020 2070 7275 6e69 6e67 5f69         pruning_i
-0003e870: 6e64 6963 6573 203d 2074 6f72 6368 2e63  ndices = torch.c
-0003e880: 6174 280a 2020 2020 2020 2020 2020 2020  at(.            
-0003e890: 2020 2020 2020 2020 5b70 7275 6e69 6e67          [pruning
-0003e8a0: 5f69 6e64 6963 6573 2b67 726f 7570 5f73  _indices+group_s
-0003e8b0: 697a 652a 6920 666f 7220 6920 696e 2072  ize*i for i in r
-0003e8c0: 616e 6765 2863 685f 6772 6f75 7073 295d  ange(ch_groups)]
-0003e8d0: 2c20 3029 0a20 2020 2020 2020 2020 2020  , 0).           
-0003e8e0: 2069 6620 7365 6c66 2e72 6f75 6e64 5f74   if self.round_t
-0003e8f0: 6f3a 0a20 2020 2020 2020 2020 2020 2020  o:.             
-0003e900: 2020 206e 5f70 7275 6e65 6420 3d20 6c65     n_pruned = le
-0003e910: 6e28 7072 756e 696e 675f 696e 6469 6365  n(pruning_indice
-0003e920: 7329 0a20 2020 2020 2020 2020 2020 2020  s).             
-0003e930: 2020 206e 5f70 7275 6e65 6420 3d20 6e5f     n_pruned = n_
-0003e940: 7072 756e 6564 202d 2028 6e5f 7072 756e  pruned - (n_prun
-0003e950: 6564 2025 2073 656c 662e 726f 756e 645f  ed % self.round_
-0003e960: 746f 290a 2020 2020 2020 2020 2020 2020  to).            
-0003e970: 2020 2020 7072 756e 696e 675f 696e 6469      pruning_indi
-0003e980: 6365 7320 3d20 7072 756e 696e 675f 696e  ces = pruning_in
-0003e990: 6469 6365 735b 3a6e 5f70 7275 6e65 645d  dices[:n_pruned]
-0003e9a0: 0a20 2020 2020 2020 2020 2020 2067 726f  .            gro
-0003e9b0: 7570 203d 2073 656c 662e 4447 2e67 6574  up = self.DG.get
-0003e9c0: 5f70 7275 6e69 6e67 5f67 726f 7570 280a  _pruning_group(.
-0003e9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0003e9e0: 6d6f 6475 6c65 2c20 7072 756e 696e 675f  module, pruning_
-0003e9f0: 666e 2c20 7072 756e 696e 675f 696e 6469  fn, pruning_indi
-0003ea00: 6365 732e 746f 6c69 7374 2829 290a 2020  ces.tolist()).  
-0003ea10: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-0003ea20: 662e 4447 2e63 6865 636b 5f70 7275 6e69  f.DG.check_pruni
-0003ea30: 6e67 5f67 726f 7570 2867 726f 7570 293a  ng_group(group):
-0003ea40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0003ea50: 2079 6965 6c64 2067 726f 7570 0a00 0000   yield group....
-0003ea60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003ea70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003ea80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003ea90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003eaa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+0003e530: 2020 2020 2020 2069 6620 6c65 6e28 676c         if len(gl
+0003e540: 6f62 616c 5f69 6d70 6f72 7461 6e63 6529  obal_importance)
+0003e550: 203d 3d20 303a 0a20 2020 2020 2020 2020   == 0:.         
+0003e560: 2020 2072 6574 7572 6e0a 2020 2020 2020     return.      
+0003e570: 2020 2020 2020 2020 2020 0a20 2020 2020            .     
+0003e580: 2020 2069 6d70 203d 2074 6f72 6368 2e63     imp = torch.c
+0003e590: 6174 285b 6c6f 6361 6c5f 696d 705b 2d31  at([local_imp[-1
+0003e5a0: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
+0003e5b0: 2020 2020 2020 2020 2020 666f 7220 6c6f            for lo
+0003e5c0: 6361 6c5f 696d 7020 696e 2067 6c6f 6261  cal_imp in globa
+0003e5d0: 6c5f 696d 706f 7274 616e 6365 5d2c 2064  l_importance], d
+0003e5e0: 696d 3d30 290a 2020 2020 2020 2020 7461  im=0).        ta
+0003e5f0: 7267 6574 5f73 7061 7273 6974 7920 3d20  rget_sparsity = 
+0003e600: 7365 6c66 2e70 6572 5f73 7465 705f 6368  self.per_step_ch
+0003e610: 5f73 7061 7273 6974 795b 7365 6c66 2e63  _sparsity[self.c
+0003e620: 7572 7265 6e74 5f73 7465 705d 0a20 2020  urrent_step].   
+0003e630: 2020 2020 206e 5f70 7275 6e65 6420 3d20       n_pruned = 
+0003e640: 6c65 6e28 696d 7029 202d 2069 6e74 280a  len(imp) - int(.
+0003e650: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0003e660: 2e69 6e69 7469 616c 5f74 6f74 616c 5f63  .initial_total_c
+0003e670: 6861 6e6e 656c 7320 2a0a 2020 2020 2020  hannels *.      
+0003e680: 2020 2020 2020 2831 202d 2074 6172 6765        (1 - targe
+0003e690: 745f 7370 6172 7369 7479 290a 2020 2020  t_sparsity).    
+0003e6a0: 2020 2020 290a 2020 2020 2020 2020 6966      ).        if
+0003e6b0: 206e 5f70 7275 6e65 6420 3c3d 2030 3a0a   n_pruned <= 0:.
+0003e6c0: 2020 2020 2020 2020 2020 2020 7265 7475              retu
+0003e6d0: 726e 0a20 2020 2020 2020 2074 6f70 6b5f  rn.        topk_
+0003e6e0: 696d 702c 205f 203d 2074 6f72 6368 2e74  imp, _ = torch.t
+0003e6f0: 6f70 6b28 696d 702c 206b 3d6e 5f70 7275  opk(imp, k=n_pru
+0003e700: 6e65 642c 206c 6172 6765 7374 3d46 616c  ned, largest=Fal
+0003e710: 7365 290a 2020 2020 2020 2020 0a20 2020  se).        .   
+0003e720: 2020 2020 2023 2067 6c6f 6261 6c20 7072       # global pr
+0003e730: 756e 696e 6720 7468 726f 7567 6820 7468  uning through th
+0003e740: 7265 7368 6f6c 6469 6e67 0a20 2020 2020  resholding.     
+0003e750: 2020 2074 6872 6573 203d 2074 6f70 6b5f     thres = topk_
+0003e760: 696d 705b 2d31 5d0a 2020 2020 2020 2020  imp[-1].        
+0003e770: 666f 7220 6772 6f75 702c 2063 685f 6772  for group, ch_gr
+0003e780: 6f75 7073 2c20 696d 7020 696e 2067 6c6f  oups, imp in glo
+0003e790: 6261 6c5f 696d 706f 7274 616e 6365 3a0a  bal_importance:.
+0003e7a0: 2020 2020 2020 2020 2020 2020 6d6f 6475              modu
+0003e7b0: 6c65 203d 2067 726f 7570 5b30 5d5b 305d  le = group[0][0]
+0003e7c0: 2e74 6172 6765 742e 6d6f 6475 6c65 0a20  .target.module. 
+0003e7d0: 2020 2020 2020 2020 2020 2070 7275 6e69             pruni
+0003e7e0: 6e67 5f66 6e20 3d20 6772 6f75 705b 305d  ng_fn = group[0]
+0003e7f0: 5b30 5d2e 6861 6e64 6c65 720a 2020 2020  [0].handler.    
+0003e800: 2020 2020 2020 2020 7072 756e 696e 675f          pruning_
+0003e810: 696e 6469 6365 7320 3d20 2869 6d70 203c  indices = (imp <
+0003e820: 3d20 7468 7265 7329 2e6e 6f6e 7a65 726f  = thres).nonzero
+0003e830: 2829 2e76 6965 7728 2d31 290a 2020 2020  ().view(-1).    
+0003e840: 2020 2020 2020 2020 6966 2063 685f 6772          if ch_gr
+0003e850: 6f75 7073 203e 2031 3a0a 2020 2020 2020  oups > 1:.      
+0003e860: 2020 2020 2020 2020 2020 6772 6f75 705f            group_
+0003e870: 7369 7a65 203d 2073 656c 662e 4447 2e67  size = self.DG.g
+0003e880: 6574 5f6f 7574 5f63 6861 6e6e 656c 7328  et_out_channels(
+0003e890: 6d6f 6475 6c65 292f 2f63 685f 6772 6f75  module)//ch_grou
+0003e8a0: 7073 0a20 2020 2020 2020 2020 2020 2020  ps.             
+0003e8b0: 2020 2070 7275 6e69 6e67 5f69 6e64 6963     pruning_indic
+0003e8c0: 6573 203d 2074 6f72 6368 2e63 6174 280a  es = torch.cat(.
+0003e8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0003e8e0: 2020 2020 5b70 7275 6e69 6e67 5f69 6e64      [pruning_ind
+0003e8f0: 6963 6573 2b67 726f 7570 5f73 697a 652a  ices+group_size*
+0003e900: 6920 666f 7220 6920 696e 2072 616e 6765  i for i in range
+0003e910: 2863 685f 6772 6f75 7073 295d 2c20 3029  (ch_groups)], 0)
+0003e920: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0003e930: 7365 6c66 2e72 6f75 6e64 5f74 6f3a 0a20  self.round_to:. 
+0003e940: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+0003e950: 5f70 7275 6e65 6420 3d20 6c65 6e28 7072  _pruned = len(pr
+0003e960: 756e 696e 675f 696e 6469 6365 7329 0a20  uning_indices). 
+0003e970: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+0003e980: 5f70 7275 6e65 6420 3d20 6e5f 7072 756e  _pruned = n_prun
+0003e990: 6564 202d 2028 6e5f 7072 756e 6564 2025  ed - (n_pruned %
+0003e9a0: 2073 656c 662e 726f 756e 645f 746f 290a   self.round_to).
+0003e9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0003e9c0: 7072 756e 696e 675f 696e 6469 6365 7320  pruning_indices 
+0003e9d0: 3d20 7072 756e 696e 675f 696e 6469 6365  = pruning_indice
+0003e9e0: 735b 3a6e 5f70 7275 6e65 645d 0a20 2020  s[:n_pruned].   
+0003e9f0: 2020 2020 2020 2020 2067 726f 7570 203d           group =
+0003ea00: 2073 656c 662e 4447 2e67 6574 5f70 7275   self.DG.get_pru
+0003ea10: 6e69 6e67 5f67 726f 7570 280a 2020 2020  ning_group(.    
+0003ea20: 2020 2020 2020 2020 2020 2020 6d6f 6475              modu
+0003ea30: 6c65 2c20 7072 756e 696e 675f 666e 2c20  le, pruning_fn, 
+0003ea40: 7072 756e 696e 675f 696e 6469 6365 732e  pruning_indices.
+0003ea50: 746f 6c69 7374 2829 290a 2020 2020 2020  tolist()).      
+0003ea60: 2020 2020 2020 6966 2073 656c 662e 4447        if self.DG
+0003ea70: 2e63 6865 636b 5f70 7275 6e69 6e67 5f67  .check_pruning_g
+0003ea80: 726f 7570 2867 726f 7570 293a 0a20 2020  roup(group):.   
+0003ea90: 2020 2020 2020 2020 2020 2020 2079 6965               yie
+0003eaa0: 6c64 2067 726f 7570 0a00 0000 0000 0000  ld group........
 0003eab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ead0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eaf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eb00: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eb10: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -16090,16 +16090,16 @@
 0003ed90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eda0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003edb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003edc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003edd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ede0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003edf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003ee00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0003ee10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0003ee00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0003ee10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0003ee20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ee30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ee40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ee50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ee60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ee70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003ee80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -16123,23 +16123,23 @@
 0003efa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003efb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003efc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003efd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003efe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003eff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0003f010: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0003f010: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0003f020: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
 0003f030: 686d 732f 7363 6865 6475 6c65 722e 7079  hms/scheduler.py
 0003f040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0003f070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0003f080: 3030 3030 3137 3400 3134 3433 3431 3434  0000174.14434144
-0003f090: 3037 3100 3032 3436 3232 0020 3000 0000  071.024622. 0...
+0003f080: 3030 3030 3137 3400 3134 3434 3632 3331  0000174.14446231
+0003f090: 3335 3300 3032 3436 3236 0020 3000 0000  353.024626. 0...
 0003f0a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f0b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f0c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f0d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f0e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f0f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -16218,16 +16218,16 @@
 0003f590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f5a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f5b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f5c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f5d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f5e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f5f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0003f600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0003f610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0003f600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0003f610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0003f620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -16251,23 +16251,23 @@
 0003f7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0003f810: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0003f810: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0003f820: 672f 7072 756e 6572 2f66 756e 6374 696f  g/pruner/functio
 0003f830: 6e2e 7079 0000 0000 0000 0000 0000 0000  n.py............
 0003f840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0003f870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0003f880: 3030 3437 3436 3700 3134 3433 3431 3434  0047467.14434144
-0003f890: 3037 3100 3032 3233 3430 0020 3000 0000  071.022340. 0...
+0003f880: 3030 3437 3437 3000 3134 3434 3632 3331  0047470.14446231
+0003f890: 3335 3300 3032 3233 3336 0020 3000 0000  353.022336. 0...
 0003f8a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f8b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f8c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f8d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f8e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f8f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0003f900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -17263,15 +17263,15 @@
 000436e0: 7261 6d65 7465 725f 616e 645f 6772 6164  rameter_and_grad
 000436f0: 286c 6179 6572 2e71 5f70 726f 6a5f 7765  (layer.q_proj_we
 00043700: 6967 6874 2c20 6b65 6570 5f69 6478 732c  ight, keep_idxs,
 00043710: 2030 290a 2020 2020 2020 2020 6966 206c   0).        if l
 00043720: 6179 6572 2e6b 5f70 726f 6a5f 7765 6967  ayer.k_proj_weig
 00043730: 6874 2069 7320 6e6f 7420 4e6f 6e65 3a0a  ht is not None:.
 00043740: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
-00043750: 722e 715f 7072 6f6a 5f77 6569 6768 7420  r.q_proj_weight 
+00043750: 722e 6b5f 7072 6f6a 5f77 6569 6768 7420  r.k_proj_weight 
 00043760: 3d20 7365 6c66 2e5f 7072 756e 655f 7061  = self._prune_pa
 00043770: 7261 6d65 7465 725f 616e 645f 6772 6164  rameter_and_grad
 00043780: 286c 6179 6572 2e6b 5f70 726f 6a5f 7765  (layer.k_proj_we
 00043790: 6967 6874 2c20 6b65 6570 5f69 6478 732c  ight, keep_idxs,
 000437a0: 2030 290a 2020 2020 2020 2020 6966 206c   0).        if l
 000437b0: 6179 6572 2e76 5f70 726f 6a5f 7765 6967  ayer.v_proj_weig
 000437c0: 6874 2069 7320 6e6f 7420 4e6f 6e65 3a0a  ht is not None:.
@@ -17549,15 +17549,15 @@
 000448c0: 5072 756e 6572 426f 785b 6f70 732e 4f50  PrunerBox[ops.OP
 000448d0: 5459 5045 2e49 4e5d 2e70 7275 6e65 5f6f  TYPE.IN].prune_o
 000448e0: 7574 5f63 6861 6e6e 656c 730a 7072 756e  ut_channels.prun
 000448f0: 655f 696e 7374 616e 6365 6e6f 726d 5f69  e_instancenorm_i
 00044900: 6e5f 6368 616e 6e65 6c73 203d 2050 7275  n_channels = Pru
 00044910: 6e65 7242 6f78 5b6f 7073 2e4f 5054 5950  nerBox[ops.OPTYP
 00044920: 452e 494e 5d2e 7072 756e 655f 696e 5f63  E.IN].prune_in_c
-00044930: 6861 6e6e 656c 7300 0000 0000 0000 0000  hannels.........
+00044930: 6861 6e6e 656c 730a 0000 0000 0000 0000  hannels.........
 00044940: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044950: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044960: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044970: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044980: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000449a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -17594,16 +17594,16 @@
 00044b90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00044c00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00044c10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00044c00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00044c10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00044c20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044c30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044c60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044c70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044c80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -17627,23 +17627,23 @@
 00044da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044e00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00044e10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00044e10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00044e20: 672f 7365 7269 616c 697a 6174 696f 6e2e  g/serialization.
 00044e30: 7079 0000 0000 0000 0000 0000 0000 0000  py..............
 00044e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044e60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00044e70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00044e80: 3030 3032 3636 3500 3134 3433 3431 3434  0002665.14434144
-00044e90: 3037 3100 3032 3230 3434 0020 3000 0000  071.022044. 0...
+00044e80: 3030 3032 3636 3500 3134 3434 3632 3331  0002665.14446231
+00044e90: 3335 3300 3032 3230 3530 0020 3000 0000  353.022050. 0...
 00044ea0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044eb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044ec0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044ed0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044ee0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044ef0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00044f00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -17786,16 +17786,16 @@
 00045790: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000457a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000457b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000457c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000457d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000457e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000457f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00045800: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-00045810: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+00045800: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+00045810: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 00045820: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045860: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045870: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045880: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -17819,23 +17819,23 @@
 000459a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000459b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000459c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000459d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000459e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000459f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045a00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00045a10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00045a10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00045a20: 672f 7574 696c 732f 0000 0000 0000 0000  g/utils/........
 00045a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045a60: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 00045a70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00045a80: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-00045a90: 3131 3000 3031 3737 3336 0020 3500 0000  110.017736. 5...
+00045a80: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+00045a90: 3336 3700 3031 3737 3535 0020 3500 0000  367.017755. 5...
 00045aa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045ab0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045ac0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045ad0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045ae0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045af0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045b00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -17882,16 +17882,16 @@
 00045d90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045da0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045db0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045dc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045dd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045de0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045df0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00045e00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00045e10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00045e00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00045e10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00045e20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045e30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045e40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045e50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045e60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045e70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045e80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -17915,23 +17915,23 @@
 00045fa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045fb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045fc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045fd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045fe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00045ff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046000: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00046010: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00046010: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00046020: 672f 7574 696c 732f 5f5f 696e 6974 5f5f  g/utils/__init__
 00046030: 2e70 7900 0000 0000 0000 0000 0000 0000  .py.............
 00046040: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046050: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046060: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00046070: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00046080: 3030 3030 3130 3100 3134 3433 3431 3434  0000101.14434144
-00046090: 3037 3100 3032 3230 3435 0020 3000 0000  071.022045. 0...
+00046080: 3030 3030 3130 3100 3134 3434 3632 3331  0000101.14446231
+00046090: 3335 3300 3032 3230 3531 0020 3000 0000  353.022051. 0...
 000460a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000460b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000460c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000460d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000460e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000460f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046100: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -18010,16 +18010,16 @@
 00046590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000465a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000465b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000465c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000465d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000465e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000465f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00046600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00046610: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+00046600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00046610: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 00046620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -18043,23 +18043,23 @@
 000467a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000467b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000467c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000467d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000467e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000467f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00046810: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00046810: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00046820: 672f 7574 696c 732f 6f70 5f63 6f75 6e74  g/utils/op_count
 00046830: 6572 2e70 7900 0000 0000 0000 0000 0000  er.py...........
 00046840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00046870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00046880: 3030 3336 3734 3700 3134 3433 3431 3434  0036747.14434144
-00046890: 3037 3100 3032 3235 3134 0020 3000 0000  071.022514. 0...
+00046880: 3030 3336 3734 3700 3134 3434 3632 3331  0036747.14446231
+00046890: 3335 3300 3032 3235 3230 0020 3000 0000  353.022520. 0...
 000468a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000468b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000468c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000468d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000468e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000468f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00046900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -19098,16 +19098,16 @@
 0004a990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004a9a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004a9b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004a9c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004a9d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004a9e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004a9f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0004aa00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0004aa10: 3838 392e 300a 0000 0000 0000 0000 0000  889.0...........
+0004aa00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0004aa10: 3634 332e 300a 0000 0000 0000 0000 0000  643.0...........
 0004aa20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004aa30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004aa40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004aa50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004aa60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004aa70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004aa80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -19131,23 +19131,23 @@
 0004aba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004abb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004abc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004abd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004abe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004abf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ac00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0004ac10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0004ac10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0004ac20: 672f 7574 696c 732f 7574 696c 732e 7079  g/utils/utils.py
 0004ac30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ac40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ac50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ac60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0004ac70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0004ac80: 3030 3132 3636 3500 3134 3433 3431 3434  0012665.14434144
-0004ac90: 3037 3100 3032 3134 3730 0020 3000 0000  071.021470. 0...
+0004ac80: 3030 3132 3636 3500 3134 3434 3632 3331  0012665.14446231
+0004ac90: 3335 3300 3032 3134 3734 0020 3000 0000  353.021474. 0...
 0004aca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004acb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004acc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004acd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ace0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004acf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ad00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -19546,16 +19546,16 @@
 0004c590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c5a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c5b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c5c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c5d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c5e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c5f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0004c600: 3238 206d 7469 6d65 3d31 3638 3531 3132  28 mtime=1685112
-0004c610: 3930 342e 3035 3031 3434 340a 0000 0000  904.0501444.....
+0004c600: 3238 206d 7469 6d65 3d31 3638 3737 3631  28 mtime=1687761
+0004c610: 3635 352e 3239 3434 3231 340a 0000 0000  655.2944214.....
 0004c620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -19579,23 +19579,23 @@
 0004c7a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c7b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c7c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c7d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c7e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c7f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0004c810: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0004c810: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0004c820: 672e 6567 672d 696e 666f 2f00 0000 0000  g.egg-info/.....
 0004c830: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c860: 0000 0000 3030 3030 3735 3500 3030 3031  ....0000755.0001
 0004c870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0004c880: 3030 3030 3030 3000 3134 3433 3431 3434  0000000.14434144
-0004c890: 3131 3000 3032 3032 3730 0020 3500 0000  110.020270. 5...
+0004c880: 3030 3030 3030 3000 3134 3434 3632 3331  0000000.14446231
+0004c890: 3336 3700 3032 3033 3037 0020 3500 0000  367.020307. 5...
 0004c8a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c8b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c8c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c8d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c8e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c8f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004c900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -19642,16 +19642,16 @@
 0004cb90: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cbb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cbc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cbd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cbe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cbf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-0004cc00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-0004cc10: 3930 342e 300a 0000 0000 0000 0000 0000  904.0...........
+0004cc00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+0004cc10: 3635 352e 300a 0000 0000 0000 0000 0000  655.0...........
 0004cc20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cc30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cc40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cc50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cc60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cc70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cc80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -19675,23 +19675,23 @@
 0004cda0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cdb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cdc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cdd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cde0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cdf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ce00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-0004ce10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+0004ce10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 0004ce20: 672e 6567 672d 696e 666f 2f50 4b47 2d49  g.egg-info/PKG-I
 0004ce30: 4e46 4f00 0000 0000 0000 0000 0000 0000  NFO.............
 0004ce40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ce50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ce60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 0004ce70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-0004ce80: 3030 3531 3031 3300 3134 3433 3431 3434  0051013.14434144
-0004ce90: 3131 3000 3032 3133 3635 0020 3000 0000  110.021365. 0...
+0004ce80: 3030 3531 3037 3600 3134 3434 3632 3331  0051076.14446231
+0004ce90: 3336 3700 3032 3134 3135 0020 3000 0000  367.021415. 0...
 0004cea0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ceb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cec0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004ced0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cee0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cef0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cf00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -19709,15 +19709,15 @@
 0004cfc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cfd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cfe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004cff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 0004d000: 4d65 7461 6461 7461 2d56 6572 7369 6f6e  Metadata-Version
 0004d010: 3a20 322e 310a 4e61 6d65 3a20 746f 7263  : 2.1.Name: torc
 0004d020: 682d 7072 756e 696e 670a 5665 7273 696f  h-pruning.Versio
-0004d030: 6e3a 2031 2e31 2e38 0a53 756d 6d61 7279  n: 1.1.8.Summary
+0004d030: 6e3a 2031 2e31 2e39 0a53 756d 6d61 7279  n: 1.1.9.Summary
 0004d040: 3a20 5374 7275 6374 7572 616c 2050 7275  : Structural Pru
 0004d050: 6e69 6e67 2066 6f72 204d 6f64 656c 2041  ning for Model A
 0004d060: 6363 656c 6572 6174 696f 6e2e 0a48 6f6d  cceleration..Hom
 0004d070: 652d 7061 6765 3a20 6874 7470 733a 2f2f  e-page: https://
 0004d080: 6769 7468 7562 2e63 6f6d 2f56 6169 6e46  github.com/VainF
 0004d090: 2f54 6f72 6368 2d50 7275 6e69 6e67 0a41  /Torch-Pruning.A
 0004d0a0: 7574 686f 723a 2047 6f6e 6766 616e 2046  uthor: Gongfan F
@@ -19794,15 +19794,15 @@
 0004d510: 2068 7265 663d 2268 7474 7073 3a2f 2f67   href="https://g
 0004d520: 6974 6875 622e 636f 6d2f 5661 696e 462f  ithub.com/VainF/
 0004d530: 546f 7263 682d 5072 756e 696e 672f 7265  Torch-Pruning/re
 0004d540: 6c65 6173 6573 2f6c 6174 6573 7422 3e3c  leases/latest"><
 0004d550: 696d 6720 7372 633d 2268 7474 7073 3a2f  img src="https:/
 0004d560: 2f69 6d67 2e73 6869 656c 6473 2e69 6f2f  /img.shields.io/
 0004d570: 6261 6467 652f 4c61 7465 7374 2532 3056  badge/Latest%20V
-0004d580: 6572 7369 6f6e 2d31 2e31 2e38 2d33 6635  ersion-1.1.8-3f5
+0004d580: 6572 7369 6f6e 2d31 2e31 2e39 2d33 6635  ersion-1.1.9-3f5
 0004d590: 3162 352e 7376 6722 2061 6c74 3d22 4c61  1b5.svg" alt="La
 0004d5a0: 7465 7374 2056 6572 7369 6f6e 223e 3c2f  test Version"></
 0004d5b0: 613e 0a20 203c 6120 6872 6566 3d22 6874  a>.  <a href="ht
 0004d5c0: 7470 733a 2f2f 636f 6c61 622e 7265 7365  tps://colab.rese
 0004d5d0: 6172 6368 2e67 6f6f 676c 652e 636f 6d2f  arch.google.com/
 0004d5e0: 6472 6976 652f 3154 5276 454c 5144 4e6a  drive/1TRvELQDNj
 0004d5f0: 3950 774d 2d45 4552 5762 4633 4951 4f79  9PwM-EERWbF3IQOy
@@ -19962,1074 +19962,1074 @@
 0004df90: 3230 3233 2e30 342e 3231 204a 6f69 6e20  2023.04.21 Join 
 0004dfa0: 6f75 7220 5465 6c65 6772 616d 206f 7220  our Telegram or 
 0004dfb0: 5765 6368 6174 2067 726f 7570 2066 6f72  Wechat group for
 0004dfc0: 2063 6173 7561 6c20 6469 7363 7573 7369   casual discussi
 0004dfd0: 6f6e 733a 0a20 202a 2054 656c 6567 7261  ons:.  * Telegra
 0004dfe0: 6d3a 2068 7474 7073 3a2f 2f74 2e6d 652f  m: https://t.me/
 0004dff0: 2b4e 776a 6242 444e 3261 6f31 6c5a 6a5a  +NwjbBDN2ao1lZjZ
-0004e000: 6c0a 2020 2a20 5765 6368 6174 3a20 3c69  l.  * Wechat: <i
+0004e000: 6c0a 2020 2a20 5765 4368 6174 3a20 3c69  l.  * WeChat: <i
 0004e010: 6d67 2077 6964 7468 3d22 3130 3022 2061  mg width="100" a
 0004e020: 6c74 3d22 696d 6167 6522 2073 7263 3d22  lt="image" src="
 0004e030: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
 0004e040: 6f6d 2f56 6169 6e46 2f54 6f72 6368 2d50  om/VainF/Torch-P
 0004e050: 7275 6e69 6e67 2f61 7373 6574 732f 3138  runing/assets/18
-0004e060: 3539 3232 3131 2f35 3932 6161 3035 342d  592211/592aa054-
-0004e070: 3762 3737 2d34 3334 632d 3935 3931 2d32  7b77-434c-9591-2
-0004e080: 3762 3633 3538 3439 6535 3322 3e0a 0a50  7b635849e53">..P
-0004e090: 6c65 6173 6520 646f 206e 6f74 2068 6573  lease do not hes
-0004e0a0: 6974 6174 6520 746f 206f 7065 6e20 6120  itate to open a 
-0004e0b0: 5b64 6973 6375 7373 696f 6e5d 2868 7474  [discussion](htt
-0004e0c0: 7073 3a2f 2f67 6974 6875 622e 636f 6d2f  ps://github.com/
-0004e0d0: 5661 696e 462f 546f 7263 682d 5072 756e  VainF/Torch-Prun
-0004e0e0: 696e 672f 6469 7363 7573 7369 6f6e 7329  ing/discussions)
-0004e0f0: 206f 7220 5b69 7373 7565 5d28 6874 7470   or [issue](http
-0004e100: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
-0004e110: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
-0004e120: 6e67 2f69 7373 7565 7329 2069 6620 796f  ng/issues) if yo
-0004e130: 7520 656e 636f 756e 7465 7220 616e 7920  u encounter any 
-0004e140: 7072 6f62 6c65 6d73 2077 6974 6820 7468  problems with th
-0004e150: 6520 6c69 6272 6172 7920 6f72 2074 6865  e library or the
-0004e160: 2070 6170 6572 2e0a 0a0a 2323 2320 2a2a   paper....### **
-0004e170: 4665 6174 7572 6573 3a2a 2a0a 2d20 5b78  Features:**.- [x
-0004e180: 5d20 5374 7275 6374 7572 616c 2070 7275  ] Structural pru
-0004e190: 6e69 6e67 2066 6f72 2043 4e4e 732c 2054  ning for CNNs, T
-0004e1a0: 7261 6e73 666f 726d 6572 732c 2044 6574  ransformers, Det
-0004e1b0: 6563 746f 7273 2c20 4c61 6e67 7561 6765  ectors, Language
-0004e1c0: 204d 6f64 656c 7320 616e 6420 4469 6666   Models and Diff
-0004e1d0: 7573 696f 6e20 4d6f 6465 6c73 2e20 506c  usion Models. Pl
-0004e1e0: 6561 7365 2072 6566 6572 2074 6f20 7468  ease refer to th
-0004e1f0: 6520 5b50 7275 6e61 6269 6c69 7479 2042  e [Prunability B
-0004e200: 656e 6368 6d61 726b 5d28 6265 6e63 686d  enchmark](benchm
-0004e210: 6172 6b73 2f70 7275 6e61 6269 6c69 7479  arks/prunability
-0004e220: 292e 0a2d 205b 785d 2048 6967 682d 6c65  )..- [x] High-le
-0004e230: 7665 6c20 7072 756e 6572 733a 205b 4d61  vel pruners: [Ma
-0004e240: 676e 6974 7564 6550 7275 6e65 725d 2868  gnitudePruner](h
-0004e250: 7474 7073 3a2f 2f61 7278 6976 2e6f 7267  ttps://arxiv.org
-0004e260: 2f61 6273 2f31 3630 382e 3038 3731 3029  /abs/1608.08710)
-0004e270: 2c20 5b42 4e53 6361 6c65 5072 756e 6572  , [BNScalePruner
-0004e280: 5d28 6874 7470 733a 2f2f 6172 7869 762e  ](https://arxiv.
-0004e290: 6f72 672f 6162 732f 3137 3038 2e30 3635  org/abs/1708.065
-0004e2a0: 3139 292c 205b 4772 6f75 704e 6f72 6d50  19), [GroupNormP
-0004e2b0: 7275 6e65 725d 2868 7474 7073 3a2f 2f61  runer](https://a
-0004e2c0: 7278 6976 2e6f 7267 2f61 6273 2f32 3330  rxiv.org/abs/230
-0004e2d0: 312e 3132 3930 3029 2c20 5261 6e64 6f6d  1.12900), Random
-0004e2e0: 5072 756e 6572 2c20 6574 632e 0a2d 205b  Pruner, etc..- [
-0004e2f0: 785d 2049 6d70 6f72 7461 6e63 6520 4372  x] Importance Cr
-0004e300: 6974 6572 6961 3a20 4c2d 7020 4e6f 726d  iteria: L-p Norm
-0004e310: 2c20 5461 796c 6f72 2c20 5261 6e64 6f6d  , Taylor, Random
-0004e320: 2c20 424e 5363 616c 696e 672c 2065 7463  , BNScaling, etc
-0004e330: 2e0a 2d20 5b78 5d20 4465 7065 6e64 656e  ..- [x] Dependen
-0004e340: 6379 2047 7261 7068 2066 6f72 2064 6570  cy Graph for dep
-0004e350: 656e 6465 6e63 7920 6d6f 6465 6c69 6e67  endency modeling
-0004e360: 2e0a 2d20 5b78 5d20 5375 7070 6f72 7465  ..- [x] Supporte
-0004e370: 6420 6d6f 6475 6c65 733a 204c 696e 6561  d modules: Linea
-0004e380: 722c 2028 5472 616e 7370 6f73 6564 2920  r, (Transposed) 
-0004e390: 436f 6e76 2c20 4e6f 726d 616c 697a 6174  Conv, Normalizat
-0004e3a0: 696f 6e2c 2050 5265 4c55 2c20 456d 6265  ion, PReLU, Embe
-0004e3b0: 6464 696e 672c 204d 756c 7469 6865 6164  dding, Multihead
-0004e3c0: 4174 7465 6e74 696f 6e2c 206e 6e2e 5061  Attention, nn.Pa
-0004e3d0: 7261 6d65 7465 7273 2061 6e64 205b 6375  rameters and [cu
-0004e3e0: 7374 6f6d 697a 6564 206d 6f64 756c 6573  stomized modules
-0004e3f0: 5d28 7465 7374 732f 7465 7374 5f63 7573  ](tests/test_cus
-0004e400: 746f 6d69 7a65 645f 6c61 7965 722e 7079  tomized_layer.py
-0004e410: 292e 0a2d 205b 785d 2053 7570 706f 7274  )..- [x] Support
-0004e420: 6564 206f 7065 7261 746f 7273 3a20 7370  ed operators: sp
-0004e430: 6c69 742c 2063 6f6e 6361 7465 6e61 7469  lit, concatenati
-0004e440: 6f6e 2c20 736b 6970 2063 6f6e 6e65 6374  on, skip connect
-0004e450: 696f 6e2c 2066 6c61 7474 656e 2c20 7265  ion, flatten, re
-0004e460: 7368 6170 652c 2076 6965 772c 2061 6c6c  shape, view, all
-0004e470: 2065 6c65 6d65 6e74 2d77 6973 6520 6f70   element-wise op
-0004e480: 732c 2065 7463 2e0a 2d20 5b78 5d20 5b4c  s, etc..- [x] [L
-0004e490: 6f77 2d6c 6576 656c 2070 7275 6e69 6e67  ow-level pruning
-0004e4a0: 2066 756e 6374 696f 6e73 5d28 746f 7263   functions](torc
-0004e4b0: 685f 7072 756e 696e 672f 7072 756e 6572  h_pruning/pruner
-0004e4c0: 2f66 756e 6374 696f 6e2e 7079 290a 2d20  /function.py).- 
-0004e4d0: 5b78 5d20 5b42 656e 6368 6d61 726b 735d  [x] [Benchmarks]
-0004e4e0: 2862 656e 6368 6d61 726b 7329 2061 6e64  (benchmarks) and
-0004e4f0: 205b 7475 746f 7269 616c 735d 2874 7574   [tutorials](tut
-0004e500: 6f72 6961 6c73 290a 2d20 5b78 5d20 4120  orials).- [x] A 
-0004e510: 5b72 6573 6f75 7263 6520 6c69 7374 5d28  [resource list](
-0004e520: 7072 6163 7469 6361 6c5f 7374 7275 6374  practical_struct
-0004e530: 7572 616c 5f70 7275 6e69 6e67 2e6d 6429  ural_pruning.md)
-0004e540: 2066 6f72 2070 7261 6374 6963 616c 2073   for practical s
-0004e550: 7472 7563 7472 7561 6c20 7072 756e 696e  tructrual prunin
-0004e560: 672e 0a20 200a 2323 2320 2a2a 544f 444f  g..  .### **TODO
-0004e570: 204c 6973 743a 2a2a 0a2d 205b 205d 2041   List:**.- [ ] A
-0004e580: 2073 7472 6f6e 6720 6261 7365 6c69 6e65   strong baseline
-0004e590: 2077 6974 6820 6261 6773 206f 6620 7472   with bags of tr
-0004e5a0: 6963 6b73 2066 726f 6d20 6578 6973 7469  icks from existi
-0004e5b0: 6e67 206d 6574 686f 6473 2e0a 2d20 5b20  ng methods..- [ 
-0004e5c0: 5d20 4120 6265 6e63 686d 6172 6b20 666f  ] A benchmark fo
-0004e5d0: 7220 5b54 6f72 6368 7669 7369 6f6e 5d28  r [Torchvision](
-0004e5e0: 6874 7470 733a 2f2f 7079 746f 7263 682e  https://pytorch.
-0004e5f0: 6f72 672f 7669 7369 6f6e 2f73 7461 626c  org/vision/stabl
-0004e600: 652f 6d6f 6465 6c73 2e68 746d 6c29 2063  e/models.html) c
-0004e610: 6f6d 7061 7469 6269 6c69 7479 2028 2a2a  ompatibility (**
-0004e620: 3831 2f38 353d 3935 2e33 252a 2a2c 203a  81/85=95.3%**, :
-0004e630: 6865 6176 795f 6368 6563 6b5f 6d61 726b  heavy_check_mark
-0004e640: 3a29 2061 6e64 205b 7469 6d6d 5d28 6874  :) and [timm](ht
-0004e650: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-0004e660: 2f68 7567 6769 6e67 6661 6365 2f70 7974  /huggingface/pyt
-0004e670: 6f72 6368 2d69 6d61 6765 2d6d 6f64 656c  orch-image-model
-0004e680: 7329 2063 6f6d 7061 7469 6269 6c69 7479  s) compatibility
-0004e690: 2e0a 2d20 5b20 5d20 5072 756e 696e 6720  ..- [ ] Pruning 
-0004e6a0: 6672 6f6d 2053 6372 6174 6368 202f 2061  from Scratch / a
-0004e6b0: 7420 496e 6974 6961 6c69 7a61 7469 6f6e  t Initialization
-0004e6c0: 2e0a 2d20 5b20 5d20 4d6f 7265 2068 6967  ..- [ ] More hig
-0004e6d0: 682d 6c65 7665 6c20 7072 756e 6572 7320  h-level pruners 
-0004e6e0: 6c69 6b65 205b 4669 7368 6572 5072 756e  like [FisherPrun
-0004e6f0: 6572 5d28 6874 7470 733a 2f2f 6172 7869  er](https://arxi
-0004e700: 762e 6f72 672f 6162 732f 3231 3038 2e30  v.org/abs/2108.0
-0004e710: 3037 3038 292c 205b 4772 6f77 696e 6752  0708), [GrowingR
-0004e720: 6567 5d28 6874 7470 733a 2f2f 6172 7869  eg](https://arxi
-0004e730: 762e 6f72 672f 6162 732f 3230 3132 2e30  v.org/abs/2012.0
-0004e740: 3932 3433 292c 2065 7463 2e0a 2d20 5b20  9243), etc..- [ 
-0004e750: 5d20 4d6f 7265 2054 7261 6e73 666f 726d  ] More Transform
-0004e760: 6572 7320 6c69 6b65 2056 6973 696f 6e20  ers like Vision 
-0004e770: 5472 616e 7366 6f72 6d65 7273 2028 3a68  Transformers (:h
-0004e780: 6561 7679 5f63 6865 636b 5f6d 6172 6b3a  eavy_check_mark:
-0004e790: 292c 2053 7769 6e20 5472 616e 7366 6f72  ), Swin Transfor
-0004e7a0: 6d65 7273 2c20 506f 6f6c 466f 726d 6572  mers, PoolFormer
-0004e7b0: 732e 0a2d 205b 205d 2042 6c6f 636b 2f4c  s..- [ ] Block/L
-0004e7c0: 6179 6572 2f44 6570 7468 2050 7275 6e69  ayer/Depth Pruni
-0004e7d0: 6e67 0a2d 205b 205d 2050 7275 6e69 6e67  ng.- [ ] Pruning
-0004e7e0: 2062 656e 6368 6d61 726b 7320 666f 7220   benchmarks for 
-0004e7f0: 4349 4641 522c 2049 6d61 6765 4e65 7420  CIFAR, ImageNet 
-0004e800: 616e 6420 434f 434f 2e0a 0a23 2320 496e  and COCO...## In
-0004e810: 7374 616c 6c61 7469 6f6e 0a0a 546f 7263  stallation..Torc
-0004e820: 682d 5072 756e 696e 6720 6973 2063 6f6d  h-Pruning is com
-0004e830: 7061 7469 626c 6520 7769 7468 2050 7954  patible with PyT
-0004e840: 6f72 6368 2031 2e78 2061 6e64 2032 2e78  orch 1.x and 2.x
-0004e850: 2e20 2a2a 5079 546f 7263 6820 312e 3132  . **PyTorch 1.12
-0004e860: 2e31 2069 7320 7265 636f 6d6d 656e 6465  .1 is recommende
-0004e870: 6421 2a2a 0a0a 6060 6062 6173 680a 7069  d!**..```bash.pi
-0004e880: 7020 696e 7374 616c 6c20 746f 7263 682d  p install torch-
-0004e890: 7072 756e 696e 6720 2320 7631 2e31 2e38  pruning # v1.1.8
-0004e8a0: 0a60 6060 0a6f 720a 6060 6062 6173 680a  .```.or.```bash.
-0004e8b0: 6769 7420 636c 6f6e 6520 6874 7470 733a  git clone https:
-0004e8c0: 2f2f 6769 7468 7562 2e63 6f6d 2f56 6169  //github.com/Vai
-0004e8d0: 6e46 2f54 6f72 6368 2d50 7275 6e69 6e67  nF/Torch-Pruning
-0004e8e0: 2e67 6974 0a60 6060 0a0a 2323 2051 7569  .git.```..## Qui
-0004e8f0: 636b 7374 6172 740a 2020 0a48 6572 6520  ckstart.  .Here 
-0004e900: 7765 2070 726f 7669 6465 2061 2071 7569  we provide a qui
-0004e910: 636b 2073 7461 7274 2066 6f72 2054 6f72  ck start for Tor
-0004e920: 6368 2d50 7275 6e69 6e67 2e20 4d6f 7265  ch-Pruning. More
-0004e930: 2065 7870 6c61 696e 6564 2064 6574 6169   explained detai
-0004e940: 6c73 2063 616e 2062 6520 666f 756e 6420  ls can be found 
-0004e950: 696e 205b 7475 746f 7261 6c73 5d28 2e2f  in [tutorals](./
-0004e960: 7475 746f 7269 616c 732f 290a 0a23 2323  tutorials/)..###
-0004e970: 2030 2e20 486f 7720 4974 2057 6f72 6b73   0. How It Works
-0004e980: 0a0a 496e 2073 7472 7563 7475 7261 6c20  ..In structural 
-0004e990: 7072 756e 696e 672c 202a 2a60 6047 726f  pruning, **``Gro
-0004e9a0: 7570 6060 2069 7320 7468 6520 6d69 6e69  up`` is the mini
-0004e9b0: 6d61 6c20 7265 6d6f 7661 626c 6520 756e  mal removable un
-0004e9c0: 6974 2077 6974 6869 6e20 6465 6570 206e  it within deep n
-0004e9d0: 6574 776f 726b 732a 2a2e 2045 6163 6820  etworks**. Each 
-0004e9e0: 6772 6f75 7020 636f 6e74 6169 6e73 2073  group contains s
-0004e9f0: 6576 6572 616c 2069 6e74 6572 6465 7065  everal interdepe
-0004ea00: 6e64 656e 7420 6c61 7965 7273 2074 6861  ndent layers tha
-0004ea10: 7420 6d75 7374 2062 6520 7072 756e 6564  t must be pruned
-0004ea20: 2073 696d 756c 7461 6e65 6f75 736c 7920   simultaneously 
-0004ea30: 746f 206d 6169 6e74 6169 6e20 7468 6520  to maintain the 
-0004ea40: 696e 7465 6772 6974 7920 6f66 2074 6865  integrity of the
-0004ea50: 2072 6573 756c 7469 6e67 2073 7472 7563   resulting struc
-0004ea60: 7475 7265 732e 2048 6f77 6576 6572 2c20  tures. However, 
-0004ea70: 6465 6570 206e 6574 776f 726b 7320 6f66  deep networks of
-0004ea80: 7465 6e20 7072 6573 656e 7420 636f 6d70  ten present comp
-0004ea90: 6c65 7820 6465 7065 6e64 656e 6369 6573  lex dependencies
-0004eaa0: 2061 6d6f 6e67 206c 6179 6572 732c 206d   among layers, m
-0004eab0: 616b 696e 6720 7374 7275 6374 7572 616c  aking structural
-0004eac0: 2070 7275 6e69 6e67 2061 2063 6861 6c6c   pruning a chall
-0004ead0: 656e 6769 6e67 2065 6e64 6561 766f 722e  enging endeavor.
-0004eae0: 2054 6869 7320 776f 726b 2061 6464 7265   This work addre
-0004eaf0: 7373 6573 2074 6869 7320 6368 616c 6c65  sses this challe
-0004eb00: 6e67 6520 6279 206f 6666 6572 696e 6720  nge by offering 
-0004eb10: 616e 2061 7574 6f6d 6174 6564 206d 6563  an automated mec
-0004eb20: 6861 6e69 736d 2c20 6060 4465 7047 7261  hanism, ``DepGra
-0004eb30: 7068 6060 2c20 666f 7220 7061 7261 6d65  ph``, for parame
-0004eb40: 7465 7220 6772 6f75 7069 6e67 2c20 7768  ter grouping, wh
-0004eb50: 6963 6820 6661 6369 6c69 7461 7465 7320  ich facilitates 
-0004eb60: 6566 666f 7274 6c65 7373 2070 7275 6e69  effortless pruni
-0004eb70: 6e67 2066 6f72 2061 2077 6964 6520 7261  ng for a wide ra
-0004eb80: 6e67 6520 6f66 2064 6565 7020 6e65 7477  nge of deep netw
-0004eb90: 6f72 6b73 2e0a 0a3c 6469 7620 616c 6967  orks...<div alig
-0004eba0: 6e3d 2263 656e 7465 7222 3e0a 3c69 6d67  n="center">.<img
-0004ebb0: 2073 7263 3d22 6173 7365 7473 2f64 6570   src="assets/dep
-0004ebc0: 2e70 6e67 2220 7769 6474 683d 2231 3030  .png" width="100
-0004ebd0: 2522 3e0a 3c2f 6469 763e 0a0a 2323 2320  %">.</div>..### 
-0004ebe0: 312e 2041 204d 696e 696d 616c 2045 7861  1. A Minimal Exa
-0004ebf0: 6d70 6c65 0a0a 6060 6070 7974 686f 6e0a  mple..```python.
-0004ec00: 696d 706f 7274 2074 6f72 6368 0a66 726f  import torch.fro
-0004ec10: 6d20 746f 7263 6876 6973 696f 6e2e 6d6f  m torchvision.mo
-0004ec20: 6465 6c73 2069 6d70 6f72 7420 7265 736e  dels import resn
-0004ec30: 6574 3138 0a69 6d70 6f72 7420 746f 7263  et18.import torc
-0004ec40: 685f 7072 756e 696e 6720 6173 2074 700a  h_pruning as tp.
-0004ec50: 0a6d 6f64 656c 203d 2072 6573 6e65 7431  .model = resnet1
-0004ec60: 3828 7072 6574 7261 696e 6564 3d54 7275  8(pretrained=Tru
-0004ec70: 6529 2e65 7661 6c28 290a 0a23 2031 2e20  e).eval()..# 1. 
-0004ec80: 6275 696c 6420 6465 7065 6e64 656e 6379  build dependency
-0004ec90: 2067 7261 7068 2066 6f72 2072 6573 6e65   graph for resne
-0004eca0: 7431 380a 4447 203d 2074 702e 4465 7065  t18.DG = tp.Depe
-0004ecb0: 6e64 656e 6379 4772 6170 6828 292e 6275  ndencyGraph().bu
-0004ecc0: 696c 645f 6465 7065 6e64 656e 6379 286d  ild_dependency(m
-0004ecd0: 6f64 656c 2c20 6578 616d 706c 655f 696e  odel, example_in
-0004ece0: 7075 7473 3d74 6f72 6368 2e72 616e 646e  puts=torch.randn
-0004ecf0: 2831 2c33 2c32 3234 2c32 3234 2929 0a0a  (1,3,224,224))..
-0004ed00: 2320 322e 2053 7065 6369 6679 2074 6865  # 2. Specify the
-0004ed10: 2074 6f2d 6265 2d70 7275 6e65 6420 6368   to-be-pruned ch
-0004ed20: 616e 6e65 6c73 2e20 4865 7265 2077 6520  annels. Here we 
-0004ed30: 7072 756e 6520 7468 6f73 6520 6368 616e  prune those chan
-0004ed40: 6e65 6c73 2069 6e64 6578 6564 2062 7920  nels indexed by 
-0004ed50: 5b32 2c20 362c 2039 5d2e 0a67 726f 7570  [2, 6, 9]..group
-0004ed60: 203d 2044 472e 6765 745f 7072 756e 696e   = DG.get_prunin
-0004ed70: 675f 6772 6f75 7028 206d 6f64 656c 2e63  g_group( model.c
-0004ed80: 6f6e 7631 2c20 7470 2e70 7275 6e65 5f63  onv1, tp.prune_c
-0004ed90: 6f6e 765f 6f75 745f 6368 616e 6e65 6c73  onv_out_channels
-0004eda0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
-0004edb0: 2029 0a0a 2320 332e 2070 7275 6e65 2061   )..# 3. prune a
-0004edc0: 6c6c 2067 726f 7570 6564 206c 6179 6572  ll grouped layer
-0004edd0: 7320 7468 6174 2061 7265 2063 6f75 706c  s that are coupl
-0004ede0: 6564 2077 6974 6820 6d6f 6465 6c2e 636f  ed with model.co
-0004edf0: 6e76 3120 2869 6e63 6c75 6465 6429 2e0a  nv1 (included)..
-0004ee00: 6966 2044 472e 6368 6563 6b5f 7072 756e  if DG.check_prun
-0004ee10: 696e 675f 6772 6f75 7028 6772 6f75 7029  ing_group(group)
-0004ee20: 3a20 2320 6176 6f69 6420 6675 6c6c 2070  : # avoid full p
-0004ee30: 7275 6e69 6e67 2c20 692e 652e 2c20 6368  runing, i.e., ch
-0004ee40: 616e 6e65 6c73 3d30 2e0a 2020 2020 6772  annels=0..    gr
-0004ee50: 6f75 702e 7072 756e 6528 290a 2020 2020  oup.prune().    
-0004ee60: 0a23 2034 2e20 5361 7665 2026 204c 6f61  .# 4. Save & Loa
-0004ee70: 640a 6d6f 6465 6c2e 7a65 726f 5f67 7261  d.model.zero_gra
-0004ee80: 6428 2920 2320 5765 2064 6f6e 2774 2077  d() # We don't w
-0004ee90: 616e 7420 746f 2073 746f 7265 2067 7261  ant to store gra
-0004eea0: 6469 656e 7420 696e 666f 726d 6174 696f  dient informatio
-0004eeb0: 6e0a 746f 7263 682e 7361 7665 286d 6f64  n.torch.save(mod
-0004eec0: 656c 2c20 276d 6f64 656c 2e70 7468 2729  el, 'model.pth')
-0004eed0: 2023 2077 6974 686f 7574 202e 7374 6174   # without .stat
-0004eee0: 655f 6469 6374 0a6d 6f64 656c 203d 2074  e_dict.model = t
-0004eef0: 6f72 6368 2e6c 6f61 6428 276d 6f64 656c  orch.load('model
-0004ef00: 2e70 7468 2729 2023 206c 6f61 6420 7468  .pth') # load th
-0004ef10: 6520 6d6f 6465 6c20 6f62 6a65 6374 0a60  e model object.`
-0004ef20: 6060 0a20 200a 5468 6520 6162 6f76 6520  ``.  .The above 
-0004ef30: 6578 616d 706c 6520 6465 6d6f 6e73 7472  example demonstr
-0004ef40: 6174 6573 2074 6865 2066 756e 6461 6d65  ates the fundame
-0004ef50: 6e74 616c 2070 7275 6e69 6e67 2070 6970  ntal pruning pip
-0004ef60: 656c 696e 6520 7573 696e 6720 4465 7047  eline using DepG
-0004ef70: 7261 7068 2e20 5468 6520 7461 7267 6574  raph. The target
-0004ef80: 206c 6179 6572 2072 6573 6e65 742e 636f   layer resnet.co
-0004ef90: 6e76 3120 6973 2063 6f75 706c 6564 2077  nv1 is coupled w
-0004efa0: 6974 6820 7365 7665 7261 6c20 6c61 7965  ith several laye
-0004efb0: 7273 2c20 7768 6963 6820 7265 7175 6972  rs, which requir
-0004efc0: 6573 2073 696d 756c 7461 6e65 6f75 7320  es simultaneous 
-0004efd0: 7265 6d6f 7661 6c20 696e 2073 7472 7563  removal in struc
-0004efe0: 7475 7261 6c20 7072 756e 696e 672e 204c  tural pruning. L
-0004eff0: 6574 2773 2070 7269 6e74 2074 6865 2067  et's print the g
-0004f000: 726f 7570 2061 6e64 206f 6273 6572 7665  roup and observe
-0004f010: 2068 6f77 2061 2070 7275 6e69 6e67 206f   how a pruning o
-0004f020: 7065 7261 7469 6f6e 2022 7472 6967 6765  peration "trigge
-0004f030: 7273 2220 6f74 6865 7220 6f6e 6573 2e20  rs" other ones. 
-0004f040: 496e 2074 6865 2066 6f6c 6c6f 7769 6e67  In the following
-0004f050: 206f 7574 7075 7473 2c20 6060 4120 3d3e   outputs, ``A =>
-0004f060: 2042 6060 206d 6561 6e73 2074 6865 2070   B`` means the p
-0004f070: 7275 6e69 6e67 206f 7065 7261 7469 6f6e  runing operation
-0004f080: 2060 6041 6060 2074 7269 6767 6572 7320   ``A`` triggers 
-0004f090: 7468 6520 7072 756e 696e 6720 6f70 6572  the pruning oper
-0004f0a0: 6174 696f 6e20 6060 4260 602e 2067 726f  ation ``B``. gro
-0004f0b0: 7570 5b30 5d20 7265 6665 7273 2074 6f20  up[0] refers to 
-0004f0c0: 7468 6520 7072 756e 696e 6720 726f 6f74  the pruning root
-0004f0d0: 2069 6e20 6060 4447 2e67 6574 5f70 7275   in ``DG.get_pru
-0004f0e0: 6e69 6e67 5f67 726f 7570 6060 2e0a 0a60  ning_group``...`
-0004f0f0: 6060 0a2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ``.-------------
-0004f100: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-0004f110: 2d2d 2d0a 2020 2020 2020 2020 2020 5072  ---.          Pr
-0004f120: 756e 696e 6720 4772 6f75 700a 2d2d 2d2d  uning Group.----
+0004e060: 3539 3232 3131 2f31 6131 3261 3634 652d  592211/1a12a64e-
+0004e070: 6139 3465 2d34 6137 342d 3839 3039 2d38  a94e-4a74-8909-8
+0004e080: 6663 3038 3836 6636 3366 3222 3e0a 0a0a  fc0886f63f2">...
+0004e090: 0a50 6c65 6173 6520 646f 206e 6f74 2068  .Please do not h
+0004e0a0: 6573 6974 6174 6520 746f 206f 7065 6e20  esitate to open 
+0004e0b0: 6120 5b64 6973 6375 7373 696f 6e5d 2868  a [discussion](h
+0004e0c0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+0004e0d0: 6d2f 5661 696e 462f 546f 7263 682d 5072  m/VainF/Torch-Pr
+0004e0e0: 756e 696e 672f 6469 7363 7573 7369 6f6e  uning/discussion
+0004e0f0: 7329 206f 7220 5b69 7373 7565 5d28 6874  s) or [issue](ht
+0004e100: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
+0004e110: 2f56 6169 6e46 2f54 6f72 6368 2d50 7275  /VainF/Torch-Pru
+0004e120: 6e69 6e67 2f69 7373 7565 7329 2069 6620  ning/issues) if 
+0004e130: 796f 7520 656e 636f 756e 7465 7220 616e  you encounter an
+0004e140: 7920 7072 6f62 6c65 6d73 2077 6974 6820  y problems with 
+0004e150: 7468 6520 6c69 6272 6172 7920 6f72 2074  the library or t
+0004e160: 6865 2070 6170 6572 2e0a 0a0a 2323 2320  he paper....### 
+0004e170: 2a2a 4665 6174 7572 6573 3a2a 2a0a 2d20  **Features:**.- 
+0004e180: 5b78 5d20 5374 7275 6374 7572 616c 2070  [x] Structural p
+0004e190: 7275 6e69 6e67 2066 6f72 2043 4e4e 732c  runing for CNNs,
+0004e1a0: 2054 7261 6e73 666f 726d 6572 732c 2044   Transformers, D
+0004e1b0: 6574 6563 746f 7273 2c20 4c61 6e67 7561  etectors, Langua
+0004e1c0: 6765 204d 6f64 656c 7320 616e 6420 4469  ge Models and Di
+0004e1d0: 6666 7573 696f 6e20 4d6f 6465 6c73 2e20  ffusion Models. 
+0004e1e0: 506c 6561 7365 2072 6566 6572 2074 6f20  Please refer to 
+0004e1f0: 7468 6520 5b50 7275 6e61 6269 6c69 7479  the [Prunability
+0004e200: 2042 656e 6368 6d61 726b 5d28 6265 6e63   Benchmark](benc
+0004e210: 686d 6172 6b73 2f70 7275 6e61 6269 6c69  hmarks/prunabili
+0004e220: 7479 292e 0a2d 205b 785d 2048 6967 682d  ty)..- [x] High-
+0004e230: 6c65 7665 6c20 7072 756e 6572 733a 205b  level pruners: [
+0004e240: 4d61 676e 6974 7564 6550 7275 6e65 725d  MagnitudePruner]
+0004e250: 2868 7474 7073 3a2f 2f61 7278 6976 2e6f  (https://arxiv.o
+0004e260: 7267 2f61 6273 2f31 3630 382e 3038 3731  rg/abs/1608.0871
+0004e270: 3029 2c20 5b42 4e53 6361 6c65 5072 756e  0), [BNScalePrun
+0004e280: 6572 5d28 6874 7470 733a 2f2f 6172 7869  er](https://arxi
+0004e290: 762e 6f72 672f 6162 732f 3137 3038 2e30  v.org/abs/1708.0
+0004e2a0: 3635 3139 292c 205b 4772 6f75 704e 6f72  6519), [GroupNor
+0004e2b0: 6d50 7275 6e65 725d 2868 7474 7073 3a2f  mPruner](https:/
+0004e2c0: 2f61 7278 6976 2e6f 7267 2f61 6273 2f32  /arxiv.org/abs/2
+0004e2d0: 3330 312e 3132 3930 3029 2c20 5261 6e64  301.12900), Rand
+0004e2e0: 6f6d 5072 756e 6572 2c20 6574 632e 0a2d  omPruner, etc..-
+0004e2f0: 205b 785d 2049 6d70 6f72 7461 6e63 6520   [x] Importance 
+0004e300: 4372 6974 6572 6961 3a20 4c2d 7020 4e6f  Criteria: L-p No
+0004e310: 726d 2c20 5461 796c 6f72 2c20 5261 6e64  rm, Taylor, Rand
+0004e320: 6f6d 2c20 424e 5363 616c 696e 672c 2065  om, BNScaling, e
+0004e330: 7463 2e0a 2d20 5b78 5d20 4465 7065 6e64  tc..- [x] Depend
+0004e340: 656e 6379 2047 7261 7068 2066 6f72 2064  ency Graph for d
+0004e350: 6570 656e 6465 6e63 7920 6d6f 6465 6c69  ependency modeli
+0004e360: 6e67 2e0a 2d20 5b78 5d20 5375 7070 6f72  ng..- [x] Suppor
+0004e370: 7465 6420 6d6f 6475 6c65 733a 204c 696e  ted modules: Lin
+0004e380: 6561 722c 2028 5472 616e 7370 6f73 6564  ear, (Transposed
+0004e390: 2920 436f 6e76 2c20 4e6f 726d 616c 697a  ) Conv, Normaliz
+0004e3a0: 6174 696f 6e2c 2050 5265 4c55 2c20 456d  ation, PReLU, Em
+0004e3b0: 6265 6464 696e 672c 204d 756c 7469 6865  bedding, Multihe
+0004e3c0: 6164 4174 7465 6e74 696f 6e2c 206e 6e2e  adAttention, nn.
+0004e3d0: 5061 7261 6d65 7465 7273 2061 6e64 205b  Parameters and [
+0004e3e0: 6375 7374 6f6d 697a 6564 206d 6f64 756c  customized modul
+0004e3f0: 6573 5d28 7465 7374 732f 7465 7374 5f63  es](tests/test_c
+0004e400: 7573 746f 6d69 7a65 645f 6c61 7965 722e  ustomized_layer.
+0004e410: 7079 292e 0a2d 205b 785d 2053 7570 706f  py)..- [x] Suppo
+0004e420: 7274 6564 206f 7065 7261 746f 7273 3a20  rted operators: 
+0004e430: 7370 6c69 742c 2063 6f6e 6361 7465 6e61  split, concatena
+0004e440: 7469 6f6e 2c20 736b 6970 2063 6f6e 6e65  tion, skip conne
+0004e450: 6374 696f 6e2c 2066 6c61 7474 656e 2c20  ction, flatten, 
+0004e460: 7265 7368 6170 652c 2076 6965 772c 2061  reshape, view, a
+0004e470: 6c6c 2065 6c65 6d65 6e74 2d77 6973 6520  ll element-wise 
+0004e480: 6f70 732c 2065 7463 2e0a 2d20 5b78 5d20  ops, etc..- [x] 
+0004e490: 5b4c 6f77 2d6c 6576 656c 2070 7275 6e69  [Low-level pruni
+0004e4a0: 6e67 2066 756e 6374 696f 6e73 5d28 746f  ng functions](to
+0004e4b0: 7263 685f 7072 756e 696e 672f 7072 756e  rch_pruning/prun
+0004e4c0: 6572 2f66 756e 6374 696f 6e2e 7079 290a  er/function.py).
+0004e4d0: 2d20 5b78 5d20 5b42 656e 6368 6d61 726b  - [x] [Benchmark
+0004e4e0: 735d 2862 656e 6368 6d61 726b 7329 2061  s](benchmarks) a
+0004e4f0: 6e64 205b 7475 746f 7269 616c 735d 2874  nd [tutorials](t
+0004e500: 7574 6f72 6961 6c73 290a 2d20 5b78 5d20  utorials).- [x] 
+0004e510: 4120 5b72 6573 6f75 7263 6520 6c69 7374  A [resource list
+0004e520: 5d28 7072 6163 7469 6361 6c5f 7374 7275  ](practical_stru
+0004e530: 6374 7572 616c 5f70 7275 6e69 6e67 2e6d  ctural_pruning.m
+0004e540: 6429 2066 6f72 2070 7261 6374 6963 616c  d) for practical
+0004e550: 2073 7472 7563 7472 7561 6c20 7072 756e   structrual prun
+0004e560: 696e 672e 0a20 200a 2323 2320 2a2a 544f  ing..  .### **TO
+0004e570: 444f 204c 6973 743a 2a2a 0a2d 205b 205d  DO List:**.- [ ]
+0004e580: 2041 2073 7472 6f6e 6720 6261 7365 6c69   A strong baseli
+0004e590: 6e65 2077 6974 6820 6261 6773 206f 6620  ne with bags of 
+0004e5a0: 7472 6963 6b73 2066 726f 6d20 6578 6973  tricks from exis
+0004e5b0: 7469 6e67 206d 6574 686f 6473 2e0a 2d20  ting methods..- 
+0004e5c0: 5b20 5d20 4120 6265 6e63 686d 6172 6b20  [ ] A benchmark 
+0004e5d0: 666f 7220 5b54 6f72 6368 7669 7369 6f6e  for [Torchvision
+0004e5e0: 5d28 6874 7470 733a 2f2f 7079 746f 7263  ](https://pytorc
+0004e5f0: 682e 6f72 672f 7669 7369 6f6e 2f73 7461  h.org/vision/sta
+0004e600: 626c 652f 6d6f 6465 6c73 2e68 746d 6c29  ble/models.html)
+0004e610: 2063 6f6d 7061 7469 6269 6c69 7479 2028   compatibility (
+0004e620: 2a2a 3831 2f38 353d 3935 2e33 252a 2a2c  **81/85=95.3%**,
+0004e630: 203a 6865 6176 795f 6368 6563 6b5f 6d61   :heavy_check_ma
+0004e640: 726b 3a29 2061 6e64 205b 7469 6d6d 5d28  rk:) and [timm](
+0004e650: 6874 7470 733a 2f2f 6769 7468 7562 2e63  https://github.c
+0004e660: 6f6d 2f68 7567 6769 6e67 6661 6365 2f70  om/huggingface/p
+0004e670: 7974 6f72 6368 2d69 6d61 6765 2d6d 6f64  ytorch-image-mod
+0004e680: 656c 7329 2063 6f6d 7061 7469 6269 6c69  els) compatibili
+0004e690: 7479 2e0a 2d20 5b20 5d20 5072 756e 696e  ty..- [ ] Prunin
+0004e6a0: 6720 6672 6f6d 2053 6372 6174 6368 202f  g from Scratch /
+0004e6b0: 2061 7420 496e 6974 6961 6c69 7a61 7469   at Initializati
+0004e6c0: 6f6e 2e0a 2d20 5b20 5d20 4d6f 7265 2068  on..- [ ] More h
+0004e6d0: 6967 682d 6c65 7665 6c20 7072 756e 6572  igh-level pruner
+0004e6e0: 7320 6c69 6b65 205b 4669 7368 6572 5072  s like [FisherPr
+0004e6f0: 756e 6572 5d28 6874 7470 733a 2f2f 6172  uner](https://ar
+0004e700: 7869 762e 6f72 672f 6162 732f 3231 3038  xiv.org/abs/2108
+0004e710: 2e30 3037 3038 292c 205b 4772 6f77 696e  .00708), [Growin
+0004e720: 6752 6567 5d28 6874 7470 733a 2f2f 6172  gReg](https://ar
+0004e730: 7869 762e 6f72 672f 6162 732f 3230 3132  xiv.org/abs/2012
+0004e740: 2e30 3932 3433 292c 2065 7463 2e0a 2d20  .09243), etc..- 
+0004e750: 5b20 5d20 4d6f 7265 2054 7261 6e73 666f  [ ] More Transfo
+0004e760: 726d 6572 7320 6c69 6b65 2056 6973 696f  rmers like Visio
+0004e770: 6e20 5472 616e 7366 6f72 6d65 7273 2028  n Transformers (
+0004e780: 3a68 6561 7679 5f63 6865 636b 5f6d 6172  :heavy_check_mar
+0004e790: 6b3a 292c 2053 7769 6e20 5472 616e 7366  k:), Swin Transf
+0004e7a0: 6f72 6d65 7273 2c20 506f 6f6c 466f 726d  ormers, PoolForm
+0004e7b0: 6572 732e 0a2d 205b 205d 2042 6c6f 636b  ers..- [ ] Block
+0004e7c0: 2f4c 6179 6572 2f44 6570 7468 2050 7275  /Layer/Depth Pru
+0004e7d0: 6e69 6e67 0a2d 205b 205d 2050 7275 6e69  ning.- [ ] Pruni
+0004e7e0: 6e67 2062 656e 6368 6d61 726b 7320 666f  ng benchmarks fo
+0004e7f0: 7220 4349 4641 522c 2049 6d61 6765 4e65  r CIFAR, ImageNe
+0004e800: 7420 616e 6420 434f 434f 2e0a 0a23 2320  t and COCO...## 
+0004e810: 496e 7374 616c 6c61 7469 6f6e 0a0a 546f  Installation..To
+0004e820: 7263 682d 5072 756e 696e 6720 6973 2063  rch-Pruning is c
+0004e830: 6f6d 7061 7469 626c 6520 7769 7468 2050  ompatible with P
+0004e840: 7954 6f72 6368 2031 2e78 2061 6e64 2032  yTorch 1.x and 2
+0004e850: 2e78 2e20 2a2a 5079 546f 7263 6820 312e  .x. **PyTorch 1.
+0004e860: 3132 2e31 2069 7320 7265 636f 6d6d 656e  12.1 is recommen
+0004e870: 6465 6421 2a2a 0a0a 6060 6062 6173 680a  ded!**..```bash.
+0004e880: 7069 7020 696e 7374 616c 6c20 746f 7263  pip install torc
+0004e890: 682d 7072 756e 696e 6720 2320 7631 2e31  h-pruning # v1.1
+0004e8a0: 2e39 0a60 6060 0a6f 720a 6060 6062 6173  .9.```.or.```bas
+0004e8b0: 680a 6769 7420 636c 6f6e 6520 6874 7470  h.git clone http
+0004e8c0: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
+0004e8d0: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
+0004e8e0: 6e67 2e67 6974 0a60 6060 0a0a 2323 2051  ng.git.```..## Q
+0004e8f0: 7569 636b 7374 6172 740a 2020 0a48 6572  uickstart.  .Her
+0004e900: 6520 7765 2070 726f 7669 6465 2061 2071  e we provide a q
+0004e910: 7569 636b 2073 7461 7274 2066 6f72 2054  uick start for T
+0004e920: 6f72 6368 2d50 7275 6e69 6e67 2e20 4d6f  orch-Pruning. Mo
+0004e930: 7265 2065 7870 6c61 696e 6564 2064 6574  re explained det
+0004e940: 6169 6c73 2063 616e 2062 6520 666f 756e  ails can be foun
+0004e950: 6420 696e 205b 7475 746f 7261 6c73 5d28  d in [tutorals](
+0004e960: 2e2f 7475 746f 7269 616c 732f 290a 0a23  ./tutorials/)..#
+0004e970: 2323 2030 2e20 486f 7720 4974 2057 6f72  ## 0. How It Wor
+0004e980: 6b73 0a0a 496e 2073 7472 7563 7475 7261  ks..In structura
+0004e990: 6c20 7072 756e 696e 672c 2061 2060 6047  l pruning, a ``G
+0004e9a0: 726f 7570 6060 2069 7320 6465 6669 6e65  roup`` is define
+0004e9b0: 6420 6173 2074 6865 206d 696e 696d 616c  d as the minimal
+0004e9c0: 2072 656d 6f76 6162 6c65 2075 6e69 7420   removable unit 
+0004e9d0: 7769 7468 696e 2064 6565 7020 6e65 7477  within deep netw
+0004e9e0: 6f72 6b73 2e20 4561 6368 2067 726f 7570  orks. Each group
+0004e9f0: 2063 6f6e 7369 7374 7320 6f66 206d 756c   consists of mul
+0004ea00: 7469 706c 6520 696e 7465 7264 6570 656e  tiple interdepen
+0004ea10: 6465 6e74 206c 6179 6572 7320 7468 6174  dent layers that
+0004ea20: 206e 6565 6420 746f 2062 6520 7072 756e   need to be prun
+0004ea30: 6564 2073 696d 756c 7461 6e65 6f75 736c  ed simultaneousl
+0004ea40: 7920 696e 206f 7264 6572 2074 6f20 7072  y in order to pr
+0004ea50: 6573 6572 7665 2074 6865 2069 6e74 6567  eserve the integ
+0004ea60: 7269 7479 206f 6620 7468 6520 7265 7375  rity of the resu
+0004ea70: 6c74 696e 6720 7374 7275 6374 7572 6573  lting structures
+0004ea80: 2e20 486f 7765 7665 722c 2064 6565 7020  . However, deep 
+0004ea90: 6e65 7477 6f72 6b73 206f 6674 656e 2065  networks often e
+0004eaa0: 7868 6962 6974 2069 6e74 7269 6361 7465  xhibit intricate
+0004eab0: 2064 6570 656e 6465 6e63 6965 7320 616d   dependencies am
+0004eac0: 6f6e 6720 6c61 7965 7273 2c20 706f 7369  ong layers, posi
+0004ead0: 6e67 2061 2073 6967 6e69 6669 6361 6e74  ng a significant
+0004eae0: 2063 6861 6c6c 656e 6765 2066 6f72 2073   challenge for s
+0004eaf0: 7472 7563 7475 7261 6c20 7072 756e 696e  tructural prunin
+0004eb00: 672e 2054 6869 7320 776f 726b 2074 6163  g. This work tac
+0004eb10: 6b6c 6573 2074 6869 7320 6368 616c 6c65  kles this challe
+0004eb20: 6e67 6520 6279 2069 6e74 726f 6475 6369  nge by introduci
+0004eb30: 6e67 2061 6e20 6175 746f 6d61 7465 6420  ng an automated 
+0004eb40: 6d65 6368 616e 6973 6d20 6361 6c6c 6564  mechanism called
+0004eb50: 2060 6044 6570 4772 6170 6860 602c 2077   ``DepGraph``, w
+0004eb60: 6869 6368 2065 6e61 626c 6573 2065 6666  hich enables eff
+0004eb70: 6f72 746c 6573 7320 7061 7261 6d65 7465  ortless paramete
+0004eb80: 7220 6772 6f75 7069 6e67 2061 6e64 2066  r grouping and f
+0004eb90: 6163 696c 6974 6174 6573 2070 7275 6e69  acilitates pruni
+0004eba0: 6e67 2066 6f72 2061 2064 6976 6572 7365  ng for a diverse
+0004ebb0: 2072 616e 6765 206f 6620 6465 6570 206e   range of deep n
+0004ebc0: 6574 776f 726b 732e 0a0a 3c64 6976 2061  etworks...<div a
+0004ebd0: 6c69 676e 3d22 6365 6e74 6572 223e 0a3c  lign="center">.<
+0004ebe0: 696d 6720 7372 633d 2261 7373 6574 732f  img src="assets/
+0004ebf0: 6465 702e 706e 6722 2077 6964 7468 3d22  dep.png" width="
+0004ec00: 3130 3025 223e 0a3c 2f64 6976 3e0a 0a23  100%">.</div>..#
+0004ec10: 2323 2031 2e20 4120 4d69 6e69 6d61 6c20  ## 1. A Minimal 
+0004ec20: 4578 616d 706c 650a 0a60 6060 7079 7468  Example..```pyth
+0004ec30: 6f6e 0a69 6d70 6f72 7420 746f 7263 680a  on.import torch.
+0004ec40: 6672 6f6d 2074 6f72 6368 7669 7369 6f6e  from torchvision
+0004ec50: 2e6d 6f64 656c 7320 696d 706f 7274 2072  .models import r
+0004ec60: 6573 6e65 7431 380a 696d 706f 7274 2074  esnet18.import t
+0004ec70: 6f72 6368 5f70 7275 6e69 6e67 2061 7320  orch_pruning as 
+0004ec80: 7470 0a0a 6d6f 6465 6c20 3d20 7265 736e  tp..model = resn
+0004ec90: 6574 3138 2870 7265 7472 6169 6e65 643d  et18(pretrained=
+0004eca0: 5472 7565 292e 6576 616c 2829 0a0a 2320  True).eval()..# 
+0004ecb0: 312e 2062 7569 6c64 2064 6570 656e 6465  1. build depende
+0004ecc0: 6e63 7920 6772 6170 6820 666f 7220 7265  ncy graph for re
+0004ecd0: 736e 6574 3138 0a44 4720 3d20 7470 2e44  snet18.DG = tp.D
+0004ece0: 6570 656e 6465 6e63 7947 7261 7068 2829  ependencyGraph()
+0004ecf0: 2e62 7569 6c64 5f64 6570 656e 6465 6e63  .build_dependenc
+0004ed00: 7928 6d6f 6465 6c2c 2065 7861 6d70 6c65  y(model, example
+0004ed10: 5f69 6e70 7574 733d 746f 7263 682e 7261  _inputs=torch.ra
+0004ed20: 6e64 6e28 312c 332c 3232 342c 3232 3429  ndn(1,3,224,224)
+0004ed30: 290a 0a23 2032 2e20 5370 6563 6966 7920  )..# 2. Specify 
+0004ed40: 7468 6520 746f 2d62 652d 7072 756e 6564  the to-be-pruned
+0004ed50: 2063 6861 6e6e 656c 732e 2048 6572 6520   channels. Here 
+0004ed60: 7765 2070 7275 6e65 2074 686f 7365 2063  we prune those c
+0004ed70: 6861 6e6e 656c 7320 696e 6465 7865 6420  hannels indexed 
+0004ed80: 6279 205b 322c 2036 2c20 395d 2e0a 6772  by [2, 6, 9]..gr
+0004ed90: 6f75 7020 3d20 4447 2e67 6574 5f70 7275  oup = DG.get_pru
+0004eda0: 6e69 6e67 5f67 726f 7570 2820 6d6f 6465  ning_group( mode
+0004edb0: 6c2e 636f 6e76 312c 2074 702e 7072 756e  l.conv1, tp.prun
+0004edc0: 655f 636f 6e76 5f6f 7574 5f63 6861 6e6e  e_conv_out_chann
+0004edd0: 656c 732c 2069 6478 733d 5b32 2c20 362c  els, idxs=[2, 6,
+0004ede0: 2039 5d20 290a 0a23 2033 2e20 7072 756e   9] )..# 3. prun
+0004edf0: 6520 616c 6c20 6772 6f75 7065 6420 6c61  e all grouped la
+0004ee00: 7965 7273 2074 6861 7420 6172 6520 636f  yers that are co
+0004ee10: 7570 6c65 6420 7769 7468 206d 6f64 656c  upled with model
+0004ee20: 2e63 6f6e 7631 2028 696e 636c 7564 6564  .conv1 (included
+0004ee30: 292e 0a69 6620 4447 2e63 6865 636b 5f70  )..if DG.check_p
+0004ee40: 7275 6e69 6e67 5f67 726f 7570 2867 726f  runing_group(gro
+0004ee50: 7570 293a 2023 2061 766f 6964 2066 756c  up): # avoid ful
+0004ee60: 6c20 7072 756e 696e 672c 2069 2e65 2e2c  l pruning, i.e.,
+0004ee70: 2063 6861 6e6e 656c 733d 302e 0a20 2020   channels=0..   
+0004ee80: 2067 726f 7570 2e70 7275 6e65 2829 0a20   group.prune(). 
+0004ee90: 2020 200a 2320 342e 2053 6176 6520 2620     .# 4. Save & 
+0004eea0: 4c6f 6164 0a6d 6f64 656c 2e7a 6572 6f5f  Load.model.zero_
+0004eeb0: 6772 6164 2829 2023 2057 6520 646f 6e27  grad() # We don'
+0004eec0: 7420 7761 6e74 2074 6f20 7374 6f72 6520  t want to store 
+0004eed0: 6772 6164 6965 6e74 2069 6e66 6f72 6d61  gradient informa
+0004eee0: 7469 6f6e 0a74 6f72 6368 2e73 6176 6528  tion.torch.save(
+0004eef0: 6d6f 6465 6c2c 2027 6d6f 6465 6c2e 7074  model, 'model.pt
+0004ef00: 6827 2920 2320 7769 7468 6f75 7420 2e73  h') # without .s
+0004ef10: 7461 7465 5f64 6963 740a 6d6f 6465 6c20  tate_dict.model 
+0004ef20: 3d20 746f 7263 682e 6c6f 6164 2827 6d6f  = torch.load('mo
+0004ef30: 6465 6c2e 7074 6827 2920 2320 6c6f 6164  del.pth') # load
+0004ef40: 2074 6865 206d 6f64 656c 206f 626a 6563   the model objec
+0004ef50: 740a 6060 600a 2020 0a54 6865 2061 626f  t.```.  .The abo
+0004ef60: 7665 2065 7861 6d70 6c65 2064 656d 6f6e  ve example demon
+0004ef70: 7374 7261 7465 7320 7468 6520 6675 6e64  strates the fund
+0004ef80: 616d 656e 7461 6c20 7072 756e 696e 6720  amental pruning 
+0004ef90: 7069 7065 6c69 6e65 2075 7369 6e67 2044  pipeline using D
+0004efa0: 6570 4772 6170 682e 2054 6865 2074 6172  epGraph. The tar
+0004efb0: 6765 7420 6c61 7965 7220 7265 736e 6574  get layer resnet
+0004efc0: 2e63 6f6e 7631 2069 7320 636f 7570 6c65  .conv1 is couple
+0004efd0: 6420 7769 7468 2073 6576 6572 616c 206c  d with several l
+0004efe0: 6179 6572 732c 2077 6869 6368 2072 6571  ayers, which req
+0004eff0: 7569 7265 7320 7369 6d75 6c74 616e 656f  uires simultaneo
+0004f000: 7573 2072 656d 6f76 616c 2069 6e20 7374  us removal in st
+0004f010: 7275 6374 7572 616c 2070 7275 6e69 6e67  ructural pruning
+0004f020: 2e20 4c65 7427 7320 7072 696e 7420 7468  . Let's print th
+0004f030: 6520 6772 6f75 7020 616e 6420 6f62 7365  e group and obse
+0004f040: 7276 6520 686f 7720 6120 7072 756e 696e  rve how a prunin
+0004f050: 6720 6f70 6572 6174 696f 6e20 2274 7269  g operation "tri
+0004f060: 6767 6572 7322 206f 7468 6572 206f 6e65  ggers" other one
+0004f070: 732e 2049 6e20 7468 6520 666f 6c6c 6f77  s. In the follow
+0004f080: 696e 6720 6f75 7470 7574 732c 2060 6041  ing outputs, ``A
+0004f090: 203d 3e20 4260 6020 6d65 616e 7320 7468   => B`` means th
+0004f0a0: 6520 7072 756e 696e 6720 6f70 6572 6174  e pruning operat
+0004f0b0: 696f 6e20 6060 4160 6020 7472 6967 6765  ion ``A`` trigge
+0004f0c0: 7273 2074 6865 2070 7275 6e69 6e67 206f  rs the pruning o
+0004f0d0: 7065 7261 7469 6f6e 2060 6042 6060 2e20  peration ``B``. 
+0004f0e0: 6772 6f75 705b 305d 2072 6566 6572 7320  group[0] refers 
+0004f0f0: 746f 2074 6865 2070 7275 6e69 6e67 2072  to the pruning r
+0004f100: 6f6f 7420 696e 2060 6044 472e 6765 745f  oot in ``DG.get_
+0004f110: 7072 756e 696e 675f 6772 6f75 7060 602e  pruning_group``.
+0004f120: 0a0a 6060 600a 2d2d 2d2d 2d2d 2d2d 2d2d  ..```.----------
 0004f130: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-0004f140: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 0a5b 305d  ------------.[0]
-0004f150: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
-0004f160: 656c 7320 6f6e 2063 6f6e 7631 2028 436f  els on conv1 (Co
-0004f170: 6e76 3264 2833 2c20 3634 2c20 6b65 726e  nv2d(3, 64, kern
-0004f180: 656c 5f73 697a 653d 2837 2c20 3729 2c20  el_size=(7, 7), 
-0004f190: 7374 7269 6465 3d28 322c 2032 292c 2070  stride=(2, 2), p
-0004f1a0: 6164 6469 6e67 3d28 332c 2033 292c 2062  adding=(3, 3), b
-0004f1b0: 6961 733d 4661 6c73 6529 2920 3d3e 2070  ias=False)) => p
-0004f1c0: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-0004f1d0: 7320 6f6e 2063 6f6e 7631 2028 436f 6e76  s on conv1 (Conv
-0004f1e0: 3264 2833 2c20 3634 2c20 6b65 726e 656c  2d(3, 64, kernel
-0004f1f0: 5f73 697a 653d 2837 2c20 3729 2c20 7374  _size=(7, 7), st
-0004f200: 7269 6465 3d28 322c 2032 292c 2070 6164  ride=(2, 2), pad
-0004f210: 6469 6e67 3d28 332c 2033 292c 2062 6961  ding=(3, 3), bia
-0004f220: 733d 4661 6c73 6529 292c 2069 6478 733d  s=False)), idxs=
-0004f230: 5b32 2c20 362c 2039 5d20 2850 7275 6e69  [2, 6, 9] (Pruni
-0004f240: 6e67 2052 6f6f 7429 0a5b 315d 2070 7275  ng Root).[1] pru
-0004f250: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
-0004f260: 6f6e 2063 6f6e 7631 2028 436f 6e76 3264  on conv1 (Conv2d
-0004f270: 2833 2c20 3634 2c20 6b65 726e 656c 5f73  (3, 64, kernel_s
-0004f280: 697a 653d 2837 2c20 3729 2c20 7374 7269  ize=(7, 7), stri
-0004f290: 6465 3d28 322c 2032 292c 2070 6164 6469  de=(2, 2), paddi
-0004f2a0: 6e67 3d28 332c 2033 292c 2062 6961 733d  ng=(3, 3), bias=
-0004f2b0: 4661 6c73 6529 2920 3d3e 2070 7275 6e65  False)) => prune
-0004f2c0: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
-0004f2d0: 2062 6e31 2028 4261 7463 684e 6f72 6d32   bn1 (BatchNorm2
-0004f2e0: 6428 3634 2c20 6570 733d 3165 2d30 352c  d(64, eps=1e-05,
-0004f2f0: 206d 6f6d 656e 7475 6d3d 302e 312c 2061   momentum=0.1, a
-0004f300: 6666 696e 653d 5472 7565 2c20 7472 6163  ffine=True, trac
-0004f310: 6b5f 7275 6e6e 696e 675f 7374 6174 733d  k_running_stats=
-0004f320: 5472 7565 2929 2c20 6964 7873 3d5b 322c  True)), idxs=[2,
-0004f330: 2036 2c20 395d 0a5b 325d 2070 7275 6e65   6, 9].[2] prune
-0004f340: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
-0004f350: 2062 6e31 2028 4261 7463 684e 6f72 6d32   bn1 (BatchNorm2
-0004f360: 6428 3634 2c20 6570 733d 3165 2d30 352c  d(64, eps=1e-05,
-0004f370: 206d 6f6d 656e 7475 6d3d 302e 312c 2061   momentum=0.1, a
-0004f380: 6666 696e 653d 5472 7565 2c20 7472 6163  ffine=True, trac
-0004f390: 6b5f 7275 6e6e 696e 675f 7374 6174 733d  k_running_stats=
-0004f3a0: 5472 7565 2929 203d 3e20 7072 756e 655f  True)) => prune_
-0004f3b0: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-0004f3c0: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f32  _ElementWiseOp_2
-0004f3d0: 3028 5265 6c75 4261 636b 7761 7264 3029  0(ReluBackward0)
-0004f3e0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
-0004f3f0: 0a5b 335d 2070 7275 6e65 5f6f 7574 5f63  .[3] prune_out_c
-0004f400: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
-0004f410: 656e 7457 6973 654f 705f 3230 2852 656c  entWiseOp_20(Rel
-0004f420: 7542 6163 6b77 6172 6430 2920 3d3e 2070  uBackward0) => p
-0004f430: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
-0004f440: 7320 6f6e 205f 456c 656d 656e 7457 6973  s on _ElementWis
-0004f450: 654f 705f 3139 284d 6178 506f 6f6c 3244  eOp_19(MaxPool2D
-0004f460: 5769 7468 496e 6469 6365 7342 6163 6b77  WithIndicesBackw
-0004f470: 6172 6430 292c 2069 6478 733d 5b32 2c20  ard0), idxs=[2, 
-0004f480: 362c 2039 5d0a 5b34 5d20 7072 756e 655f  6, 9].[4] prune_
-0004f490: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-0004f4a0: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f31  _ElementWiseOp_1
-0004f4b0: 3928 4d61 7850 6f6f 6c32 4457 6974 6849  9(MaxPool2DWithI
-0004f4c0: 6e64 6963 6573 4261 636b 7761 7264 3029  ndicesBackward0)
-0004f4d0: 203d 3e20 7072 756e 655f 6f75 745f 6368   => prune_out_ch
-0004f4e0: 616e 6e65 6c73 206f 6e20 5f45 6c65 6d65  annels on _Eleme
-0004f4f0: 6e74 5769 7365 4f70 5f31 3828 4164 6442  ntWiseOp_18(AddB
-0004f500: 6163 6b77 6172 6430 292c 2069 6478 733d  ackward0), idxs=
-0004f510: 5b32 2c20 362c 2039 5d0a 5b35 5d20 7072  [2, 6, 9].[5] pr
-0004f520: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-0004f530: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
-0004f540: 4f70 5f31 3928 4d61 7850 6f6f 6c32 4457  Op_19(MaxPool2DW
-0004f550: 6974 6849 6e64 6963 6573 4261 636b 7761  ithIndicesBackwa
-0004f560: 7264 3029 203d 3e20 7072 756e 655f 696e  rd0) => prune_in
-0004f570: 5f63 6861 6e6e 656c 7320 6f6e 206c 6179  _channels on lay
-0004f580: 6572 312e 302e 636f 6e76 3120 2843 6f6e  er1.0.conv1 (Con
-0004f590: 7632 6428 3634 2c20 3634 2c20 6b65 726e  v2d(64, 64, kern
-0004f5a0: 656c 5f73 697a 653d 2833 2c20 3329 2c20  el_size=(3, 3), 
-0004f5b0: 7374 7269 6465 3d28 312c 2031 292c 2070  stride=(1, 1), p
-0004f5c0: 6164 6469 6e67 3d28 312c 2031 292c 2062  adding=(1, 1), b
-0004f5d0: 6961 733d 4661 6c73 6529 292c 2069 6478  ias=False)), idx
-0004f5e0: 733d 5b32 2c20 362c 2039 5d0a 5b36 5d20  s=[2, 6, 9].[6] 
-0004f5f0: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-0004f600: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-0004f610: 7365 4f70 5f31 3828 4164 6442 6163 6b77  seOp_18(AddBackw
-0004f620: 6172 6430 2920 3d3e 2070 7275 6e65 5f6f  ard0) => prune_o
-0004f630: 7574 5f63 6861 6e6e 656c 7320 6f6e 206c  ut_channels on l
-0004f640: 6179 6572 312e 302e 626e 3220 2842 6174  ayer1.0.bn2 (Bat
-0004f650: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
-0004f660: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
-0004f670: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
-0004f680: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
-0004f690: 5f73 7461 7473 3d54 7275 6529 292c 2069  _stats=True)), i
-0004f6a0: 6478 733d 5b32 2c20 362c 2039 5d0a 5b37  dxs=[2, 6, 9].[7
-0004f6b0: 5d20 7072 756e 655f 6f75 745f 6368 616e  ] prune_out_chan
-0004f6c0: 6e65 6c73 206f 6e20 5f45 6c65 6d65 6e74  nels on _Element
-0004f6d0: 5769 7365 4f70 5f31 3828 4164 6442 6163  WiseOp_18(AddBac
-0004f6e0: 6b77 6172 6430 2920 3d3e 2070 7275 6e65  kward0) => prune
-0004f6f0: 5f6f 7574 5f63 6861 6e6e 656c 7320 6f6e  _out_channels on
-0004f700: 205f 456c 656d 656e 7457 6973 654f 705f   _ElementWiseOp_
-0004f710: 3137 2852 656c 7542 6163 6b77 6172 6430  17(ReluBackward0
-0004f720: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
-0004f730: 5d0a 5b38 5d20 7072 756e 655f 6f75 745f  ].[8] prune_out_
-0004f740: 6368 616e 6e65 6c73 206f 6e20 5f45 6c65  channels on _Ele
-0004f750: 6d65 6e74 5769 7365 4f70 5f31 3728 5265  mentWiseOp_17(Re
-0004f760: 6c75 4261 636b 7761 7264 3029 203d 3e20  luBackward0) => 
-0004f770: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-0004f780: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-0004f790: 7365 4f70 5f31 3628 4164 6442 6163 6b77  seOp_16(AddBackw
-0004f7a0: 6172 6430 292c 2069 6478 733d 5b32 2c20  ard0), idxs=[2, 
-0004f7b0: 362c 2039 5d0a 5b39 5d20 7072 756e 655f  6, 9].[9] prune_
-0004f7c0: 6f75 745f 6368 616e 6e65 6c73 206f 6e20  out_channels on 
-0004f7d0: 5f45 6c65 6d65 6e74 5769 7365 4f70 5f31  _ElementWiseOp_1
-0004f7e0: 3728 5265 6c75 4261 636b 7761 7264 3029  7(ReluBackward0)
-0004f7f0: 203d 3e20 7072 756e 655f 696e 5f63 6861   => prune_in_cha
-0004f800: 6e6e 656c 7320 6f6e 206c 6179 6572 312e  nnels on layer1.
-0004f810: 312e 636f 6e76 3120 2843 6f6e 7632 6428  1.conv1 (Conv2d(
-0004f820: 3634 2c20 3634 2c20 6b65 726e 656c 5f73  64, 64, kernel_s
-0004f830: 697a 653d 2833 2c20 3329 2c20 7374 7269  ize=(3, 3), stri
-0004f840: 6465 3d28 312c 2031 292c 2070 6164 6469  de=(1, 1), paddi
-0004f850: 6e67 3d28 312c 2031 292c 2062 6961 733d  ng=(1, 1), bias=
-0004f860: 4661 6c73 6529 292c 2069 6478 733d 5b32  False)), idxs=[2
-0004f870: 2c20 362c 2039 5d0a 5b31 305d 2070 7275  , 6, 9].[10] pru
-0004f880: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
-0004f890: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
-0004f8a0: 705f 3136 2841 6464 4261 636b 7761 7264  p_16(AddBackward
-0004f8b0: 3029 203d 3e20 7072 756e 655f 6f75 745f  0) => prune_out_
-0004f8c0: 6368 616e 6e65 6c73 206f 6e20 6c61 7965  channels on laye
-0004f8d0: 7231 2e31 2e62 6e32 2028 4261 7463 684e  r1.1.bn2 (BatchN
-0004f8e0: 6f72 6d32 6428 3634 2c20 6570 733d 3165  orm2d(64, eps=1e
-0004f8f0: 2d30 352c 206d 6f6d 656e 7475 6d3d 302e  -05, momentum=0.
-0004f900: 312c 2061 6666 696e 653d 5472 7565 2c20  1, affine=True, 
-0004f910: 7472 6163 6b5f 7275 6e6e 696e 675f 7374  track_running_st
-0004f920: 6174 733d 5472 7565 2929 2c20 6964 7873  ats=True)), idxs
-0004f930: 3d5b 322c 2036 2c20 395d 0a5b 3131 5d20  =[2, 6, 9].[11] 
-0004f940: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-0004f950: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-0004f960: 7365 4f70 5f31 3628 4164 6442 6163 6b77  seOp_16(AddBackw
-0004f970: 6172 6430 2920 3d3e 2070 7275 6e65 5f6f  ard0) => prune_o
-0004f980: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
-0004f990: 456c 656d 656e 7457 6973 654f 705f 3135  ElementWiseOp_15
-0004f9a0: 2852 656c 7542 6163 6b77 6172 6430 292c  (ReluBackward0),
-0004f9b0: 2069 6478 733d 5b32 2c20 362c 2039 5d0a   idxs=[2, 6, 9].
-0004f9c0: 5b31 325d 2070 7275 6e65 5f6f 7574 5f63  [12] prune_out_c
-0004f9d0: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
-0004f9e0: 656e 7457 6973 654f 705f 3135 2852 656c  entWiseOp_15(Rel
-0004f9f0: 7542 6163 6b77 6172 6430 2920 3d3e 2070  uBackward0) => p
-0004fa00: 7275 6e65 5f69 6e5f 6368 616e 6e65 6c73  rune_in_channels
-0004fa10: 206f 6e20 6c61 7965 7232 2e30 2e64 6f77   on layer2.0.dow
-0004fa20: 6e73 616d 706c 652e 3020 2843 6f6e 7632  nsample.0 (Conv2
-0004fa30: 6428 3634 2c20 3132 382c 206b 6572 6e65  d(64, 128, kerne
-0004fa40: 6c5f 7369 7a65 3d28 312c 2031 292c 2073  l_size=(1, 1), s
-0004fa50: 7472 6964 653d 2832 2c20 3229 2c20 6269  tride=(2, 2), bi
-0004fa60: 6173 3d46 616c 7365 2929 2c20 6964 7873  as=False)), idxs
-0004fa70: 3d5b 322c 2036 2c20 395d 0a5b 3133 5d20  =[2, 6, 9].[13] 
-0004fa80: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
-0004fa90: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
-0004faa0: 7365 4f70 5f31 3528 5265 6c75 4261 636b  seOp_15(ReluBack
-0004fab0: 7761 7264 3029 203d 3e20 7072 756e 655f  ward0) => prune_
-0004fac0: 696e 5f63 6861 6e6e 656c 7320 6f6e 206c  in_channels on l
-0004fad0: 6179 6572 322e 302e 636f 6e76 3120 2843  ayer2.0.conv1 (C
-0004fae0: 6f6e 7632 6428 3634 2c20 3132 382c 206b  onv2d(64, 128, k
-0004faf0: 6572 6e65 6c5f 7369 7a65 3d28 332c 2033  ernel_size=(3, 3
-0004fb00: 292c 2073 7472 6964 653d 2832 2c20 3229  ), stride=(2, 2)
-0004fb10: 2c20 7061 6464 696e 673d 2831 2c20 3129  , padding=(1, 1)
-0004fb20: 2c20 6269 6173 3d46 616c 7365 2929 2c20  , bias=False)), 
-0004fb30: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
-0004fb40: 3134 5d20 7072 756e 655f 6f75 745f 6368  14] prune_out_ch
-0004fb50: 616e 6e65 6c73 206f 6e20 6c61 7965 7231  annels on layer1
-0004fb60: 2e31 2e62 6e32 2028 4261 7463 684e 6f72  .1.bn2 (BatchNor
-0004fb70: 6d32 6428 3634 2c20 6570 733d 3165 2d30  m2d(64, eps=1e-0
-0004fb80: 352c 206d 6f6d 656e 7475 6d3d 302e 312c  5, momentum=0.1,
-0004fb90: 2061 6666 696e 653d 5472 7565 2c20 7472   affine=True, tr
-0004fba0: 6163 6b5f 7275 6e6e 696e 675f 7374 6174  ack_running_stat
-0004fbb0: 733d 5472 7565 2929 203d 3e20 7072 756e  s=True)) => prun
-0004fbc0: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
-0004fbd0: 6e20 6c61 7965 7231 2e31 2e63 6f6e 7632  n layer1.1.conv2
-0004fbe0: 2028 436f 6e76 3264 2836 342c 2036 342c   (Conv2d(64, 64,
-0004fbf0: 206b 6572 6e65 6c5f 7369 7a65 3d28 332c   kernel_size=(3,
-0004fc00: 2033 292c 2073 7472 6964 653d 2831 2c20   3), stride=(1, 
-0004fc10: 3129 2c20 7061 6464 696e 673d 2831 2c20  1), padding=(1, 
-0004fc20: 3129 2c20 6269 6173 3d46 616c 7365 2929  1), bias=False))
-0004fc30: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
-0004fc40: 0a5b 3135 5d20 7072 756e 655f 6f75 745f  .[15] prune_out_
-0004fc50: 6368 616e 6e65 6c73 206f 6e20 6c61 7965  channels on laye
-0004fc60: 7231 2e30 2e62 6e32 2028 4261 7463 684e  r1.0.bn2 (BatchN
-0004fc70: 6f72 6d32 6428 3634 2c20 6570 733d 3165  orm2d(64, eps=1e
-0004fc80: 2d30 352c 206d 6f6d 656e 7475 6d3d 302e  -05, momentum=0.
-0004fc90: 312c 2061 6666 696e 653d 5472 7565 2c20  1, affine=True, 
-0004fca0: 7472 6163 6b5f 7275 6e6e 696e 675f 7374  track_running_st
-0004fcb0: 6174 733d 5472 7565 2929 203d 3e20 7072  ats=True)) => pr
-0004fcc0: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
-0004fcd0: 206f 6e20 6c61 7965 7231 2e30 2e63 6f6e   on layer1.0.con
-0004fce0: 7632 2028 436f 6e76 3264 2836 342c 2036  v2 (Conv2d(64, 6
-0004fcf0: 342c 206b 6572 6e65 6c5f 7369 7a65 3d28  4, kernel_size=(
-0004fd00: 332c 2033 292c 2073 7472 6964 653d 2831  3, 3), stride=(1
-0004fd10: 2c20 3129 2c20 7061 6464 696e 673d 2831  , 1), padding=(1
-0004fd20: 2c20 3129 2c20 6269 6173 3d46 616c 7365  , 1), bias=False
-0004fd30: 2929 2c20 6964 7873 3d5b 322c 2036 2c20  )), idxs=[2, 6, 
-0004fd40: 395d 0a2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  9].-------------
-0004fd50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-0004fd60: 2d2d 2d0a 6060 600a 466f 7220 6d6f 7265  ---.```.For more
-0004fd70: 2064 6574 6169 6c73 2061 626f 7574 2067   details about g
-0004fd80: 726f 7570 696e 672c 2070 6c65 6173 6520  rouping, please 
-0004fd90: 7265 6665 7220 746f 205b 7475 746f 7269  refer to [tutori
-0004fda0: 616c 732f 3220 2d20 4578 706c 6f72 696e  als/2 - Explorin
-0004fdb0: 6720 4465 7065 6e64 656e 6379 2047 726f  g Dependency Gro
-0004fdc0: 7570 735d 2868 7474 7073 3a2f 2f67 6974  ups](https://git
-0004fdd0: 6875 622e 636f 6d2f 5661 696e 462f 546f  hub.com/VainF/To
-0004fde0: 7263 682d 5072 756e 696e 672f 626c 6f62  rch-Pruning/blob
-0004fdf0: 2f6d 6173 7465 722f 7475 746f 7269 616c  /master/tutorial
-0004fe00: 732f 3225 3230 2d25 3230 4578 706c 6f72  s/2%20-%20Explor
-0004fe10: 696e 6725 3230 4465 7065 6e64 656e 6379  ing%20Dependency
-0004fe20: 2532 3047 726f 7570 732e 6970 796e 6229  %20Groups.ipynb)
-0004fe30: 0a20 200a 2323 2323 2048 6f77 2074 6f20  .  .#### How to 
-0004fe40: 7363 616e 2061 6c6c 2067 726f 7570 7320  scan all groups 
-0004fe50: 2841 6476 616e 6365 6429 3a0a 5765 2063  (Advanced):.We c
-0004fe60: 616e 2075 7365 2060 6044 472e 6765 745f  an use ``DG.get_
-0004fe70: 616c 6c5f 6772 6f75 7073 2869 676e 6f72  all_groups(ignor
-0004fe80: 6564 5f6c 6179 6572 732c 2072 6f6f 745f  ed_layers, root_
-0004fe90: 6d6f 6475 6c65 5f74 7970 6573 2960 6020  module_types)`` 
-0004fea0: 746f 2073 6361 6e20 616c 6c20 6772 6f75  to scan all grou
-0004feb0: 7073 2073 6571 7565 6e74 6961 6c6c 792e  ps sequentially.
-0004fec0: 2045 6163 6820 6772 6f75 7020 7769 6c6c   Each group will
-0004fed0: 2062 6567 696e 2077 6974 6820 6120 6c61   begin with a la
-0004fee0: 7965 7220 7468 6174 206d 6174 6368 6573  yer that matches
-0004fef0: 2061 2074 7970 6520 696e 2074 6865 2022   a type in the "
-0004ff00: 726f 6f74 5f6d 6f64 756c 655f 7479 7065  root_module_type
-0004ff10: 7322 2070 6172 616d 6574 6572 2e20 4e6f  s" parameter. No
-0004ff20: 7465 2074 6861 7420 4447 2e67 6574 5f61  te that DG.get_a
-0004ff30: 6c6c 5f67 726f 7570 7320 6973 206f 6e6c  ll_groups is onl
-0004ff40: 7920 7265 7370 6f6e 7369 626c 6520 666f  y responsible fo
-0004ff50: 7220 6772 6f75 7069 6e67 2061 6e64 2064  r grouping and d
-0004ff60: 6f65 7320 6e6f 7420 6861 7665 2061 6e79  oes not have any
-0004ff70: 206b 6e6f 776c 6564 6765 206f 7220 756e   knowledge or un
-0004ff80: 6465 7273 7461 6e64 696e 6720 6f66 2077  derstanding of w
-0004ff90: 6869 6368 2070 6172 616d 6574 6572 7320  hich parameters 
-0004ffa0: 7368 6f75 6c64 2062 6520 7072 756e 6564  should be pruned
-0004ffb0: 2e20 5468 6572 6566 6f72 652c 2069 7420  . Therefore, it 
-0004ffc0: 6973 206e 6563 6573 7361 7279 2074 6f20  is necessary to 
-0004ffd0: 7370 6563 6966 7920 7468 6520 7072 756e  specify the prun
-0004ffe0: 696e 6720 6964 7873 2075 7369 6e67 2020  ing idxs using  
-0004fff0: 6060 6772 6f75 702e 7072 756e 6528 6964  ``group.prune(id
-00050000: 7873 3d69 6478 7329 6060 2e0a 0a60 6060  xs=idxs)``...```
-00050010: 7079 7468 6f6e 0a66 6f72 2067 726f 7570  python.for group
-00050020: 2069 6e20 4447 2e67 6574 5f61 6c6c 5f67   in DG.get_all_g
-00050030: 726f 7570 7328 6967 6e6f 7265 645f 6c61  roups(ignored_la
-00050040: 7965 7273 3d5b 6d6f 6465 6c2e 636f 6e76  yers=[model.conv
-00050050: 315d 2c20 726f 6f74 5f6d 6f64 756c 655f  1], root_module_
-00050060: 7479 7065 733d 5b6e 6e2e 436f 6e76 3264  types=[nn.Conv2d
-00050070: 2c20 6e6e 2e4c 696e 6561 725d 293a 0a20  , nn.Linear]):. 
-00050080: 2020 2023 2068 616e 646c 6520 6772 6f75     # handle grou
-00050090: 7073 2069 6e20 7365 7175 656e 7469 616c  ps in sequential
-000500a0: 206f 7264 6572 0a20 2020 2069 6478 7320   order.    idxs 
-000500b0: 3d20 5b32 2c34 2c36 5d20 2320 796f 7572  = [2,4,6] # your
-000500c0: 2070 7275 6e69 6e67 2069 6e64 6963 6573   pruning indices
-000500d0: 0a20 2020 2067 726f 7570 2e70 7275 6e65  .    group.prune
-000500e0: 2869 6478 733d 6964 7873 290a 2020 2020  (idxs=idxs).    
-000500f0: 7072 696e 7428 6772 6f75 7029 0a60 6060  print(group).```
-00050100: 0a0a 2323 2320 322e 2048 6967 682d 6c65  ..### 2. High-le
-00050110: 7665 6c20 5072 756e 6572 730a 0a4c 6576  vel Pruners..Lev
-00050120: 6572 6167 696e 6720 7468 6520 4465 7065  eraging the Depe
-00050130: 6e64 656e 6379 4772 6170 682c 2077 6520  ndencyGraph, we 
-00050140: 6465 7665 6c6f 7065 6420 7365 7665 7261  developed severa
-00050150: 6c20 6869 6768 2d6c 6576 656c 2070 7275  l high-level pru
-00050160: 6e65 7273 2069 6e20 7468 6973 2072 6570  ners in this rep
-00050170: 6f73 6974 6f72 7920 746f 2066 6163 696c  ository to facil
-00050180: 6974 6174 6520 6566 666f 7274 6c65 7373  itate effortless
-00050190: 2070 7275 6e69 6e67 2e20 4279 2073 7065   pruning. By spe
-000501a0: 6369 6679 696e 6720 7468 6520 6465 7369  cifying the desi
-000501b0: 7265 6420 6368 616e 6e65 6c20 7370 6172  red channel spar
-000501c0: 7369 7479 2c20 796f 7520 6361 6e20 7072  sity, you can pr
-000501d0: 756e 6520 7468 6520 656e 7469 7265 206d  une the entire m
-000501e0: 6f64 656c 2061 6e64 2066 696e 652d 7475  odel and fine-tu
-000501f0: 6e65 2069 7420 7573 696e 6720 796f 7572  ne it using your
-00050200: 206f 776e 2074 7261 696e 696e 6720 636f   own training co
-00050210: 6465 2e20 466f 7220 6465 7461 696c 6564  de. For detailed
-00050220: 2069 6e66 6f72 6d61 7469 6f6e 206f 6e20   information on 
-00050230: 7468 6973 2070 726f 6365 7373 2c20 706c  this process, pl
-00050240: 6561 7365 2072 6566 6572 2074 6f20 5b74  ease refer to [t
-00050250: 6869 7320 7475 746f 7269 616c 5d28 6874  his tutorial](ht
-00050260: 7470 733a 2f2f 6769 7468 7562 2e63 6f6d  tps://github.com
-00050270: 2f56 6169 6e46 2f54 6f72 6368 2d50 7275  /VainF/Torch-Pru
-00050280: 6e69 6e67 2f62 6c6f 622f 6d61 7374 6572  ning/blob/master
-00050290: 2f74 7574 6f72 6961 6c73 2f31 2532 302d  /tutorials/1%20-
-000502a0: 2532 3043 7573 746f 6d69 7a65 2532 3059  %20Customize%20Y
-000502b0: 6f75 7225 3230 4f77 6e25 3230 5072 756e  our%20Own%20Prun
-000502c0: 6572 732e 6970 796e 6229 2c20 7768 6963  ers.ipynb), whic
-000502d0: 6820 7368 6f77 7320 686f 7720 746f 2069  h shows how to i
-000502e0: 6d70 6c65 6d65 6e74 2061 205b 736c 696d  mplement a [slim
-000502f0: 6d69 6e67 5d28 6874 7470 733a 2f2f 6172  ming](https://ar
-00050300: 7869 762e 6f72 672f 6162 732f 3137 3038  xiv.org/abs/1708
-00050310: 2e30 3635 3139 2920 7072 756e 6572 2066  .06519) pruner f
-00050320: 726f 6d20 7363 7261 7463 682e 2041 6464  rom scratch. Add
-00050330: 6974 696f 6e61 6c6c 792c 2079 6f75 2063  itionally, you c
-00050340: 616e 2066 696e 6420 6d6f 7265 2070 7261  an find more pra
-00050350: 6374 6963 616c 2065 7861 6d70 6c65 7320  ctical examples 
-00050360: 696e 205b 6265 6e63 686d 6172 6b73 2f6d  in [benchmarks/m
-00050370: 6169 6e2e 7079 5d28 6265 6e63 686d 6172  ain.py](benchmar
-00050380: 6b73 2f6d 6169 6e2e 7079 292e 0a0a 6060  ks/main.py)...``
-00050390: 6070 7974 686f 6e0a 696d 706f 7274 2074  `python.import t
-000503a0: 6f72 6368 0a66 726f 6d20 746f 7263 6876  orch.from torchv
-000503b0: 6973 696f 6e2e 6d6f 6465 6c73 2069 6d70  ision.models imp
-000503c0: 6f72 7420 7265 736e 6574 3138 0a69 6d70  ort resnet18.imp
-000503d0: 6f72 7420 746f 7263 685f 7072 756e 696e  ort torch_prunin
-000503e0: 6720 6173 2074 700a 0a6d 6f64 656c 203d  g as tp..model =
-000503f0: 2072 6573 6e65 7431 3828 7072 6574 7261   resnet18(pretra
-00050400: 696e 6564 3d54 7275 6529 0a0a 2320 496d  ined=True)..# Im
-00050410: 706f 7274 616e 6365 2063 7269 7465 7269  portance criteri
-00050420: 610a 6578 616d 706c 655f 696e 7075 7473  a.example_inputs
-00050430: 203d 2074 6f72 6368 2e72 616e 646e 2831   = torch.randn(1
-00050440: 2c20 332c 2032 3234 2c20 3232 3429 0a69  , 3, 224, 224).i
-00050450: 6d70 203d 2074 702e 696d 706f 7274 616e  mp = tp.importan
-00050460: 6365 2e54 6179 6c6f 7249 6d70 6f72 7461  ce.TaylorImporta
-00050470: 6e63 6528 290a 0a69 676e 6f72 6564 5f6c  nce()..ignored_l
-00050480: 6179 6572 7320 3d20 5b5d 0a66 6f72 206d  ayers = [].for m
-00050490: 2069 6e20 6d6f 6465 6c2e 6d6f 6475 6c65   in model.module
-000504a0: 7328 293a 0a20 2020 2069 6620 6973 696e  s():.    if isin
-000504b0: 7374 616e 6365 286d 2c20 746f 7263 682e  stance(m, torch.
-000504c0: 6e6e 2e4c 696e 6561 7229 2061 6e64 206d  nn.Linear) and m
-000504d0: 2e6f 7574 5f66 6561 7475 7265 7320 3d3d  .out_features ==
-000504e0: 2031 3030 303a 0a20 2020 2020 2020 2069   1000:.        i
-000504f0: 676e 6f72 6564 5f6c 6179 6572 732e 6170  gnored_layers.ap
-00050500: 7065 6e64 286d 2920 2320 444f 204e 4f54  pend(m) # DO NOT
-00050510: 2070 7275 6e65 2074 6865 2066 696e 616c   prune the final
-00050520: 2063 6c61 7373 6966 6965 7221 0a0a 6974   classifier!..it
-00050530: 6572 6174 6976 655f 7374 6570 7320 3d20  erative_steps = 
-00050540: 3520 2320 7072 6f67 7265 7373 6976 6520  5 # progressive 
-00050550: 7072 756e 696e 670a 7072 756e 6572 203d  pruning.pruner =
-00050560: 2074 702e 7072 756e 6572 2e4d 6167 6e69   tp.pruner.Magni
-00050570: 7475 6465 5072 756e 6572 280a 2020 2020  tudePruner(.    
-00050580: 6d6f 6465 6c2c 0a20 2020 2065 7861 6d70  model,.    examp
-00050590: 6c65 5f69 6e70 7574 732c 0a20 2020 2069  le_inputs,.    i
-000505a0: 6d70 6f72 7461 6e63 653d 696d 702c 0a20  mportance=imp,. 
-000505b0: 2020 2069 7465 7261 7469 7665 5f73 7465     iterative_ste
-000505c0: 7073 3d69 7465 7261 7469 7665 5f73 7465  ps=iterative_ste
-000505d0: 7073 2c0a 2020 2020 6368 5f73 7061 7273  ps,.    ch_spars
-000505e0: 6974 793d 302e 352c 2023 2072 656d 6f76  ity=0.5, # remov
-000505f0: 6520 3530 2520 6368 616e 6e65 6c73 2c20  e 50% channels, 
-00050600: 5265 734e 6574 3138 203d 207b 3634 2c20  ResNet18 = {64, 
-00050610: 3132 382c 2032 3536 2c20 3531 327d 203d  128, 256, 512} =
-00050620: 3e20 5265 734e 6574 3138 5f48 616c 6620  > ResNet18_Half 
-00050630: 3d20 7b33 322c 2036 342c 2031 3238 2c20  = {32, 64, 128, 
-00050640: 3235 367d 0a20 2020 2069 676e 6f72 6564  256}.    ignored
-00050650: 5f6c 6179 6572 733d 6967 6e6f 7265 645f  _layers=ignored_
-00050660: 6c61 7965 7273 2c0a 290a 0a62 6173 655f  layers,.)..base_
-00050670: 6d61 6373 2c20 6261 7365 5f6e 7061 7261  macs, base_npara
-00050680: 6d73 203d 2074 702e 7574 696c 732e 636f  ms = tp.utils.co
-00050690: 756e 745f 6f70 735f 616e 645f 7061 7261  unt_ops_and_para
-000506a0: 6d73 286d 6f64 656c 2c20 6578 616d 706c  ms(model, exampl
-000506b0: 655f 696e 7075 7473 290a 666f 7220 6920  e_inputs).for i 
-000506c0: 696e 2072 616e 6765 2869 7465 7261 7469  in range(iterati
-000506d0: 7665 5f73 7465 7073 293a 0a20 2020 2069  ve_steps):.    i
-000506e0: 6620 6973 696e 7374 616e 6365 2869 6d70  f isinstance(imp
-000506f0: 2c20 7470 2e69 6d70 6f72 7461 6e63 652e  , tp.importance.
-00050700: 5461 796c 6f72 496d 706f 7274 616e 6365  TaylorImportance
-00050710: 293a 0a20 2020 2020 2020 2023 2054 6179  ):.        # Tay
-00050720: 6c6f 7220 6578 7061 6e73 696f 6e20 7265  lor expansion re
-00050730: 7175 6972 6573 2067 7261 6469 656e 7473  quires gradients
-00050740: 2066 6f72 2069 6d70 6f72 7461 6e63 6520   for importance 
-00050750: 6573 7469 6d61 7469 6f6e 0a20 2020 2020  estimation.     
-00050760: 2020 206c 6f73 7320 3d20 6d6f 6465 6c28     loss = model(
-00050770: 6578 616d 706c 655f 696e 7075 7473 292e  example_inputs).
-00050780: 7375 6d28 2920 2320 6120 6475 6d6d 7920  sum() # a dummy 
-00050790: 6c6f 7373 2066 6f72 2054 6179 6c6f 7249  loss for TaylorI
-000507a0: 6d70 6f72 7461 6e63 650a 2020 2020 2020  mportance.      
-000507b0: 2020 6c6f 7373 2e62 6163 6b77 6172 6428    loss.backward(
-000507c0: 2920 2320 6265 666f 7265 2070 7275 6e65  ) # before prune
-000507d0: 722e 7374 6570 2829 0a20 2020 2070 7275  r.step().    pru
-000507e0: 6e65 722e 7374 6570 2829 0a20 2020 206d  ner.step().    m
-000507f0: 6163 732c 206e 7061 7261 6d73 203d 2074  acs, nparams = t
-00050800: 702e 7574 696c 732e 636f 756e 745f 6f70  p.utils.count_op
-00050810: 735f 616e 645f 7061 7261 6d73 286d 6f64  s_and_params(mod
-00050820: 656c 2c20 6578 616d 706c 655f 696e 7075  el, example_inpu
-00050830: 7473 290a 2020 2020 2320 6669 6e65 7475  ts).    # finetu
-00050840: 6e65 2079 6f75 7220 6d6f 6465 6c20 6865  ne your model he
-00050850: 7265 0a20 2020 2023 2066 696e 6574 756e  re.    # finetun
-00050860: 6528 6d6f 6465 6c29 0a20 2020 2023 202e  e(model).    # .
-00050870: 2e2e 0a60 6060 0a0a 2323 2323 2053 7061  ...```..#### Spa
-00050880: 7273 6520 5472 6169 6e69 6e67 0a53 6f6d  rse Training.Som
-00050890: 6520 7072 756e 6572 7320 6c69 6b65 205b  e pruners like [
-000508a0: 424e 5363 616c 6550 7275 6e65 725d 2868  BNScalePruner](h
-000508b0: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
-000508c0: 6d2f 5661 696e 462f 546f 7263 682d 5072  m/VainF/Torch-Pr
-000508d0: 756e 696e 672f 626c 6f62 2f64 6435 3939  uning/blob/dd599
-000508e0: 3231 3336 3564 3732 6163 6232 3835 3764  21365d72acb2857d
-000508f0: 3364 3734 6637 3563 3033 6534 3737 3036  3d74f75c03e47706
-00050900: 3066 622f 746f 7263 685f 7072 756e 696e  0fb/torch_prunin
-00050910: 672f 7072 756e 6572 2f61 6c67 6f72 6974  g/pruner/algorit
-00050920: 686d 732f 6261 7463 686e 6f72 6d5f 7363  hms/batchnorm_sc
-00050930: 616c 655f 7072 756e 6572 2e70 7923 4c34  ale_pruner.py#L4
-00050940: 3529 2061 6e64 205b 4772 6f75 704e 6f72  5) and [GroupNor
-00050950: 6d50 7275 6e65 725d 2868 7474 7073 3a2f  mPruner](https:/
-00050960: 2f67 6974 6875 622e 636f 6d2f 5661 696e  /github.com/Vain
-00050970: 462f 546f 7263 682d 5072 756e 696e 672f  F/Torch-Pruning/
-00050980: 626c 6f62 2f64 6435 3939 3231 3336 3564  blob/dd59921365d
-00050990: 3732 6163 6232 3835 3764 3364 3734 6637  72acb2857d3d74f7
-000509a0: 3563 3033 6534 3737 3036 3066 622f 746f  5c03e477060fb/to
-000509b0: 7263 685f 7072 756e 696e 672f 7072 756e  rch_pruning/prun
-000509c0: 6572 2f61 6c67 6f72 6974 686d 732f 6772  er/algorithms/gr
-000509d0: 6f75 705f 6e6f 726d 5f70 7275 6e65 722e  oup_norm_pruner.
-000509e0: 7079 234c 3533 2920 7265 7175 6972 6520  py#L53) require 
-000509f0: 7370 6172 7365 2074 7261 696e 696e 6720  sparse training 
-00050a00: 6265 666f 7265 2070 7275 6e69 6e67 2e20  before pruning. 
-00050a10: 5468 6973 2063 616e 2062 6520 6561 7369  This can be easi
-00050a20: 6c79 2061 6368 6965 7665 6420 6279 2069  ly achieved by i
-00050a30: 6e73 6572 7469 6e67 206a 7573 7420 6f6e  nserting just on
-00050a40: 6520 6c69 6e65 206f 6620 636f 6465 2060  e line of code `
-00050a50: 6070 7275 6e65 722e 7265 6775 6c61 7269  `pruner.regulari
-00050a60: 7a65 286d 6f64 656c 2960 6020 696e 2079  ze(model)`` in y
-00050a70: 6f75 7220 7472 6169 6e69 6e67 2073 6372  our training scr
-00050a80: 6970 742e 2054 6865 2070 7275 6e65 7220  ipt. The pruner 
-00050a90: 7769 6c6c 2075 7064 6174 6520 7468 6520  will update the 
-00050aa0: 6772 6164 6965 6e74 206f 6620 7472 6169  gradient of trai
-00050ab0: 6e61 626c 6520 7061 7261 6d65 7465 7273  nable parameters
-00050ac0: 2e0a 6060 6070 7974 686f 6e0a 666f 7220  ..```python.for 
-00050ad0: 6570 6f63 6820 696e 2072 616e 6765 2865  epoch in range(e
-00050ae0: 706f 6368 7329 3a0a 2020 2020 6d6f 6465  pochs):.    mode
-00050af0: 6c2e 7472 6169 6e28 290a 2020 2020 666f  l.train().    fo
-00050b00: 7220 692c 2028 6461 7461 2c20 7461 7267  r i, (data, targ
-00050b10: 6574 2920 696e 2065 6e75 6d65 7261 7465  et) in enumerate
-00050b20: 2874 7261 696e 5f6c 6f61 6465 7229 3a0a  (train_loader):.
-00050b30: 2020 2020 2020 2020 6461 7461 2c20 7461          data, ta
-00050b40: 7267 6574 203d 2064 6174 612e 746f 2864  rget = data.to(d
-00050b50: 6576 6963 6529 2c20 7461 7267 6574 2e74  evice), target.t
-00050b60: 6f28 6465 7669 6365 290a 2020 2020 2020  o(device).      
-00050b70: 2020 6f70 7469 6d69 7a65 722e 7a65 726f    optimizer.zero
-00050b80: 5f67 7261 6428 290a 2020 2020 2020 2020  _grad().        
-00050b90: 6f75 7420 3d20 6d6f 6465 6c28 6461 7461  out = model(data
-00050ba0: 290a 2020 2020 2020 2020 6c6f 7373 203d  ).        loss =
-00050bb0: 2046 2e63 726f 7373 5f65 6e74 726f 7079   F.cross_entropy
-00050bc0: 286f 7574 2c20 7461 7267 6574 290a 2020  (out, target).  
-00050bd0: 2020 2020 2020 6c6f 7373 2e62 6163 6b77        loss.backw
-00050be0: 6172 6428 290a 2020 2020 2020 2020 7072  ard().        pr
-00050bf0: 756e 6572 2e72 6567 756c 6172 697a 6528  uner.regularize(
-00050c00: 6d6f 6465 6c29 2023 203c 3d3d 2066 6f72  model) # <== for
-00050c10: 2073 7061 7273 6520 6c65 6172 6e69 6e67   sparse learning
-00050c20: 0a20 2020 2020 2020 206f 7074 696d 697a  .        optimiz
-00050c30: 6572 2e73 7465 7028 290a 6060 600a 0a23  er.step().```..#
-00050c40: 2323 2320 496e 7465 7261 6374 6976 6520  ### Interactive 
-00050c50: 5072 756e 696e 6720 2841 6476 616e 6365  Pruning (Advance
-00050c60: 6429 0a41 6c6c 2068 6967 682d 6c65 7665  d).All high-leve
-00050c70: 6c20 7072 756e 6572 7320 7375 7070 6f72  l pruners suppor
-00050c80: 7420 696e 7465 7261 6374 6976 6520 7072  t interactive pr
-00050c90: 756e 696e 672e 2055 7365 2060 6070 7275  uning. Use ``pru
-00050ca0: 6e65 722e 7374 6570 2869 6e74 6572 6163  ner.step(interac
-00050cb0: 7469 7665 3d54 7275 6529 6060 2074 6f20  tive=True)`` to 
-00050cc0: 6765 7420 616c 6c20 6772 6f75 7073 2061  get all groups a
-00050cd0: 6e64 2069 6e74 6572 6163 7469 7665 6c79  nd interactively
-00050ce0: 2070 7275 6e65 2074 6865 6d20 6279 2063   prune them by c
-00050cf0: 616c 6c69 6e67 2060 6067 726f 7570 2e70  alling ``group.p
-00050d00: 7275 6e65 2829 6060 2e20 5468 6973 2066  rune()``. This f
-00050d10: 6561 7475 7265 2069 7320 7573 6566 756c  eature is useful
-00050d20: 2069 6620 796f 7520 7761 6e74 2074 6f20   if you want to 
-00050d30: 636f 6e74 726f 6c2f 6d6f 6e69 746f 7220  control/monitor 
-00050d40: 7468 6520 7072 756e 696e 6720 7072 6f63  the pruning proc
-00050d50: 6573 732e 0a0a 6060 6070 7974 686f 6e0a  ess...```python.
-00050d60: 666f 7220 6920 696e 2072 616e 6765 2869  for i in range(i
-00050d70: 7465 7261 7469 7665 5f73 7465 7073 293a  terative_steps):
-00050d80: 0a20 2020 2066 6f72 2067 726f 7570 2069  .    for group i
-00050d90: 6e20 7072 756e 6572 2e73 7465 7028 696e  n pruner.step(in
-00050da0: 7465 7261 6374 6976 653d 5472 7565 293a  teractive=True):
-00050db0: 2023 2057 6172 6e69 6e67 3a20 6772 6f75   # Warning: grou
-00050dc0: 7073 206d 7573 7420 6265 2068 616e 646c  ps must be handl
-00050dd0: 6564 2073 6571 7565 6e74 6961 6c6c 792e  ed sequentially.
-00050de0: 2044 6f20 6e6f 7420 6b65 6570 2074 6865   Do not keep the
-00050df0: 6d20 6173 2061 206c 6973 742e 0a20 2020  m as a list..   
-00050e00: 2020 2020 2070 7269 6e74 2867 726f 7570       print(group
-00050e10: 2920 0a20 2020 2020 2020 2023 2064 6f20  ) .        # do 
-00050e20: 7768 6174 6576 6572 2079 6f75 206c 696b  whatever you lik
-00050e30: 6520 7769 7468 2074 6865 2067 726f 7570  e with the group
-00050e40: 200a 2020 2020 2020 2020 6465 702c 2069   .        dep, i
-00050e50: 6478 7320 3d20 6772 6f75 705b 305d 2023  dxs = group[0] #
-00050e60: 2067 6574 2074 6865 2069 6478 730a 2020   get the idxs.  
-00050e70: 2020 2020 2020 7461 7267 6574 5f6d 6f64        target_mod
-00050e80: 756c 6520 3d20 6465 702e 7461 7267 6574  ule = dep.target
-00050e90: 2e6d 6f64 756c 6520 2320 6765 7420 7468  .module # get th
-00050ea0: 6520 726f 6f74 206d 6f64 756c 650a 2020  e root module.  
-00050eb0: 2020 2020 2020 7072 756e 696e 675f 666e        pruning_fn
-00050ec0: 203d 2064 6570 2e68 616e 646c 6572 2023   = dep.handler #
-00050ed0: 2067 6574 2074 6865 2070 7275 6e69 6e67   get the pruning
-00050ee0: 2066 756e 6374 696f 6e0a 2020 2020 2020   function.      
-00050ef0: 200a 2020 2020 2020 2020 2320 446f 6e27   .        # Don'
-00050f00: 7420 666f 7267 6574 2074 6f20 7072 756e  t forget to prun
-00050f10: 6520 7468 6520 6772 6f75 700a 2020 2020  e the group.    
-00050f20: 2020 2020 6772 6f75 702e 7072 756e 6528      group.prune(
-00050f30: 290a 2020 2020 2020 2020 2020 0a20 2020  ).          .   
-00050f40: 2020 2020 2023 2067 726f 7570 2e70 7275       # group.pru
-00050f50: 6e65 2869 6478 733d 5b30 2c20 322c 2036  ne(idxs=[0, 2, 6
-00050f60: 5d29 2023 2049 7420 6973 2065 7665 6e20  ]) # It is even 
-00050f70: 706f 7373 6962 6c65 2074 6f20 6368 616e  possible to chan
-00050f80: 6765 2074 6865 2070 7275 6e69 6e67 2062  ge the pruning b
-00050f90: 6568 6176 696f 7572 2077 6974 6820 7468  ehaviour with th
-00050fa0: 6520 6964 7873 2070 6172 616d 6574 6572  e idxs parameter
-00050fb0: 0a20 2020 206d 6163 732c 206e 7061 7261  .    macs, npara
-00050fc0: 6d73 203d 2074 702e 7574 696c 732e 636f  ms = tp.utils.co
-00050fd0: 756e 745f 6f70 735f 616e 645f 7061 7261  unt_ops_and_para
-00050fe0: 6d73 286d 6f64 656c 2c20 6578 616d 706c  ms(model, exampl
-00050ff0: 655f 696e 7075 7473 290a 2020 2020 2320  e_inputs).    # 
-00051000: 6669 6e65 7475 6e65 2079 6f75 7220 6d6f  finetune your mo
-00051010: 6465 6c20 6865 7265 0a20 2020 2023 2066  del here.    # f
-00051020: 696e 6574 756e 6528 6d6f 6465 6c29 0a20  inetune(model). 
-00051030: 2020 2023 202e 2e2e 0a60 6060 0a0a 2323     # ....```..##
-00051040: 2323 2047 726f 7570 2d6c 6576 656c 2050  ## Group-level P
-00051050: 7275 6e69 6e67 0a0a 5769 7468 2044 6570  runing..With Dep
-00051060: 4772 6170 682c 2069 7420 6973 2065 6173  Graph, it is eas
-00051070: 7920 746f 2064 6573 6967 6e20 736f 6d65  y to design some
-00051080: 2022 6772 6f75 702d 6c65 7665 6c22 2063   "group-level" c
-00051090: 7269 7465 7269 6120 746f 2065 7374 696d  riteria to estim
-000510a0: 6174 6520 7468 6520 696d 706f 7274 616e  ate the importan
-000510b0: 6365 206f 6620 6120 7768 6f6c 6520 6772  ce of a whole gr
-000510c0: 6f75 7020 7261 7468 6572 2074 6861 6e20  oup rather than 
-000510d0: 6120 7369 6e67 6c65 206c 6179 6572 2e20  a single layer. 
-000510e0: 496e 2054 6f72 6368 2d70 7275 6e69 6e67  In Torch-pruning
-000510f0: 2c20 616c 6c20 7072 756e 6572 7320 776f  , all pruners wo
-00051100: 726b 2069 6e20 7468 6520 6772 6f75 7020  rk in the group 
-00051110: 6c65 7665 6c2e 0a0a 3c64 6976 2061 6c69  level...<div ali
-00051120: 676e 3d22 6365 6e74 6572 223e 0a3c 696d  gn="center">.<im
-00051130: 6720 7372 633d 2261 7373 6574 732f 6772  g src="assets/gr
-00051140: 6f75 705f 7370 6172 7369 7479 2e70 6e67  oup_sparsity.png
-00051150: 2220 7769 6474 683d 2238 3025 223e 0a3c  " width="80%">.<
-00051160: 2f64 6976 3e0a 0a23 2323 2033 2e20 5361  /div>..### 3. Sa
-00051170: 7665 2026 204c 6f61 640a 2020 2020 2020  ve & Load.      
-00051180: 2020 2020 0a54 6865 2066 6f6c 6c6f 7769      .The followi
-00051190: 6e67 2073 6372 6970 7420 7361 7665 7320  ng script saves 
-000511a0: 7468 6520 7768 6f6c 6520 6d6f 6465 6c20  the whole model 
-000511b0: 6f62 6a65 6374 2028 7374 7275 6374 7572  object (structur
-000511c0: 652b 7765 6967 6874 7329 2061 7320 6120  e+weights) as a 
-000511d0: 276d 6f64 656c 2e70 7468 272e 200a 6060  'model.pth'. .``
-000511e0: 6070 7974 686f 6e0a 6d6f 6465 6c2e 7a65  `python.model.ze
-000511f0: 726f 5f67 7261 6428 2920 2320 5765 2064  ro_grad() # We d
-00051200: 6f6e 2774 2077 616e 7420 746f 2073 746f  on't want to sto
-00051210: 7265 2067 7261 6469 656e 7420 696e 666f  re gradient info
-00051220: 726d 6174 696f 6e0a 746f 7263 682e 7361  rmation.torch.sa
-00051230: 7665 286d 6f64 656c 2c20 276d 6f64 656c  ve(model, 'model
-00051240: 2e70 7468 2729 2023 2077 6974 686f 7574  .pth') # without
-00051250: 202e 7374 6174 655f 6469 6374 0a6d 6f64   .state_dict.mod
-00051260: 656c 203d 2074 6f72 6368 2e6c 6f61 6428  el = torch.load(
-00051270: 276d 6f64 656c 2e70 7468 2729 2023 206c  'model.pth') # l
-00051280: 6f61 6420 7468 6520 7072 756e 6564 206d  oad the pruned m
-00051290: 6f64 656c 0a60 6060 0a0a 2a2a 4578 7065  odel.```..**Expe
-000512a0: 7269 6d65 6e74 616c 2046 6561 7475 7265  rimental Feature
-000512b0: 732a 2a3a 2052 652d 6372 6561 7465 2070  s**: Re-create p
-000512c0: 7275 6e65 6420 6d6f 6465 6c73 2066 726f  runed models fro
-000512d0: 6d20 756e 7072 756e 6564 206f 6e65 7320  m unpruned ones 
-000512e0: 7573 696e 6720 6060 7470 2e73 7461 7465  using ``tp.state
-000512f0: 5f64 6963 7460 6020 616e 6420 6060 7470  _dict`` and ``tp
-00051300: 2e6c 6f61 645f 7374 6174 655f 6469 6374  .load_state_dict
-00051310: 6060 2e0a 6060 6070 7974 686f 6e0a 2320  ``..```python.# 
-00051320: 7361 7665 2074 6865 2070 7275 6e65 6420  save the pruned 
-00051330: 7374 6174 655f 6469 6374 2c20 7768 6963  state_dict, whic
-00051340: 6820 696e 636c 7564 6573 2062 6f74 6820  h includes both 
-00051350: 7072 756e 6564 2070 6172 616d 6574 6572  pruned parameter
-00051360: 7320 616e 6420 6d6f 6469 6669 6564 2061  s and modified a
-00051370: 7474 7269 6275 7465 730a 7374 6174 655f  ttributes.state_
-00051380: 6469 6374 203d 2074 702e 7374 6174 655f  dict = tp.state_
-00051390: 6469 6374 2870 7275 6e65 645f 6d6f 6465  dict(pruned_mode
-000513a0: 6c29 2023 2074 6865 2070 7275 6e65 6420  l) # the pruned 
-000513b0: 6d6f 6465 6c2c 2065 2e67 2e2c 2061 2072  model, e.g., a r
-000513c0: 6573 6e65 742d 3138 2d68 616c 660a 746f  esnet-18-half.to
-000513d0: 7263 682e 7361 7665 2873 7461 7465 5f64  rch.save(state_d
-000513e0: 6963 742c 2027 7072 756e 6564 2e70 7468  ict, 'pruned.pth
-000513f0: 2729 0a0a 2320 6372 6561 7465 2061 206e  ')..# create a n
-00051400: 6577 206d 6f64 656c 2c20 652e 672e 2072  ew model, e.g. r
-00051410: 6573 6e65 7431 380a 6e65 775f 6d6f 6465  esnet18.new_mode
-00051420: 6c20 3d20 7265 736e 6574 3138 2829 2e65  l = resnet18().e
-00051430: 7661 6c28 290a 0a23 206c 6f61 6420 7468  val()..# load th
-00051440: 6520 7072 756e 6564 2073 7461 7465 5f64  e pruned state_d
-00051450: 6963 7420 696e 746f 2074 6865 2075 6e70  ict into the unp
-00051460: 7275 6e65 6420 6d6f 6465 6c2e 0a6c 6f61  runed model..loa
-00051470: 6465 645f 7374 6174 655f 6469 6374 203d  ded_state_dict =
-00051480: 2074 6f72 6368 2e6c 6f61 6428 2770 7275   torch.load('pru
-00051490: 6e65 642e 7074 6827 2c20 6d61 705f 6c6f  ned.pth', map_lo
-000514a0: 6361 7469 6f6e 3d27 6370 7527 290a 7470  cation='cpu').tp
-000514b0: 2e6c 6f61 645f 7374 6174 655f 6469 6374  .load_state_dict
-000514c0: 286e 6577 5f6d 6f64 656c 2c20 7374 6174  (new_model, stat
-000514d0: 655f 6469 6374 3d6c 6f61 6465 645f 7374  e_dict=loaded_st
-000514e0: 6174 655f 6469 6374 290a 7072 696e 7428  ate_dict).print(
-000514f0: 6e65 775f 6d6f 6465 6c29 2023 2054 6869  new_model) # Thi
-00051500: 7320 7769 6c6c 2062 6520 6120 7072 756e  s will be a prun
-00051510: 6564 206d 6f64 656c 2e0a 6060 600a 5265  ed model..```.Re
-00051520: 6665 7220 746f 205b 7465 7374 732f 7465  fer to [tests/te
-00051530: 7374 5f73 6572 6961 6c69 7a61 7469 6f6e  st_serialization
-00051540: 2e70 795d 2874 6573 7473 2f74 6573 745f  .py](tests/test_
-00051550: 7365 7269 616c 697a 6174 696f 6e2e 7079  serialization.py
-00051560: 2920 666f 7220 616e 2056 6954 2065 7861  ) for an ViT exa
-00051570: 6d70 6c65 2e20 496e 2074 6869 7320 6578  mple. In this ex
-00051580: 616d 706c 652c 2077 6520 7769 6c6c 2070  ample, we will p
-00051590: 7275 6e65 2074 6865 206d 6f64 656c 2061  rune the model a
-000515a0: 6e64 206d 6f64 6966 7920 736f 6d65 2061  nd modify some a
-000515b0: 7474 7269 6275 7465 7320 6c69 6b65 2060  ttributes like `
-000515c0: 606d 6f64 656c 2e68 6964 6465 6e5f 6469  `model.hidden_di
-000515d0: 6d73 6060 2e0a 2020 2020 2020 2020 2020  ms``..          
-000515e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000515f0: 2020 2020 2020 2020 2020 2020 2020 0a20                . 
-00051600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00051610: 2020 2020 2020 2020 2020 2020 2020 200a                 .
-00051620: 2323 2320 342e 204c 6f77 2d6c 6576 656c  ### 4. Low-level
-00051630: 2050 7275 6e69 6e67 2046 756e 6374 696f   Pruning Functio
-00051640: 6e73 0a0a 5768 696c 6520 6974 2069 7320  ns..While it is 
-00051650: 706f 7373 6962 6c65 2074 6f20 6d61 6e75  possible to manu
-00051660: 616c 6c79 2070 7275 6e65 2079 6f75 7220  ally prune your 
-00051670: 6d6f 6465 6c20 7573 696e 6720 6c6f 772d  model using low-
-00051680: 6c65 7665 6c20 6675 6e63 7469 6f6e 732c  level functions,
-00051690: 2074 6869 7320 6170 7072 6f61 6368 2063   this approach c
-000516a0: 616e 2062 6520 7175 6974 6520 6c61 626f  an be quite labo
-000516b0: 7269 6f75 732c 2061 7320 6974 2072 6571  rious, as it req
-000516c0: 7569 7265 7320 6361 7265 6675 6c20 6d61  uires careful ma
-000516d0: 6e61 6765 6d65 6e74 206f 6620 7468 6520  nagement of the 
-000516e0: 6173 736f 6369 6174 6564 2064 6570 656e  associated depen
-000516f0: 6465 6e63 6965 732e 2041 7320 6120 7265  dencies. As a re
-00051700: 7375 6c74 2c20 7765 2072 6563 6f6d 6d65  sult, we recomme
-00051710: 6e64 2075 7469 6c69 7a69 6e67 2074 6865  nd utilizing the
-00051720: 2061 666f 7265 6d65 6e74 696f 6e65 6420   aforementioned 
-00051730: 6869 6768 2d6c 6576 656c 2070 7275 6e65  high-level prune
-00051740: 7273 2074 6f20 7374 7265 616d 6c69 6e65  rs to streamline
-00051750: 2074 6865 2070 7275 6e69 6e67 2070 726f   the pruning pro
-00051760: 6365 7373 2e0a 0a60 6060 7079 7468 6f6e  cess...```python
-00051770: 0a74 702e 7072 756e 655f 636f 6e76 5f6f  .tp.prune_conv_o
-00051780: 7574 5f63 6861 6e6e 656c 7328 206d 6f64  ut_channels( mod
-00051790: 656c 2e63 6f6e 7631 2c20 6964 7873 3d5b  el.conv1, idxs=[
-000517a0: 322c 362c 395d 2029 0a0a 2320 6669 7820  2,6,9] )..# fix 
-000517b0: 7468 6520 6272 6f6b 656e 2064 6570 656e  the broken depen
-000517c0: 6465 6e63 6965 7320 6d61 6e75 616c 6c79  dencies manually
-000517d0: 0a74 702e 7072 756e 655f 6261 7463 686e  .tp.prune_batchn
-000517e0: 6f72 6d5f 6f75 745f 6368 616e 6e65 6c73  orm_out_channels
-000517f0: 2820 6d6f 6465 6c2e 626e 312c 2069 6478  ( model.bn1, idx
-00051800: 733d 5b32 2c36 2c39 5d20 290a 7470 2e70  s=[2,6,9] ).tp.p
-00051810: 7275 6e65 5f63 6f6e 765f 696e 5f63 6861  rune_conv_in_cha
-00051820: 6e6e 656c 7328 206d 6f64 656c 2e6c 6179  nnels( model.lay
-00051830: 6572 325b 305d 2e63 6f6e 7631 2c20 6964  er2[0].conv1, id
-00051840: 7873 3d5b 322c 362c 395d 2029 0a2e 2e2e  xs=[2,6,9] )....
-00051850: 0a60 6060 0a0a 5468 6520 666f 6c6c 6f77  .```..The follow
-00051860: 696e 6720 7072 756e 696e 6720 6675 6e63  ing pruning func
-00051870: 7469 6f6e 7320 6172 6520 6176 6169 6c61  tions are availa
-00051880: 626c 653a 0a60 6060 7079 7468 6f6e 0a27  ble:.```python.'
-00051890: 7072 756e 655f 636f 6e76 5f6f 7574 5f63  prune_conv_out_c
-000518a0: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
-000518b0: 5f63 6f6e 765f 696e 5f63 6861 6e6e 656c  _conv_in_channel
-000518c0: 7327 2c0a 2770 7275 6e65 5f64 6570 7468  s',.'prune_depth
-000518d0: 7769 7365 5f63 6f6e 765f 6f75 745f 6368  wise_conv_out_ch
-000518e0: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-000518f0: 6465 7074 6877 6973 655f 636f 6e76 5f69  depthwise_conv_i
-00051900: 6e5f 6368 616e 6e65 6c73 272c 0a27 7072  n_channels',.'pr
-00051910: 756e 655f 6261 7463 686e 6f72 6d5f 6f75  une_batchnorm_ou
-00051920: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
-00051930: 756e 655f 6261 7463 686e 6f72 6d5f 696e  une_batchnorm_in
-00051940: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-00051950: 6e65 5f6c 696e 6561 725f 6f75 745f 6368  ne_linear_out_ch
-00051960: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-00051970: 6c69 6e65 6172 5f69 6e5f 6368 616e 6e65  linear_in_channe
-00051980: 6c73 272c 0a27 7072 756e 655f 7072 656c  ls',.'prune_prel
-00051990: 755f 6f75 745f 6368 616e 6e65 6c73 272c  u_out_channels',
-000519a0: 0a27 7072 756e 655f 7072 656c 755f 696e  .'prune_prelu_in
-000519b0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-000519c0: 6e65 5f6c 6179 6572 6e6f 726d 5f6f 7574  ne_layernorm_out
-000519d0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-000519e0: 6e65 5f6c 6179 6572 6e6f 726d 5f69 6e5f  ne_layernorm_in_
-000519f0: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
-00051a00: 655f 656d 6265 6464 696e 675f 6f75 745f  e_embedding_out_
-00051a10: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
-00051a20: 655f 656d 6265 6464 696e 675f 696e 5f63  e_embedding_in_c
-00051a30: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
-00051a40: 5f70 6172 616d 6574 6572 5f6f 7574 5f63  _parameter_out_c
-00051a50: 6861 6e6e 656c 7327 2c0a 2770 7275 6e65  hannels',.'prune
-00051a60: 5f70 6172 616d 6574 6572 5f69 6e5f 6368  _parameter_in_ch
-00051a70: 616e 6e65 6c73 272c 0a27 7072 756e 655f  annels',.'prune_
-00051a80: 6d75 6c74 6968 6561 645f 6174 7465 6e74  multihead_attent
-00051a90: 696f 6e5f 6f75 745f 6368 616e 6e65 6c73  ion_out_channels
-00051aa0: 272c 0a27 7072 756e 655f 6d75 6c74 6968  ',.'prune_multih
-00051ab0: 6561 645f 6174 7465 6e74 696f 6e5f 696e  ead_attention_in
-00051ac0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-00051ad0: 6e65 5f67 726f 7570 6e6f 726d 5f6f 7574  ne_groupnorm_out
-00051ae0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
-00051af0: 6e65 5f67 726f 7570 6e6f 726d 5f69 6e5f  ne_groupnorm_in_
-00051b00: 6368 616e 6e65 6c73 272c 0a27 7072 756e  channels',.'prun
-00051b10: 655f 696e 7374 616e 6365 6e6f 726d 5f6f  e_instancenorm_o
-00051b20: 7574 5f63 6861 6e6e 656c 7327 2c0a 2770  ut_channels',.'p
-00051b30: 7275 6e65 5f69 6e73 7461 6e63 656e 6f72  rune_instancenor
-00051b40: 6d5f 696e 5f63 6861 6e6e 656c 7327 2c0a  m_in_channels',.
-00051b50: 6060 600a 0a0a 0a23 2323 2035 2e20 4375  ```....### 5. Cu
-00051b60: 7374 6f6d 697a 6564 204c 6179 6572 730a  stomized Layers.
-00051b70: 0a50 6c65 6173 6520 7265 6665 7220 746f  .Please refer to
-00051b80: 205b 7465 7374 732f 7465 7374 5f63 7573   [tests/test_cus
-00051b90: 746f 6d69 7a65 645f 6c61 7965 722e 7079  tomized_layer.py
-00051ba0: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
-00051bb0: 2e63 6f6d 2f56 6169 6e46 2f54 6f72 6368  .com/VainF/Torch
-00051bc0: 2d50 7275 6e69 6e67 2f62 6c6f 622f 6d61  -Pruning/blob/ma
-00051bd0: 7374 6572 2f74 6573 7473 2f74 6573 745f  ster/tests/test_
-00051be0: 6375 7374 6f6d 697a 6564 5f6c 6179 6572  customized_layer
-00051bf0: 2e70 7929 2e0a 0a23 2323 2036 2e20 4265  .py)...### 6. Be
-00051c00: 6e63 686d 6172 6b73 0a0a 4f75 7220 7265  nchmarks..Our re
-00051c10: 7375 6c74 7320 6f6e 207b 5265 734e 6574  sults on {ResNet
-00051c20: 2d35 3620 2f20 4349 4641 522d 3130 202f  -56 / CIFAR-10 /
-00051c30: 2032 2e30 3078 7d0a 0a7c 204d 6574 686f   2.00x}..| Metho
-00051c40: 6420 7c20 4261 7365 2028 2529 207c 2050  d | Base (%) | P
-00051c50: 7275 6e65 6420 2825 2920 7c20 245c 4465  runed (%) | $\De
-00051c60: 6c74 6124 2041 6363 2028 2529 207c 2053  lta$ Acc (%) | S
-00051c70: 7065 6564 2055 7020 7c0a 7c3a 2d2d 2020  peed Up |.|:--  
-00051c80: 2020 7c3a 2d2d 3a20 207c 3a2d 2d3a 2020    |:--:  |:--:  
-00051c90: 2020 7c3a 2d2d 3a20 7c3a 2d2d 3a20 2020    |:--: |:--:   
-00051ca0: 2020 207c 0a7c 204e 4950 5320 5b5b 315d     |.| NIPS [[1]
-00051cb0: 5d28 2331 2920 207c 202d 2020 2020 7c20  ](#1)  | -    | 
-00051cc0: 2d20 2020 2020 207c 2d30 2e30 3320 7c20  -      |-0.03 | 
-00051cd0: 312e 3736 7820 2020 207c 0a7c 2047 656f  1.76x    |.| Geo
-00051ce0: 6d65 7472 6963 205b 5b32 5d5d 2823 3229  metric [[2]](#2)
-00051cf0: 207c 2039 332e 3539 207c 2039 332e 3236   | 93.59 | 93.26
-00051d00: 207c 202d 302e 3333 207c 2031 2e37 3078   | -0.33 | 1.70x
-00051d10: 207c 0a7c 2050 6f6c 6172 205b 5b33 5d5d   |.| Polar [[3]]
-00051d20: 2823 3329 2020 7c20 3933 2e38 3020 7c20  (#3)  | 93.80 | 
-00051d30: 3933 2e38 3320 7c20 2b30 2e30 3320 7c31  93.83 | +0.03 |1
-00051d40: 2e38 3878 207c 0a7c 2043 5020 205b 5b34  .88x |.| CP  [[4
-00051d50: 5d5d 2823 3429 2020 207c 2039 322e 3830  ]](#4)   | 92.80
-00051d60: 207c 2039 312e 3830 207c 202d 312e 3030   | 91.80 | -1.00
-00051d70: 207c 322e 3030 7820 7c0a 7c20 414d 4320   |2.00x |.| AMC 
-00051d80: 5b5b 355d 5d28 2335 2920 2020 7c20 3932  [[5]](#5)   | 92
-00051d90: 2e38 3020 7c20 3931 2e39 3020 7c20 2d30  .80 | 91.90 | -0
-00051da0: 2e39 3020 7c32 2e30 3078 207c 0a7c 2048  .90 |2.00x |.| H
-00051db0: 5261 6e6b 205b 5b36 5d5d 2823 3629 207c  Rank [[6]](#6) |
-00051dc0: 2039 332e 3236 207c 2039 322e 3137 207c   93.26 | 92.17 |
-00051dd0: 202d 302e 3039 207c 322e 3030 7820 7c0a   -0.09 |2.00x |.
-00051de0: 7c20 5346 5020 205b 5b37 5d5d 2823 3729  | SFP  [[7]](#7)
-00051df0: 2020 7c20 3933 2e35 3920 7c20 3933 2e33    | 93.59 | 93.3
-00051e00: 3620 7c20 2b30 2e32 3320 7c32 2e31 3178  6 | +0.23 |2.11x
-00051e10: 207c 0a7c 2052 6573 5265 7020 5b5b 385d   |.| ResRep [[8]
-00051e20: 5d28 2338 2920 7c20 3933 2e37 3120 7c20  ](#8) | 93.71 | 
-00051e30: 3933 2e37 3120 7c20 2b30 2e30 3020 7c32  93.71 | +0.00 |2
-00051e40: 2e31 3278 207c 0a7c 7c0a 7c20 4f75 7273  .12x |.||.| Ours
-00051e50: 2d4c 3120 7c20 3933 2e35 3320 7c20 3932  -L1 | 93.53 | 92
-00051e60: 2e39 3320 7c20 2d30 2e36 3020 7c20 322e  .93 | -0.60 | 2.
-00051e70: 3132 7820 7c0a 7c20 4f75 7273 2d42 4e20  12x |.| Ours-BN 
-00051e80: 7c20 3933 2e35 3320 7c20 3933 2e32 3920  | 93.53 | 93.29 
-00051e90: 7c20 2d30 2e32 3420 7c20 322e 3132 7820  | -0.24 | 2.12x 
-00051ea0: 7c0a 7c20 4f75 7273 2d47 726f 7570 207c  |.| Ours-Group |
-00051eb0: 2039 332e 3533 207c 2039 332e 3737 207c   93.53 | 93.77 |
-00051ec0: 202b 302e 3338 207c 2032 2e31 3378 207c   +0.38 | 2.13x |
-00051ed0: 0a0a 506c 6561 7365 2072 6566 6572 2074  ..Please refer t
-00051ee0: 6f20 5b62 656e 6368 6d61 726b 735d 2862  o [benchmarks](b
-00051ef0: 656e 6368 6d61 726b 7329 2066 6f72 206d  enchmarks) for m
-00051f00: 6f72 6520 6465 7461 696c 732e 0a0a 2323  ore details...##
-00051f10: 2320 372e 2053 6572 6965 7320 6f66 2057  # 7. Series of W
-00051f20: 6f72 6b73 0a3e 202a 2a4c 4c4d 2d50 7275  orks.> **LLM-Pru
-00051f30: 6e65 723a 204f 6e20 7468 6520 5374 7275  ner: On the Stru
-00051f40: 6374 7572 616c 2050 7275 6e69 6e67 206f  ctural Pruning o
-00051f50: 6620 4c61 7267 6520 4c61 6e67 7561 6765  f Large Language
-00051f60: 204d 6f64 656c 732a 2a20 5b5b 5072 6f6a   Models** [[Proj
-00051f70: 6563 745d 5d28 6874 7470 733a 2f2f 6769  ect]](https://gi
-00051f80: 7468 7562 2e63 6f6d 2f68 6f72 7365 6565  thub.com/horseee
-00051f90: 2f4c 4c4d 2d50 7275 6e65 7229 205b 5b61  /LLM-Pruner) [[a
-00051fa0: 7258 6976 5d5d 2868 7474 7073 3a2f 2f61  rXiv]](https://a
-00051fb0: 7278 6976 2e6f 7267 2f61 6273 2f32 3330  rxiv.org/abs/230
-00051fc0: 352e 3131 3632 3729 2020 200a 3e20 2a58  5.11627)   .> *X
-00051fd0: 696e 7969 6e20 4d61 2c20 476f 6e67 6661  inyin Ma, Gongfa
-00051fe0: 6e20 4661 6e67 2c20 5869 6e63 6861 6f20  n Fang, Xinchao 
-00051ff0: 5761 6e67 2a20 2020 0a0a 3e20 2a2a 5374  Wang*   ..> **St
-00052000: 7275 6374 7572 616c 2050 7275 6e69 6e67  ructural Pruning
-00052010: 2066 6f72 2044 6966 6675 7369 6f6e 204d   for Diffusion M
-00052020: 6f64 656c 732a 2a20 5b5b 5072 6f6a 6563  odels** [[Projec
-00052030: 745d 5d28 6874 7470 733a 2f2f 6769 7468  t]](https://gith
-00052040: 7562 2e63 6f6d 2f56 6169 6e46 2f44 6966  ub.com/VainF/Dif
-00052050: 662d 5072 756e 696e 6729 205b 5b61 7278  f-Pruning) [[arx
-00052060: 6976 5d5d 2868 7474 7073 3a2f 2f61 7278  iv]](https://arx
-00052070: 6976 2e6f 7267 2f61 6273 2f32 3330 352e  iv.org/abs/2305.
-00052080: 3130 3932 3429 2020 0a3e 202a 476f 6e67  10924)  .> *Gong
-00052090: 6661 6e20 4661 6e67 2c20 5869 6e79 696e  fan Fang, Xinyin
-000520a0: 204d 612c 2058 696e 6368 616f 2057 616e   Ma, Xinchao Wan
-000520b0: 672a 2020 2020 0a0a 0a23 2320 4369 7461  g*    ...## Cita
-000520c0: 7469 6f6e 0a60 6060 0a40 696e 7072 6f63  tion.```.@inproc
-000520d0: 6565 6469 6e67 737b 6661 6e67 3230 3233  eedings{fang2023
-000520e0: 6465 7067 7261 7068 2c0a 2020 7469 746c  depgraph,.  titl
-000520f0: 653d 7b44 6570 6772 6170 683a 2054 6f77  e={Depgraph: Tow
-00052100: 6172 6473 2061 6e79 2073 7472 7563 7475  ards any structu
-00052110: 7261 6c20 7072 756e 696e 677d 2c0a 2020  ral pruning},.  
-00052120: 6175 7468 6f72 3d7b 4661 6e67 2c20 476f  author={Fang, Go
-00052130: 6e67 6661 6e20 616e 6420 4d61 2c20 5869  ngfan and Ma, Xi
-00052140: 6e79 696e 2061 6e64 2053 6f6e 672c 204d  nyin and Song, M
-00052150: 696e 676c 6920 616e 6420 4d69 2c20 4d69  ingli and Mi, Mi
-00052160: 6368 6165 6c20 4269 2061 6e64 2057 616e  chael Bi and Wan
-00052170: 672c 2058 696e 6368 616f 7d2c 0a20 2062  g, Xinchao},.  b
-00052180: 6f6f 6b74 6974 6c65 3d7b 5072 6f63 6565  ooktitle={Procee
-00052190: 6469 6e67 7320 6f66 2074 6865 2049 4545  dings of the IEE
-000521a0: 452f 4356 4620 436f 6e66 6572 656e 6365  E/CVF Conference
-000521b0: 206f 6e20 436f 6d70 7574 6572 2056 6973   on Computer Vis
-000521c0: 696f 6e20 616e 6420 5061 7474 6572 6e20  ion and Pattern 
-000521d0: 5265 636f 676e 6974 696f 6e7d 2c0a 2020  Recognition},.  
-000521e0: 7061 6765 733d 7b31 3630 3931 2d2d 3136  pages={16091--16
-000521f0: 3130 317d 2c0a 2020 7965 6172 3d7b 3230  101},.  year={20
-00052200: 3233 7d0a 7d0a 6060 600a 0a00 0000 0000  23}.}.```.......
-00052210: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00052220: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00052230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
+0004f140: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2020  ------.         
+0004f150: 2050 7275 6e69 6e67 2047 726f 7570 0a2d   Pruning Group.-
+0004f160: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+0004f170: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
+0004f180: 5b30 5d20 7072 756e 655f 6f75 745f 6368  [0] prune_out_ch
+0004f190: 616e 6e65 6c73 206f 6e20 636f 6e76 3120  annels on conv1 
+0004f1a0: 2843 6f6e 7632 6428 332c 2036 342c 206b  (Conv2d(3, 64, k
+0004f1b0: 6572 6e65 6c5f 7369 7a65 3d28 372c 2037  ernel_size=(7, 7
+0004f1c0: 292c 2073 7472 6964 653d 2832 2c20 3229  ), stride=(2, 2)
+0004f1d0: 2c20 7061 6464 696e 673d 2833 2c20 3329  , padding=(3, 3)
+0004f1e0: 2c20 6269 6173 3d46 616c 7365 2929 203d  , bias=False)) =
+0004f1f0: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
+0004f200: 6e65 6c73 206f 6e20 636f 6e76 3120 2843  nels on conv1 (C
+0004f210: 6f6e 7632 6428 332c 2036 342c 206b 6572  onv2d(3, 64, ker
+0004f220: 6e65 6c5f 7369 7a65 3d28 372c 2037 292c  nel_size=(7, 7),
+0004f230: 2073 7472 6964 653d 2832 2c20 3229 2c20   stride=(2, 2), 
+0004f240: 7061 6464 696e 673d 2833 2c20 3329 2c20  padding=(3, 3), 
+0004f250: 6269 6173 3d46 616c 7365 2929 2c20 6964  bias=False)), id
+0004f260: 7873 3d5b 322c 2036 2c20 395d 2028 5072  xs=[2, 6, 9] (Pr
+0004f270: 756e 696e 6720 526f 6f74 290a 5b31 5d20  uning Root).[1] 
+0004f280: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
+0004f290: 6c73 206f 6e20 636f 6e76 3120 2843 6f6e  ls on conv1 (Con
+0004f2a0: 7632 6428 332c 2036 342c 206b 6572 6e65  v2d(3, 64, kerne
+0004f2b0: 6c5f 7369 7a65 3d28 372c 2037 292c 2073  l_size=(7, 7), s
+0004f2c0: 7472 6964 653d 2832 2c20 3229 2c20 7061  tride=(2, 2), pa
+0004f2d0: 6464 696e 673d 2833 2c20 3329 2c20 6269  dding=(3, 3), bi
+0004f2e0: 6173 3d46 616c 7365 2929 203d 3e20 7072  as=False)) => pr
+0004f2f0: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
+0004f300: 206f 6e20 626e 3120 2842 6174 6368 4e6f   on bn1 (BatchNo
+0004f310: 726d 3264 2836 342c 2065 7073 3d31 652d  rm2d(64, eps=1e-
+0004f320: 3035 2c20 6d6f 6d65 6e74 756d 3d30 2e31  05, momentum=0.1
+0004f330: 2c20 6166 6669 6e65 3d54 7275 652c 2074  , affine=True, t
+0004f340: 7261 636b 5f72 756e 6e69 6e67 5f73 7461  rack_running_sta
+0004f350: 7473 3d54 7275 6529 292c 2069 6478 733d  ts=True)), idxs=
+0004f360: 5b32 2c20 362c 2039 5d0a 5b32 5d20 7072  [2, 6, 9].[2] pr
+0004f370: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
+0004f380: 206f 6e20 626e 3120 2842 6174 6368 4e6f   on bn1 (BatchNo
+0004f390: 726d 3264 2836 342c 2065 7073 3d31 652d  rm2d(64, eps=1e-
+0004f3a0: 3035 2c20 6d6f 6d65 6e74 756d 3d30 2e31  05, momentum=0.1
+0004f3b0: 2c20 6166 6669 6e65 3d54 7275 652c 2074  , affine=True, t
+0004f3c0: 7261 636b 5f72 756e 6e69 6e67 5f73 7461  rack_running_sta
+0004f3d0: 7473 3d54 7275 6529 2920 3d3e 2070 7275  ts=True)) => pru
+0004f3e0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+0004f3f0: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+0004f400: 705f 3230 2852 656c 7542 6163 6b77 6172  p_20(ReluBackwar
+0004f410: 6430 292c 2069 6478 733d 5b32 2c20 362c  d0), idxs=[2, 6,
+0004f420: 2039 5d0a 5b33 5d20 7072 756e 655f 6f75   9].[3] prune_ou
+0004f430: 745f 6368 616e 6e65 6c73 206f 6e20 5f45  t_channels on _E
+0004f440: 6c65 6d65 6e74 5769 7365 4f70 5f32 3028  lementWiseOp_20(
+0004f450: 5265 6c75 4261 636b 7761 7264 3029 203d  ReluBackward0) =
+0004f460: 3e20 7072 756e 655f 6f75 745f 6368 616e  > prune_out_chan
+0004f470: 6e65 6c73 206f 6e20 5f45 6c65 6d65 6e74  nels on _Element
+0004f480: 5769 7365 4f70 5f31 3928 4d61 7850 6f6f  WiseOp_19(MaxPoo
+0004f490: 6c32 4457 6974 6849 6e64 6963 6573 4261  l2DWithIndicesBa
+0004f4a0: 636b 7761 7264 3029 2c20 6964 7873 3d5b  ckward0), idxs=[
+0004f4b0: 322c 2036 2c20 395d 0a5b 345d 2070 7275  2, 6, 9].[4] pru
+0004f4c0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+0004f4d0: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+0004f4e0: 705f 3139 284d 6178 506f 6f6c 3244 5769  p_19(MaxPool2DWi
+0004f4f0: 7468 496e 6469 6365 7342 6163 6b77 6172  thIndicesBackwar
+0004f500: 6430 2920 3d3e 2070 7275 6e65 5f6f 7574  d0) => prune_out
+0004f510: 5f63 6861 6e6e 656c 7320 6f6e 205f 456c  _channels on _El
+0004f520: 656d 656e 7457 6973 654f 705f 3138 2841  ementWiseOp_18(A
+0004f530: 6464 4261 636b 7761 7264 3029 2c20 6964  ddBackward0), id
+0004f540: 7873 3d5b 322c 2036 2c20 395d 0a5b 355d  xs=[2, 6, 9].[5]
+0004f550: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+0004f560: 656c 7320 6f6e 205f 456c 656d 656e 7457  els on _ElementW
+0004f570: 6973 654f 705f 3139 284d 6178 506f 6f6c  iseOp_19(MaxPool
+0004f580: 3244 5769 7468 496e 6469 6365 7342 6163  2DWithIndicesBac
+0004f590: 6b77 6172 6430 2920 3d3e 2070 7275 6e65  kward0) => prune
+0004f5a0: 5f69 6e5f 6368 616e 6e65 6c73 206f 6e20  _in_channels on 
+0004f5b0: 6c61 7965 7231 2e30 2e63 6f6e 7631 2028  layer1.0.conv1 (
+0004f5c0: 436f 6e76 3264 2836 342c 2036 342c 206b  Conv2d(64, 64, k
+0004f5d0: 6572 6e65 6c5f 7369 7a65 3d28 332c 2033  ernel_size=(3, 3
+0004f5e0: 292c 2073 7472 6964 653d 2831 2c20 3129  ), stride=(1, 1)
+0004f5f0: 2c20 7061 6464 696e 673d 2831 2c20 3129  , padding=(1, 1)
+0004f600: 2c20 6269 6173 3d46 616c 7365 2929 2c20  , bias=False)), 
+0004f610: 6964 7873 3d5b 322c 2036 2c20 395d 0a5b  idxs=[2, 6, 9].[
+0004f620: 365d 2070 7275 6e65 5f6f 7574 5f63 6861  6] prune_out_cha
+0004f630: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+0004f640: 7457 6973 654f 705f 3138 2841 6464 4261  tWiseOp_18(AddBa
+0004f650: 636b 7761 7264 3029 203d 3e20 7072 756e  ckward0) => prun
+0004f660: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
+0004f670: 6e20 6c61 7965 7231 2e30 2e62 6e32 2028  n layer1.0.bn2 (
+0004f680: 4261 7463 684e 6f72 6d32 6428 3634 2c20  BatchNorm2d(64, 
+0004f690: 6570 733d 3165 2d30 352c 206d 6f6d 656e  eps=1e-05, momen
+0004f6a0: 7475 6d3d 302e 312c 2061 6666 696e 653d  tum=0.1, affine=
+0004f6b0: 5472 7565 2c20 7472 6163 6b5f 7275 6e6e  True, track_runn
+0004f6c0: 696e 675f 7374 6174 733d 5472 7565 2929  ing_stats=True))
+0004f6d0: 2c20 6964 7873 3d5b 322c 2036 2c20 395d  , idxs=[2, 6, 9]
+0004f6e0: 0a5b 375d 2070 7275 6e65 5f6f 7574 5f63  .[7] prune_out_c
+0004f6f0: 6861 6e6e 656c 7320 6f6e 205f 456c 656d  hannels on _Elem
+0004f700: 656e 7457 6973 654f 705f 3138 2841 6464  entWiseOp_18(Add
+0004f710: 4261 636b 7761 7264 3029 203d 3e20 7072  Backward0) => pr
+0004f720: 756e 655f 6f75 745f 6368 616e 6e65 6c73  une_out_channels
+0004f730: 206f 6e20 5f45 6c65 6d65 6e74 5769 7365   on _ElementWise
+0004f740: 4f70 5f31 3728 5265 6c75 4261 636b 7761  Op_17(ReluBackwa
+0004f750: 7264 3029 2c20 6964 7873 3d5b 322c 2036  rd0), idxs=[2, 6
+0004f760: 2c20 395d 0a5b 385d 2070 7275 6e65 5f6f  , 9].[8] prune_o
+0004f770: 7574 5f63 6861 6e6e 656c 7320 6f6e 205f  ut_channels on _
+0004f780: 456c 656d 656e 7457 6973 654f 705f 3137  ElementWiseOp_17
+0004f790: 2852 656c 7542 6163 6b77 6172 6430 2920  (ReluBackward0) 
+0004f7a0: 3d3e 2070 7275 6e65 5f6f 7574 5f63 6861  => prune_out_cha
+0004f7b0: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+0004f7c0: 7457 6973 654f 705f 3136 2841 6464 4261  tWiseOp_16(AddBa
+0004f7d0: 636b 7761 7264 3029 2c20 6964 7873 3d5b  ckward0), idxs=[
+0004f7e0: 322c 2036 2c20 395d 0a5b 395d 2070 7275  2, 6, 9].[9] pru
+0004f7f0: 6e65 5f6f 7574 5f63 6861 6e6e 656c 7320  ne_out_channels 
+0004f800: 6f6e 205f 456c 656d 656e 7457 6973 654f  on _ElementWiseO
+0004f810: 705f 3137 2852 656c 7542 6163 6b77 6172  p_17(ReluBackwar
+0004f820: 6430 2920 3d3e 2070 7275 6e65 5f69 6e5f  d0) => prune_in_
+0004f830: 6368 616e 6e65 6c73 206f 6e20 6c61 7965  channels on laye
+0004f840: 7231 2e31 2e63 6f6e 7631 2028 436f 6e76  r1.1.conv1 (Conv
+0004f850: 3264 2836 342c 2036 342c 206b 6572 6e65  2d(64, 64, kerne
+0004f860: 6c5f 7369 7a65 3d28 332c 2033 292c 2073  l_size=(3, 3), s
+0004f870: 7472 6964 653d 2831 2c20 3129 2c20 7061  tride=(1, 1), pa
+0004f880: 6464 696e 673d 2831 2c20 3129 2c20 6269  dding=(1, 1), bi
+0004f890: 6173 3d46 616c 7365 2929 2c20 6964 7873  as=False)), idxs
+0004f8a0: 3d5b 322c 2036 2c20 395d 0a5b 3130 5d20  =[2, 6, 9].[10] 
+0004f8b0: 7072 756e 655f 6f75 745f 6368 616e 6e65  prune_out_channe
+0004f8c0: 6c73 206f 6e20 5f45 6c65 6d65 6e74 5769  ls on _ElementWi
+0004f8d0: 7365 4f70 5f31 3628 4164 6442 6163 6b77  seOp_16(AddBackw
+0004f8e0: 6172 6430 2920 3d3e 2070 7275 6e65 5f6f  ard0) => prune_o
+0004f8f0: 7574 5f63 6861 6e6e 656c 7320 6f6e 206c  ut_channels on l
+0004f900: 6179 6572 312e 312e 626e 3220 2842 6174  ayer1.1.bn2 (Bat
+0004f910: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
+0004f920: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
+0004f930: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
+0004f940: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
+0004f950: 5f73 7461 7473 3d54 7275 6529 292c 2069  _stats=True)), i
+0004f960: 6478 733d 5b32 2c20 362c 2039 5d0a 5b31  dxs=[2, 6, 9].[1
+0004f970: 315d 2070 7275 6e65 5f6f 7574 5f63 6861  1] prune_out_cha
+0004f980: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+0004f990: 7457 6973 654f 705f 3136 2841 6464 4261  tWiseOp_16(AddBa
+0004f9a0: 636b 7761 7264 3029 203d 3e20 7072 756e  ckward0) => prun
+0004f9b0: 655f 6f75 745f 6368 616e 6e65 6c73 206f  e_out_channels o
+0004f9c0: 6e20 5f45 6c65 6d65 6e74 5769 7365 4f70  n _ElementWiseOp
+0004f9d0: 5f31 3528 5265 6c75 4261 636b 7761 7264  _15(ReluBackward
+0004f9e0: 3029 2c20 6964 7873 3d5b 322c 2036 2c20  0), idxs=[2, 6, 
+0004f9f0: 395d 0a5b 3132 5d20 7072 756e 655f 6f75  9].[12] prune_ou
+0004fa00: 745f 6368 616e 6e65 6c73 206f 6e20 5f45  t_channels on _E
+0004fa10: 6c65 6d65 6e74 5769 7365 4f70 5f31 3528  lementWiseOp_15(
+0004fa20: 5265 6c75 4261 636b 7761 7264 3029 203d  ReluBackward0) =
+0004fa30: 3e20 7072 756e 655f 696e 5f63 6861 6e6e  > prune_in_chann
+0004fa40: 656c 7320 6f6e 206c 6179 6572 322e 302e  els on layer2.0.
+0004fa50: 646f 776e 7361 6d70 6c65 2e30 2028 436f  downsample.0 (Co
+0004fa60: 6e76 3264 2836 342c 2031 3238 2c20 6b65  nv2d(64, 128, ke
+0004fa70: 726e 656c 5f73 697a 653d 2831 2c20 3129  rnel_size=(1, 1)
+0004fa80: 2c20 7374 7269 6465 3d28 322c 2032 292c  , stride=(2, 2),
+0004fa90: 2062 6961 733d 4661 6c73 6529 292c 2069   bias=False)), i
+0004faa0: 6478 733d 5b32 2c20 362c 2039 5d0a 5b31  dxs=[2, 6, 9].[1
+0004fab0: 335d 2070 7275 6e65 5f6f 7574 5f63 6861  3] prune_out_cha
+0004fac0: 6e6e 656c 7320 6f6e 205f 456c 656d 656e  nnels on _Elemen
+0004fad0: 7457 6973 654f 705f 3135 2852 656c 7542  tWiseOp_15(ReluB
+0004fae0: 6163 6b77 6172 6430 2920 3d3e 2070 7275  ackward0) => pru
+0004faf0: 6e65 5f69 6e5f 6368 616e 6e65 6c73 206f  ne_in_channels o
+0004fb00: 6e20 6c61 7965 7232 2e30 2e63 6f6e 7631  n layer2.0.conv1
+0004fb10: 2028 436f 6e76 3264 2836 342c 2031 3238   (Conv2d(64, 128
+0004fb20: 2c20 6b65 726e 656c 5f73 697a 653d 2833  , kernel_size=(3
+0004fb30: 2c20 3329 2c20 7374 7269 6465 3d28 322c  , 3), stride=(2,
+0004fb40: 2032 292c 2070 6164 6469 6e67 3d28 312c   2), padding=(1,
+0004fb50: 2031 292c 2062 6961 733d 4661 6c73 6529   1), bias=False)
+0004fb60: 292c 2069 6478 733d 5b32 2c20 362c 2039  ), idxs=[2, 6, 9
+0004fb70: 5d0a 5b31 345d 2070 7275 6e65 5f6f 7574  ].[14] prune_out
+0004fb80: 5f63 6861 6e6e 656c 7320 6f6e 206c 6179  _channels on lay
+0004fb90: 6572 312e 312e 626e 3220 2842 6174 6368  er1.1.bn2 (Batch
+0004fba0: 4e6f 726d 3264 2836 342c 2065 7073 3d31  Norm2d(64, eps=1
+0004fbb0: 652d 3035 2c20 6d6f 6d65 6e74 756d 3d30  e-05, momentum=0
+0004fbc0: 2e31 2c20 6166 6669 6e65 3d54 7275 652c  .1, affine=True,
+0004fbd0: 2074 7261 636b 5f72 756e 6e69 6e67 5f73   track_running_s
+0004fbe0: 7461 7473 3d54 7275 6529 2920 3d3e 2070  tats=True)) => p
+0004fbf0: 7275 6e65 5f6f 7574 5f63 6861 6e6e 656c  rune_out_channel
+0004fc00: 7320 6f6e 206c 6179 6572 312e 312e 636f  s on layer1.1.co
+0004fc10: 6e76 3220 2843 6f6e 7632 6428 3634 2c20  nv2 (Conv2d(64, 
+0004fc20: 3634 2c20 6b65 726e 656c 5f73 697a 653d  64, kernel_size=
+0004fc30: 2833 2c20 3329 2c20 7374 7269 6465 3d28  (3, 3), stride=(
+0004fc40: 312c 2031 292c 2070 6164 6469 6e67 3d28  1, 1), padding=(
+0004fc50: 312c 2031 292c 2062 6961 733d 4661 6c73  1, 1), bias=Fals
+0004fc60: 6529 292c 2069 6478 733d 5b32 2c20 362c  e)), idxs=[2, 6,
+0004fc70: 2039 5d0a 5b31 355d 2070 7275 6e65 5f6f   9].[15] prune_o
+0004fc80: 7574 5f63 6861 6e6e 656c 7320 6f6e 206c  ut_channels on l
+0004fc90: 6179 6572 312e 302e 626e 3220 2842 6174  ayer1.0.bn2 (Bat
+0004fca0: 6368 4e6f 726d 3264 2836 342c 2065 7073  chNorm2d(64, eps
+0004fcb0: 3d31 652d 3035 2c20 6d6f 6d65 6e74 756d  =1e-05, momentum
+0004fcc0: 3d30 2e31 2c20 6166 6669 6e65 3d54 7275  =0.1, affine=Tru
+0004fcd0: 652c 2074 7261 636b 5f72 756e 6e69 6e67  e, track_running
+0004fce0: 5f73 7461 7473 3d54 7275 6529 2920 3d3e  _stats=True)) =>
+0004fcf0: 2070 7275 6e65 5f6f 7574 5f63 6861 6e6e   prune_out_chann
+0004fd00: 656c 7320 6f6e 206c 6179 6572 312e 302e  els on layer1.0.
+0004fd10: 636f 6e76 3220 2843 6f6e 7632 6428 3634  conv2 (Conv2d(64
+0004fd20: 2c20 3634 2c20 6b65 726e 656c 5f73 697a  , 64, kernel_siz
+0004fd30: 653d 2833 2c20 3329 2c20 7374 7269 6465  e=(3, 3), stride
+0004fd40: 3d28 312c 2031 292c 2070 6164 6469 6e67  =(1, 1), padding
+0004fd50: 3d28 312c 2031 292c 2062 6961 733d 4661  =(1, 1), bias=Fa
+0004fd60: 6c73 6529 292c 2069 6478 733d 5b32 2c20  lse)), idxs=[2, 
+0004fd70: 362c 2039 5d0a 2d2d 2d2d 2d2d 2d2d 2d2d  6, 9].----------
+0004fd80: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+0004fd90: 2d2d 2d2d 2d2d 0a60 6060 0a46 6f72 206d  ------.```.For m
+0004fda0: 6f72 6520 6465 7461 696c 7320 6162 6f75  ore details abou
+0004fdb0: 7420 6772 6f75 7069 6e67 2c20 706c 6561  t grouping, plea
+0004fdc0: 7365 2072 6566 6572 2074 6f20 5b74 7574  se refer to [tut
+0004fdd0: 6f72 6961 6c73 2f32 202d 2045 7870 6c6f  orials/2 - Explo
+0004fde0: 7269 6e67 2044 6570 656e 6465 6e63 7920  ring Dependency 
+0004fdf0: 4772 6f75 7073 5d28 6874 7470 733a 2f2f  Groups](https://
+0004fe00: 6769 7468 7562 2e63 6f6d 2f56 6169 6e46  github.com/VainF
+0004fe10: 2f54 6f72 6368 2d50 7275 6e69 6e67 2f62  /Torch-Pruning/b
+0004fe20: 6c6f 622f 6d61 7374 6572 2f74 7574 6f72  lob/master/tutor
+0004fe30: 6961 6c73 2f32 2532 302d 2532 3045 7870  ials/2%20-%20Exp
+0004fe40: 6c6f 7269 6e67 2532 3044 6570 656e 6465  loring%20Depende
+0004fe50: 6e63 7925 3230 4772 6f75 7073 2e69 7079  ncy%20Groups.ipy
+0004fe60: 6e62 290a 2020 0a23 2323 2320 486f 7720  nb).  .#### How 
+0004fe70: 746f 2073 6361 6e20 616c 6c20 6772 6f75  to scan all grou
+0004fe80: 7073 2028 4164 7661 6e63 6564 293a 0a57  ps (Advanced):.W
+0004fe90: 6520 6361 6e20 7573 6520 6060 4447 2e67  e can use ``DG.g
+0004fea0: 6574 5f61 6c6c 5f67 726f 7570 7328 6967  et_all_groups(ig
+0004feb0: 6e6f 7265 645f 6c61 7965 7273 2c20 726f  nored_layers, ro
+0004fec0: 6f74 5f6d 6f64 756c 655f 7479 7065 7329  ot_module_types)
+0004fed0: 6060 2074 6f20 7363 616e 2061 6c6c 2067  `` to scan all g
+0004fee0: 726f 7570 7320 7365 7175 656e 7469 616c  roups sequential
+0004fef0: 6c79 2e20 4561 6368 2067 726f 7570 2077  ly. Each group w
+0004ff00: 696c 6c20 6265 6769 6e20 7769 7468 2061  ill begin with a
+0004ff10: 206c 6179 6572 2074 6861 7420 6d61 7463   layer that matc
+0004ff20: 6865 7320 6120 7479 7065 2069 6e20 7468  hes a type in th
+0004ff30: 6520 2272 6f6f 745f 6d6f 6475 6c65 5f74  e "root_module_t
+0004ff40: 7970 6573 2220 7061 7261 6d65 7465 722e  ypes" parameter.
+0004ff50: 204e 6f74 6520 7468 6174 2044 472e 6765   Note that DG.ge
+0004ff60: 745f 616c 6c5f 6772 6f75 7073 2069 7320  t_all_groups is 
+0004ff70: 6f6e 6c79 2072 6573 706f 6e73 6962 6c65  only responsible
+0004ff80: 2066 6f72 2067 726f 7570 696e 6720 616e   for grouping an
+0004ff90: 6420 646f 6573 206e 6f74 2068 6176 6520  d does not have 
+0004ffa0: 616e 7920 6b6e 6f77 6c65 6467 6520 6f72  any knowledge or
+0004ffb0: 2075 6e64 6572 7374 616e 6469 6e67 206f   understanding o
+0004ffc0: 6620 7768 6963 6820 7061 7261 6d65 7465  f which paramete
+0004ffd0: 7273 2073 686f 756c 6420 6265 2070 7275  rs should be pru
+0004ffe0: 6e65 642e 2054 6865 7265 666f 7265 2c20  ned. Therefore, 
+0004fff0: 6974 2069 7320 6e65 6365 7373 6172 7920  it is necessary 
+00050000: 746f 2073 7065 6369 6679 2074 6865 2070  to specify the p
+00050010: 7275 6e69 6e67 2069 6478 7320 7573 696e  runing idxs usin
+00050020: 6720 2060 6067 726f 7570 2e70 7275 6e65  g  ``group.prune
+00050030: 2869 6478 733d 6964 7873 2960 602e 0a0a  (idxs=idxs)``...
+00050040: 6060 6070 7974 686f 6e0a 666f 7220 6772  ```python.for gr
+00050050: 6f75 7020 696e 2044 472e 6765 745f 616c  oup in DG.get_al
+00050060: 6c5f 6772 6f75 7073 2869 676e 6f72 6564  l_groups(ignored
+00050070: 5f6c 6179 6572 733d 5b6d 6f64 656c 2e63  _layers=[model.c
+00050080: 6f6e 7631 5d2c 2072 6f6f 745f 6d6f 6475  onv1], root_modu
+00050090: 6c65 5f74 7970 6573 3d5b 6e6e 2e43 6f6e  le_types=[nn.Con
+000500a0: 7632 642c 206e 6e2e 4c69 6e65 6172 5d29  v2d, nn.Linear])
+000500b0: 3a0a 2020 2020 2320 6861 6e64 6c65 2067  :.    # handle g
+000500c0: 726f 7570 7320 696e 2073 6571 7565 6e74  roups in sequent
+000500d0: 6961 6c20 6f72 6465 720a 2020 2020 6964  ial order.    id
+000500e0: 7873 203d 205b 322c 342c 365d 2023 2079  xs = [2,4,6] # y
+000500f0: 6f75 7220 7072 756e 696e 6720 696e 6469  our pruning indi
+00050100: 6365 730a 2020 2020 6772 6f75 702e 7072  ces.    group.pr
+00050110: 756e 6528 6964 7873 3d69 6478 7329 0a20  une(idxs=idxs). 
+00050120: 2020 2070 7269 6e74 2867 726f 7570 290a     print(group).
+00050130: 6060 600a 0a23 2323 2032 2e20 4869 6768  ```..### 2. High
+00050140: 2d6c 6576 656c 2050 7275 6e65 7273 0a0a  -level Pruners..
+00050150: 4c65 7665 7261 6769 6e67 2074 6865 2044  Leveraging the D
+00050160: 6570 656e 6465 6e63 7947 7261 7068 2c20  ependencyGraph, 
+00050170: 7765 2064 6576 656c 6f70 6564 2073 6576  we developed sev
+00050180: 6572 616c 2068 6967 682d 6c65 7665 6c20  eral high-level 
+00050190: 7072 756e 6572 7320 696e 2074 6869 7320  pruners in this 
+000501a0: 7265 706f 7369 746f 7279 2074 6f20 6661  repository to fa
+000501b0: 6369 6c69 7461 7465 2065 6666 6f72 746c  cilitate effortl
+000501c0: 6573 7320 7072 756e 696e 672e 2042 7920  ess pruning. By 
+000501d0: 7370 6563 6966 7969 6e67 2074 6865 2064  specifying the d
+000501e0: 6573 6972 6564 2063 6861 6e6e 656c 2073  esired channel s
+000501f0: 7061 7273 6974 792c 2079 6f75 2063 616e  parsity, you can
+00050200: 2070 7275 6e65 2074 6865 2065 6e74 6972   prune the entir
+00050210: 6520 6d6f 6465 6c20 616e 6420 6669 6e65  e model and fine
+00050220: 2d74 756e 6520 6974 2075 7369 6e67 2079  -tune it using y
+00050230: 6f75 7220 6f77 6e20 7472 6169 6e69 6e67  our own training
+00050240: 2063 6f64 652e 2046 6f72 2064 6574 6169   code. For detai
+00050250: 6c65 6420 696e 666f 726d 6174 696f 6e20  led information 
+00050260: 6f6e 2074 6869 7320 7072 6f63 6573 732c  on this process,
+00050270: 2070 6c65 6173 6520 7265 6665 7220 746f   please refer to
+00050280: 205b 7468 6973 2074 7574 6f72 6961 6c5d   [this tutorial]
+00050290: 2868 7474 7073 3a2f 2f67 6974 6875 622e  (https://github.
+000502a0: 636f 6d2f 5661 696e 462f 546f 7263 682d  com/VainF/Torch-
+000502b0: 5072 756e 696e 672f 626c 6f62 2f6d 6173  Pruning/blob/mas
+000502c0: 7465 722f 7475 746f 7269 616c 732f 3125  ter/tutorials/1%
+000502d0: 3230 2d25 3230 4375 7374 6f6d 697a 6525  20-%20Customize%
+000502e0: 3230 596f 7572 2532 304f 776e 2532 3050  20Your%20Own%20P
+000502f0: 7275 6e65 7273 2e69 7079 6e62 292c 2077  runers.ipynb), w
+00050300: 6869 6368 2073 686f 7773 2068 6f77 2074  hich shows how t
+00050310: 6f20 696d 706c 656d 656e 7420 6120 5b73  o implement a [s
+00050320: 6c69 6d6d 696e 675d 2868 7474 7073 3a2f  limming](https:/
+00050330: 2f61 7278 6976 2e6f 7267 2f61 6273 2f31  /arxiv.org/abs/1
+00050340: 3730 382e 3036 3531 3929 2070 7275 6e65  708.06519) prune
+00050350: 7220 6672 6f6d 2073 6372 6174 6368 2e20  r from scratch. 
+00050360: 4164 6469 7469 6f6e 616c 6c79 2c20 796f  Additionally, yo
+00050370: 7520 6361 6e20 6669 6e64 206d 6f72 6520  u can find more 
+00050380: 7072 6163 7469 6361 6c20 6578 616d 706c  practical exampl
+00050390: 6573 2069 6e20 5b62 656e 6368 6d61 726b  es in [benchmark
+000503a0: 732f 6d61 696e 2e70 795d 2862 656e 6368  s/main.py](bench
+000503b0: 6d61 726b 732f 6d61 696e 2e70 7929 2e0a  marks/main.py)..
+000503c0: 0a60 6060 7079 7468 6f6e 0a69 6d70 6f72  .```python.impor
+000503d0: 7420 746f 7263 680a 6672 6f6d 2074 6f72  t torch.from tor
+000503e0: 6368 7669 7369 6f6e 2e6d 6f64 656c 7320  chvision.models 
+000503f0: 696d 706f 7274 2072 6573 6e65 7431 380a  import resnet18.
+00050400: 696d 706f 7274 2074 6f72 6368 5f70 7275  import torch_pru
+00050410: 6e69 6e67 2061 7320 7470 0a0a 6d6f 6465  ning as tp..mode
+00050420: 6c20 3d20 7265 736e 6574 3138 2870 7265  l = resnet18(pre
+00050430: 7472 6169 6e65 643d 5472 7565 290a 0a23  trained=True)..#
+00050440: 2049 6d70 6f72 7461 6e63 6520 6372 6974   Importance crit
+00050450: 6572 6961 0a65 7861 6d70 6c65 5f69 6e70  eria.example_inp
+00050460: 7574 7320 3d20 746f 7263 682e 7261 6e64  uts = torch.rand
+00050470: 6e28 312c 2033 2c20 3232 342c 2032 3234  n(1, 3, 224, 224
+00050480: 290a 696d 7020 3d20 7470 2e69 6d70 6f72  ).imp = tp.impor
+00050490: 7461 6e63 652e 5461 796c 6f72 496d 706f  tance.TaylorImpo
+000504a0: 7274 616e 6365 2829 0a0a 6967 6e6f 7265  rtance()..ignore
+000504b0: 645f 6c61 7965 7273 203d 205b 5d0a 666f  d_layers = [].fo
+000504c0: 7220 6d20 696e 206d 6f64 656c 2e6d 6f64  r m in model.mod
+000504d0: 756c 6573 2829 3a0a 2020 2020 6966 2069  ules():.    if i
+000504e0: 7369 6e73 7461 6e63 6528 6d2c 2074 6f72  sinstance(m, tor
+000504f0: 6368 2e6e 6e2e 4c69 6e65 6172 2920 616e  ch.nn.Linear) an
+00050500: 6420 6d2e 6f75 745f 6665 6174 7572 6573  d m.out_features
+00050510: 203d 3d20 3130 3030 3a0a 2020 2020 2020   == 1000:.      
+00050520: 2020 6967 6e6f 7265 645f 6c61 7965 7273    ignored_layers
+00050530: 2e61 7070 656e 6428 6d29 2023 2044 4f20  .append(m) # DO 
+00050540: 4e4f 5420 7072 756e 6520 7468 6520 6669  NOT prune the fi
+00050550: 6e61 6c20 636c 6173 7369 6669 6572 210a  nal classifier!.
+00050560: 0a69 7465 7261 7469 7665 5f73 7465 7073  .iterative_steps
+00050570: 203d 2035 2023 2070 726f 6772 6573 7369   = 5 # progressi
+00050580: 7665 2070 7275 6e69 6e67 0a70 7275 6e65  ve pruning.prune
+00050590: 7220 3d20 7470 2e70 7275 6e65 722e 4d61  r = tp.pruner.Ma
+000505a0: 676e 6974 7564 6550 7275 6e65 7228 0a20  gnitudePruner(. 
+000505b0: 2020 206d 6f64 656c 2c0a 2020 2020 6578     model,.    ex
+000505c0: 616d 706c 655f 696e 7075 7473 2c0a 2020  ample_inputs,.  
+000505d0: 2020 696d 706f 7274 616e 6365 3d69 6d70    importance=imp
+000505e0: 2c0a 2020 2020 6974 6572 6174 6976 655f  ,.    iterative_
+000505f0: 7374 6570 733d 6974 6572 6174 6976 655f  steps=iterative_
+00050600: 7374 6570 732c 0a20 2020 2063 685f 7370  steps,.    ch_sp
+00050610: 6172 7369 7479 3d30 2e35 2c20 2320 7265  arsity=0.5, # re
+00050620: 6d6f 7665 2035 3025 2063 6861 6e6e 656c  move 50% channel
+00050630: 732c 2052 6573 4e65 7431 3820 3d20 7b36  s, ResNet18 = {6
+00050640: 342c 2031 3238 2c20 3235 362c 2035 3132  4, 128, 256, 512
+00050650: 7d20 3d3e 2052 6573 4e65 7431 385f 4861  } => ResNet18_Ha
+00050660: 6c66 203d 207b 3332 2c20 3634 2c20 3132  lf = {32, 64, 12
+00050670: 382c 2032 3536 7d0a 2020 2020 6967 6e6f  8, 256}.    igno
+00050680: 7265 645f 6c61 7965 7273 3d69 676e 6f72  red_layers=ignor
+00050690: 6564 5f6c 6179 6572 732c 0a29 0a0a 6261  ed_layers,.)..ba
+000506a0: 7365 5f6d 6163 732c 2062 6173 655f 6e70  se_macs, base_np
+000506b0: 6172 616d 7320 3d20 7470 2e75 7469 6c73  arams = tp.utils
+000506c0: 2e63 6f75 6e74 5f6f 7073 5f61 6e64 5f70  .count_ops_and_p
+000506d0: 6172 616d 7328 6d6f 6465 6c2c 2065 7861  arams(model, exa
+000506e0: 6d70 6c65 5f69 6e70 7574 7329 0a66 6f72  mple_inputs).for
+000506f0: 2069 2069 6e20 7261 6e67 6528 6974 6572   i in range(iter
+00050700: 6174 6976 655f 7374 6570 7329 3a0a 2020  ative_steps):.  
+00050710: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
+00050720: 696d 702c 2074 702e 696d 706f 7274 616e  imp, tp.importan
+00050730: 6365 2e54 6179 6c6f 7249 6d70 6f72 7461  ce.TaylorImporta
+00050740: 6e63 6529 3a0a 2020 2020 2020 2020 2320  nce):.        # 
+00050750: 5461 796c 6f72 2065 7870 616e 7369 6f6e  Taylor expansion
+00050760: 2072 6571 7569 7265 7320 6772 6164 6965   requires gradie
+00050770: 6e74 7320 666f 7220 696d 706f 7274 616e  nts for importan
+00050780: 6365 2065 7374 696d 6174 696f 6e0a 2020  ce estimation.  
+00050790: 2020 2020 2020 6c6f 7373 203d 206d 6f64        loss = mod
+000507a0: 656c 2865 7861 6d70 6c65 5f69 6e70 7574  el(example_input
+000507b0: 7329 2e73 756d 2829 2023 2061 2064 756d  s).sum() # a dum
+000507c0: 6d79 206c 6f73 7320 666f 7220 5461 796c  my loss for Tayl
+000507d0: 6f72 496d 706f 7274 616e 6365 0a20 2020  orImportance.   
+000507e0: 2020 2020 206c 6f73 732e 6261 636b 7761       loss.backwa
+000507f0: 7264 2829 2023 2062 6566 6f72 6520 7072  rd() # before pr
+00050800: 756e 6572 2e73 7465 7028 290a 2020 2020  uner.step().    
+00050810: 7072 756e 6572 2e73 7465 7028 290a 2020  pruner.step().  
+00050820: 2020 6d61 6373 2c20 6e70 6172 616d 7320    macs, nparams 
+00050830: 3d20 7470 2e75 7469 6c73 2e63 6f75 6e74  = tp.utils.count
+00050840: 5f6f 7073 5f61 6e64 5f70 6172 616d 7328  _ops_and_params(
+00050850: 6d6f 6465 6c2c 2065 7861 6d70 6c65 5f69  model, example_i
+00050860: 6e70 7574 7329 0a20 2020 2023 2066 696e  nputs).    # fin
+00050870: 6574 756e 6520 796f 7572 206d 6f64 656c  etune your model
+00050880: 2068 6572 650a 2020 2020 2320 6669 6e65   here.    # fine
+00050890: 7475 6e65 286d 6f64 656c 290a 2020 2020  tune(model).    
+000508a0: 2320 2e2e 2e0a 6060 600a 0a23 2323 2320  # ....```..#### 
+000508b0: 5370 6172 7365 2054 7261 696e 696e 670a  Sparse Training.
+000508c0: 536f 6d65 2070 7275 6e65 7273 206c 696b  Some pruners lik
+000508d0: 6520 5b42 4e53 6361 6c65 5072 756e 6572  e [BNScalePruner
+000508e0: 5d28 6874 7470 733a 2f2f 6769 7468 7562  ](https://github
+000508f0: 2e63 6f6d 2f56 6169 6e46 2f54 6f72 6368  .com/VainF/Torch
+00050900: 2d50 7275 6e69 6e67 2f62 6c6f 622f 6464  -Pruning/blob/dd
+00050910: 3539 3932 3133 3635 6437 3261 6362 3238  59921365d72acb28
+00050920: 3537 6433 6437 3466 3735 6330 3365 3437  57d3d74f75c03e47
+00050930: 3730 3630 6662 2f74 6f72 6368 5f70 7275  7060fb/torch_pru
+00050940: 6e69 6e67 2f70 7275 6e65 722f 616c 676f  ning/pruner/algo
+00050950: 7269 7468 6d73 2f62 6174 6368 6e6f 726d  rithms/batchnorm
+00050960: 5f73 6361 6c65 5f70 7275 6e65 722e 7079  _scale_pruner.py
+00050970: 234c 3435 2920 616e 6420 5b47 726f 7570  #L45) and [Group
+00050980: 4e6f 726d 5072 756e 6572 5d28 6874 7470  NormPruner](http
+00050990: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f56  s://github.com/V
+000509a0: 6169 6e46 2f54 6f72 6368 2d50 7275 6e69  ainF/Torch-Pruni
+000509b0: 6e67 2f62 6c6f 622f 6464 3539 3932 3133  ng/blob/dd599213
+000509c0: 3635 6437 3261 6362 3238 3537 6433 6437  65d72acb2857d3d7
+000509d0: 3466 3735 6330 3365 3437 3730 3630 6662  4f75c03e477060fb
+000509e0: 2f74 6f72 6368 5f70 7275 6e69 6e67 2f70  /torch_pruning/p
+000509f0: 7275 6e65 722f 616c 676f 7269 7468 6d73  runer/algorithms
+00050a00: 2f67 726f 7570 5f6e 6f72 6d5f 7072 756e  /group_norm_prun
+00050a10: 6572 2e70 7923 4c35 3329 2072 6571 7569  er.py#L53) requi
+00050a20: 7265 2073 7061 7273 6520 7472 6169 6e69  re sparse traini
+00050a30: 6e67 2062 6566 6f72 6520 7072 756e 696e  ng before prunin
+00050a40: 672e 2054 6869 7320 6361 6e20 6265 2065  g. This can be e
+00050a50: 6173 696c 7920 6163 6869 6576 6564 2062  asily achieved b
+00050a60: 7920 696e 7365 7274 696e 6720 6a75 7374  y inserting just
+00050a70: 206f 6e65 206c 696e 6520 6f66 2063 6f64   one line of cod
+00050a80: 6520 6060 7072 756e 6572 2e72 6567 756c  e ``pruner.regul
+00050a90: 6172 697a 6528 6d6f 6465 6c29 6060 2069  arize(model)`` i
+00050aa0: 6e20 796f 7572 2074 7261 696e 696e 6720  n your training 
+00050ab0: 7363 7269 7074 2e20 5468 6520 7072 756e  script. The prun
+00050ac0: 6572 2077 696c 6c20 7570 6461 7465 2074  er will update t
+00050ad0: 6865 2067 7261 6469 656e 7420 6f66 2074  he gradient of t
+00050ae0: 7261 696e 6162 6c65 2070 6172 616d 6574  rainable paramet
+00050af0: 6572 732e 0a60 6060 7079 7468 6f6e 0a66  ers..```python.f
+00050b00: 6f72 2065 706f 6368 2069 6e20 7261 6e67  or epoch in rang
+00050b10: 6528 6570 6f63 6873 293a 0a20 2020 206d  e(epochs):.    m
+00050b20: 6f64 656c 2e74 7261 696e 2829 0a20 2020  odel.train().   
+00050b30: 2066 6f72 2069 2c20 2864 6174 612c 2074   for i, (data, t
+00050b40: 6172 6765 7429 2069 6e20 656e 756d 6572  arget) in enumer
+00050b50: 6174 6528 7472 6169 6e5f 6c6f 6164 6572  ate(train_loader
+00050b60: 293a 0a20 2020 2020 2020 2064 6174 612c  ):.        data,
+00050b70: 2074 6172 6765 7420 3d20 6461 7461 2e74   target = data.t
+00050b80: 6f28 6465 7669 6365 292c 2074 6172 6765  o(device), targe
+00050b90: 742e 746f 2864 6576 6963 6529 0a20 2020  t.to(device).   
+00050ba0: 2020 2020 206f 7074 696d 697a 6572 2e7a       optimizer.z
+00050bb0: 6572 6f5f 6772 6164 2829 0a20 2020 2020  ero_grad().     
+00050bc0: 2020 206f 7574 203d 206d 6f64 656c 2864     out = model(d
+00050bd0: 6174 6129 0a20 2020 2020 2020 206c 6f73  ata).        los
+00050be0: 7320 3d20 462e 6372 6f73 735f 656e 7472  s = F.cross_entr
+00050bf0: 6f70 7928 6f75 742c 2074 6172 6765 7429  opy(out, target)
+00050c00: 0a20 2020 2020 2020 206c 6f73 732e 6261  .        loss.ba
+00050c10: 636b 7761 7264 2829 0a20 2020 2020 2020  ckward().       
+00050c20: 2070 7275 6e65 722e 7265 6775 6c61 7269   pruner.regulari
+00050c30: 7a65 286d 6f64 656c 2920 2320 3c3d 3d20  ze(model) # <== 
+00050c40: 666f 7220 7370 6172 7365 206c 6561 726e  for sparse learn
+00050c50: 696e 670a 2020 2020 2020 2020 6f70 7469  ing.        opti
+00050c60: 6d69 7a65 722e 7374 6570 2829 0a60 6060  mizer.step().```
+00050c70: 0a0a 2323 2323 2049 6e74 6572 6163 7469  ..#### Interacti
+00050c80: 7665 2050 7275 6e69 6e67 2028 4164 7661  ve Pruning (Adva
+00050c90: 6e63 6564 290a 416c 6c20 6869 6768 2d6c  nced).All high-l
+00050ca0: 6576 656c 2070 7275 6e65 7273 2073 7570  evel pruners sup
+00050cb0: 706f 7274 2069 6e74 6572 6163 7469 7665  port interactive
+00050cc0: 2070 7275 6e69 6e67 2e20 5573 6520 6060   pruning. Use ``
+00050cd0: 7072 756e 6572 2e73 7465 7028 696e 7465  pruner.step(inte
+00050ce0: 7261 6374 6976 653d 5472 7565 2960 6020  ractive=True)`` 
+00050cf0: 746f 2067 6574 2061 6c6c 2067 726f 7570  to get all group
+00050d00: 7320 616e 6420 696e 7465 7261 6374 6976  s and interactiv
+00050d10: 656c 7920 7072 756e 6520 7468 656d 2062  ely prune them b
+00050d20: 7920 6361 6c6c 696e 6720 6060 6772 6f75  y calling ``grou
+00050d30: 702e 7072 756e 6528 2960 602e 2054 6869  p.prune()``. Thi
+00050d40: 7320 6665 6174 7572 6520 6973 2075 7365  s feature is use
+00050d50: 6675 6c20 6966 2079 6f75 2077 616e 7420  ful if you want 
+00050d60: 746f 2063 6f6e 7472 6f6c 2f6d 6f6e 6974  to control/monit
+00050d70: 6f72 2074 6865 2070 7275 6e69 6e67 2070  or the pruning p
+00050d80: 726f 6365 7373 2e0a 0a60 6060 7079 7468  rocess...```pyth
+00050d90: 6f6e 0a66 6f72 2069 2069 6e20 7261 6e67  on.for i in rang
+00050da0: 6528 6974 6572 6174 6976 655f 7374 6570  e(iterative_step
+00050db0: 7329 3a0a 2020 2020 666f 7220 6772 6f75  s):.    for grou
+00050dc0: 7020 696e 2070 7275 6e65 722e 7374 6570  p in pruner.step
+00050dd0: 2869 6e74 6572 6163 7469 7665 3d54 7275  (interactive=Tru
+00050de0: 6529 3a20 2320 5761 726e 696e 673a 2067  e): # Warning: g
+00050df0: 726f 7570 7320 6d75 7374 2062 6520 6861  roups must be ha
+00050e00: 6e64 6c65 6420 7365 7175 656e 7469 616c  ndled sequential
+00050e10: 6c79 2e20 446f 206e 6f74 206b 6565 7020  ly. Do not keep 
+00050e20: 7468 656d 2061 7320 6120 6c69 7374 2e0a  them as a list..
+00050e30: 2020 2020 2020 2020 7072 696e 7428 6772          print(gr
+00050e40: 6f75 7029 200a 2020 2020 2020 2020 2320  oup) .        # 
+00050e50: 646f 2077 6861 7465 7665 7220 796f 7520  do whatever you 
+00050e60: 6c69 6b65 2077 6974 6820 7468 6520 6772  like with the gr
+00050e70: 6f75 7020 0a20 2020 2020 2020 2064 6570  oup .        dep
+00050e80: 2c20 6964 7873 203d 2067 726f 7570 5b30  , idxs = group[0
+00050e90: 5d20 2320 6765 7420 7468 6520 6964 7873  ] # get the idxs
+00050ea0: 0a20 2020 2020 2020 2074 6172 6765 745f  .        target_
+00050eb0: 6d6f 6475 6c65 203d 2064 6570 2e74 6172  module = dep.tar
+00050ec0: 6765 742e 6d6f 6475 6c65 2023 2067 6574  get.module # get
+00050ed0: 2074 6865 2072 6f6f 7420 6d6f 6475 6c65   the root module
+00050ee0: 0a20 2020 2020 2020 2070 7275 6e69 6e67  .        pruning
+00050ef0: 5f66 6e20 3d20 6465 702e 6861 6e64 6c65  _fn = dep.handle
+00050f00: 7220 2320 6765 7420 7468 6520 7072 756e  r # get the prun
+00050f10: 696e 6720 6675 6e63 7469 6f6e 0a20 2020  ing function.   
+00050f20: 2020 2020 0a20 2020 2020 2020 2023 2044      .        # D
+00050f30: 6f6e 2774 2066 6f72 6765 7420 746f 2070  on't forget to p
+00050f40: 7275 6e65 2074 6865 2067 726f 7570 0a20  rune the group. 
+00050f50: 2020 2020 2020 2067 726f 7570 2e70 7275         group.pru
+00050f60: 6e65 2829 0a20 2020 2020 2020 2020 200a  ne().          .
+00050f70: 2020 2020 2020 2020 2320 6772 6f75 702e          # group.
+00050f80: 7072 756e 6528 6964 7873 3d5b 302c 2032  prune(idxs=[0, 2
+00050f90: 2c20 365d 2920 2320 4974 2069 7320 6576  , 6]) # It is ev
+00050fa0: 656e 2070 6f73 7369 626c 6520 746f 2063  en possible to c
+00050fb0: 6861 6e67 6520 7468 6520 7072 756e 696e  hange the prunin
+00050fc0: 6720 6265 6861 7669 6f75 7220 7769 7468  g behaviour with
+00050fd0: 2074 6865 2069 6478 7320 7061 7261 6d65   the idxs parame
+00050fe0: 7465 720a 2020 2020 6d61 6373 2c20 6e70  ter.    macs, np
+00050ff0: 6172 616d 7320 3d20 7470 2e75 7469 6c73  arams = tp.utils
+00051000: 2e63 6f75 6e74 5f6f 7073 5f61 6e64 5f70  .count_ops_and_p
+00051010: 6172 616d 7328 6d6f 6465 6c2c 2065 7861  arams(model, exa
+00051020: 6d70 6c65 5f69 6e70 7574 7329 0a20 2020  mple_inputs).   
+00051030: 2023 2066 696e 6574 756e 6520 796f 7572   # finetune your
+00051040: 206d 6f64 656c 2068 6572 650a 2020 2020   model here.    
+00051050: 2320 6669 6e65 7475 6e65 286d 6f64 656c  # finetune(model
+00051060: 290a 2020 2020 2320 2e2e 2e0a 6060 600a  ).    # ....```.
+00051070: 0a23 2323 2320 4772 6f75 702d 6c65 7665  .#### Group-leve
+00051080: 6c20 5072 756e 696e 670a 0a57 6974 6820  l Pruning..With 
+00051090: 4465 7047 7261 7068 2c20 6974 2069 7320  DepGraph, it is 
+000510a0: 6561 7379 2074 6f20 6465 7369 676e 2073  easy to design s
+000510b0: 6f6d 6520 2267 726f 7570 2d6c 6576 656c  ome "group-level
+000510c0: 2220 6372 6974 6572 6961 2074 6f20 6573  " criteria to es
+000510d0: 7469 6d61 7465 2074 6865 2069 6d70 6f72  timate the impor
+000510e0: 7461 6e63 6520 6f66 2061 2077 686f 6c65  tance of a whole
+000510f0: 2067 726f 7570 2072 6174 6865 7220 7468   group rather th
+00051100: 616e 2061 2073 696e 676c 6520 6c61 7965  an a single laye
+00051110: 722e 2049 6e20 546f 7263 682d 7072 756e  r. In Torch-prun
+00051120: 696e 672c 2061 6c6c 2070 7275 6e65 7273  ing, all pruners
+00051130: 2077 6f72 6b20 696e 2074 6865 2067 726f   work in the gro
+00051140: 7570 206c 6576 656c 2e0a 0a3c 6469 7620  up level...<div 
+00051150: 616c 6967 6e3d 2263 656e 7465 7222 3e0a  align="center">.
+00051160: 3c69 6d67 2073 7263 3d22 6173 7365 7473  <img src="assets
+00051170: 2f67 726f 7570 5f73 7061 7273 6974 792e  /group_sparsity.
+00051180: 706e 6722 2077 6964 7468 3d22 3830 2522  png" width="80%"
+00051190: 3e0a 3c2f 6469 763e 0a0a 2323 2320 332e  >.</div>..### 3.
+000511a0: 2053 6176 6520 2620 4c6f 6164 0a20 2020   Save & Load.   
+000511b0: 2020 2020 2020 200a 5468 6520 666f 6c6c         .The foll
+000511c0: 6f77 696e 6720 7363 7269 7074 2073 6176  owing script sav
+000511d0: 6573 2074 6865 2077 686f 6c65 206d 6f64  es the whole mod
+000511e0: 656c 206f 626a 6563 7420 2873 7472 7563  el object (struc
+000511f0: 7475 7265 2b77 6569 6768 7473 2920 6173  ture+weights) as
+00051200: 2061 2027 6d6f 6465 6c2e 7074 6827 2e20   a 'model.pth'. 
+00051210: 0a60 6060 7079 7468 6f6e 0a6d 6f64 656c  .```python.model
+00051220: 2e7a 6572 6f5f 6772 6164 2829 2023 2057  .zero_grad() # W
+00051230: 6520 646f 6e27 7420 7761 6e74 2074 6f20  e don't want to 
+00051240: 7374 6f72 6520 6772 6164 6965 6e74 2069  store gradient i
+00051250: 6e66 6f72 6d61 7469 6f6e 0a74 6f72 6368  nformation.torch
+00051260: 2e73 6176 6528 6d6f 6465 6c2c 2027 6d6f  .save(model, 'mo
+00051270: 6465 6c2e 7074 6827 2920 2320 7769 7468  del.pth') # with
+00051280: 6f75 7420 2e73 7461 7465 5f64 6963 740a  out .state_dict.
+00051290: 6d6f 6465 6c20 3d20 746f 7263 682e 6c6f  model = torch.lo
+000512a0: 6164 2827 6d6f 6465 6c2e 7074 6827 2920  ad('model.pth') 
+000512b0: 2320 6c6f 6164 2074 6865 2070 7275 6e65  # load the prune
+000512c0: 6420 6d6f 6465 6c0a 6060 600a 0a2a 2a45  d model.```..**E
+000512d0: 7870 6572 696d 656e 7461 6c20 4665 6174  xperimental Feat
+000512e0: 7572 6573 2a2a 3a20 5265 2d63 7265 6174  ures**: Re-creat
+000512f0: 6520 7072 756e 6564 206d 6f64 656c 7320  e pruned models 
+00051300: 6672 6f6d 2075 6e70 7275 6e65 6420 6f6e  from unpruned on
+00051310: 6573 2075 7369 6e67 2060 6074 702e 7374  es using ``tp.st
+00051320: 6174 655f 6469 6374 6060 2061 6e64 2060  ate_dict`` and `
+00051330: 6074 702e 6c6f 6164 5f73 7461 7465 5f64  `tp.load_state_d
+00051340: 6963 7460 602e 0a60 6060 7079 7468 6f6e  ict``..```python
+00051350: 0a23 2073 6176 6520 7468 6520 7072 756e  .# save the prun
+00051360: 6564 2073 7461 7465 5f64 6963 742c 2077  ed state_dict, w
+00051370: 6869 6368 2069 6e63 6c75 6465 7320 626f  hich includes bo
+00051380: 7468 2070 7275 6e65 6420 7061 7261 6d65  th pruned parame
+00051390: 7465 7273 2061 6e64 206d 6f64 6966 6965  ters and modifie
+000513a0: 6420 6174 7472 6962 7574 6573 0a73 7461  d attributes.sta
+000513b0: 7465 5f64 6963 7420 3d20 7470 2e73 7461  te_dict = tp.sta
+000513c0: 7465 5f64 6963 7428 7072 756e 6564 5f6d  te_dict(pruned_m
+000513d0: 6f64 656c 2920 2320 7468 6520 7072 756e  odel) # the prun
+000513e0: 6564 206d 6f64 656c 2c20 652e 672e 2c20  ed model, e.g., 
+000513f0: 6120 7265 736e 6574 2d31 382d 6861 6c66  a resnet-18-half
+00051400: 0a74 6f72 6368 2e73 6176 6528 7374 6174  .torch.save(stat
+00051410: 655f 6469 6374 2c20 2770 7275 6e65 642e  e_dict, 'pruned.
+00051420: 7074 6827 290a 0a23 2063 7265 6174 6520  pth')..# create 
+00051430: 6120 6e65 7720 6d6f 6465 6c2c 2065 2e67  a new model, e.g
+00051440: 2e20 7265 736e 6574 3138 0a6e 6577 5f6d  . resnet18.new_m
+00051450: 6f64 656c 203d 2072 6573 6e65 7431 3828  odel = resnet18(
+00051460: 292e 6576 616c 2829 0a0a 2320 6c6f 6164  ).eval()..# load
+00051470: 2074 6865 2070 7275 6e65 6420 7374 6174   the pruned stat
+00051480: 655f 6469 6374 2069 6e74 6f20 7468 6520  e_dict into the 
+00051490: 756e 7072 756e 6564 206d 6f64 656c 2e0a  unpruned model..
+000514a0: 6c6f 6164 6564 5f73 7461 7465 5f64 6963  loaded_state_dic
+000514b0: 7420 3d20 746f 7263 682e 6c6f 6164 2827  t = torch.load('
+000514c0: 7072 756e 6564 2e70 7468 272c 206d 6170  pruned.pth', map
+000514d0: 5f6c 6f63 6174 696f 6e3d 2763 7075 2729  _location='cpu')
+000514e0: 0a74 702e 6c6f 6164 5f73 7461 7465 5f64  .tp.load_state_d
+000514f0: 6963 7428 6e65 775f 6d6f 6465 6c2c 2073  ict(new_model, s
+00051500: 7461 7465 5f64 6963 743d 6c6f 6164 6564  tate_dict=loaded
+00051510: 5f73 7461 7465 5f64 6963 7429 0a70 7269  _state_dict).pri
+00051520: 6e74 286e 6577 5f6d 6f64 656c 2920 2320  nt(new_model) # 
+00051530: 5468 6973 2077 696c 6c20 6265 2061 2070  This will be a p
+00051540: 7275 6e65 6420 6d6f 6465 6c2e 0a60 6060  runed model..```
+00051550: 0a52 6566 6572 2074 6f20 5b74 6573 7473  .Refer to [tests
+00051560: 2f74 6573 745f 7365 7269 616c 697a 6174  /test_serializat
+00051570: 696f 6e2e 7079 5d28 7465 7374 732f 7465  ion.py](tests/te
+00051580: 7374 5f73 6572 6961 6c69 7a61 7469 6f6e  st_serialization
+00051590: 2e70 7929 2066 6f72 2061 6e20 5669 5420  .py) for an ViT 
+000515a0: 6578 616d 706c 652e 2049 6e20 7468 6973  example. In this
+000515b0: 2065 7861 6d70 6c65 2c20 7765 2077 696c   example, we wil
+000515c0: 6c20 7072 756e 6520 7468 6520 6d6f 6465  l prune the mode
+000515d0: 6c20 616e 6420 6d6f 6469 6679 2073 6f6d  l and modify som
+000515e0: 6520 6174 7472 6962 7574 6573 206c 696b  e attributes lik
+000515f0: 6520 6060 6d6f 6465 6c2e 6869 6464 656e  e ``model.hidden
+00051600: 5f64 696d 7360 602e 0a20 2020 2020 2020  _dims``..       
+00051610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00051620: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00051630: 200a 2020 2020 2020 2020 2020 2020 2020   .              
+00051640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00051650: 2020 0a23 2323 2034 2e20 4c6f 772d 6c65    .### 4. Low-le
+00051660: 7665 6c20 5072 756e 696e 6720 4675 6e63  vel Pruning Func
+00051670: 7469 6f6e 730a 0a57 6869 6c65 2069 7420  tions..While it 
+00051680: 6973 2070 6f73 7369 626c 6520 746f 206d  is possible to m
+00051690: 616e 7561 6c6c 7920 7072 756e 6520 796f  anually prune yo
+000516a0: 7572 206d 6f64 656c 2075 7369 6e67 206c  ur model using l
+000516b0: 6f77 2d6c 6576 656c 2066 756e 6374 696f  ow-level functio
+000516c0: 6e73 2c20 7468 6973 2061 7070 726f 6163  ns, this approac
+000516d0: 6820 6361 6e20 6265 2071 7569 7465 206c  h can be quite l
+000516e0: 6162 6f72 696f 7573 2c20 6173 2069 7420  aborious, as it 
+000516f0: 7265 7175 6972 6573 2063 6172 6566 756c  requires careful
+00051700: 206d 616e 6167 656d 656e 7420 6f66 2074   management of t
+00051710: 6865 2061 7373 6f63 6961 7465 6420 6465  he associated de
+00051720: 7065 6e64 656e 6369 6573 2e20 4173 2061  pendencies. As a
+00051730: 2072 6573 756c 742c 2077 6520 7265 636f   result, we reco
+00051740: 6d6d 656e 6420 7574 696c 697a 696e 6720  mmend utilizing 
+00051750: 7468 6520 6166 6f72 656d 656e 7469 6f6e  the aforemention
+00051760: 6564 2068 6967 682d 6c65 7665 6c20 7072  ed high-level pr
+00051770: 756e 6572 7320 746f 2073 7472 6561 6d6c  uners to streaml
+00051780: 696e 6520 7468 6520 7072 756e 696e 6720  ine the pruning 
+00051790: 7072 6f63 6573 732e 0a0a 6060 6070 7974  process...```pyt
+000517a0: 686f 6e0a 7470 2e70 7275 6e65 5f63 6f6e  hon.tp.prune_con
+000517b0: 765f 6f75 745f 6368 616e 6e65 6c73 2820  v_out_channels( 
+000517c0: 6d6f 6465 6c2e 636f 6e76 312c 2069 6478  model.conv1, idx
+000517d0: 733d 5b32 2c36 2c39 5d20 290a 0a23 2066  s=[2,6,9] )..# f
+000517e0: 6978 2074 6865 2062 726f 6b65 6e20 6465  ix the broken de
+000517f0: 7065 6e64 656e 6369 6573 206d 616e 7561  pendencies manua
+00051800: 6c6c 790a 7470 2e70 7275 6e65 5f62 6174  lly.tp.prune_bat
+00051810: 6368 6e6f 726d 5f6f 7574 5f63 6861 6e6e  chnorm_out_chann
+00051820: 656c 7328 206d 6f64 656c 2e62 6e31 2c20  els( model.bn1, 
+00051830: 6964 7873 3d5b 322c 362c 395d 2029 0a74  idxs=[2,6,9] ).t
+00051840: 702e 7072 756e 655f 636f 6e76 5f69 6e5f  p.prune_conv_in_
+00051850: 6368 616e 6e65 6c73 2820 6d6f 6465 6c2e  channels( model.
+00051860: 6c61 7965 7232 5b30 5d2e 636f 6e76 312c  layer2[0].conv1,
+00051870: 2069 6478 733d 5b32 2c36 2c39 5d20 290a   idxs=[2,6,9] ).
+00051880: 2e2e 2e0a 6060 600a 0a54 6865 2066 6f6c  ....```..The fol
+00051890: 6c6f 7769 6e67 2070 7275 6e69 6e67 2066  lowing pruning f
+000518a0: 756e 6374 696f 6e73 2061 7265 2061 7661  unctions are ava
+000518b0: 696c 6162 6c65 3a0a 6060 6070 7974 686f  ilable:.```pytho
+000518c0: 6e0a 2770 7275 6e65 5f63 6f6e 765f 6f75  n.'prune_conv_ou
+000518d0: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
+000518e0: 756e 655f 636f 6e76 5f69 6e5f 6368 616e  une_conv_in_chan
+000518f0: 6e65 6c73 272c 0a27 7072 756e 655f 6465  nels',.'prune_de
+00051900: 7074 6877 6973 655f 636f 6e76 5f6f 7574  pthwise_conv_out
+00051910: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+00051920: 6e65 5f64 6570 7468 7769 7365 5f63 6f6e  ne_depthwise_con
+00051930: 765f 696e 5f63 6861 6e6e 656c 7327 2c0a  v_in_channels',.
+00051940: 2770 7275 6e65 5f62 6174 6368 6e6f 726d  'prune_batchnorm
+00051950: 5f6f 7574 5f63 6861 6e6e 656c 7327 2c0a  _out_channels',.
+00051960: 2770 7275 6e65 5f62 6174 6368 6e6f 726d  'prune_batchnorm
+00051970: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
+00051980: 7072 756e 655f 6c69 6e65 6172 5f6f 7574  prune_linear_out
+00051990: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+000519a0: 6e65 5f6c 696e 6561 725f 696e 5f63 6861  ne_linear_in_cha
+000519b0: 6e6e 656c 7327 2c0a 2770 7275 6e65 5f70  nnels',.'prune_p
+000519c0: 7265 6c75 5f6f 7574 5f63 6861 6e6e 656c  relu_out_channel
+000519d0: 7327 2c0a 2770 7275 6e65 5f70 7265 6c75  s',.'prune_prelu
+000519e0: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
+000519f0: 7072 756e 655f 6c61 7965 726e 6f72 6d5f  prune_layernorm_
+00051a00: 6f75 745f 6368 616e 6e65 6c73 272c 0a27  out_channels',.'
+00051a10: 7072 756e 655f 6c61 7965 726e 6f72 6d5f  prune_layernorm_
+00051a20: 696e 5f63 6861 6e6e 656c 7327 2c0a 2770  in_channels',.'p
+00051a30: 7275 6e65 5f65 6d62 6564 6469 6e67 5f6f  rune_embedding_o
+00051a40: 7574 5f63 6861 6e6e 656c 7327 2c0a 2770  ut_channels',.'p
+00051a50: 7275 6e65 5f65 6d62 6564 6469 6e67 5f69  rune_embedding_i
+00051a60: 6e5f 6368 616e 6e65 6c73 272c 0a27 7072  n_channels',.'pr
+00051a70: 756e 655f 7061 7261 6d65 7465 725f 6f75  une_parameter_ou
+00051a80: 745f 6368 616e 6e65 6c73 272c 0a27 7072  t_channels',.'pr
+00051a90: 756e 655f 7061 7261 6d65 7465 725f 696e  une_parameter_in
+00051aa0: 5f63 6861 6e6e 656c 7327 2c0a 2770 7275  _channels',.'pru
+00051ab0: 6e65 5f6d 756c 7469 6865 6164 5f61 7474  ne_multihead_att
+00051ac0: 656e 7469 6f6e 5f6f 7574 5f63 6861 6e6e  ention_out_chann
+00051ad0: 656c 7327 2c0a 2770 7275 6e65 5f6d 756c  els',.'prune_mul
+00051ae0: 7469 6865 6164 5f61 7474 656e 7469 6f6e  tihead_attention
+00051af0: 5f69 6e5f 6368 616e 6e65 6c73 272c 0a27  _in_channels',.'
+00051b00: 7072 756e 655f 6772 6f75 706e 6f72 6d5f  prune_groupnorm_
+00051b10: 6f75 745f 6368 616e 6e65 6c73 272c 0a27  out_channels',.'
+00051b20: 7072 756e 655f 6772 6f75 706e 6f72 6d5f  prune_groupnorm_
+00051b30: 696e 5f63 6861 6e6e 656c 7327 2c0a 2770  in_channels',.'p
+00051b40: 7275 6e65 5f69 6e73 7461 6e63 656e 6f72  rune_instancenor
+00051b50: 6d5f 6f75 745f 6368 616e 6e65 6c73 272c  m_out_channels',
+00051b60: 0a27 7072 756e 655f 696e 7374 616e 6365  .'prune_instance
+00051b70: 6e6f 726d 5f69 6e5f 6368 616e 6e65 6c73  norm_in_channels
+00051b80: 272c 0a60 6060 0a0a 0a0a 2323 2320 352e  ',.```....### 5.
+00051b90: 2043 7573 746f 6d69 7a65 6420 4c61 7965   Customized Laye
+00051ba0: 7273 0a0a 506c 6561 7365 2072 6566 6572  rs..Please refer
+00051bb0: 2074 6f20 5b74 6573 7473 2f74 6573 745f   to [tests/test_
+00051bc0: 6375 7374 6f6d 697a 6564 5f6c 6179 6572  customized_layer
+00051bd0: 2e70 795d 2868 7474 7073 3a2f 2f67 6974  .py](https://git
+00051be0: 6875 622e 636f 6d2f 5661 696e 462f 546f  hub.com/VainF/To
+00051bf0: 7263 682d 5072 756e 696e 672f 626c 6f62  rch-Pruning/blob
+00051c00: 2f6d 6173 7465 722f 7465 7374 732f 7465  /master/tests/te
+00051c10: 7374 5f63 7573 746f 6d69 7a65 645f 6c61  st_customized_la
+00051c20: 7965 722e 7079 292e 0a0a 2323 2320 362e  yer.py)...### 6.
+00051c30: 2042 656e 6368 6d61 726b 730a 0a4f 7572   Benchmarks..Our
+00051c40: 2072 6573 756c 7473 206f 6e20 7b52 6573   results on {Res
+00051c50: 4e65 742d 3536 202f 2043 4946 4152 2d31  Net-56 / CIFAR-1
+00051c60: 3020 2f20 322e 3030 787d 0a0a 7c20 4d65  0 / 2.00x}..| Me
+00051c70: 7468 6f64 207c 2042 6173 6520 2825 2920  thod | Base (%) 
+00051c80: 7c20 5072 756e 6564 2028 2529 207c 2024  | Pruned (%) | $
+00051c90: 5c44 656c 7461 2420 4163 6320 2825 2920  \Delta$ Acc (%) 
+00051ca0: 7c20 5370 6565 6420 5570 207c 0a7c 3a2d  | Speed Up |.|:-
+00051cb0: 2d20 2020 207c 3a2d 2d3a 2020 7c3a 2d2d  -    |:--:  |:--
+00051cc0: 3a20 2020 207c 3a2d 2d3a 207c 3a2d 2d3a  :    |:--: |:--:
+00051cd0: 2020 2020 2020 7c0a 7c20 4e49 5053 205b        |.| NIPS [
+00051ce0: 5b31 5d5d 2823 3129 2020 7c20 2d20 2020  [1]](#1)  | -   
+00051cf0: 207c 202d 2020 2020 2020 7c2d 302e 3033   | -      |-0.03
+00051d00: 207c 2031 2e37 3678 2020 2020 7c0a 7c20   | 1.76x    |.| 
+00051d10: 4765 6f6d 6574 7269 6320 5b5b 325d 5d28  Geometric [[2]](
+00051d20: 2332 2920 7c20 3933 2e35 3920 7c20 3933  #2) | 93.59 | 93
+00051d30: 2e32 3620 7c20 2d30 2e33 3320 7c20 312e  .26 | -0.33 | 1.
+00051d40: 3730 7820 7c0a 7c20 506f 6c61 7220 5b5b  70x |.| Polar [[
+00051d50: 335d 5d28 2333 2920 207c 2039 332e 3830  3]](#3)  | 93.80
+00051d60: 207c 2039 332e 3833 207c 202b 302e 3033   | 93.83 | +0.03
+00051d70: 207c 312e 3838 7820 7c0a 7c20 4350 2020   |1.88x |.| CP  
+00051d80: 5b5b 345d 5d28 2334 2920 2020 7c20 3932  [[4]](#4)   | 92
+00051d90: 2e38 3020 7c20 3931 2e38 3020 7c20 2d31  .80 | 91.80 | -1
+00051da0: 2e30 3020 7c32 2e30 3078 207c 0a7c 2041  .00 |2.00x |.| A
+00051db0: 4d43 205b 5b35 5d5d 2823 3529 2020 207c  MC [[5]](#5)   |
+00051dc0: 2039 322e 3830 207c 2039 312e 3930 207c   92.80 | 91.90 |
+00051dd0: 202d 302e 3930 207c 322e 3030 7820 7c0a   -0.90 |2.00x |.
+00051de0: 7c20 4852 616e 6b20 5b5b 365d 5d28 2336  | HRank [[6]](#6
+00051df0: 2920 7c20 3933 2e32 3620 7c20 3932 2e31  ) | 93.26 | 92.1
+00051e00: 3720 7c20 2d30 2e30 3920 7c32 2e30 3078  7 | -0.09 |2.00x
+00051e10: 207c 0a7c 2053 4650 2020 5b5b 375d 5d28   |.| SFP  [[7]](
+00051e20: 2337 2920 207c 2039 332e 3539 207c 2039  #7)  | 93.59 | 9
+00051e30: 332e 3336 207c 202b 302e 3233 207c 322e  3.36 | +0.23 |2.
+00051e40: 3131 7820 7c0a 7c20 5265 7352 6570 205b  11x |.| ResRep [
+00051e50: 5b38 5d5d 2823 3829 207c 2039 332e 3731  [8]](#8) | 93.71
+00051e60: 207c 2039 332e 3731 207c 202b 302e 3030   | 93.71 | +0.00
+00051e70: 207c 322e 3132 7820 7c0a 7c7c 0a7c 204f   |2.12x |.||.| O
+00051e80: 7572 732d 4c31 207c 2039 332e 3533 207c  urs-L1 | 93.53 |
+00051e90: 2039 322e 3933 207c 202d 302e 3630 207c   92.93 | -0.60 |
+00051ea0: 2032 2e31 3278 207c 0a7c 204f 7572 732d   2.12x |.| Ours-
+00051eb0: 424e 207c 2039 332e 3533 207c 2039 332e  BN | 93.53 | 93.
+00051ec0: 3239 207c 202d 302e 3234 207c 2032 2e31  29 | -0.24 | 2.1
+00051ed0: 3278 207c 0a7c 204f 7572 732d 4772 6f75  2x |.| Ours-Grou
+00051ee0: 7020 7c20 3933 2e35 3320 7c20 3933 2e37  p | 93.53 | 93.7
+00051ef0: 3720 7c20 2b30 2e33 3820 7c20 322e 3133  7 | +0.38 | 2.13
+00051f00: 7820 7c0a 0a50 6c65 6173 6520 7265 6665  x |..Please refe
+00051f10: 7220 746f 205b 6265 6e63 686d 6172 6b73  r to [benchmarks
+00051f20: 5d28 6265 6e63 686d 6172 6b73 2920 666f  ](benchmarks) fo
+00051f30: 7220 6d6f 7265 2064 6574 6169 6c73 2e0a  r more details..
+00051f40: 0a23 2323 2037 2e20 5365 7269 6573 206f  .### 7. Series o
+00051f50: 6620 576f 726b 730a 3e20 2a2a 4c4c 4d2d  f Works.> **LLM-
+00051f60: 5072 756e 6572 3a20 4f6e 2074 6865 2053  Pruner: On the S
+00051f70: 7472 7563 7475 7261 6c20 5072 756e 696e  tructural Prunin
+00051f80: 6720 6f66 204c 6172 6765 204c 616e 6775  g of Large Langu
+00051f90: 6167 6520 4d6f 6465 6c73 2a2a 205b 5b50  age Models** [[P
+00051fa0: 726f 6a65 6374 5d5d 2868 7474 7073 3a2f  roject]](https:/
+00051fb0: 2f67 6974 6875 622e 636f 6d2f 686f 7273  /github.com/hors
+00051fc0: 6565 652f 4c4c 4d2d 5072 756e 6572 2920  eee/LLM-Pruner) 
+00051fd0: 5b5b 6172 5869 765d 5d28 6874 7470 733a  [[arXiv]](https:
+00051fe0: 2f2f 6172 7869 762e 6f72 672f 6162 732f  //arxiv.org/abs/
+00051ff0: 3233 3035 2e31 3136 3237 2920 2020 0a3e  2305.11627)   .>
+00052000: 202a 5869 6e79 696e 204d 612c 2047 6f6e   *Xinyin Ma, Gon
+00052010: 6766 616e 2046 616e 672c 2058 696e 6368  gfan Fang, Xinch
+00052020: 616f 2057 616e 672a 2020 200a 0a3e 202a  ao Wang*   ..> *
+00052030: 2a53 7472 7563 7475 7261 6c20 5072 756e  *Structural Prun
+00052040: 696e 6720 666f 7220 4469 6666 7573 696f  ing for Diffusio
+00052050: 6e20 4d6f 6465 6c73 2a2a 205b 5b50 726f  n Models** [[Pro
+00052060: 6a65 6374 5d5d 2868 7474 7073 3a2f 2f67  ject]](https://g
+00052070: 6974 6875 622e 636f 6d2f 5661 696e 462f  ithub.com/VainF/
+00052080: 4469 6666 2d50 7275 6e69 6e67 2920 5b5b  Diff-Pruning) [[
+00052090: 6172 7869 765d 5d28 6874 7470 733a 2f2f  arxiv]](https://
+000520a0: 6172 7869 762e 6f72 672f 6162 732f 3233  arxiv.org/abs/23
+000520b0: 3035 2e31 3039 3234 2920 200a 3e20 2a47  05.10924)  .> *G
+000520c0: 6f6e 6766 616e 2046 616e 672c 2058 696e  ongfan Fang, Xin
+000520d0: 7969 6e20 4d61 2c20 5869 6e63 6861 6f20  yin Ma, Xinchao 
+000520e0: 5761 6e67 2a20 2020 200a 0a0a 2323 2043  Wang*    ...## C
+000520f0: 6974 6174 696f 6e0a 6060 600a 4069 6e70  itation.```.@inp
+00052100: 726f 6365 6564 696e 6773 7b66 616e 6732  roceedings{fang2
+00052110: 3032 3364 6570 6772 6170 682c 0a20 2074  023depgraph,.  t
+00052120: 6974 6c65 3d7b 4465 7067 7261 7068 3a20  itle={Depgraph: 
+00052130: 546f 7761 7264 7320 616e 7920 7374 7275  Towards any stru
+00052140: 6374 7572 616c 2070 7275 6e69 6e67 7d2c  ctural pruning},
+00052150: 0a20 2061 7574 686f 723d 7b46 616e 672c  .  author={Fang,
+00052160: 2047 6f6e 6766 616e 2061 6e64 204d 612c   Gongfan and Ma,
+00052170: 2058 696e 7969 6e20 616e 6420 536f 6e67   Xinyin and Song
+00052180: 2c20 4d69 6e67 6c69 2061 6e64 204d 692c  , Mingli and Mi,
+00052190: 204d 6963 6861 656c 2042 6920 616e 6420   Michael Bi and 
+000521a0: 5761 6e67 2c20 5869 6e63 6861 6f7d 2c0a  Wang, Xinchao},.
+000521b0: 2020 626f 6f6b 7469 746c 653d 7b50 726f    booktitle={Pro
+000521c0: 6365 6564 696e 6773 206f 6620 7468 6520  ceedings of the 
+000521d0: 4945 4545 2f43 5646 2043 6f6e 6665 7265  IEEE/CVF Confere
+000521e0: 6e63 6520 6f6e 2043 6f6d 7075 7465 7220  nce on Computer 
+000521f0: 5669 7369 6f6e 2061 6e64 2050 6174 7465  Vision and Patte
+00052200: 726e 2052 6563 6f67 6e69 7469 6f6e 7d2c  rn Recognition},
+00052210: 0a20 2070 6167 6573 3d7b 3136 3039 312d  .  pages={16091-
+00052220: 2d31 3631 3031 7d2c 0a20 2079 6561 723d  -16101},.  year=
+00052230: 7b32 3032 337d 0a7d 0a60 6060 0a0a 0000  {2023}.}.```....
 00052240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052260: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052270: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052280: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052290: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000522a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -21082,16 +21082,16 @@
 00052590: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000525a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000525b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000525c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000525d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000525e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000525f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00052600: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00052610: 3930 342e 300a 0000 0000 0000 0000 0000  904.0...........
+00052600: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00052610: 3635 352e 300a 0000 0000 0000 0000 0000  655.0...........
 00052620: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052630: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052640: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052650: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052660: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052670: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052680: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -21115,23 +21115,23 @@
 000527a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000527b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000527c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000527d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000527e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000527f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052800: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00052810: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00052810: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00052820: 672e 6567 672d 696e 666f 2f53 4f55 5243  g.egg-info/SOURC
 00052830: 4553 2e74 7874 0000 0000 0000 0000 0000  ES.txt..........
 00052840: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052850: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052860: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00052870: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00052880: 3030 3032 3630 3000 3134 3433 3431 3434  0002600.14434144
-00052890: 3131 3000 3032 3231 3532 0020 3000 0000  110.022152. 0...
+00052880: 3030 3032 3630 3000 3134 3434 3632 3331  0002600.14446231
+00052890: 3336 3700 3032 3231 3731 0020 3000 0000  367.022171. 0...
 000528a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000528b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000528c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000528d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000528e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000528f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00052900: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -21274,16 +21274,16 @@
 00053190: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000531a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000531b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000531c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000531d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000531e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000531f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00053200: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00053210: 3930 342e 300a 0000 0000 0000 0000 0000  904.0...........
+00053200: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00053210: 3635 352e 300a 0000 0000 0000 0000 0000  655.0...........
 00053220: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053260: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053270: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053280: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -21307,23 +21307,23 @@
 000533a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000533b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000533c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000533d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000533e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000533f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053400: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00053410: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00053410: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00053420: 672e 6567 672d 696e 666f 2f64 6570 656e  g.egg-info/depen
 00053430: 6465 6e63 795f 6c69 6e6b 732e 7478 7400  dency_links.txt.
 00053440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053460: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00053470: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00053480: 3030 3030 3030 3100 3134 3433 3431 3434  0000001.14434144
-00053490: 3131 3000 3032 3433 3336 0020 3000 0000  110.024336. 0...
+00053480: 3030 3030 3030 3100 3134 3434 3632 3331  0000001.14446231
+00053490: 3336 3700 3032 3433 3535 0020 3000 0000  367.024355. 0...
 000534a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000534b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000534c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000534d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000534e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000534f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053500: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -21402,16 +21402,16 @@
 00053990: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000539a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000539b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000539c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000539d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000539e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000539f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00053a00: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00053a10: 3930 342e 300a 0000 0000 0000 0000 0000  904.0...........
+00053a00: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00053a10: 3635 352e 300a 0000 0000 0000 0000 0000  655.0...........
 00053a20: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053a30: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053a40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053a50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053a60: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053a70: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053a80: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -21435,23 +21435,23 @@
 00053ba0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053bb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053bc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053bd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053be0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053bf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053c00: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00053c10: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00053c10: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00053c20: 672e 6567 672d 696e 666f 2f72 6571 7569  g.egg-info/requi
 00053c30: 7265 732e 7478 7400 0000 0000 0000 0000  res.txt.........
 00053c40: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053c50: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053c60: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00053c70: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00053c80: 3030 3030 3031 3400 3134 3433 3431 3434  0000014.14434144
-00053c90: 3131 3000 3032 3236 3633 0020 3000 0000  110.022663. 0...
+00053c80: 3030 3030 3031 3400 3134 3434 3632 3331  0000014.14446231
+00053c90: 3336 3700 3032 3237 3032 0020 3000 0000  367.022702. 0...
 00053ca0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053cb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053cc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053cd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053ce0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053cf0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00053d00: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
@@ -21530,16 +21530,16 @@
 00054190: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000541a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000541b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000541c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000541d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000541e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000541f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
-00054200: 3232 206d 7469 6d65 3d31 3638 3531 3132  22 mtime=1685112
-00054210: 3930 342e 300a 0000 0000 0000 0000 0000  904.0...........
+00054200: 3232 206d 7469 6d65 3d31 3638 3737 3631  22 mtime=1687761
+00054210: 3635 352e 300a 0000 0000 0000 0000 0000  655.0...........
 00054220: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054230: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054240: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054250: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054260: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054270: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054280: 0000 0000 0000 0000 0000 0000 0000 0000  ................
@@ -21563,23 +21563,23 @@
 000543a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000543b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000543c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000543d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000543e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000543f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054400: 746f 7263 682d 7072 756e 696e 672d 312e  torch-pruning-1.
-00054410: 312e 382f 746f 7263 685f 7072 756e 696e  1.8/torch_prunin
+00054410: 312e 392f 746f 7263 685f 7072 756e 696e  1.9/torch_prunin
 00054420: 672e 6567 672d 696e 666f 2f74 6f70 5f6c  g.egg-info/top_l
 00054430: 6576 656c 2e74 7874 0000 0000 0000 0000  evel.txt........
 00054440: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054450: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054460: 0000 0000 3030 3030 3634 3400 3030 3031  ....0000644.0001
 00054470: 3735 3100 3030 3030 3137 3300 3030 3030  751.0000173.0000
-00054480: 3030 3030 3031 3600 3134 3433 3431 3434  0000016.14434144
-00054490: 3131 3000 3032 3330 3137 0020 3000 0000  110.023017. 0...
+00054480: 3030 3030 3031 3600 3134 3434 3632 3331  0000016.14446231
+00054490: 3336 3700 3032 3330 3336 0020 3000 0000  367.023036. 0...
 000544a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000544b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000544c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000544d0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000544e0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 000544f0: 0000 0000 0000 0000 0000 0000 0000 0000  ................
 00054500: 0075 7374 6172 0030 3072 756e 6e65 7200  .ustar.00runner.
```


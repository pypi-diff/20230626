# Comparing `tmp/scludam-1.0.2-py3-none-any.whl.zip` & `tmp/scludam-1.0.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,31 +1,31 @@
-Zip file size: 109632 bytes, number of entries: 29
--rw-rw-r--  2.0 unx      442 b- defN 22-Oct-08 02:29 scludam/__init__.py
--rw-rw-r--  2.0 unx    37505 b- defN 22-Oct-08 02:29 scludam/detection.py
--rw-rw-r--  2.0 unx    24162 b- defN 22-Oct-08 02:29 scludam/fetcher.py
--rw-rw-r--  2.0 unx    23573 b- defN 22-Oct-07 16:22 scludam/hkde.py
+Zip file size: 112086 bytes, number of entries: 29
+-rw-rw-r--  2.0 unx     2206 b- defN 23-Jun-25 22:01 scludam/__init__.py
+-rw-rw-r--  2.0 unx    37516 b- defN 23-Jun-25 22:01 scludam/detection.py
+-rw-rw-r--  2.0 unx    24162 b- defN 22-Nov-12 22:56 scludam/fetcher.py
+-rw-rw-r--  2.0 unx    24190 b- defN 23-Jun-25 22:01 scludam/hkde.py
 -rw-rw-r--  2.0 unx     7447 b- defN 22-Oct-08 02:29 scludam/masker.py
--rw-rw-r--  2.0 unx    14274 b- defN 22-Sep-28 13:24 scludam/membership.py
--rw-rw-r--  2.0 unx    21987 b- defN 22-Oct-08 02:29 scludam/pipeline.py
--rw-rw-r--  2.0 unx    27623 b- defN 22-Oct-08 02:29 scludam/plots.py
--rw-rw-r--  2.0 unx     4848 b- defN 22-Jul-24 14:45 scludam/rutils.py
--rw-rw-r--  2.0 unx    31035 b- defN 22-Sep-04 13:02 scludam/shdbscan.py
--rw-rw-r--  2.0 unx    23021 b- defN 22-Aug-27 02:10 scludam/stat_tests.py
+-rw-rw-r--  2.0 unx    17840 b- defN 23-Jun-25 22:01 scludam/membership.py
+-rw-rw-r--  2.0 unx    22099 b- defN 23-Jun-25 22:01 scludam/pipeline.py
+-rw-rw-r--  2.0 unx    27623 b- defN 22-Dec-17 02:40 scludam/plots.py
+-rw-rw-r--  2.0 unx     4816 b- defN 23-Jun-25 22:01 scludam/rutils.py
+-rw-rw-r--  2.0 unx    31030 b- defN 23-Jun-25 22:01 scludam/shdbscan.py
+-rw-rw-r--  2.0 unx    23021 b- defN 23-Jun-25 13:46 scludam/stat_tests.py
 -rw-rw-r--  2.0 unx    38632 b- defN 22-Jul-13 00:03 scludam/synthetic.py
--rw-rw-r--  2.0 unx     2609 b- defN 22-Jul-23 22:59 scludam/type_utils.py
--rw-rw-r--  2.0 unx     7928 b- defN 22-Oct-08 03:01 scludam/utils.py
--rw-rw-r--  2.0 unx    21653 b- defN 22-Oct-08 02:29 tests/test_detection.py
--rw-rw-r--  2.0 unx    15589 b- defN 22-Oct-08 02:29 tests/test_fetcher.py
--rw-rw-r--  2.0 unx    11107 b- defN 22-Sep-19 13:22 tests/test_hkde.py
--rw-rw-r--  2.0 unx     7610 b- defN 22-Aug-03 11:24 tests/test_membership.py
--rw-rw-r--  2.0 unx     8361 b- defN 22-Oct-08 02:47 tests/test_pipeline.py
+-rw-rw-r--  2.0 unx     2609 b- defN 22-Nov-12 22:56 scludam/type_utils.py
+-rw-rw-r--  2.0 unx     7928 b- defN 23-Jun-25 21:59 scludam/utils.py
+-rw-rw-r--  2.0 unx    21700 b- defN 23-Jun-25 22:01 tests/test_detection.py
+-rw-rw-r--  2.0 unx    15589 b- defN 22-Dec-16 04:18 tests/test_fetcher.py
+-rw-rw-r--  2.0 unx    12421 b- defN 23-Jun-25 22:01 tests/test_hkde.py
+-rw-rw-r--  2.0 unx     9993 b- defN 23-Jun-25 22:01 tests/test_membership.py
+-rw-rw-r--  2.0 unx    10177 b- defN 23-Jun-25 22:01 tests/test_pipeline.py
 -rw-rw-r--  2.0 unx    17500 b- defN 22-Jul-13 00:03 tests/test_shdbscan.py
--rw-rw-r--  2.0 unx     5054 b- defN 22-Aug-27 02:10 tests/test_stat_tests.py
+-rw-rw-r--  2.0 unx     5054 b- defN 22-Dec-16 04:18 tests/test_stat_tests.py
 -rw-rw-r--  2.0 unx    16673 b- defN 22-Jun-09 03:55 tests/test_synthetic.py
 -rw-rw-r--  2.0 unx     3009 b- defN 22-Aug-27 02:10 tests/test_utils.py
 -rw-rw-r--  2.0 unx      611 b- defN 22-Jul-13 00:03 tests/utils.py
--rw-rw-r--  2.0 unx    35149 b- defN 22-Oct-08 03:14 scludam-1.0.2.dist-info/LICENSE
--rw-rw-r--  2.0 unx     5230 b- defN 22-Oct-08 03:14 scludam-1.0.2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 22-Oct-08 03:14 scludam-1.0.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx       14 b- defN 22-Oct-08 03:14 scludam-1.0.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2236 b- defN 22-Oct-08 03:14 scludam-1.0.2.dist-info/RECORD
-29 files, 414974 bytes uncompressed, 106150 bytes compressed:  74.4%
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Jun-25 22:10 scludam-1.0.3.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     5384 b- defN 23-Jun-25 22:10 scludam-1.0.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-25 22:10 scludam-1.0.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       14 b- defN 23-Jun-25 22:10 scludam-1.0.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2238 b- defN 23-Jun-25 22:10 scludam-1.0.3.dist-info/RECORD
+29 files, 426723 bytes uncompressed, 108604 bytes compressed:  74.5%
```

## zipnote {}

```diff
@@ -66,23 +66,23 @@
 
 Filename: tests/test_utils.py
 Comment: 
 
 Filename: tests/utils.py
 Comment: 
 
-Filename: scludam-1.0.2.dist-info/LICENSE
+Filename: scludam-1.0.3.dist-info/LICENSE
 Comment: 
 
-Filename: scludam-1.0.2.dist-info/METADATA
+Filename: scludam-1.0.3.dist-info/METADATA
 Comment: 
 
-Filename: scludam-1.0.2.dist-info/WHEEL
+Filename: scludam-1.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: scludam-1.0.2.dist-info/top_level.txt
+Filename: scludam-1.0.3.dist-info/top_level.txt
 Comment: 
 
-Filename: scludam-1.0.2.dist-info/RECORD
+Filename: scludam-1.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scludam/__init__.py

```diff
@@ -1,11 +1,44 @@
-"""Scludam.
+# scludam, Star CLUster Detection And Membership estimation package
+# Copyright (C) 2022  Simón Pedro González
 
-Star Cluster Detection and Membership Probability Calculation
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
 
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+
+# You should have received a copy of the GNU General Public License
+# along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+"""
+
+Star Cluster Detection and Membership Probability Calculation main module.
+
+Simplified API:
+    - :class:`~scludam.detection.CountPeakDetector`,
+      :func:`~scludam.detection.default_mask`,
+      :func:`~scludam.detection.extend_1dmask` from :py:mod:`~scludam.detection`
+    - :class:`~scludam.fetcher.Query`, :func:`~scludam.fetcher.search_object`,
+      :func:`~scludam.fetcher.search_table`,
+      :func:`~scludam.fetcher.search_objects_near_data`,
+      :func:`~scludam.fetcher.search_table` from :py:mod:`~scludam.fetcher`
+    - :class:`~scludam.hkde.HKDE`,
+      :class:`~scludam.hkde.PluginSelector`,
+      :class:`~scludam.hkde.RuleOfThumbSelector` from :py:mod:`~scludam.hkde`
+    - :class:`~scludam.membership.DBME` from :py:mod:`~scludam.membership`
+    - :class:`~scludam.pipeline.DEP` from :py:mod:`~scludam.pipeline`
+    - :class:`~scludam.shdbscan.SHDBSCAN` from :py:mod:`~scludam.shdbscan`
+    - :class:`~scludam.stat_tests.DipDistTest`,
+      :class:`~scludam.stat_tests.HopkinsTest`,
+      :class:`~scludam.stat_tests.RipleysKTest` from :py:mod:`~scludam.stat_tests`
 """
 
 from .detection import CountPeakDetector, default_mask, extend_1dmask
 from .fetcher import Query, search_object, search_objects_near_data, search_table
 from .hkde import HKDE, PluginSelector, RuleOfThumbSelector
 from .membership import DBME
 from .pipeline import DEP
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## scludam/detection.py

```diff
@@ -77,15 +77,15 @@
     produce a good estimate of the local density of the background
     of the bin over which it is applied. This mask is used in the
     method applied by González-Alejo (2020) [1]_.
 
     References
     ----------
     .. [1] Alejo, A.D., González, J.F., González, S. P. (2020).
-        Estudio de membresı́a de cúmulos estelares utilizando Gaia DR2.
+        Estudio de membresía de cúmulos estelares utilizando Gaia DR2.
         Cuaderno de Resúmenes 62a Reunión Anual Asociación
         Argentina de Astronomía, Rosario, Provincia de Santa Fe, 64.
 
     Examples
     --------
     .. literalinclude:: ../../examples/detection/default_mask.py
         :language: python
@@ -599,14 +599,15 @@
         if not self.nyquist_offset:
             self._offsets = np.atleast_2d(np.zeros(dim))
         else:
             values = np.vstack((np.array(self.bin_shape) / 2, np.zeros(dim))).T
             combinations = np.array(np.meshgrid(*values)).T.reshape((-1, dim))
             self._offsets = np.flip(combinations, axis=0)
 
+    @beartype
     def detect(self, data: Numeric2DArray):
         """Detect peaks in the provided data.
 
         Uses the configuration provided in the class attributes.
 
         Parameters
         ----------
@@ -723,15 +724,14 @@
             if self.min_sigma_dif is not None:
                 detection_img[sharp < self.min_sigma_dif * std] = 0
 
             clusters_idx = peak_local_max(detection_img, **peak_detection_params).T
 
             _, peak_count = clusters_idx.shape
             if peak_count != 0:
-
                 iter_indcs = clusters_idx.T
                 iter_counts = sharp[tuple(clusters_idx)]
                 iter_scores = normalized[tuple(clusters_idx)]
 
                 limits = [
                     [
                         (
```

## scludam/hkde.py

```diff
@@ -21,15 +21,15 @@
 from the Plugin or Rule Of Thumb (scott or silverman) methods. Variable errors and
 covariances can be added to the matrices.
 
 """
 
 from abc import abstractmethod
 from numbers import Number
-from typing import Optional, Tuple, Union
+from typing import List, Optional, Tuple, Union
 
 import matplotlib.pyplot as plt
 import numpy as np
 from attrs import define, field, validators
 from beartype import beartype
 from rpy2.robjects import r
 from scipy.stats import multivariate_normal
@@ -159,22 +159,27 @@
 
     Attributes
     ----------
     rule : str, optional
         Name of the rule of thumb to use, by default "scott".
         Can be "scott" or "silverman".
 
+    diag : bool, optional
+        Whether to use the diagonal bandwidth,
+        by default False.
+
     Raises
     ------
     ValueError
         If rule is not "scott" or "silverman".
 
     """
 
     rule: str = field(default="scott", validator=validators.in_(["scott", "silverman"]))
+    diag: bool = False
 
     def _scotts_factor(self, data):
         n = data.shape[0]
         d = data.shape[1]
         return np.power(n, -1.0 / (d + 4))
 
     def _silverman_factor(self, data):
@@ -190,15 +195,23 @@
         else:
             raise ValueError("Invalid rule")
 
     def _get_data_covariance(self, data, weights: Union[Numeric2DArray, None]):
         kws = {}
         if weights is not None:
             kws["aweights"] = weights
-        data_covariance = np.cov(data, rowvar=False, bias=False, **kws)
+        if self.diag:
+            data_covariance = np.diagflat(
+                [
+                    np.cov(data[:, i], rowvar=False, bias=False, **kws)
+                    for i in range(data.shape[1])
+                ]
+            )
+        else:
+            data_covariance = np.cov(data, rowvar=False, bias=False, **kws)
         data_covariance = cov_nearest(data_covariance)
         return data_covariance
 
     @beartype
     def get_bw(self, data: Numeric2DArray, weights: Union[None, Numeric1DArray] = None):
         """Calculate bandwith matrix using the rule of thumb.
 
@@ -247,23 +260,26 @@
         :language: python
         :linenos:
     .. image:: ../../examples/hkde/hkde.png
 
     """
 
     # input attrs
-    bw: Union[BandwidthSelector, Number, NumericArray, str] = field(
+    bw: Union[BandwidthSelector, Number, NumericArray, List[Number], str] = field(
         default=PluginSelector(),
-        validator=_type(Union[BandwidthSelector, Number, NumericArray, str]),
+        validator=_type(
+            Union[BandwidthSelector, Number, List[Number], NumericArray, str]
+        ),
     )
 
     # internal attrs
     _kernels: ArrayLike = None
     _weights: Numeric1DArrayLike = None
     _covariances: NumericArray = None
+    _base_bw: NumericArray = None
     _data: Numeric2DArray = None
     _n: int = None
     _d: int = None
     _n_eff: int = None
     _eff_mask: ArrayLike = None
     _maxs: Numeric1DArray = None
     _mins: Numeric1DArray = None
@@ -376,21 +392,24 @@
     def _get_bw_matrices(self, data: Numeric2DArray):
         if isinstance(self.bw, BandwidthSelector):
             bw_matrix = self.bw.get_bw(data[self._eff_mask])
         elif isinstance(self.bw, str):
             bw_matrix = RuleOfThumbSelector(rule=self.bw).get_bw(
                 data[self._eff_mask], self._weights[self._eff_mask]
             )
-        elif isinstance(self.bw, np.ndarray):
+        elif isinstance(self.bw, np.ndarray) or isinstance(self.bw, list):
+            if isinstance(self.bw, list):
+                self.bw = np.array(self.bw)
             if len(self.bw.shape) == 1 and self.bw.shape[0] == self._d:
                 bw_matrix = np.diag(self.bw)
             elif self.bw.shape == (self._d, self._d):
                 bw_matrix = self.bw
             else:
                 raise ValueError("Incorrect shape of bandwidth array")
+        self._base_bw = bw_matrix
         return np.repeat(bw_matrix[:, np.newaxis], self._n, 1).swapaxes(0, 1)
 
     def _get_cov_matrices(
         self,
         data: Numeric2DArray,
         err: Numeric2DArray = None,
         corr: NumericArray = None,
```

## scludam/membership.py

```diff
@@ -14,14 +14,15 @@
 # You should have received a copy of the GNU General Public License
 # along with this program.  If not, see <https://www.gnu.org/licenses/>.
 
 
 """Module for Density Based Membership Estimation."""
 
 from copy import deepcopy
+from typing import List, Union
 
 import numpy as np
 from attrs import Factory, define, field, validators
 
 from scludam.hkde import HKDE
 from scludam.plots import (
     pairprobaplot,
@@ -65,17 +66,18 @@
         *  ``per_class_per_iter``: the bandwidth of the kernels is different for
            each class and iteration. There will be one estimator per class which will
            be updated in each iteration, recalculating the bandwith each time.
 
     kde_leave1out : bool
         Whether to use leave-one-out KDE estimation, by default True.
 
-    pdf_estimator : :class:`~scludam.hkde.HKDE`
+    pdf_estimator : Union[:class:`~scludam.hkde.HKDE`, List[HKDE]]
         Estimator used to estimate the density, by default an instance of HKDE with
-        default parameters.
+        default parameters. If list is provided, it is asumed either 1 per class or
+        first for first class and 2nd for the rest.
 
     n_classes: int
         Number of detected classes. Only available after the
         :func:`~scludam.membership.DBME.fit` method is called.
     labels : Numeric1DArray
         Labels of the classes, only available after the
         :func:`~scludam.membership.DBME.fit` method is called.
@@ -103,22 +105,25 @@
     # intput attrs
     n_iters: int = field(default=2, validator=[_type(int), validators.gt(0)])
     kde_leave1out: bool = field(default=True, validator=_type(bool))
     kernel_calculation_mode: str = field(
         validator=validators.in_(["same", "per_class", "per_class_per_iter"]),
         default="per_class",
     )
-    pdf_estimator: HKDE = field(default=HKDE(), validator=_type(HKDE))
+    pdf_estimator: HKDE = field(
+        default=HKDE(), validator=_type(Union[HKDE, List[HKDE]])
+    )
 
     # internal attrs
     _n: int = None
     _d: int = None
     _unique_labels: Numeric1DArray = None
     _data: Numeric2DArray = None
     _estimators: list = Factory(list)
+    _n_estimators: int = None
     _iter_priors: list = Factory(list)
     _iter_counts: list = Factory(list)
     _iter_label_diff: list = Factory(list)
     _iter_labels: list = Factory(list)
 
     # output attrs
     n_classes: int = None
@@ -187,51 +192,137 @@
     #     yclu = 1 - yfie1
     #     new_posteriors = np.concatenate(
     #         (yfie1.reshape(-1, 1), yclu.reshape(-1, 1)), axis=1
     #     )
 
     #     return new_posteriors
 
-    def _get_densities(
+    def _fit_several_estimators(
         self,
         data: Numeric2DArray,
         err: OptionalNumeric2DArray,
         corr: OptionalNumericArray,
         weights: OptionalNumeric2DArray,
     ):
-        densities = np.zeros((self._n, self.n_classes))
+        # estimator(s) fitting
+        first_iter = not self._estimators
+        each_time = self.kernel_calculation_mode == "per_class_per_iter"
+        if first_iter or each_time:
+            self._estimators = []
+            if self._n_estimators == 2:
+                # one for first class and copy the 2nd for the other classes
+                # no need to copy the first one
+                self._estimators.append(
+                    self.pdf_estimator[0].fit(
+                        data=data,
+                        err=err,
+                        corr=corr,
+                        weights=weights[:, 0],
+                    )
+                )
+                for i in range(1, self.n_classes):
+                    self._estimators.append(
+                        deepcopy(self.pdf_estimator[1]).fit(
+                            data=data,
+                            err=err,
+                            corr=corr,
+                            weights=weights[:, i],
+                        ),
+                    )
+            else:  # assume n_estimators == n_classes
+                for i in range(self.n_classes):
+                    self._estimators.append(
+                        self.pdf_estimator[i].fit(
+                            data=data,
+                            err=err,
+                            corr=corr,
+                            weights=weights[:, i],
+                        ),
+                    )
 
+    def _fit_one_estimator(
+        self,
+        data: Numeric2DArray,
+        err: OptionalNumeric2DArray,
+        corr: OptionalNumericArray,
+        weights: OptionalNumeric2DArray,
+    ):
         # estimator(s) fitting
-        if not self._estimators or self.kernel_calculation_mode == "per_class_per_iter":
+        first_iter = not self._estimators
+        each_time = self.kernel_calculation_mode == "per_class_per_iter"
+
+        if first_iter or each_time:
             if self.kernel_calculation_mode == "same":
                 self._estimators = [self.pdf_estimator.fit(data, err, corr)]
             else:
                 self._estimators = []
                 for i in range(self.n_classes):
                     self._estimators.append(
                         deepcopy(self.pdf_estimator).fit(
                             data=data,
                             err=err,
                             corr=corr,
                             weights=weights[:, i],
                         ),
                     )
 
+    def _get_densities(
+        self,
+        data: Numeric2DArray,
+        err: OptionalNumeric2DArray,
+        corr: OptionalNumericArray,
+        weights: OptionalNumeric2DArray,
+    ):
+        densities = np.zeros((self._n, self.n_classes))
+
+        if self._n_estimators == 1:
+            self._fit_one_estimator(data, err, corr, weights)
+        else:
+            self._fit_several_estimators(data, err, corr, weights)
+
         # pdf estimation
         for i in range(self.n_classes):
             if self.kernel_calculation_mode == "same":
                 class_estimator = self._estimators[0]
             else:
                 class_estimator = self._estimators[i]
             densities[:, i] = class_estimator.set_weights(weights[:, i]).pdf(
                 data, leave1out=self.kde_leave1out
             )
 
         return densities
 
+    def _validate_n_estimators(self):
+        # if self.pdf_estimator is class HKDE, set _n_estimators to 1
+        if isinstance(self.pdf_estimator, HKDE):
+            self._n_estimators = 1
+            return
+        else:
+            # is list of hkde
+            self._n_estimators = len(self.pdf_estimator)
+        if self._n_estimators == 1:
+            # normal case, return
+            self.pdf_estimator = self.pdf_estimator[0]
+            return
+        # if len > 1 then kernel_calculation_mode should be
+        # either per_class or per_class_per_iter
+        else:
+            if self.kernel_calculation_mode not in [
+                "per_class",
+                "per_class_per_iter",
+            ]:
+                raise ValueError("kernel_calculation_mode and n_estimators mismatch")
+            # now check against n_classes, n_estimators can be only
+            # either == n_classes, or 2 (field, clusters)
+            # we assume n_classes = 1 is not possible because
+            # in fit we already returned init_proba in that case
+            if self._n_estimators != self.n_classes and self._n_estimators != 2:
+                raise ValueError("n_estimators should be 1, 2 or n_classes")
+        return
+
     def fit(
         self,
         data: Numeric2DArray,
         init_proba: Numeric2DArray,
         err: OptionalNumeric2DArray = None,
         corr: OptionalNumericArray = None,
     ):
@@ -280,14 +371,16 @@
         self._update_class_mixtures(posteriors=init_proba)
 
         # case no clusters found
         if self.n_classes == 1:
             # there are no populations to fit
             return self
 
+        self._validate_n_estimators()
+
         for i in range(self.n_iters):
             # is copy actually needed?
             previous_posteriors = self.posteriors.copy()
             weights = previous_posteriors
             densities = self._get_densities(data, err, corr, weights)
             self.posteriors = self._get_posteriors(densities)
             self._update_class_mixtures(self.posteriors)
```

## scludam/pipeline.py

```diff
@@ -34,14 +34,15 @@
 import warnings
 from typing import List, Optional
 
 import numpy as np
 import pandas as pd
 from astropy.table.table import Table
 from attrs import Factory, define, field, validators
+from beartype import beartype
 from sklearn.preprocessing import MinMaxScaler, RobustScaler
 
 from scludam.detection import CountPeakDetector, DetectionResult
 from scludam.fetcher import search_objects_near_data, simbad2gaiacolnames
 from scludam.masker import RangeMasker
 from scludam.membership import DBME
 from scludam.plots import plot_objects, scatter2dprobaplot
@@ -239,15 +240,14 @@
             is_clusterable = np.sum(trs) >= trs.size / 2
         else:
             raise ValueError("test_mode must be one of 'any', 'all', 'majority'")
         self.is_clusterable.append(is_clusterable)
         return is_clusterable
 
     def _estimate_membership(self, df: pd.DataFrame, count: int, center: np.ndarray):
-
         data = df[self.mem_cols].values
 
         # create a clusterer for the data
         # with the configuration defined by the user
         clusterer = copy.deepcopy(self.clusterer)
         if clusterer.min_cluster_size and clusterer.clusterer is None:
             clusterer.min_cluster_size = int(count)
@@ -279,14 +279,15 @@
         # estimate membershipts
         estimator = copy.deepcopy(self.estimator)
         estimator.fit(data=data, init_proba=clusterer.proba, err=err, corr=corr)
         self.estimators.append(estimator)
 
         return estimator.posteriors
 
+    @beartype
     def fit(self, df: pd.DataFrame):
         """Perform the detection and membership estimation.
 
         NaNs are dropped from the dataframe copy and are
         not taken into account.
 
         Parameters
@@ -326,15 +327,14 @@
 
         global_proba = []
 
         # scatter_with_coors(df[["pmra", "pmdec"]], self.detection_result.centers)
         # plt.show()
         print(f"found {self.detection_result.centers.shape[0]} overdensities")
         for i, center in enumerate(self.detection_result.centers):
-
             count = self.detection_result.counts[i]
             sigma = self.detection_result.sigmas[i]
             mask = self._get_region_mask(df, center, sigma)
             region_df = df[mask]
 
             # test
             print(f"testing peak {i}...")
@@ -375,14 +375,15 @@
         self.n_estimated = self.proba.shape[1] - 1
 
         return self
 
     def _is_fitted(self):
         return self.proba is not None
 
+    @beartype
     def proba_df(self):
         """Return the data frame with the probabilities.
 
         Returns the full dataframe used for the process
         added columns for the labels and the probabilites.
 
         Returns
@@ -404,14 +405,15 @@
         df["label"] = self.labels
         return pd.concat(
             [self._df.reset_index(drop=True), df.reset_index(drop=True)],
             axis=1,
             sort=False,
         )
 
+    @beartype
     def write(self, path: str, **kwargs):
         """Write the data frame with the probabilities to a file.
 
         Writes the data frame used for the process
         with labels and probabilities to a file. kwargs are
         passed to the astropy.table.Table.write method.
         Default kwargs are "overwrite"=True and
@@ -435,14 +437,15 @@
         default_kws = {
             "overwrite": True,
             "format": "fits",
         }
         default_kws.update(kwargs)
         return table.write(path, **default_kws)
 
+    @beartype
     def cm_diagram(
         self,
         cols: str = ["bp_rp", "phot_g_mean_mag"],
         plotcols: Optional[List[str]] = None,
         plot_objects: bool = True,
         **kwargs,
     ):
@@ -485,14 +488,15 @@
         df = self._df[cols]
         ax = scatter2dprobaplot(df, self.proba, self.labels, plotcols, **kwargs)
         ax.invert_yaxis()
         if plot_objects:
             self._plot_objects(ax, cols)
         return ax
 
+    @beartype
     def radec_plot(
         self,
         cols: str = ["ra", "dec"],
         plotcols: Optional[List[str]] = None,
         plot_objects: bool = True,
         **kwargs,
     ):
@@ -536,14 +540,15 @@
         df = self._df[cols]
         ax = scatter2dprobaplot(df, self.proba, self.labels, plotcols, **kwargs)
         ax.invert_xaxis()
         if plot_objects:
             self._plot_objects(ax, cols)
         return ax
 
+    @beartype
     def scatterplot(
         self,
         cols: List[str] = ["pmra", "pmdec"],
         plotcols: Optional[List[str]] = None,
         plot_objects: bool = True,
         **kwargs,
     ):
```

## scludam/rutils.py

```diff
@@ -33,15 +33,15 @@
 def disable_r_warnings():
     """Disable R runtime warnings."""
     warnings.filterwarnings("ignore", category=RRuntimeWarning)
 
 
 def disable_r_console_output():
     """Disable R console output in python."""
-    # flush any R console output
+
     def add_to_stdout(line):
         pass
 
     def add_to_stderr(line):
         pass
 
     rcallbacks.consolewrite_print = add_to_stdout
```

## scludam/shdbscan.py

```diff
@@ -226,22 +226,20 @@
                 raise ValueError(
                     "Either min_cluster_size or clusterer must be provided."
                 )
         elif value < 1:
             raise ValueError("min_cluster_size must be greater than 1.")
 
     def _cluster(self, data: Numeric2DArray):
-
         if self._centers is not None or self.auto_allow_single_cluster:
             allow_single_cluster = False
         else:
             allow_single_cluster = self.allow_single_cluster
 
         if self.clusterer is None:
-
             if self.min_samples is None:
                 min_samples = self.min_cluster_size
             else:
                 min_samples = self.min_samples
 
             self.clusterer = HDBSCAN(
                 min_samples=min_samples,
@@ -256,15 +254,14 @@
 
         self.clusterer.fit(data)
 
         unique_labels = np.sort(np.unique(self.clusterer.labels_))
 
         # if no clusters found & auto_toggle_single_cluster & not allow_single_cluster
         if np.all(unique_labels == -1) and self.auto_allow_single_cluster:
-
             self.clusterer = HDBSCAN(
                 min_samples=self.clusterer.min_samples,
                 min_cluster_size=self.clusterer.min_cluster_size,
                 allow_single_cluster=True,
                 metric=self.clusterer.metric,
                 prediction_data=True,
             )
@@ -272,15 +269,14 @@
 
         return self.clusterer
 
     # get the most "conservative" cluster probabilities
     # accounting for outlier scores and labeling given
     # by the clusterer
     def _get_proba(self):
-
         # Outlier score is an implementation of GLOSH, it catches local outliers as
         # well as just points that are far away from everything.
         # Thus a point can be "in"
         # a cluster, and have a label, but be sufficiently far from an
         # otherwise very
         # dense core that is is anomalous in that local region of space
         # (i.e. it is weird
@@ -418,15 +414,14 @@
 
     def _center_based_cluster_selection(
         self,
         data: Numeric2DArray,
         labels: Numeric1DArray,
         input_centers: Numeric2DArrayLike,
     ):
-
         # compares input cluster centers with obtained cluster centers
         # if input cluster centers are less than obtained, then select
         # onbtained clusters that match input centers the best
 
         cluster_labels = self._unique_labels[self._unique_labels != -1]
         cluster_centers = np.array(
             [
```

## tests/test_detection.py

```diff
@@ -478,15 +478,16 @@
             bin_shape=[1, 1], mask=mask, min_count=6
         )._remove_low_density_regions(data)
 
 
 @pytest.mark.parametrize(
     "data, mask, bin_shape",
     [
-        (np.zeros(10), [1, 1], [1, 1]),
+        # this is controlled by beartype now
+        # (np.zeros(10), [1, 1], [1, 1]),
         (diagonal1_data(), [1, 1, 1], [1, 1]),
         (diagonal1_data(), [1, 1], [1, 1, 1]),
     ],
 )
 def test_detect_errors(data, mask, bin_shape):
     with pytest.raises(ValueError):
         CountPeakDetector(bin_shape=bin_shape, mask=mask).detect(data)
```

## tests/test_hkde.py

```diff
@@ -1,16 +1,17 @@
 import numpy as np
 import pytest
 from astropy.table.table import Table
+from scipy.stats import gaussian_kde
 from scipy.stats._multivariate import multi_rv_frozen
 from sklearn.datasets import load_iris
 from utils import raises_exception
 
 from scludam import HKDE
-from scludam.hkde import PluginSelector
+from scludam.hkde import PluginSelector, RuleOfThumbSelector
 from scludam.hkde import r as rsession
 from scludam.rutils import disable_r_console_output, disable_r_warnings, load_r_packages
 from scludam.utils import Colnames
 
 disable_r_console_output()
 disable_r_warnings()
 
@@ -130,14 +131,42 @@
             assert np.all(np.isclose(np.asarray(rsession("x")), data))
 
             bw = PluginSelector(
                 nstage=nstage, pilot=pilot, binned=binned, diag=diag
             ).get_bw(data)
             assert np.all(np.isclose(correct_result, bw))
 
+    def test_rule_of_thimb_selector__yields_equal_results_than_scipy(self):
+        iris = load_iris()
+        data = iris.data
+
+        hkde = HKDE(bw=RuleOfThumbSelector(rule="silverman")).fit(data)
+        hkde_res = hkde.pdf(data, leave1out=False)
+
+        scipy_kde = gaussian_kde(data.T, bw_method="silverman")
+        scipy_res = scipy_kde.evaluate(data.T)
+
+        # selector
+        rots = RuleOfThumbSelector(rule="silverman")
+        assert rots._silverman_factor(data) == scipy_kde.silverman_factor()
+        assert rots._scotts_factor(data) == scipy_kde.scotts_factor()
+        weights = np.ones(data.shape[0])
+        assert np.allclose(
+            rots._get_data_covariance(data, weights), scipy_kde._data_covariance
+        )
+        assert np.allclose(scipy_res, hkde_res)
+
+    def test_rule_of_thumb_selector_diag__yields_equal_results_than_scipy(self):
+        zero_corr_dataset = np.array([[8, 6, 4, 6, 8], [10, 12, 14, 16, 18]]).T
+        scipy_kde = gaussian_kde(zero_corr_dataset.T, bw_method="silverman")
+        scipy_cov = scipy_kde.covariance
+        rots = RuleOfThumbSelector(rule="silverman", diag=True)
+        hkde_cov = HKDE(bw=rots).fit(zero_corr_dataset)._covariances[0]
+        assert np.allclose(scipy_cov, hkde_cov)
+
 
 class TestHKDE:
     @pytest.mark.parametrize(
         "bw, exception, correct",
         [
             (
                 PluginSelector(),
```

## tests/test_membership.py

```diff
@@ -234,7 +234,72 @@
     labels = get_labels(
         sample2c[["p_pm_field", "p_pm_cluster1", "p_pm_cluster2"]].values
     )
     init_proba = one_hot_encode(labels)
     dbme = DBME().fit(data=data, init_proba=init_proba)
     densities = dbme._get_densities(data, None, None, dbme.posteriors)
     assert np.allclose(dbme._get_posteriors(densities).sum(axis=1), np.ones(len(data)))
+
+
+def test_multiple_estimator_configuration_errors(sample2c):
+    data = sample2c[["pmra", "pmdec"]].values
+    labels = get_labels(
+        sample2c[["p_pm_field", "p_pm_cluster1", "p_pm_cluster2"]].values
+    )
+    init_proba = one_hot_encode(labels)
+
+    dbme = DBME(
+        # problema es que tenemos 3 clases y 4 estimadores
+        pdf_estimator=[HKDE(), HKDE(), HKDE(), HKDE()],
+    )
+    message = "n_estimators should be 1, 2 or n_classes"
+    with pytest.raises(ValueError, match=message):
+        dbme.fit(data=data, init_proba=init_proba)
+
+
+def test_multiple_estimator_configuration_ok(sample2c):
+    data = sample2c[["pmra", "pmdec"]].values
+    labels = get_labels(
+        sample2c[["p_pm_field", "p_pm_cluster1", "p_pm_cluster2"]].values
+    )
+    init_proba = one_hot_encode(labels)
+    bws = [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]]
+    dbme = DBME(
+        pdf_estimator=[HKDE(bw=bws[0]), HKDE(bw=bws[1]), HKDE(bw=bws[2])],
+    )
+    dbme.fit(data=data, init_proba=init_proba)
+    assert dbme._is_fitted()
+    assert len(dbme._estimators) == 3
+    for i, estimator in enumerate(dbme._estimators):
+        assert isinstance(estimator, HKDE)
+        assert np.allclose(estimator.bw, bws[i])
+        # que se haya usado el bw correcto
+        # la 1er covarianza tenga diagonal igual al bw
+        diag = estimator._covariances[init_proba[:, i] > 0][0].diagonal()
+        assert np.allclose(diag, bws[i])
+
+
+def test_two_estimator_configuration_ok(sample2c):
+    data = sample2c[["pmra", "pmdec"]].values
+    labels = get_labels(
+        sample2c[["p_pm_field", "p_pm_cluster1", "p_pm_cluster2"]].values
+    )
+    init_proba = one_hot_encode(labels)
+    bws = [[0.1, 0.1], [0.2, 0.2]]
+    dbme = DBME(
+        # debería usar 1er para 1era clase y 2ndo para el resto
+        pdf_estimator=[HKDE(bw=bws[0]), HKDE(bw=bws[1])],
+    )
+    dbme.fit(data=data, init_proba=init_proba)
+    assert dbme._is_fitted()
+    assert len(dbme._estimators) == 3
+    for i, estimator in enumerate(dbme._estimators):
+        assert isinstance(estimator, HKDE)
+        if i == 2:
+            j = 1
+        else:
+            j = i
+        assert np.allclose(estimator.bw, bws[j])
+        # que se haya usado el bw correcto
+        # la 1er covarianza tenga diagonal igual al bw
+        diag = estimator._covariances[init_proba[:, i] > 0][0].diagonal()
+        assert np.allclose(diag, bws[j])
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## tests/test_pipeline.py

```diff
@@ -261,7 +261,72 @@
         test_cols=[["ra", "dec"]],
         det_cols=["pmra", "pmdec", "parallax"],
         mem_cols=["pmra", "pmdec"],
     ).fit(sample2cdf)
     # all field probabilities
     assert dep.proba.shape == (sample2cdf.shape[0], 1)
     assert np.allclose(dep.proba, 1)
+
+
+def test_multiple_estimators_configuration():
+    df = sample2cdf.copy()
+    dep1 = DEP(
+        # # Detector configuration for the detection step
+        det_cols=["pmra", "pmdec", "parallax"],
+        detector=CountPeakDetector(
+            bin_shape=[0.7, 0.7, 0.1],
+            min_score=6,
+            max_n_peaks=1,
+        ),
+        sample_sigma_factor=1.5,
+        mem_cols=["pmra", "pmdec"],
+        estimator=DBME(
+            pdf_estimator=HKDE(
+                bw=RuleOfThumbSelector(rule="scott", diag=True),
+            ),
+            kernel_calculation_mode="per_class",
+        ),
+    ).fit(df)
+
+    fe1 = dep1.estimators[0]._estimators[0]
+    fbw1 = fe1._base_bw
+    ce1 = dep1.estimators[0]._estimators[1]
+    cbw1 = ce1._base_bw
+
+    bw1 = [fbw1[0, 0], fbw1[1, 1]]
+    bw2 = [cbw1[0, 0], cbw1[1, 1]]
+
+    dep2 = DEP(
+        # # Detector configuration for the detection step
+        det_cols=["pmra", "pmdec", "parallax"],
+        detector=CountPeakDetector(
+            bin_shape=[0.7, 0.7, 0.1],
+            min_score=6,
+            max_n_peaks=1,
+        ),
+        sample_sigma_factor=1.5,
+        mem_cols=["pmra", "pmdec"],
+        estimator=DBME(
+            pdf_estimator=[
+                HKDE(bw=bw1),
+                HKDE(bw=bw2),
+            ],
+            kernel_calculation_mode="per_class",
+        ),
+    ).fit(df)
+
+    fe1 = dep1.estimators[0]._estimators[0]
+    fcov1 = fe1._covariances[0]
+    ce1 = dep1.estimators[0]._estimators[1]
+    ccov1 = ce1._covariances[0]
+
+    fe2 = dep2.estimators[0]._estimators[0]
+    fcov2 = fe2._covariances[0]
+    ce2 = dep2.estimators[0]._estimators[1]
+    ccov2 = ce2._covariances[0]
+
+    assert np.allclose(fcov1, fcov2)
+    assert np.allclose(ccov1, ccov2)
+
+    p1 = dep1.proba_df()
+    p2 = dep2.proba_df()
+    assert np.allclose(p1, p2)
```

## Comparing `scludam-1.0.2.dist-info/LICENSE` & `scludam-1.0.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `scludam-1.0.2.dist-info/METADATA` & `scludam-1.0.3.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: scludam
-Version: 1.0.2
+Version: 1.0.3
 Summary: Star cluster detection and membership estimation based on GAIA data.
 Home-page: http://packages.python.org/scludam
 Author: Simón Pedro González
 Author-email: simon.pedro.g@gmail.com
 License: GPL-3
 Keywords: star cluster detection membership probabilities
 Classifier: Development Status :: 2 - Pre-Alpha
@@ -48,46 +48,48 @@
 [![Documentation Status](https://img.shields.io/badge/docs-passing-success)](https://simonpedrogonzalez.github.io/scludam-docs/index.html)
 [![PyPI](https://img.shields.io/pypi/v/scludam)](https://pypi.org/project/scludam/)
 [![Python 3.7.6+](https://img.shields.io/badge/python-3.7.6+-blue.svg)](https://github.com/simonpedrogonzalez/scludam)
 [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://github.com/simonpedrogonzalez/scludam)
 [![License](https://img.shields.io/badge/License-GNU-blue.svg)](https://tldrlegal.com/license/gnu-lesser-general-public-license-v3-(lgpl-3))
 
 
-**scludam** (**S**tar **CLU**ster **D**etection **A**nd **M**embership estimation) is a Python package for GAIA catalogues **data fetching**, **star cluster detection** and **star cluster membership estimation**.
+**SCLUDAM** (**S**tar **CLU**ster **D**etection **A**nd **M**embership estimation) is a Python package for GAIA catalogues **data fetching**, **star cluster detection** and **star cluster membership estimation**.
 
 ### Repository and issues
 [https://github.com/simonpedrogonzalez/scludam](https://github.com/simonpedrogonzalez/scludam)
 
 ### Authors
-- Simón Pedro González
-email: [simon.pedro.g@gmail.com](simon.pedro.g@gmail.com)
+- Simón Pedro González. 
+Email: [simon.pedro.g@gmail.com](simon.pedro.g@gmail.com)
 
 ### Features
-Currently **scludam** is a work in progress. Modules and features already included are:
+Included modules and features are:
 
-- **fetcher**: simple query builder to get data from the GAIA catalogue more easily, and some extra useful functions.
+- **fetcher**: Query builder for easy access to GAIA catalogues data, functions to get catalogues and SIMBAD objects information.
 
-- **stat_tests**: set of 3 clusterability tests that can be used to detect the presence of a cluster in a sample.
+- **stat_tests**: Set of three clusterability tests that can be used to detect the presence of a cluster in a sample.
 
-- **synthetic**: classes that can be used to generate synthetic astrometric samples by specifying the distributions to use and parameter values.
+- **synthetic**: Classes that can be used to generate synthetic astrometric samples by specifying the distributions and parameter values.
 
-- **detection**: classes that can be used to detect the presence of a cluster in a sample.
+- **detection**: Detection of star clusters in a sample using an improved version of the Star Counts algorithm.
 
-- **shdbscan**: soft clustering based on the **HDBSCAN** algorithm.
+- **shdbscan**: Soft clustering based on the **HDBSCAN** algorithm.
 
-- **hkde**: kernel density estimation with variable bandwidth.
+- **hkde**: Kernel density estimation with per-observation or per-dimension variable bandwidth.
 
-- **membership**: membership calculation based on **hkde** smoothing.
+- **membership**: Membership probability estimation based on **hkde** smoothing.
 
-- **pipeline**: pipeline for the detection and membership estimation, with default values and convenience functions.
+- **pipeline**: Pipeline for the detection and membership estimation, with default values and convenience functions.
+
+- **plots**: Plot detection and membership estimation results alongside SIMBAD objects for better result interpretation.
 
 --------------------------------------------------------------------------------
 
 ### Requirements
-You need **Python 3.7.6+** and **R 3.6.3+** to run scludam. It is recommended to install scludam in a separate environment created with pyenv or conda, to avoid dependencies issues with other preinstalled packages you may have in the base environment. The following dependencies will be installed with SCLUDAM:
+**Python 3.7.6+** and **R 3.6.3+** are needed to run SCLUDAM. It is recommended to install scludam in a separate environment created with pyenv or conda, to avoid dependencies issues with other preinstalled packages in the base environment. The following dependencies will be installed along with SCLUDAM:
 
 - numpy>=1.21.6
 - matplotlib>=3.4.1
 - scipy>=1.7.3
 - astropy>=4.3.1
 - astroquery>=0.4.6
 - pandas>=1.3.5
@@ -114,18 +116,18 @@
 ### Update scludam in a Conda environment
 ```
 conda activate myscludamenv
 python -m pip install -U scludam
 python -m pip show scludam
 ```
 
-### User install
+### Simple user install
 Install from PyPi:
 ```python -m pip install scludam```
 
-### User update
+### Simple user update
 Update from PyPi:
 ```python -m pip install -U scludam```
 
 ### Dev install
 Clone the repo and run the following command in the cloned directory (with your environment activated):
 ```python -m pip install -e .[dev]```
```

## Comparing `scludam-1.0.2.dist-info/RECORD` & `scludam-1.0.3.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-scludam/__init__.py,sha256=OF-c4s139vFCPVvnhWua_cNLQBfQDfYThfHxpKXt6UE,442
-scludam/detection.py,sha256=5dbQfgJ76cNC-0-Zxr1pacOO2blrvdwo65U0SPfRKdc,37505
+scludam/__init__.py,sha256=qA1uv9DzxQv7J20FImLDZz573KDsx959DWj0Rf4uIm0,2206
+scludam/detection.py,sha256=Ppwh06hF-JV1-7PP77cq77qEi1wmy5JxZcHGyc4xLPk,37516
 scludam/fetcher.py,sha256=gTUdQThPb-4wMxD_SaEbJ3jXOQ4Jnyx-KZUonArbb4A,24162
-scludam/hkde.py,sha256=ZHacGCokhadNYvpWY5OZKeZRadgh4iYoweqCjce7KKg,23573
+scludam/hkde.py,sha256=oxwfCJiMLQtzmVj-rQP2S998iTdfWicjAZBmdFdzxqc,24190
 scludam/masker.py,sha256=n0Esyvy8ltK-KC_jKA0lLYtnzLANB8EbcMWD0OhTeW4,7447
-scludam/membership.py,sha256=PIiIcaoYfJNX-1XzQcKcqg4BLvuziioIW0PG0jROLX0,14274
-scludam/pipeline.py,sha256=MU5I4G2uWer8Kf8I3AVLpexZYR_vuo4F2uTtqlbbanQ,21987
+scludam/membership.py,sha256=nyZyYaJkiS4KMpj0QAYbQlWXivkPdjcAZeCRqazzoNs,17840
+scludam/pipeline.py,sha256=zBVIf0MHsEcInHXTiThIf4GsOwoQCQVf3dI0XOZmjVM,22099
 scludam/plots.py,sha256=4suZOevaQ5mn4-nJRn01zOGnBcxeeam7KRCKwkAuAcg,27623
-scludam/rutils.py,sha256=irbryl549KvHlzkgzvAWVM1S1K4FAgMLJBshfdpiNVs,4848
-scludam/shdbscan.py,sha256=GXFCgNSFRdgxHSXuufBX-uKF76v1E7PlcbhtVy_qHc0,31035
+scludam/rutils.py,sha256=Q77YqMUbTVnoBEDzWA3NMfb2nZn0xdy1uxit-DC9-Dk,4816
+scludam/shdbscan.py,sha256=3YWmhdMy9CSFP3QTLuv47xNYG_s_xzkXFnKV-YJepHs,31030
 scludam/stat_tests.py,sha256=U7JSKG4PaT6wohaH_wR9DDqZe_45UjOEkzLx9HRwFTk,23021
 scludam/synthetic.py,sha256=tBMyPyM1FyRXTkpiideq48agrn__RjGyoAEwnI0FOBg,38632
 scludam/type_utils.py,sha256=GhsRBF5TK65WZkomBeTL3gD4nZr-4wEUP7QmjOBg1-U,2609
 scludam/utils.py,sha256=uP391UX7m3xyaGfReFrnedHvvdIw4mZ1du1wnmhcFGQ,7928
-tests/test_detection.py,sha256=DE4j9HXVbjPijvJ_cT0up152WjaXp7V-lYdq8JVUPwI,21653
+tests/test_detection.py,sha256=S7lwT7VtjHlygIPPxS5ztJyq5FIeuSVyyhztjl4iGpo,21700
 tests/test_fetcher.py,sha256=JPenI8vhFfYWqFgNgbXzvY2IkjGHSjgIda3nbUu4oj4,15589
-tests/test_hkde.py,sha256=r7Yxsr-WWQ72AIqZs1dWGzaLLeWi38M4ndBGefp4X7s,11107
-tests/test_membership.py,sha256=MW56C7_XCl_CwNGHyt5fHYTE_hADtox6b9bn1btN944,7610
-tests/test_pipeline.py,sha256=0BR2P0uSDuLK4-kaiwZiVgoOjola84gWMlRkvuGo29Q,8361
+tests/test_hkde.py,sha256=PNPX-64EHD7C_YLRHsG4UkBeTjr1RnMegH4-JdfWxNI,12421
+tests/test_membership.py,sha256=bhaGpr2KTGTnFHVzhVC8hBTYvl1K6w4kBAR8S5EouMw,9993
+tests/test_pipeline.py,sha256=A_H6wXnKeunFeVm4UHk5T4J2d_AM3aXMAiY1kQgAtEI,10177
 tests/test_shdbscan.py,sha256=CM8FIqpfXDTUn2OOH8jdH6aIyR9fk5lA9Qx8foITmHs,17500
 tests/test_stat_tests.py,sha256=PMlJVP8lw8llm_BCawk0EewdUGUy_Wq769WMaMgAsbo,5054
 tests/test_synthetic.py,sha256=0YeYIjRM4kWCKAeVvtIWNpIxgg2iDrYqEDbQUitZqmE,16673
 tests/test_utils.py,sha256=oCXOmGRvnWWVTVGnO44AYm0N9S8weLQWLPbwo9AYTkM,3009
 tests/utils.py,sha256=dEG0wWDV1twH_epZACm7WMeq5Sp-RNmW92la4BfMqno,611
-scludam-1.0.2.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-scludam-1.0.2.dist-info/METADATA,sha256=8_-y4Y5rFEOiAoGmfJVsNYh86yI2T0jzr_X1NQ1ecgU,5230
-scludam-1.0.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-scludam-1.0.2.dist-info/top_level.txt,sha256=o1ZchBQ1yRGNlnzajlmexEPcjlq8wyPsyFafvNVKmqw,14
-scludam-1.0.2.dist-info/RECORD,,
+scludam-1.0.3.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+scludam-1.0.3.dist-info/METADATA,sha256=oM6-kwlrSXSkFz1mhHskq7vQcp9c8Qh0B7OMk6K9W2E,5384
+scludam-1.0.3.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+scludam-1.0.3.dist-info/top_level.txt,sha256=o1ZchBQ1yRGNlnzajlmexEPcjlq8wyPsyFafvNVKmqw,14
+scludam-1.0.3.dist-info/RECORD,,
```


# Comparing `tmp/data_gradients-0.0.9-py3-none-any.whl.zip` & `tmp/data_gradients-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,103 +1,109 @@
-Zip file size: 383255 bytes, number of entries: 101
--rw-r--r--  2.0 unx       22 b- defN 23-Jun-15 15:53 data_gradients/__init__.py
--rw-r--r--  2.0 unx      189 b- defN 23-Jun-15 15:53 data_gradients/requirements.txt
--rw-r--r--  2.0 unx      231 b- defN 23-Jun-15 15:53 data_gradients/assets/__init__.py
--rw-r--r--  2.0 unx     2755 b- defN 23-Jun-15 15:53 data_gradients/assets/assets_container.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/assets/css/test.css
--rw-r--r--  2.0 unx     3245 b- defN 23-Jun-15 15:53 data_gradients/assets/html/basic_info_fe.html
--rw-r--r--  2.0 unx     8301 b- defN 23-Jun-15 15:53 data_gradients/assets/html/doc_template.html
--rw-r--r--  2.0 unx      123 b- defN 23-Jun-15 15:53 data_gradients/assets/html/test.html
--rw-r--r--  2.0 unx    51292 b- defN 23-Jun-15 15:53 data_gradients/assets/images/chart_demo.png
--rw-r--r--  2.0 unx   139838 b- defN 23-Jun-15 15:53 data_gradients/assets/images/info.png
--rw-r--r--  2.0 unx    36081 b- defN 23-Jun-15 15:53 data_gradients/assets/images/logo.png
--rw-r--r--  2.0 unx    75086 b- defN 23-Jun-15 15:53 data_gradients/assets/images/warning.png
--rw-r--r--  2.0 unx      333 b- defN 23-Jun-15 15:53 data_gradients/assets/text/lorem_ipsum.txt
--rw-r--r--  2.0 unx       12 b- defN 23-Jun-15 15:53 data_gradients/assets/text/test.txt
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/__init__.py
--rw-r--r--  2.0 unx     1846 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/base.py
--rw-r--r--  2.0 unx     1171 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/detection.py
--rw-r--r--  2.0 unx     1323 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/segmentation.py
--rw-r--r--  2.0 unx     1193 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/adapters/__init__.py
--rw-r--r--  2.0 unx     3613 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/adapters/dataset_adapter.py
--rw-r--r--  2.0 unx     6256 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/adapters/tensor_extractor.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/formatters/__init__.py
--rw-r--r--  2.0 unx      725 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/formatters/base.py
--rw-r--r--  2.0 unx    11284 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/formatters/detection.py
--rw-r--r--  2.0 unx     6895 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/formatters/segmentation.py
--rw-r--r--  2.0 unx     1865 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/formatters/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/preprocessors/__init__.py
--rw-r--r--  2.0 unx      994 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/preprocessors/base.py
--rw-r--r--  2.0 unx     5189 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/preprocessors/contours.py
--rw-r--r--  2.0 unx     2527 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/preprocessors/detection.py
--rw-r--r--  2.0 unx     1836 b- defN 23-Jun-15 15:53 data_gradients/batch_processors/preprocessors/segmentation.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/common/__init__.py
--rw-r--r--  2.0 unx       66 b- defN 23-Jun-15 15:53 data_gradients/common/decorators/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 23-Jun-15 15:53 data_gradients/common/decorators/decorators.py
--rw-r--r--  2.0 unx      140 b- defN 23-Jun-15 15:53 data_gradients/common/factories/__init__.py
--rw-r--r--  2.0 unx     3694 b- defN 23-Jun-15 15:53 data_gradients/common/factories/base_factory.py
--rw-r--r--  2.0 unx      206 b- defN 23-Jun-15 15:53 data_gradients/common/factories/feature_extractors_factory.py
--rw-r--r--  2.0 unx      569 b- defN 23-Jun-15 15:53 data_gradients/common/factories/list_factory.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/common/registry/__init__.py
--rw-r--r--  2.0 unx     1245 b- defN 23-Jun-15 15:53 data_gradients/common/registry/registry.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/config/__init__.py
--rw-r--r--  2.0 unx      674 b- defN 23-Jun-15 15:53 data_gradients/config/detection.yaml
--rw-r--r--  2.0 unx      756 b- defN 23-Jun-15 15:53 data_gradients/config/segmentation.yaml
--rw-r--r--  2.0 unx     4511 b- defN 23-Jun-15 15:53 data_gradients/config/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/datasets/__init__.py
--rw-r--r--  2.0 unx     2265 b- defN 23-Jun-15 15:53 data_gradients/datasets/bdd_dataset.py
--rw-r--r--  2.0 unx     1548 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/__init__.py
--rw-r--r--  2.0 unx     1176 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/abstract_feature_extractor.py
--rw-r--r--  2.0 unx     4445 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/features.py
--rw-r--r--  2.0 unx     1750 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/utils.py
--rw-r--r--  2.0 unx      325 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/__init__.py
--rw-r--r--  2.0 unx     2337 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/heatmap.py
--rw-r--r--  2.0 unx     2991 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/image_average_brightness.py
--rw-r--r--  2.0 unx     4395 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/image_color_distribution.py
--rw-r--r--  2.0 unx     3312 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/image_resolution.py
--rw-r--r--  2.0 unx     3036 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/sample_visualization.py
--rw-r--r--  2.0 unx     4930 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/common/summary.py
--rw-r--r--  2.0 unx      782 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/__init__.py
--rw-r--r--  2.0 unx     2554 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/bounding_boxes_area.py
--rw-r--r--  2.0 unx     5625 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py
--rw-r--r--  2.0 unx     2186 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py
--rw-r--r--  2.0 unx     2915 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py
--rw-r--r--  2.0 unx     2408 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/classes_count.py
--rw-r--r--  2.0 unx     2049 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py
--rw-r--r--  2.0 unx     2567 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/classes_per_image_count.py
--rw-r--r--  2.0 unx     1971 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/object_detection/sample_visualization.py
--rw-r--r--  2.0 unx      947 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/__init__.py
--rw-r--r--  2.0 unx     2725 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/bounding_boxes_area.py
--rw-r--r--  2.0 unx     2858 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py
--rw-r--r--  2.0 unx     2652 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/classes_count.py
--rw-r--r--  2.0 unx     2110 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py
--rw-r--r--  2.0 unx     2850 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/classes_per_image_count.py
--rw-r--r--  2.0 unx     2396 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/components_convexity.py
--rw-r--r--  2.0 unx     3815 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/components_erosion.py
--rw-r--r--  2.0 unx     2204 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/components_per_image_count.py
--rw-r--r--  2.0 unx     2674 b- defN 23-Jun-15 15:53 data_gradients/feature_extractors/segmentation/sample_visualization.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/managers/__init__.py
--rw-r--r--  2.0 unx    12060 b- defN 23-Jun-15 15:53 data_gradients/managers/abstract_manager.py
--rw-r--r--  2.0 unx     4519 b- defN 23-Jun-15 15:53 data_gradients/managers/detection_manager.py
--rw-r--r--  2.0 unx     4864 b- defN 23-Jun-15 15:53 data_gradients/managers/segmentation_manager.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/utils/__init__.py
--rw-r--r--  2.0 unx      797 b- defN 23-Jun-15 15:53 data_gradients/utils/detection.py
--rw-r--r--  2.0 unx     1052 b- defN 23-Jun-15 15:53 data_gradients/utils/image_processing.py
--rw-r--r--  2.0 unx     2550 b- defN 23-Jun-15 15:53 data_gradients/utils/pdf_writer.py
--rw-r--r--  2.0 unx     3701 b- defN 23-Jun-15 15:53 data_gradients/utils/utils.py
--rw-r--r--  2.0 unx      169 b- defN 23-Jun-15 15:53 data_gradients/utils/common/__init__.py
--rw-r--r--  2.0 unx      249 b- defN 23-Jun-15 15:53 data_gradients/utils/data_classes/__init__.py
--rw-r--r--  2.0 unx      260 b- defN 23-Jun-15 15:53 data_gradients/utils/data_classes/contour.py
--rw-r--r--  2.0 unx     3060 b- defN 23-Jun-15 15:53 data_gradients/utils/data_classes/data_samples.py
--rw-r--r--  2.0 unx     3975 b- defN 23-Jun-15 15:53 data_gradients/utils/data_classes/extractor_results.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-15 15:53 data_gradients/visualize/__init__.py
--rw-r--r--  2.0 unx     5437 b- defN 23-Jun-15 15:53 data_gradients/visualize/detection.py
--rw-r--r--  2.0 unx     5092 b- defN 23-Jun-15 15:53 data_gradients/visualize/images.py
--rw-r--r--  2.0 unx    11756 b- defN 23-Jun-15 15:53 data_gradients/visualize/plot_options.py
--rw-r--r--  2.0 unx    16700 b- defN 23-Jun-15 15:53 data_gradients/visualize/seaborn_renderer.py
--rw-r--r--  2.0 unx    11341 b- defN 23-Jun-15 15:58 data_gradients-0.0.9.dist-info/LICENSE.md
--rw-r--r--  2.0 unx     8393 b- defN 23-Jun-15 15:58 data_gradients-0.0.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-15 15:58 data_gradients-0.0.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 23-Jun-15 15:58 data_gradients-0.0.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    10656 b- defN 23-Jun-15 15:58 data_gradients-0.0.9.dist-info/RECORD
-101 files, 564936 bytes uncompressed, 365543 bytes compressed:  35.3%
+Zip file size: 392579 bytes, number of entries: 107
+-rw-r--r--  2.0 unx       22 b- defN 23-Jun-26 07:49 data_gradients/__init__.py
+-rw-r--r--  2.0 unx      205 b- defN 23-Jun-26 07:49 data_gradients/requirements.txt
+-rw-r--r--  2.0 unx      231 b- defN 23-Jun-26 07:49 data_gradients/assets/__init__.py
+-rw-r--r--  2.0 unx     2755 b- defN 23-Jun-26 07:49 data_gradients/assets/assets_container.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/assets/css/test.css
+-rw-r--r--  2.0 unx     3245 b- defN 23-Jun-26 07:49 data_gradients/assets/html/basic_info_fe.html
+-rw-r--r--  2.0 unx     8301 b- defN 23-Jun-26 07:49 data_gradients/assets/html/doc_template.html
+-rw-r--r--  2.0 unx      123 b- defN 23-Jun-26 07:49 data_gradients/assets/html/test.html
+-rw-r--r--  2.0 unx    51292 b- defN 23-Jun-26 07:49 data_gradients/assets/images/chart_demo.png
+-rw-r--r--  2.0 unx   139838 b- defN 23-Jun-26 07:49 data_gradients/assets/images/info.png
+-rw-r--r--  2.0 unx    36081 b- defN 23-Jun-26 07:49 data_gradients/assets/images/logo.png
+-rw-r--r--  2.0 unx    75086 b- defN 23-Jun-26 07:49 data_gradients/assets/images/warning.png
+-rw-r--r--  2.0 unx      333 b- defN 23-Jun-26 07:49 data_gradients/assets/text/lorem_ipsum.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jun-26 07:49 data_gradients/assets/text/test.txt
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/__init__.py
+-rw-r--r--  2.0 unx     1636 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/base.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/detection.py
+-rw-r--r--  2.0 unx     1236 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/segmentation.py
+-rw-r--r--  2.0 unx     1193 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/adapters/__init__.py
+-rw-r--r--  2.0 unx     4621 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/adapters/dataset_adapter.py
+-rw-r--r--  2.0 unx     6698 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/adapters/tensor_extractor.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/__init__.py
+-rw-r--r--  2.0 unx      725 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/base.py
+-rw-r--r--  2.0 unx     8757 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/detection.py
+-rw-r--r--  2.0 unx     6940 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/segmentation.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/__init__.py
+-rw-r--r--  2.0 unx      994 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/base.py
+-rw-r--r--  2.0 unx     5189 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/contours.py
+-rw-r--r--  2.0 unx     2527 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/detection.py
+-rw-r--r--  2.0 unx     1836 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/segmentation.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/common/__init__.py
+-rw-r--r--  2.0 unx       66 b- defN 23-Jun-26 07:49 data_gradients/common/decorators/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 23-Jun-26 07:49 data_gradients/common/decorators/decorators.py
+-rw-r--r--  2.0 unx      140 b- defN 23-Jun-26 07:49 data_gradients/common/factories/__init__.py
+-rw-r--r--  2.0 unx     3694 b- defN 23-Jun-26 07:49 data_gradients/common/factories/base_factory.py
+-rw-r--r--  2.0 unx      206 b- defN 23-Jun-26 07:49 data_gradients/common/factories/feature_extractors_factory.py
+-rw-r--r--  2.0 unx      569 b- defN 23-Jun-26 07:49 data_gradients/common/factories/list_factory.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/common/registry/__init__.py
+-rw-r--r--  2.0 unx     1245 b- defN 23-Jun-26 07:49 data_gradients/common/registry/registry.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/config/__init__.py
+-rw-r--r--  2.0 unx      705 b- defN 23-Jun-26 07:49 data_gradients/config/detection.yaml
+-rw-r--r--  2.0 unx      756 b- defN 23-Jun-26 07:49 data_gradients/config/segmentation.yaml
+-rw-r--r--  2.0 unx     4511 b- defN 23-Jun-26 07:49 data_gradients/config/utils.py
+-rw-r--r--  2.0 unx      157 b- defN 23-Jun-26 07:49 data_gradients/config/data/__init__.py
+-rw-r--r--  2.0 unx     8713 b- defN 23-Jun-26 07:49 data_gradients/config/data/caching_utils.py
+-rw-r--r--  2.0 unx     8640 b- defN 23-Jun-26 07:49 data_gradients/config/data/data_config.py
+-rw-r--r--  2.0 unx     2894 b- defN 23-Jun-26 07:49 data_gradients/config/data/questions.py
+-rw-r--r--  2.0 unx      231 b- defN 23-Jun-26 07:49 data_gradients/config/data/typing.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/datasets/__init__.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jun-26 07:49 data_gradients/datasets/bdd_dataset.py
+-rw-r--r--  2.0 unx     1565 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/__init__.py
+-rw-r--r--  2.0 unx     1176 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/abstract_feature_extractor.py
+-rw-r--r--  2.0 unx     4445 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/features.py
+-rw-r--r--  2.0 unx     1750 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/utils.py
+-rw-r--r--  2.0 unx      325 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/__init__.py
+-rw-r--r--  2.0 unx     2337 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/heatmap.py
+-rw-r--r--  2.0 unx     2971 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/image_average_brightness.py
+-rw-r--r--  2.0 unx     4478 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/image_color_distribution.py
+-rw-r--r--  2.0 unx     3382 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/image_resolution.py
+-rw-r--r--  2.0 unx     3219 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/sample_visualization.py
+-rw-r--r--  2.0 unx     4930 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/summary.py
+-rw-r--r--  2.0 unx      790 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/__init__.py
+-rw-r--r--  2.0 unx     2757 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_area.py
+-rw-r--r--  2.0 unx     5663 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py
+-rw-r--r--  2.0 unx     2382 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py
+-rw-r--r--  2.0 unx     2877 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py
+-rw-r--r--  2.0 unx     2667 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/classes_frequency.py
+-rw-r--r--  2.0 unx     2815 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py
+-rw-r--r--  2.0 unx     2479 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py
+-rw-r--r--  2.0 unx     1971 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/sample_visualization.py
+-rw-r--r--  2.0 unx      958 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/__init__.py
+-rw-r--r--  2.0 unx     2776 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/bounding_boxes_area.py
+-rw-r--r--  2.0 unx     2809 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py
+-rw-r--r--  2.0 unx     2754 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/classes_frequency.py
+-rw-r--r--  2.0 unx     2835 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py
+-rw-r--r--  2.0 unx     2526 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py
+-rw-r--r--  2.0 unx     2363 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/component_frequency_per_image.py
+-rw-r--r--  2.0 unx     2303 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/components_convexity.py
+-rw-r--r--  2.0 unx     3808 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/components_erosion.py
+-rw-r--r--  2.0 unx     2674 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/sample_visualization.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/managers/__init__.py
+-rw-r--r--  2.0 unx    11429 b- defN 23-Jun-26 07:49 data_gradients/managers/abstract_manager.py
+-rw-r--r--  2.0 unx     5498 b- defN 23-Jun-26 07:49 data_gradients/managers/detection_manager.py
+-rw-r--r--  2.0 unx     5401 b- defN 23-Jun-26 07:49 data_gradients/managers/segmentation_manager.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/utils/__init__.py
+-rw-r--r--  2.0 unx     2760 b- defN 23-Jun-26 07:49 data_gradients/utils/detection.py
+-rw-r--r--  2.0 unx     1052 b- defN 23-Jun-26 07:49 data_gradients/utils/image_processing.py
+-rw-r--r--  2.0 unx     2550 b- defN 23-Jun-26 07:49 data_gradients/utils/pdf_writer.py
+-rw-r--r--  2.0 unx     3705 b- defN 23-Jun-26 07:49 data_gradients/utils/summary_writer.py
+-rw-r--r--  2.0 unx     2561 b- defN 23-Jun-26 07:49 data_gradients/utils/utils.py
+-rw-r--r--  2.0 unx      169 b- defN 23-Jun-26 07:49 data_gradients/utils/common/__init__.py
+-rw-r--r--  2.0 unx      249 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/__init__.py
+-rw-r--r--  2.0 unx      260 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/contour.py
+-rw-r--r--  2.0 unx     3060 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/data_samples.py
+-rw-r--r--  2.0 unx     3975 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/extractor_results.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/visualize/__init__.py
+-rw-r--r--  2.0 unx     5437 b- defN 23-Jun-26 07:49 data_gradients/visualize/detection.py
+-rw-r--r--  2.0 unx     5092 b- defN 23-Jun-26 07:49 data_gradients/visualize/images.py
+-rw-r--r--  2.0 unx    11756 b- defN 23-Jun-26 07:49 data_gradients/visualize/plot_options.py
+-rw-r--r--  2.0 unx    16700 b- defN 23-Jun-26 07:49 data_gradients/visualize/seaborn_renderer.py
+-rw-r--r--  2.0 unx    11341 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx     9226 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    11250 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/RECORD
+107 files, 593382 bytes uncompressed, 373903 bytes compressed:  37.0%
```

## zipnote {}

```diff
@@ -129,14 +129,29 @@
 
 Filename: data_gradients/config/segmentation.yaml
 Comment: 
 
 Filename: data_gradients/config/utils.py
 Comment: 
 
+Filename: data_gradients/config/data/__init__.py
+Comment: 
+
+Filename: data_gradients/config/data/caching_utils.py
+Comment: 
+
+Filename: data_gradients/config/data/data_config.py
+Comment: 
+
+Filename: data_gradients/config/data/questions.py
+Comment: 
+
+Filename: data_gradients/config/data/typing.py
+Comment: 
+
 Filename: data_gradients/datasets/__init__.py
 Comment: 
 
 Filename: data_gradients/datasets/bdd_dataset.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/__init__.py
@@ -183,53 +198,53 @@
 
 Filename: data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py
 Comment: 
 
-Filename: data_gradients/feature_extractors/object_detection/classes_count.py
+Filename: data_gradients/feature_extractors/object_detection/classes_frequency.py
 Comment: 
 
-Filename: data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py
+Filename: data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py
 Comment: 
 
-Filename: data_gradients/feature_extractors/object_detection/classes_per_image_count.py
+Filename: data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/object_detection/sample_visualization.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/segmentation/__init__.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/segmentation/bounding_boxes_area.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py
 Comment: 
 
-Filename: data_gradients/feature_extractors/segmentation/classes_count.py
+Filename: data_gradients/feature_extractors/segmentation/classes_frequency.py
+Comment: 
+
+Filename: data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py
 Comment: 
 
-Filename: data_gradients/feature_extractors/segmentation/classes_per_image_count.py
+Filename: data_gradients/feature_extractors/segmentation/component_frequency_per_image.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/segmentation/components_convexity.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/segmentation/components_erosion.py
 Comment: 
 
-Filename: data_gradients/feature_extractors/segmentation/components_per_image_count.py
-Comment: 
-
 Filename: data_gradients/feature_extractors/segmentation/sample_visualization.py
 Comment: 
 
 Filename: data_gradients/managers/__init__.py
 Comment: 
 
 Filename: data_gradients/managers/abstract_manager.py
@@ -249,14 +264,17 @@
 
 Filename: data_gradients/utils/image_processing.py
 Comment: 
 
 Filename: data_gradients/utils/pdf_writer.py
 Comment: 
 
+Filename: data_gradients/utils/summary_writer.py
+Comment: 
+
 Filename: data_gradients/utils/utils.py
 Comment: 
 
 Filename: data_gradients/utils/common/__init__.py
 Comment: 
 
 Filename: data_gradients/utils/data_classes/__init__.py
@@ -282,23 +300,23 @@
 
 Filename: data_gradients/visualize/plot_options.py
 Comment: 
 
 Filename: data_gradients/visualize/seaborn_renderer.py
 Comment: 
 
-Filename: data_gradients-0.0.9.dist-info/LICENSE.md
+Filename: data_gradients-0.1.1.dist-info/LICENSE.md
 Comment: 
 
-Filename: data_gradients-0.0.9.dist-info/METADATA
+Filename: data_gradients-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: data_gradients-0.0.9.dist-info/WHEEL
+Filename: data_gradients-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: data_gradients-0.0.9.dist-info/top_level.txt
+Filename: data_gradients-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: data_gradients-0.0.9.dist-info/RECORD
+Filename: data_gradients-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## data_gradients/__init__.py

```diff
@@ -1 +1 @@
-__version__ = "0.0.9"
+__version__ = "0.1.1"
```

## data_gradients/requirements.txt

```diff
@@ -1,17 +1,18 @@
 hydra-core>=1.2.0
 omegaconf>=2.2.3
 pygments>=2.13.0
 tqdm>=4.64.1
+appdirs>=1.4.4
 opencv-python
 Pillow
 tensorboard
 torch
 torchvision
 numpy
 matplotlib
 scipy
 rapidfuzz
 coverage~=5.3.1
 seaborn
 xhtml2pdf
-jinja2
+jinja2
```

## data_gradients/batch_processors/base.py

```diff
@@ -24,15 +24,7 @@
         self.batch_preprocessor = batch_preprocessor
 
     def process(self, unprocessed_batch: Union[Tuple, List, Mapping], split: str) -> Iterable[ImageSample]:
         images, labels = self.dataset_adapter.extract(unprocessed_batch)
         images, labels = self.batch_formatter.format(images, labels)
         for sample in self.batch_preprocessor.preprocess(images, labels, split):
             yield sample
-
-    @property
-    def images_route(self) -> List[str]:
-        return self.dataset_adapter.images_route
-
-    @property
-    def labels_route(self) -> List[str]:
-        return self.dataset_adapter.labels_route
```

## data_gradients/batch_processors/detection.py

```diff
@@ -1,26 +1,25 @@
-from typing import Optional, Callable, List
+from typing import List
 
 from data_gradients.batch_processors.base import BatchProcessor
 from data_gradients.batch_processors.adapters.dataset_adapter import DatasetAdapter
 from data_gradients.batch_processors.formatters.detection import DetectionBatchFormatter
 from data_gradients.batch_processors.preprocessors.detection import DetectionBatchPreprocessor
+from data_gradients.config.data.data_config import DetectionDataConfig
 
 
 class DetectionBatchProcessor(BatchProcessor):
     def __init__(
         self,
         *,
+        data_config: DetectionDataConfig,
         class_names: List[str],
         class_names_to_use: List[str],
         n_image_channels: int = 3,
-        images_extractor: Optional[Callable] = None,
-        labels_extractor: Optional[Callable] = None,
     ):
-        dataset_adapter = DatasetAdapter(
-            images_extractor=images_extractor,
-            labels_extractor=labels_extractor,
+        dataset_adapter = DatasetAdapter(data_config=data_config)
+        formatter = DetectionBatchFormatter(
+            data_config=data_config, class_names=class_names, class_names_to_use=class_names_to_use, n_image_channels=n_image_channels
         )
-        formatter = DetectionBatchFormatter(class_names=class_names, class_names_to_use=class_names_to_use, n_image_channels=n_image_channels)
         preprocessor = DetectionBatchPreprocessor(class_names=class_names)
 
         super().__init__(dataset_adapter=dataset_adapter, batch_formatter=formatter, batch_preprocessor=preprocessor)
```

## data_gradients/batch_processors/segmentation.py

```diff
@@ -1,31 +1,28 @@
-from typing import Optional, Callable, List
+from typing import List
 
 from data_gradients.batch_processors.base import BatchProcessor
 from data_gradients.batch_processors.adapters.dataset_adapter import DatasetAdapter
 from data_gradients.batch_processors.preprocessors.segmentation import SegmentationBatchPreprocessor
 from data_gradients.batch_processors.formatters.segmentation import SegmentationBatchFormatter
+from data_gradients.config.data.data_config import SegmentationDataConfig
 
 
 class SegmentationBatchProcessor(BatchProcessor):
     def __init__(
         self,
         *,
+        data_config: SegmentationDataConfig,
         class_names: List[str],
         class_names_to_use: List[str],
         n_image_channels: int = 3,
         threshold_value: float = 0.5,
-        images_extractor: Optional[Callable] = None,
-        labels_extractor: Optional[Callable] = None,
     ):
 
-        dataset_adapter = DatasetAdapter(
-            images_extractor=images_extractor,
-            labels_extractor=labels_extractor,
-        )
+        dataset_adapter = DatasetAdapter(data_config=data_config)
         formatter = SegmentationBatchFormatter(
             class_names=class_names,
             class_names_to_use=class_names_to_use,
             n_image_channels=n_image_channels,
             threshold_value=threshold_value,
         )
         preprocessor = SegmentationBatchPreprocessor(class_names=class_names)
```

## data_gradients/batch_processors/adapters/dataset_adapter.py

```diff
@@ -1,68 +1,92 @@
-from typing import Optional, Callable, Union, Tuple, List, Mapping, Any
+from typing import Callable, Union, Tuple, List, Mapping
 
 import PIL
 import numpy as np
 import torch
 from torchvision.transforms import transforms
 
-from data_gradients.batch_processors.adapters.tensor_extractor import TensorExtractor
+from data_gradients.batch_processors.adapters.tensor_extractor import get_tensor_extractor_options
+from data_gradients.config.data.data_config import DataConfig
+from data_gradients.config.data.questions import Question, text_to_yellow
+
+SupportedData = Union[Tuple, List, Mapping, Tuple, List]
 
 
 class DatasetAdapter:
     """Class responsible to convert raw batch (coming from dataloader) into a batch of image and a batch of labels."""
 
-    def __init__(self, images_extractor: Optional[Callable] = None, labels_extractor: Optional[Callable] = None):
-        """
-        :param images_extractor:    (Optional) function that takes the dataloader output and extract the images.
-                                    If None, the user will need to input it manually in a following prompt.
-        :param labels_extractor:    (Optional) function that takes the dataloader output and extract the labels.
-                                    If None, the user will need to input it manually in a following prompt.
-        """
-        self._tensor_extractor = {0: images_extractor, 1: labels_extractor}
+    def __init__(self, data_config: DataConfig):
+        self.data_config = data_config
 
-    def extract(self, objs: Union[Tuple, List, Mapping]) -> Tuple[torch.Tensor, torch.Tensor]:
+    def extract(self, data: SupportedData) -> Tuple[torch.Tensor, torch.Tensor]:
         """Convert raw batch (coming from dataloader) into a batch of image and a batch of labels.
 
-        :param objs: Raw batch (coming from dataloader without any modification).
+        :param data: Raw batch (coming from dataloader without any modification).
         :return:
             - images: Batch of images
             - labels: Batch of labels
         """
-        if isinstance(objs, (Tuple, List)) and len(objs) == 2:
-            images = objs[0] if isinstance(objs[0], torch.Tensor) else self._to_tensor(objs[0], tuple_idx=0)
-            labels = objs[1] if isinstance(objs[1], torch.Tensor) else self._to_tensor(objs[1], tuple_idx=1)
-        elif isinstance(objs, (Mapping, Tuple, List)):
-            images = self._extract_tensor_from_container(objs, 0)
-            labels = self._extract_tensor_from_container(objs, 1)
+        images = self._extract_images(data)
+        labels = self._extract_labels(data)
+        return self._to_torch(images), self._to_torch(labels)
+
+    def _extract_images(self, data: SupportedData) -> torch.Tensor:
+        images_extractor = self._get_images_extractor(data)
+        return images_extractor(data)
+
+    def _extract_labels(self, data: SupportedData) -> torch.Tensor:
+        labels_extractor = self._get_labels_extractor(data)
+        return labels_extractor(data)
+
+    def _get_images_extractor(self, data: SupportedData) -> Callable[[SupportedData], torch.Tensor]:
+        if self.data_config.images_extractor is not None:
+            return self.data_config.get_images_extractor()
+
+        # We use the heuristic that a tuple of 2 should represent (image, label) in this order
+        if isinstance(data, (Tuple, List)) and len(data) == 2:
+            if isinstance(data[0], (torch.Tensor, np.ndarray, PIL.Image.Image)):
+                self.data_config.images_extractor = "[0]"  # We save it for later use
+                return self.data_config.get_images_extractor()  # This will return a callable
+
+        # Otherwise, we ask the user how to map data -> image
+        if isinstance(data, (Tuple, List, Mapping, Tuple, List)):
+            description, options = get_tensor_extractor_options(data)
+            question = Question(question=f"Which tensor represents your {text_to_yellow('Image(s)')} ?", options=options)
+            return self.data_config.get_images_extractor(question=question, hint=description)
+
+        raise NotImplementedError(
+            f"Got object {type(data)} from Data Iterator which is not supported!\n"
+            f"Please implement a custom `images_extractor` for your dataset. "
+            f"You can find more detail about this in our documentation: https://github.com/Deci-AI/data-gradients"
+        )
+
+    def _get_labels_extractor(self, data: SupportedData) -> Callable[[SupportedData], torch.Tensor]:
+        if self.data_config.labels_extractor is not None:
+            return self.data_config.get_labels_extractor()
+
+        # We use the heuristic that a tuple of 2 should represent (image, label) in this order
+        if isinstance(data, (Tuple, List)) and len(data) == 2:
+            if isinstance(data[1], (torch.Tensor, np.ndarray, PIL.Image.Image)):
+                self.data_config.labels_extractor = "[1]"  # We save it for later use
+                return self.data_config.get_labels_extractor()  # This will return a callable
+
+        # Otherwise, we ask the user how to map data -> labels
+        if isinstance(data, (Tuple, List, Mapping, Tuple, List)):
+            description, options = get_tensor_extractor_options(data)
+            question = Question(question=f"Which tensor represents your {text_to_yellow('Label(s)')} ?", options=options)
+            return self.data_config.get_labels_extractor(question=question, hint=description)
+
+        raise NotImplementedError(
+            f"Got object {type(data)} from Data Iterator which is not supported!\n"
+            f"Please implement a custom `labels_extractor` for your dataset. "
+            f"You can find more detail about this in our documentation: https://github.com/Deci-AI/data-gradients"
+        )
+
+    @staticmethod
+    def _to_torch(tensor: Union[np.ndarray, PIL.Image.Image, torch.Tensor]) -> torch.Tensor:
+        if isinstance(tensor, np.ndarray):
+            return torch.from_numpy(tensor)
+        elif isinstance(tensor, PIL.Image.Image):
+            return transforms.ToTensor()(tensor)
         else:
-            raise NotImplementedError(f"Got object {type(objs)} from Iterator - supporting dict, tuples and lists Only!")
-        return images, labels
-
-    def _to_tensor(self, objs: Union[np.ndarray, PIL.Image.Image, Mapping], tuple_idx: int) -> torch.Tensor:
-        if isinstance(objs, np.ndarray):
-            return torch.from_numpy(objs)
-        elif isinstance(objs, PIL.Image.Image):
-            return transforms.ToTensor()(objs)
-        else:
-            return self._extract_tensor_from_container(objs=objs, tuple_idx=tuple_idx)
-
-    def _extract_tensor_from_container(self, objs: Any, tuple_idx: int) -> torch.Tensor:
-        mapping_fn = self._get_tensor_extractor(tuple_idx=tuple_idx, objs=objs)
-        return mapping_fn(objs)
-
-    def _get_tensor_extractor(self, objs: Any, tuple_idx: int) -> Union[Callable, TensorExtractor]:
-        if self._tensor_extractor[tuple_idx] is None:
-            self._tensor_extractor[tuple_idx] = TensorExtractor(objs=objs, name="image(s)" if (tuple_idx == 0) else "label(s)")
-        return self._tensor_extractor[tuple_idx]
-
-    @property
-    def images_route(self) -> List[str]:
-        """Represent the path (route) to extract the images from the raw batch (coming from dataloader)."""
-        tensor_finder = self._tensor_extractor[0]
-        return tensor_finder.path_to_tensor if isinstance(tensor_finder, TensorExtractor) else []
-
-    @property
-    def labels_route(self) -> List[str]:
-        """Represent the path (route) to extract the labels from the raw batch (coming from dataloader)."""
-        tensor_finder = self._tensor_extractor[1]
-        return tensor_finder.path_to_tensor if isinstance(tensor_finder, TensorExtractor) else []
+            return tensor
```

## data_gradients/batch_processors/adapters/tensor_extractor.py

```diff
@@ -1,141 +1,131 @@
-from typing import Mapping, Optional, Any, List, Tuple, Sequence, Union
-import json
+from typing import Mapping, Dict, Any, List, Tuple, Sequence, Union
 import re
+import json
 
 from PIL import Image
 import torch
 from numpy import ndarray
-from torch import Tensor
-from data_gradients.utils.utils import ask_user
 
 
-class TensorExtractor:
-    """Extract the tensor of interest (could be image, label, ..) out of a batch raw output (coming from dataloader output).
-    This is done by asking the user what field is the relevant one.
+def get_tensor_extractor_options(objs: Any) -> Tuple[str, Dict[str, str]]:
+    """Extract out of objs all the potential fields of type [torch.Tensor, np.ndarray, PIL.Image], and then
+    asks the user to input which of the above keys mapping is the right one in order to retrieve the correct data (either images or labels).
+
+    :param objs: Dictionary following the pattern: {"path.to.object: object_type": "path.to.object"}
     """
+    objects_mapping: List[Tuple[str, str]] = []  # Placeholder for list of (path.to.object, object_type)
+    nested_object_mapping = extract_object_mapping(objs, current_path="", objects_mapping=objects_mapping)
+    description = "This is how your data is structured: \n"
+    description += f"data = {json.dumps(nested_object_mapping, indent=4)}"
+
+    options = {f"data{path_to_object}: {object_type}": path_to_object for path_to_object, object_type in objects_mapping}
+    return description, options
+
+
+def extract_object_mapping(current_object: Any, current_path: str, objects_mapping: List[Tuple[str, str]]) -> Any:
+    """Recursive function for "digging" into the mapping object it received and save a "path" to the target.
+    Target is defined as one of [torch.Tensor, np.ndarray, PIL.Image]. If got Mapping / Sequence -> continue recursion.
+
+    :param current_object:  Recursively returned object
+    :param current_path:    Current path - not achieved a target yet
+    :param objects_mapping: List of tuples (path.to.object, object_type)
+    """
+    if isinstance(current_object, Mapping):
+        printable_map = {}
+        for k, v in current_object.items():
+            new_path = f"{current_path}.{k}" if current_path else f".{k}"
+            printable_map[k] = extract_object_mapping(v, new_path, objects_mapping)
+    elif isinstance(current_object, Sequence) and not isinstance(current_object, str):
+        if all(isinstance(v, (int, float)) for v in current_object):
+            printable_map = "List[float|int]"
+            objects_mapping.append((current_path, printable_map))
+        elif all(isinstance(v, str) for v in current_object):
+            printable_map = "List[str]"
+            objects_mapping.append((current_path, printable_map))
+        else:
+            printable_map = []
+            for i, v in enumerate(current_object):
+                new_path = f"{current_path}[{i}]"
+                printable_map.append(extract_object_mapping(v, new_path, objects_mapping))
+    elif isinstance(current_object, int):
+        printable_map = "int"
+        objects_mapping.append((current_path, printable_map))
+    elif isinstance(current_object, float):
+        printable_map = "float"
+        objects_mapping.append((current_path, printable_map))
+    elif isinstance(current_object, str):
+        printable_map = "String"
+        objects_mapping.append((current_path, printable_map))
+    elif isinstance(current_object, torch.Tensor):
+        printable_map = "Tensor"
+        objects_mapping.append((current_path, printable_map))
+    elif isinstance(current_object, ndarray):
+        printable_map = "ndarray"
+        objects_mapping.append((current_path, printable_map))
+    elif isinstance(current_object, Image.Image):
+        printable_map = "PIL Image"
+        objects_mapping.append((current_path, printable_map))
+    else:
+        printable_map = f"Unsupported object: '{current_object.__name__}'"
+        objects_mapping.append((current_path, printable_map))
+    return printable_map
+
+
+class DataLookupError(Exception):
+    def __init__(self, exception: Exception, keys_to_reach_object: List[Union[str, int]]):
+        self.keys_to_reach_object = keys_to_reach_object
+        err_msg = (
+            "\n     => Error happened during tensor mapping between dataset and DataGradients.\n"
+            f'It seems that the key mapping to access to the tensor is incorrect: key_mapping="{self.keys_to_reach_object}".\n'
+            f'Failed with exception: "{exception}"\n\n'
+            f"Possible source of the error:\n"
+            f"      1. You are using the same cache as a previous run that was done with different datasets.\n"
+            f"          -> In that case you should use a different report title (recommended), or alternatively deactivate the cache.\n"
+            f"      2. Your training and validation datasets/dataloaders provide data that is structured differently.\n"
+            "           e.g. train_data returns data={'image': ..., 'labels': ...} while valid_data returns data={'images': ..., 'all_labels': ...}.\n"
+            "           -> This case is not supported by DataGradients, so you need to implement a unique dataset/loader class and use it.\n"
+            f"      3. You passed a non-valid key mapping when defining `images_extractor` or `labels_extractor`.\n"
+            f"          -> Please go over your key mapping and make sure it respects the format defined in the documentation.\n\n"
+        )
+        super().__init__(err_msg)
 
-    def __init__(self, objs: Any, name: str):
-        self.path_to_tensor: Optional[List[str]] = self.prompt_user_for_data_keys(objs=objs, name=name)
 
-    def __call__(self, objs: Any) -> Tensor:
-        return self.traverse_nested_data_structure(data=objs, keys=self.path_to_tensor)
+class NestedDataLookup:
+    """Callable that allows to traverse a data structure according to an input path."""
 
-    @staticmethod
-    def parse_path(path: str) -> List[Union[str, int]]:
-        """Parse the path to an object into a list of indexes.
-
-        >>> parse_path("field1.field12[0]") # parsing path to {"field1": {"field12": [<object>, ...], ...}, ...}
-        ["field1", "field12", 0]  # data["field1"]["field12"][0] = <object>
-
-        :param path: Path to the object as a string
-        """
-        pattern = r"\.|\[(\d+)\]"
-
-        result = re.split(pattern, path)
-        result = [int(x) if x.isdigit() else x for x in result if x and x != "."]
-
-        return result
-
-    @staticmethod
-    def prompt_user_for_data_keys(objs: Any, name: str) -> List[str]:
-        """Extract out of objs all the potential fields of type [torch.Tensor, np.ndarray, PIL.Image], and then
-        asks the user to input which of the above keys mapping is the right one in order to retrieve the correct data (either images or labels).
-
-        :param objs:        Dictionary of json-like structure.
-        :param name:        The type of your targeted field ('image', 'label', ...). This is only for display purpose.
-        :return:            List of keys that if you iterate with the Get Operation (d[k]) through all of them, you will get the data you intended.
-                            e.g. ["field1", "field12", 0]  # objs["field1"]["field12"][0] = <object>
-        """
-
-        paths = []
-        printable_mapping = TensorExtractor.objects_mapping(objs, path="", targets=paths)
-        printable_mapping = json.dumps(printable_mapping, indent=4)
-        printable_mapping = "This is the structure of your data: \ndata = " + printable_mapping
-        main_question = f"Which object maps to your {name} ?"
-
-        options = [f"- {name} = data{k}: {v}" for k, v in paths]
-        selected_option = ask_user(main_question=main_question, options=options, optional_description=printable_mapping)
-
-        start_index = selected_option.find("data") + len("data")
-        end_index = selected_option.find(":", start_index)
-        selected_path = selected_option[start_index:end_index].strip()
-
-        keys = TensorExtractor.parse_path(selected_path)
-        return keys
-
-    @staticmethod
-    def is_valid_json(myjson: str) -> bool:
-        """Check if an object is a JSON serialized object or not.
-
-        :param myjson: any object
-        :return: boolean if myjson is a JSON object
-        """
+    def __init__(self, object_path: str):
+        self.keys_to_reach_object = extract_keys_from_path(object_path=object_path)
+
+    def __call__(self, data: Any) -> Any:
         try:
-            json.loads(myjson)
-        except ValueError:
-            return False
-        else:
-            return True
+            return traverse_nested_data_structure(data=data, keys=self.keys_to_reach_object)
+        except Exception as e:
+            raise DataLookupError(exception=e, keys_to_reach_object=self.keys_to_reach_object) from e
 
-    @staticmethod
-    def objects_mapping(obj: Any, path: str, targets: List[Tuple[str, str]]) -> Any:
-        """Recursive function for "digging" into the mapping object it received and save a "path" to the target.
-        Target is defined as one of [torch.Tensor, np.ndarray, PIL.Image]. If got Mapping / Sequence -> continue recursion.
-
-        :param obj:     Recursively returned object
-        :param path:    Current path - not achieved a target yet
-        :param targets: List of tuples (path.to.object, object_type)
-        """
-        if isinstance(obj, Mapping):
-            printable_map = {}
-            for k, v in obj.items():
-                new_path = f"{path}.{k}" if path else k
-                printable_map[k] = TensorExtractor.objects_mapping(v, new_path, targets)
-        elif isinstance(obj, Sequence) and not isinstance(obj, str):
-            if all(isinstance(v, (int, float)) for v in obj):
-                printable_map = "List[float|int]"
-                targets.append((path, printable_map))
-            elif all(isinstance(v, str) for v in obj):
-                printable_map = "List[str]"
-                targets.append((path, printable_map))
-            else:
-                printable_map = []
-                for i, v in enumerate(obj):
-                    new_path = f"{path}[{i}]"
-                    printable_map.append(TensorExtractor.objects_mapping(v, new_path, targets))
-        elif isinstance(obj, int):
-            printable_map = "int"
-            targets.append((path, printable_map))
-        elif isinstance(obj, float):
-            printable_map = "float"
-            targets.append((path, printable_map))
-        elif isinstance(obj, str):
-            printable_map = "String"
-            targets.append((path, printable_map))
-        elif isinstance(obj, torch.Tensor):
-            printable_map = "Tensor"
-            targets.append((path, printable_map))
-        elif isinstance(obj, ndarray):
-            printable_map = "ndarray"
-            targets.append((path, printable_map))
-        elif isinstance(obj, Image.Image):
-            printable_map = "PIL Image"
-            targets.append((path, printable_map))
-        else:
-            raise RuntimeError(
-                f"Unsupported object! Object found has a type of {type(obj)} which is not supported for now.\n"
-                f"Supported types: [Mapping, Sequence, String, Tensor, Numpy array, PIL Image]"
-            )
-        return printable_map
-
-    @staticmethod
-    def traverse_nested_data_structure(data: Mapping, keys: List[str]) -> Any:
-        """Traverse a nested data structure and returns the value at the specified key path.
-
-        :param data:    Nested data structure like dict, defaultdict or OrderedDict
-        :param keys:    List of strings representing the keys in the data structure
-        :return:        Value at the specified key path in the data structure
-        """
-        for key in keys:
-            data = data[key]
-        return data
+
+def extract_keys_from_path(object_path: str) -> List[Union[str, int]]:
+    """Parse the path to an object into a list of indexes.
+
+    >> extract_keys_from_path("field1.field12[0]") # Which originally represents {"field1": {"field12": [<object>, ...], ...}, ...}
+    ["field1", "field12", 0]  # Can be used like this: data["field1"]["field12"][0] = <object>
+
+    :param object_path: Path to the object as a string
+    """
+    pattern = r"\.|\[(\d+)\]"
+
+    result = re.split(pattern, object_path)
+    result = [int(x) if x.isdigit() else x for x in result if x and x != "."]
+
+    return result
+
+
+def traverse_nested_data_structure(data: Union[List, Mapping], keys: List[str]) -> Any:
+    """Traverse a nested data structure and returns the value at the specified key path.
+
+    :param data:    Nested data structure like dict, defaultdict or OrderedDict
+    :param keys:    List of strings representing the keys in the data structure
+    :return:        Value at the specified key path in the data structure
+    """
+    for key in keys:
+        data = data[key]
+    return data
```

## data_gradients/batch_processors/formatters/detection.py

```diff
@@ -2,44 +2,48 @@
 
 import torch
 from torch import Tensor
 
 from data_gradients.batch_processors.utils import check_all_integers
 from data_gradients.batch_processors.formatters.base import BatchFormatter
 from data_gradients.batch_processors.formatters.utils import ensure_images_shape, ensure_channel_first, drop_nan
-from data_gradients.utils.utils import ask_user
+from data_gradients.config.data.data_config import DetectionDataConfig
+from data_gradients.batch_processors.formatters.utils import DatasetFormatError
 
 
-class UnsupportedDetectionBatchFormatError(Exception):
+class UnsupportedDetectionBatchFormatError(DatasetFormatError):
     def __init__(self, batch_format: tuple):
         grouped_batch_format = "(Batch_size x padding_size x 5) with 5: (class_id + 4 bbox coordinates))"
         flat_batch_format = "(N, 6) with 6: (image_id + class_id + 4 bbox coordinates)"
         super().__init__(
             f"Supported format for detection is not supported. Supported formats are:\n- {grouped_batch_format}\n- {flat_batch_format}\n Got: {batch_format}"
         )
 
 
 class DetectionBatchFormatter(BatchFormatter):
     """Detection formatter class"""
 
     def __init__(
         self,
+        data_config: DetectionDataConfig,
         class_names: List[str],
         class_names_to_use: List[str],
         n_image_channels: int,
         xyxy_converter: Optional[Callable[[Tensor], Tensor]] = None,
         label_first: Optional[bool] = None,
     ):
         """
         :param class_names:         List of all class names in the dataset. The index should represent the class_id.
         :param class_names_to_use:  List of class names that we should use for analysis.
         :param n_image_channels:    Number of image channels (3 for RGB, 1 for Gray Scale, ...)
         :param xyxy_converter:      Function to convert the bboxes to the `xyxy` format.
         :param label_first:         Whether the annotated_bboxes states with labels, or with the bboxes. (typically label_xyxy vs xyxy_label)
         """
+        self.data_config = data_config
+
         class_names_to_use = set(class_names_to_use)
         self.class_ids_to_use = [class_id for class_id, class_name in enumerate(class_names) if class_name in class_names_to_use]
 
         self.n_image_channels = n_image_channels
         self.xyxy_converter = xyxy_converter
         self.label_first = label_first
 
@@ -60,22 +64,17 @@
 
         labels = drop_nan(labels)
 
         images = ensure_channel_first(images, n_image_channels=self.n_image_channels)
         images = ensure_images_shape(images, n_image_channels=self.n_image_channels)
         labels = self.ensure_labels_shape(annotated_bboxes=labels)
 
-        if self.label_first is None or self.xyxy_converter is None:
-            self.show_annotated_bboxes(annotated_bboxes=labels)
-
-        if self.label_first is None:
-            self.label_first = self.ask_user_is_label_first()
-
-        if self.xyxy_converter is None:
-            self.xyxy_converter = self.ask_user_xyxy_converter()
+        targets_sample_str = f"Here's a sample of how your labels look like:\nEach line corresponds to a bounding box.\n{labels[0, :4, :]}"
+        self.label_first = self.data_config.get_is_label_first(hint=targets_sample_str)
+        self.xyxy_converter = self.data_config.get_xyxy_converter(hint=targets_sample_str)
 
         if 0 <= images.min() and images.max() <= 1:
             images *= 255
             images = images.to(torch.uint8)
 
         labels = self.convert_to_label_xyxy(
             annotated_bboxes=labels,
@@ -121,84 +120,20 @@
         if not check_all_integers(labels):
             raise RuntimeError(f"Labels should all be integers, but got {labels}")
 
         xyxy_bboxes = xyxy_converter(bboxes)
 
         if xyxy_bboxes.max().item() < 2:
             h, w = image_shape
-            bboxes[..., 0::2] *= w
-            bboxes[..., 1::2] *= h
+            xyxy_bboxes[..., 0::2] *= w
+            xyxy_bboxes[..., 1::2] *= h
 
         return torch.cat([labels, xyxy_bboxes], dim=-1)
 
     @staticmethod
-    def show_annotated_bboxes(annotated_bboxes: Tensor) -> None:
-        """Show an example of the annotated bounding boxes."""
-        print("\n========================================================================")
-        print("SAMPLE BOUNDING BOXES")
-        print("========================================================================")
-        print("Here's a sample of how your labels look like:")
-        print("Each line corresponds to a bounding box, with the format you specified earlier.")
-        print(annotated_bboxes[0, :3, :])
-        print("")
-
-    @staticmethod
-    def ask_user_is_label_first() -> bool:
-        is_label_first_descriptions = {
-            "Label comes first (e.g. [class_id, x1, y1, x2, y2])": True,
-            "Bounding box comes first (e.g. [x1, y1, x2, y2, class_id])": False,
-        }
-        selected_option = ask_user(
-            main_question="Which comes first in your annotations, the class id or the bounding box?",
-            options=list(is_label_first_descriptions.keys()),
-        )
-        return is_label_first_descriptions[selected_option]
-
-    @staticmethod
-    def ask_user_xyxy_converter() -> Callable[[Tensor], Tensor]:
-        xyxy_converter_descriptions = {
-            "xyxy: x- left, y-top, x-right, y-bottom": lambda x: x,
-            "xywh: x-left, y-top, width, height": DetectionBatchFormatter.xywh_to_xyxy,
-            "cxcywh: x-center, y-center, width, height": DetectionBatchFormatter.cxcywh_to_xyxy,
-        }
-        selected_option = ask_user(
-            main_question="What is the format of the bounding boxes?",
-            options=list(xyxy_converter_descriptions.keys()),
-        )
-        return xyxy_converter_descriptions[selected_option]
-
-    @staticmethod
-    def cxcywh_to_xyxy(bboxes: Tensor) -> Tensor:
-        """Transform bboxes from CX-CY-W-H format to XYXY format.
-
-        :param bboxes:  BBoxes of shape (..., 4) in CX-CY-W-H format
-        :return:        BBoxes of shape (..., 4) in XYXY format
-        """
-        cx, cy, w, h = bboxes[..., 0], bboxes[..., 1], bboxes[..., 2], bboxes[..., 3]
-        x1 = cx - 0.5 * w
-        y1 = cy - 0.5 * h
-        x2 = x1 + w
-        y2 = y1 + h
-
-        return torch.stack([x1, y1, x2, y2], dim=-1)
-
-    @staticmethod
-    def xywh_to_xyxy(bboxes: Tensor) -> Tensor:
-        """Transform bboxes from XYWH format to XYXY format.
-
-        :param bboxes:  BBoxes of shape (..., 4) in XYWH format
-        :return:        BBoxes of shape (..., 4) in XYXY format
-        """
-        x1, y1, w, h = bboxes[..., 0], bboxes[..., 1], bboxes[..., 2], bboxes[..., 3]
-        x2 = x1 + w
-        y2 = y1 + h
-
-        return torch.stack([x1, y1, x2, y2], dim=-1)
-
-    @staticmethod
     def filter_non_relevant_annotations(bboxes: torch.Tensor, class_ids_to_use: List[int]) -> List[torch.Tensor]:
         """Filter the bounding box tensors to keep only the ones with relevant label; also removes padding.
 
         :param bboxes:              Bounding box tensors with shape [batch_size, padding_size, 5], where 5 represents (label, x, y, x, y).
         :param class_ids_to_use:    List of class ids to keep use.
         :return: List of filtered bounding box tensors, each of shape [n_bbox, 5],
                  where n_bbox is the number of bounding boxes with a label in the `valid_labels` list.
```

## data_gradients/batch_processors/formatters/segmentation.py

```diff
@@ -1,16 +1,16 @@
 from typing import Optional, List, Tuple
 
 import torch
 from torch import Tensor
 
-from data_gradients.utils.utils import ask_user
+from data_gradients.config.data.questions import ask_user
 from data_gradients.batch_processors.formatters.base import BatchFormatter
 from data_gradients.batch_processors.utils import check_all_integers, to_one_hot
-from data_gradients.batch_processors.formatters.utils import ensure_images_shape, ensure_channel_first, drop_nan
+from data_gradients.batch_processors.formatters.utils import DatasetFormatError, ensure_images_shape, ensure_channel_first, drop_nan
 
 
 class SegmentationBatchFormatter(BatchFormatter):
     """
     Segmentation formatter class
     """
 
@@ -102,15 +102,15 @@
 
         if check_all_integers(unique_values):
             return labels
         elif 0 <= min(unique_values) and max(unique_values) <= 1 and check_all_integers(unique_values * 255):
             return labels * 255
         else:
             if n_classes > 1:
-                raise NotImplementedError(f"Not supporting soft-labeling for number of classes > 1!\nGot {n_classes} classes.")
+                raise DatasetFormatError(f"Not supporting soft-labeling for number of classes > 1!\nGot {n_classes} classes.")
             labels = SegmentationBatchFormatter.binary_mask_above_threshold(labels=labels, threshold_value=threshold_value)
         return labels
 
     @staticmethod
     def is_soft_labels(labels: Tensor) -> bool:
         unique_values = torch.unique(labels)
         if check_all_integers(unique_values):
@@ -136,21 +136,21 @@
             labels = labels.unsqueeze(1)  # Probably (B, H, W)
             return labels
         elif labels.dim() == 4:
             total_n_classes = n_classes + len(ignore_labels)
             valid_n_classes = (total_n_classes, 1)
             input_n_classes = labels.shape[1]
             if input_n_classes not in valid_n_classes and labels.shape[-1] not in valid_n_classes:
-                raise ValueError(
+                raise DatasetFormatError(
                     f"Labels batch shape should be [BS, N, W, H] where N is either 1 or n_classes + len(ignore_labels)"
                     f" ({total_n_classes}). Got: {input_n_classes}"
                 )
             return labels
         else:
-            raise ValueError(f"Labels batch shape should be [BatchSize x Channels x Width x Height]. Got {labels.shape}")
+            raise DatasetFormatError(f"Labels batch shape should be [BatchSize x Channels x Width x Height]. Got {labels.shape}")
 
     @staticmethod
     def binary_mask_above_threshold(labels: Tensor, threshold_value: float) -> Tensor:
         # Support only for binary segmentation
         labels = torch.where(
             labels > threshold_value,
             torch.ones_like(labels),
```

## data_gradients/batch_processors/formatters/utils.py

```diff
@@ -1,13 +1,17 @@
 import torch
 from torch import Tensor
 
 from data_gradients.batch_processors.utils import channels_last_to_first
 
 
+class DatasetFormatError(Exception):
+    ...
+
+
 def drop_nan(tensor: Tensor) -> Tensor:
     """Remove rows containing NaN values from a given PyTorch tensor.
 
     :param tensor:  Tensor with shape (N, M) where N is the number of rows and M is the number of columns.
     :return:        Tensor with the same number of columns as the input tensor, but without rows containing NaN.
     """
     nans = torch.isnan(tensor)
```

## data_gradients/config/detection.yaml

```diff
@@ -4,21 +4,22 @@
       - SummaryStats
       - ImagesResolution
       - ImageColorDistribution
       - ImagesAverageBrightness
   - name: Object Detection Features
     features:
       - DetectionSampleVisualization:
-          n_rows: 6
-          n_cols: 2
+          n_rows: 3
+          n_cols: 4
           stack_splits_vertically: True
       - DetectionClassHeatmap:
           n_rows: 6
           n_cols: 2
           heatmap_shape: [200, 200]
       - DetectionBoundingBoxArea
       - DetectionBoundingBoxPerImageCount
       - DetectionBoundingBoxSize
       - DetectionClassFrequency
       - DetectionClassesPerImageCount
       - DetectionBoundingBoxIoU:
           num_bins: 10
+          class_agnostic: true
```

## data_gradients/feature_extractors/__init__.py

```diff
@@ -1,13 +1,9 @@
 from .abstract_feature_extractor import AbstractFeatureExtractor
-from .common import (
-    ImagesAverageBrightness,
-    ImageColorDistribution,
-    ImagesResolution,
-)
+from .common import ImagesAverageBrightness, ImageColorDistribution, ImagesResolution, SummaryStats
 from .segmentation import (
     SegmentationBoundingBoxArea,
     SegmentationBoundingBoxResolution,
     SegmentationClassFrequency,
     SegmentationClassHeatmap,
     SegmentationClassesPerImageCount,
     SegmentationComponentsConvexity,
@@ -27,14 +23,15 @@
 )
 
 __all__ = [
     "AbstractFeatureExtractor",
     "ImagesAverageBrightness",
     "ImageColorDistribution",
     "ImagesResolution",
+    "SummaryStats",
     "SegmentationBoundingBoxArea",
     "SegmentationBoundingBoxResolution",
     "SegmentationClassFrequency",
     "SegmentationClassHeatmap",
     "SegmentationClassesPerImageCount",
     "SegmentationComponentsConvexity",
     "SegmentationComponentsErosion",
```

## data_gradients/feature_extractors/common/image_average_brightness.py

```diff
@@ -61,16 +61,16 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Average Brightness of Images."
+        return "Image Brightness Distribution"
 
     @property
     def description(self) -> str:
         return (
-            "Distribution of the average 'lightness' of images (as L channel pixel value distribution in CIELAB color). \n"
-            "Image brightness distribution can reveal differences between the train and validation set. I.e. if "
-            "the train set contains only day images while the validation set contains night images. "
+            "This graph shows the distribution of the image brightness of each dataset. \n"
+            "This may for instance uncover differences between the training and validation sets, "
+            "such as the presence of exclusively daytime images in the training set and nighttime images in the validation set."
         )
```

## data_gradients/feature_extractors/common/image_color_distribution.py

```diff
@@ -87,17 +87,16 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Distribution of Colors"
+        return "Color Distribution"
 
     @property
     def description(self) -> str:
         return (
-            "Distribution of RBG or Grayscale intensity (0-255) over the whole dataset."
-            "Assumes RGB Channel ordering: \n"
-            "Can reveal differences in the nature of the images in the two datasets or in the augmentation. I.e., if the mean "
-            "of one of the colors is shifted between the datasets, it might indicate wrong augmentation. "
+            "Here's a comparison of RGB or grayscale intensity intensity (0-255) distributions across the entire dataset, assuming RGB channel ordering. \n"
+            "It can reveal discrepancies in the image characteristics between the two datasets, as well as potential flaws in the augmentation process. \n"
+            "E.g., a notable difference in the mean value of a specific color between the two datasets may indicate an issue with augmentation."
         )
```

## data_gradients/feature_extractors/common/image_resolution.py

```diff
@@ -72,15 +72,16 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Image height and width distribution"
+        return "Image Width and Height Distribution"
 
     @property
     def description(self) -> str:
         return (
-            "The distribution of the image resolutions as a discrete histogram. \n Note that if images are "
-            "rescaled or padded, this plot will show the size after rescaling and padding. "
+            "These histograms depict the distributions of image height and width. "
+            "It's important to note that if certain images have been rescaled or padded, the histograms will represent the size after "
+            "the rescaling and padding operations."
         )
```

## data_gradients/feature_extractors/common/sample_visualization.py

```diff
@@ -63,11 +63,17 @@
     @property
     def title(self) -> str:
         return "Visualization of Samples"
 
     @property
     def description(self) -> str:
         return (
-            f"Visualization of {self.n_rows * self.n_cols} samples per split. "
-            f"This can be useful to make sure the mapping of class_names to class_ids is done correctly, "
-            f"but also to get a better understanding of what your dataset is made of.."
+            "The sample visualization feature provides a visual representation of images and labels. "
+            "This visualization aids in understanding of the composition of the dataset."
+        )
+
+    @property
+    def notice(self) -> str:
+        return (
+            f"Only {self.n_cols * self.n_rows} random samples are shown.<br/>"
+            f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
         )
```

## data_gradients/feature_extractors/object_detection/__init__.py

```diff
@@ -1,13 +1,13 @@
 from .bounding_boxes_area import DetectionBoundingBoxArea
 from .bounding_boxes_per_image_count import DetectionBoundingBoxPerImageCount
 from .bounding_boxes_resolution import DetectionBoundingBoxSize
-from .classes_count import DetectionClassFrequency
+from .classes_frequency import DetectionClassFrequency
 from .classes_heatmap_per_class import DetectionClassHeatmap
-from .classes_per_image_count import DetectionClassesPerImageCount
+from .classes_frequency_per_image import DetectionClassesPerImageCount
 from .sample_visualization import DetectionSampleVisualization
 from .bounding_boxes_iou import DetectionBoundingBoxIoU
 
 __all__ = [
     "DetectionBoundingBoxArea",
     "DetectionBoundingBoxPerImageCount",
     "DetectionBoundingBoxSize",
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_area.py

```diff
@@ -27,23 +27,25 @@
                     "relative_bbox_area": 100 * (bbox_area / image_area),
                 }
             )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
+        max_area = min(100, df["relative_bbox_area"].max())
         plot_options = ViolinPlotOptions(
             x_label_key="relative_bbox_area",
             x_label_name="Bounding Box Area (in % of image)",
             y_label_key="class_name",
             y_label_name="Class",
             order_key="class_id",
             title=self.title,
             x_ticks_rotation=None,
             labels_key="split",
+            x_lim=(0, max_area),
             bandwidth=0.4,
         )
 
         json = dict(
             train=dict(df[df["split"] == "train"]["relative_bbox_area"].describe()), val=dict(df[df["split"] == "val"]["relative_bbox_area"].describe())
         )
 
@@ -52,16 +54,17 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Distribution of Bounding Boxes Area per Class."
+        return "Distribution of Bounding Box Area"
 
     @property
     def description(self) -> str:
         return (
-            "The distribution of the areas of the boxes of the different classes.\n"
-            "The size of the objects can significantly affect the performance of your model. "
-            "If certain classes tend to have smaller objects, the model might struggle to segment them, especially if the resolution of the images is low "
+            "This graph shows the distribution of bounding box area for each class. "
+            "This can highlight distribution gap in object size between the training and validation splits, which can harm the model performance. \n"
+            "Another thing to keep in mind is that having too many very small objects may indicate that your are down sizing your original image to a "
+            "low resolution that is not appropriate for your objects."
         )
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py

```diff
@@ -13,15 +13,15 @@
 
 
 @register_feature_extractor()
 class DetectionBoundingBoxIoU(AbstractFeatureExtractor):
     """Feature Extractor to compute the pairwise IoU of bounding boxes per image.
     This feature extractor helps to identify duplicate/highly overlapping bounding boxes."""
 
-    def __init__(self, num_bins: int, class_agnostic: bool = False):
+    def __init__(self, num_bins: int = 10, class_agnostic: bool = True):
         """
         :param num_bins: Number of bins to use for the heatmap plot.
         :param class_agnostic: If True, only check IoU of bounding boxes of the same class.
         """
         self.data = []
         self.num_bins = num_bins
         self.class_agnostic = class_agnostic
@@ -117,21 +117,21 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Intersection of bounding boxes"
+        return "Intersection of Bounding Boxes"
 
     @property
     def description(self) -> str:
         description = (
-            "The distribution of the box IoU with respect to other boxes in the sample. "
-            "The heatmap shows the percentage of boxes with IoU in range [0..T] for each class. "
+            "The distribution of the box Intersection over Union (IoU) with respect to other boxes in the sample. "
+            "The heatmap shows the percentage of boxes overlap with IoU in range [0..T] for each class. "
         )
         if self.class_agnostic:
             description += "Intersection of all boxes are considered (Regardless of classes of corresponding bboxes)."
         else:
             description += "Only intersection of boxes of same class are considered."
         return description
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py

```diff
@@ -33,14 +33,15 @@
         plot_options = Hist2DPlotOptions(
             x_label_key="n_components",
             x_label_name="Number of bounding box per Image",
             title=self.title,
             kde=False,
             labels_key="split",
             individual_plots_key="split",
+            stat="percent",
             x_ticks_rotation=None,
             sharey=True,
             labels_palette=LABELS_PALETTE,
         )
 
         json = dict(
             train=dict(df_class_count[df_class_count["split"] == "train"]["n_components"].describe()),
@@ -48,12 +49,16 @@
         )
 
         feature = Feature(data=df_class_count, plot_options=plot_options, json=json)
         return feature
 
     @property
     def title(self) -> str:
-        return "Number of bounding box per image."
+        return "Distribution of Bounding Box per image"
 
     @property
     def description(self) -> str:
-        return "The total number of bounding box per image. This helps understanding how many bounding boxes an image typically includes."
+        return (
+            "These graphs shows how many bounding boxes appear in images. \n"
+            "This can typically be valuable to know when you observe a very high number of bounding boxes per image, "
+            "as some models include a parameter to filter the top k results."
+        )
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py

```diff
@@ -39,15 +39,15 @@
             y_label_name="Height (in % of image)",
             title=self.title,
             x_lim=(0, 100),
             y_lim=(0, 100),
             x_ticks_rotation=None,
             labels_key="split",
             individual_plots_key="split",
-            tight_layout=True,
+            tight_layout=False,
             sharey=True,
             labels_palette=LABELS_PALETTE,
         )
 
         train_description = df[df["split"] == "train"].describe()
         train_json = {"relative_width": dict(train_description["relative_width"]), "relative_height": dict(train_description["relative_height"])}
 
@@ -60,15 +60,15 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Distribution of Bounding Boxes Height and Width."
+        return "Distribution of Bounding Box Width and Height"
 
     @property
     def description(self) -> str:
         return (
-            "Width, Height of the bounding-boxes surrounding every object across all images. Plotted per-class on a heat-map.\n"
-            "A large variation in object sizes within a class can make it harder for the model to recognize the objects."
+            "These heat maps illustrate the distribution of bounding box width and height per class. \n"
+            "Large variations in object size can affect the model's ability to accurately recognize objects."
         )
```

## data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py

```diff
@@ -30,16 +30,24 @@
             x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
             split_heatmap[class_id, y1:y2, x1:x2] += 1
 
         self.heatmaps_per_split[sample.split] = split_heatmap
 
     @property
     def title(self) -> str:
-        return "Heatmap of Bounding Boxes"
+        return "Bounding Boxes Density"
 
     @property
     def description(self) -> str:
         return (
-            "Show the areas of high density of Bounding Boxes. This can be useful to understand if the objects are positioned in the right area.\n"
-            f"Note that only top {self.n_cols * self.n_rows} classes are shown. "
-            f" You can increase the number of classes by setting `DetectionClassHeatmap` with `n_classes_to_show`"
+            "The heatmap represents areas of high object density within the images, providing insights into the spatial distribution of objects. "
+            "By examining the heatmap, you can quickly identify if objects are predominantly concentrated in specific regions or if they are evenly "
+            "distributed throughout the scene. This information can serve as a heuristic to assess if the objects are positioned appropriately "
+            "within the expected areas of interest."
+        )
+
+    @property
+    def notice(self) -> str:
+        return (
+            f"Only the {self.n_cols * self.n_rows} classes with highest density are shown.<br/>"
+            f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
         )
```

## data_gradients/feature_extractors/segmentation/__init__.py

```diff
@@ -1,15 +1,15 @@
 from .bounding_boxes_area import SegmentationBoundingBoxArea
 from .bounding_boxes_resolution import SegmentationBoundingBoxResolution
-from .classes_count import SegmentationClassFrequency
+from .classes_frequency import SegmentationClassFrequency
 from .classes_heatmap_per_class import SegmentationClassHeatmap
-from .classes_per_image_count import SegmentationClassesPerImageCount
+from .classes_frequency_per_image import SegmentationClassesPerImageCount
 from .components_convexity import SegmentationComponentsConvexity
 from .components_erosion import SegmentationComponentsErosion
-from .components_per_image_count import SegmentationComponentsPerImageCount
+from .component_frequency_per_image import SegmentationComponentsPerImageCount
 from .sample_visualization import SegmentationSampleVisualization
 
 __all__ = [
     "SegmentationBoundingBoxArea",
     "SegmentationBoundingBoxResolution",
     "SegmentationClassFrequency",
     "SegmentationClassHeatmap",
```

## data_gradients/feature_extractors/segmentation/bounding_boxes_area.py

```diff
@@ -55,16 +55,17 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Distribution of Bounding Boxes Area per Class."
+        return "Distribution of Object Area"
 
     @property
     def description(self) -> str:
         return (
-            "The distribution of the areas of the boxes that bound connected components of the different classes as a histogram.\n"
-            "The size of the objects can significantly affect the performance of your model. "
-            "If certain classes tend to have smaller objects, the model might struggle to segment them, especially if the resolution of the images is low "
+            "This graph shows the distribution of object area for each class. "
+            "This can highlight distribution gap in object size between the training and validation splits, which can harm the model performance. \n"
+            "Another thing to keep in mind is that having too many very small objects may indicate that your are down sizing your original image to a "
+            "low resolution that is not appropriate for your objects."
         )
```

## data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py

```diff
@@ -39,15 +39,15 @@
             y_label_name="Height (in % of image)",
             title=self.title,
             x_lim=(0, 100),
             y_lim=(0, 100),
             x_ticks_rotation=None,
             labels_key="split",
             individual_plots_key="split",
-            tight_layout=True,
+            tight_layout=False,
             sharey=True,
             labels_palette=LABELS_PALETTE,
         )
 
         train_description = df[df["split"] == "train"].describe()
         train_json = {"width": dict(train_description["width"]), "height": dict(train_description["height"])}
 
@@ -61,15 +61,15 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Distribution of Bounding Boxes Height and Width."
+        return "Distribution of Object Width and Height"
 
     @property
     def description(self) -> str:
         return (
-            "Width, Height of the bounding-boxes surrounding every object across all images. Plotted per-class on a heat-map.\n"
-            "A large variation in object sizes within a class can make it harder for the model to recognize the objects."
+            "These heat maps illustrate the distribution of objects width and height per class. \n"
+            "Large variations in object size can affect the model's ability to accurately recognize objects."
         )
```

## data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py

```diff
@@ -31,16 +31,24 @@
 
         split_heatmap = self.heatmaps_per_split.get(sample.split, np.zeros((len(sample.class_names), *self.heatmap_shape)))
         split_heatmap += resized_masks
         self.heatmaps_per_split[sample.split] = split_heatmap
 
     @property
     def title(self) -> str:
-        return "Heatmap of Segmentation Masks"
+        return "Objects Density"
 
     @property
     def description(self) -> str:
         return (
-            "Show the areas of high density of Bounding Boxes. This can be useful to understand if the objects are positioned in the right area.\n"
-            f"Note that only top {self.n_cols * self.n_rows} classes are shown. "
-            f" You can increase the number of classes by setting `SegmentationClassHeatmap` with `n_classes_to_show`"
+            "The heatmap represents areas of high object density within the images, providing insights into the spatial distribution of objects. "
+            "By examining the heatmap, you can quickly identify if objects are predominantly concentrated in specific regions or if they are evenly "
+            "distributed throughout the scene. This information can serve as a heuristic to assess if the objects are positioned appropriately "
+            "within the expected areas of interest."
+        )
+
+    @property
+    def notice(self) -> str:
+        return (
+            f"Only the {self.n_cols * self.n_rows} classes with highest density are shown.<br/>"
+            f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
         )
```

## data_gradients/feature_extractors/segmentation/components_convexity.py

```diff
@@ -47,17 +47,15 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Components Convexity."
+        return "Objects Convexity"
 
     @property
     def description(self) -> str:
         return (
-            "Mean of the convexity measure across all components VS Class ID.\n"
-            "Convexity measure of a component is defined by ("
-            "component_perimeter-convex_hull_perimeter)/convex_hull_perimeter.\n"
-            "High values can imply complex structures which might be difficult to segment."
+            "This graph depicts the convexity distribution of objects in both training and validation sets. \n"
+            "Higher convexity values suggest complex structures that may pose challenges for accurate segmentation."
         )
```

## data_gradients/feature_extractors/segmentation/components_erosion.py

```diff
@@ -56,20 +56,20 @@
             val=dict(df[df["split"] == "val"]["percent_change_of_n_components"].describe()),
         )
 
         return Feature(data=df, plot_options=plot_options, json=json)
 
     @property
     def title(self) -> str:
-        return "Components Stability to Erosion."
+        return "Objects Stability to Erosion"
 
     @property
     def description(self) -> str:
         return (
-            "An assessment of object stability under morphological opening - erosion followed by dilation. "
+            "Assessment of object stability under morphological opening - erosion followed by dilation. "
             "When a lot of components are small then the number of components decrease which means we might have "
             "noise in our annotations (i.e 'sprinkles')."
         )
         # FIXME: Can this also lead to increase of components, when breaking existing component into 2?
 
     def apply_mask_opening(self, mask: np.ndarray, kernel_shape: Tuple[int, int]) -> np.ndarray:
         """Opening is just another name of erosion followed by dilation.
```

## data_gradients/managers/abstract_manager.py

```diff
@@ -1,76 +1,65 @@
 import os
 import abc
 import logging
-import json
+import traceback
 from typing import Iterable, List, Dict, Optional
 from itertools import zip_longest
 from logging import getLogger
-from datetime import datetime
 from tqdm import tqdm
 
 from data_gradients.feature_extractors import AbstractFeatureExtractor
 from data_gradients.batch_processors.base import BatchProcessor
 from data_gradients.feature_extractors.common import SummaryStats
-from data_gradients.utils.utils import copy_files_by_list
 from data_gradients.visualize.seaborn_renderer import SeabornRenderer
+from data_gradients.utils.pdf_writer import ResultsContainer, Section, FeatureSummary
+from data_gradients.utils.summary_writer import SummaryWriter
+from data_gradients.config.data.data_config import DataConfig
 
-from data_gradients.utils.pdf_writer import ResultsContainer, Section, FeatureSummary, PDFWriter, assets
 
-logging.basicConfig(level=logging.WARNING)
+logging.basicConfig(level=logging.INFO)
 
 logger = getLogger(__name__)
 
 
 class AnalysisManagerAbstract(abc.ABC):
     """
     Main dataset analyzer manager abstract class.
     """
 
     def __init__(
         self,
         *,
         report_title: str,
+        data_config: DataConfig,
         train_data: Iterable,
         val_data: Optional[Iterable] = None,
         report_subtitle: Optional[str] = None,
         log_dir: Optional[str] = None,
         batch_processor: BatchProcessor,
         grouped_feature_extractors: Dict[str, List[AbstractFeatureExtractor]],
-        id_to_name: Dict,
         batches_early_stop: Optional[int] = None,
     ):
         """
         :param report_title:        Title of the report. Will be used to save the report
         :param report_subtitle:     Subtitle of the report
         :param train_data:          Iterable object contains images and labels of the training dataset
         :param val_data:            Iterable object contains images and labels of the validation dataset
         :param log_dir:             Directory where to save the logs. By default uses the current working directory
         :param batch_processor:     Batch processor object to be used before extracting features
         :param grouped_feature_extractors:  List of feature extractors to be used
         :param id_to_name:          Dictionary mapping class IDs to class names
         :param batches_early_stop:  Maximum number of batches to run in training (early stop)
         """
-        # Static parameters
-        if log_dir is None:
-            log_dir = os.path.join(os.getcwd(), "logs", report_title.replace(" ", "_"))
-            logger.info(f"`log_dir` was not set, so the logs will be saved in {log_dir}")
-
-        session_id = datetime.now().strftime("%Y%m%d-%H%M%S")
-        self.report_name = "Report.pdf"
-        self.log_filename = "summary.json"
-        self.log_dir = log_dir  # Main logging directory. Latest run results will be saved here.
-        self.archive_dir = os.path.join(log_dir, "archive_" + session_id)  # A duplicate of the results will be saved here as well.
-
-        self.report_title = report_title
-        self.report_subtitle = report_subtitle or datetime.strftime(datetime.now(), "%m:%H %B %d, %Y")
-
-        # WRITERS
         self.renderer = SeabornRenderer()
-        self.pdf_writer = PDFWriter(title=report_title, subtitle=report_subtitle, html_template=assets.html.doc_template)
+        self.summary_writer = SummaryWriter(report_title=report_title, report_subtitle=report_subtitle, log_dir=log_dir)
+
+        self.data_config_cache_name = f"{self.summary_writer.run_name}.json"
+        self.data_config = data_config
+        self.data_config.fill_missing_params_with_cache(cache_filename=self.data_config_cache_name)
 
         # DATA
         if batches_early_stop:
             logger.info(f"Running with `batches_early_stop={batches_early_stop}`: Only the first {batches_early_stop} batches will be analyzed.")
         self.batches_early_stop = batches_early_stop
         self.train_size = len(train_data) if hasattr(train_data, "__len__") else None
         self.val_size = len(val_data) if hasattr(val_data, "__len__") else None
@@ -95,16 +84,16 @@
         """
 
         print(
             f"  - Executing analysis with: \n"
             f"  - batches_early_stop: {self.batches_early_stop} \n"
             f"  - len(train_data): {self.train_size} \n"
             f"  - len(val_data): {self.val_size} \n"
-            f"  - log directory: {self.log_dir} \n"
-            f"  - Archive directory: {self.archive_dir} \n"
+            f"  - log directory: {self.summary_writer.log_dir} \n"
+            f"  - Archive directory: {self.summary_writer.archive_dir} \n"
             f"  - feature extractor list: {self.grouped_feature_extractors}"
         )
 
         datasets_tqdm = tqdm(
             zip_longest(self.train_iter, self.val_iter, fillvalue=None),
             desc="Analyzing... ",
             total=self.n_batches,
@@ -155,29 +144,28 @@
                 try:
                     feature = feature_extractor.aggregate()
                     f = self.renderer.render(feature.data, feature.plot_options)
                     feature_json = feature.json
                     feature_error = ""
                 except Exception as e:
                     f = None
-                    feature_json = {"error": str(e)}
-                    feature_error = (
-                        f"Feature extraction error. Check out the log file for more details:<br/>"
-                        f"<em>{os.path.join(self.archive_dir, self.log_filename)}</em>"
-                    )
+                    error_description = traceback.format_exception(type(e), e, e.__traceback__)
+                    feature_json = {"error": error_description}
+                    feature_error = f"Feature extraction error. Check out the log file for more details:<br/>" f"<em>{self.summary_writer.errors_path}</em>"
+                    self.summary_writer.add_error(title=feature_extractor.title, error=error_description)
 
                 if f is not None:
                     image_name = feature_extractor.__class__.__name__ + ".png"
-                    image_path = os.path.join(self.archive_dir, image_name)
+                    image_path = os.path.join(self.summary_writer.archive_dir, image_name)
                     f.savefig(image_path, dpi=300)
                     images_created.append(image_path)
                 else:
                     image_path = None
 
-                self.write_json(data=dict(title=feature_extractor.title, data=feature_json), output_dir=self.archive_dir, filename=self.log_filename)
+                self.summary_writer.add_feature_stats(title=feature_extractor.title, stats=feature_json)
 
                 if feature_error:
                     warning = feature_error
                 elif isinstance(feature_extractor, SummaryStats) and (interrupted or (self.batches_early_stop and self._stopped_early)):
                     warning = self._create_samples_iterated_warning()
                 else:
                     warning = feature_extractor.warning
@@ -191,29 +179,36 @@
                         notice=feature_extractor.notice,
                     )
                 )
             summary.add_section(section)
 
         print("Dataset successfully analyzed!")
         print("Starting to write the report, this may take around 10 seconds...")
+        self.summary_writer.set_pdf_summary(pdf_summary=summary)
+        self.summary_writer.set_data_config(data_config_dict=self.data_config.to_json())
+        self.summary_writer.write()
 
-        self.pdf_writer.write(results_container=summary, output_filename=os.path.join(self.archive_dir, self.report_name))
-        copy_files_by_list(source_dir=self.archive_dir, dest_dir=self.log_dir, file_list=[self.log_filename, self.report_name])
+        # Save cache in a specific Folder
+        self.data_config.write_to_json(filename=self.data_config_cache_name)
 
         # Cleanup of generated images
         for image_created in images_created:
             os.remove(image_created)
 
     def close(self):
         """Safe logging closing"""
         print(f'{"*" * 100}')
         print("We have finished evaluating your dataset!")
+        print()
+        print("The cache of your DataConfig object can be found in:")
+        print(f"    - {os.path.join(self.data_config.DEFAULT_CACHE_DIR, self.data_config_cache_name)}")
+        print()
         print("The results can be seen in:")
-        print(f"    - {self.log_dir}")
-        print(f"    - {self.archive_dir}")
+        print(f"    - {self.summary_writer.log_dir}")
+        print(f"    - {self.summary_writer.archive_dir}")
 
     def run(self):
         """
         Run method activating build, execute, post process and close the manager.
         """
         interrupted = False
         try:
@@ -234,30 +229,14 @@
         if self.train_size is None or self.val_size is None:
             return self.batches_early_stop
 
         n_batches_available = max(self.train_size, self.val_size)
         n_batches_early_stop = self.batches_early_stop or float("inf")
         return min(n_batches_early_stop, n_batches_available)
 
-    @staticmethod
-    def write_json(data: Dict, output_dir: str, filename: str):
-        os.makedirs(output_dir, exist_ok=True)
-        output_path = os.path.join(output_dir, filename)
-
-        if os.path.exists(output_path):
-            with open(output_path, "r") as f:
-                json_dict = json.load(f)
-        else:
-            json_dict = {}
-
-        json_dict["features"] = json_dict.get("features", []) + [data]
-
-        with open(output_path, "w") as f:
-            json.dump(json_dict, f, indent=4)
-
     def _create_samples_iterated_warning(self) -> str:
         if self.train_size is None or self._train_batch_size is None:
             total_train_samples = "unknown amount of "
             portion_train = ""
         else:
             total_train_samples = self.train_size * self._train_batch_size
             portion_train = f" ({self._train_iters_done/total_train_samples:.1%})"
```

## data_gradients/managers/detection_manager.py

```diff
@@ -1,55 +1,70 @@
 import os
-from typing import Optional, Iterable, Dict, Callable, List
+from typing import Optional, Iterable, Callable, List
+import torch
 
 from data_gradients.managers.abstract_manager import AnalysisManagerAbstract
 from data_gradients.config.utils import load_report_feature_extractors
 from data_gradients.batch_processors.detection import DetectionBatchProcessor
+from data_gradients.config.data.data_config import DetectionDataConfig
+from data_gradients.config.data.typing import SupportedDataType
 
 
 class DetectionAnalysisManager(AnalysisManagerAbstract):
     """Main detection manager class.
     Definition of task name, task-related preprocessor and parsing related configuration file
     """
 
     def __init__(
         self,
         *,
         report_title: str,
         train_data: Iterable,
         val_data: Optional[Iterable] = None,
         report_subtitle: Optional[str] = None,
+        config_path: Optional[str] = None,
+        log_dir: Optional[str] = None,
+        use_cache: bool = False,
         class_names: Optional[List[str]] = None,
         class_names_to_use: Optional[List[str]] = None,
         n_classes: Optional[int] = None,
-        config_path: Optional[str] = None,
-        log_dir: Optional[str] = None,
-        id_to_name: Optional[Dict] = None,
-        batches_early_stop: int = 999,
-        images_extractor: Callable = None,
-        labels_extractor: Callable = None,
+        images_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
+        labels_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
+        is_label_first: Optional[bool] = None,
+        bbox_format: Optional[str] = None,
         n_image_channels: int = 3,
+        batches_early_stop: int = 999,
     ):
         """
         Constructor of detection manager which controls the analyzer
         :param report_title:            Title of the report. Will be used to save the report
         :param report_subtitle:         Subtitle of the report
         :param class_names:             List of all class names in the dataset. The index should represent the class_id.
         :param class_names_to_use:      List of class names that we should use for analysis.
         :param n_classes:               Number of classes. Mutually exclusive with `class_names`.
         :param train_data:              Iterable object contains images and labels of the training dataset
         :param val_data:                Iterable object contains images and labels of the validation dataset
         :param config_path:             Full path the hydra configuration file. If None, the default configuration will be used.
         :param log_dir:                 Directory where to save the logs. By default uses the current working directory
-        :param id_to_name:              Class ID to class names mapping (Dictionary)
         :param batches_early_stop:      Maximum number of batches to run in training (early stop)
-        :param images_extractor:
-        :param labels_extractor:
-        :param n_image_channels:      Number of channels for each image in the dataset
+        :param use_cache:               Whether to use cache or not for the configuration of the data.
+        :param images_extractor:        Function extracting the image(s) out of the data output.
+        :param labels_extractor:        Function extracting the label(s) out of the data output.
+        :param is_label_first:          Whether the labels are in the first dimension or not.
+                                            > (class_id, x, y, w, h) for instance, as opposed to (x, y, w, h, class_id)
+        :param bbox_format:             Format of the bounding boxes. 'xyxy', 'xywh' or 'cxcywh'
+        :param n_image_channels:        Number of channels for each image in the dataset
         """
+        data_config = DetectionDataConfig(
+            use_cache=use_cache,
+            images_extractor=images_extractor,
+            labels_extractor=labels_extractor,
+            is_label_first=is_label_first,
+            xyxy_converter=bbox_format,
+        )
 
         # Check values of `n_classes` and `class_names` to define `class_names`.
         if n_classes and class_names:
             raise RuntimeError("`class_names` and `n_classes` cannot be specified at the same time")
         elif n_classes is None and class_names is None:
             raise RuntimeError("Either `class_names` or `n_classes` must be specified")
         class_names = class_names if class_names else list(map(str, range(n_classes)))
@@ -65,27 +80,26 @@
         if config_path is None:
             config_dir, config_name = None, "detection"
         else:
             config_path = os.path.abspath(config_path)
             config_dir, config_name = os.path.dirname(config_path), os.path.basename(config_path).split(".")[0]
 
         batch_processor = DetectionBatchProcessor(
-            images_extractor=images_extractor,
-            labels_extractor=labels_extractor,
+            data_config=data_config,
             n_image_channels=n_image_channels,
             class_names=class_names,
             class_names_to_use=class_names_to_use,
         )
 
         feature_extractors = load_report_feature_extractors(config_name=config_name, config_dir=config_dir)
 
         super().__init__(
+            data_config=data_config,
             report_title=report_title,
             report_subtitle=report_subtitle,
             train_data=train_data,
             val_data=val_data,
             batch_processor=batch_processor,
             grouped_feature_extractors=feature_extractors,
             log_dir=log_dir,
-            id_to_name=id_to_name,
             batches_early_stop=batches_early_stop,
         )
```

## data_gradients/managers/segmentation_manager.py

```diff
@@ -1,39 +1,42 @@
 import os
-from typing import Optional, Iterable, Dict, Callable, List
+from typing import Optional, Iterable, Callable, List
+import torch
 
 from data_gradients.managers.abstract_manager import AnalysisManagerAbstract
 from data_gradients.config.utils import load_report_feature_extractors
 from data_gradients.batch_processors.segmentation import SegmentationBatchProcessor
+from data_gradients.config.data.data_config import SegmentationDataConfig
+from data_gradients.config.data.typing import SupportedDataType
 
 
 class SegmentationAnalysisManager(AnalysisManagerAbstract):
     """
     Main semantic segmentation manager class.
     Definition of task name, task-related preprocessor and parsing related configuration file
     """
 
     def __init__(
         self,
         *,
         report_title: str,
-        class_names: Optional[List[str]] = None,
-        class_names_to_use: Optional[List[str]] = None,
-        n_classes: Optional[int] = None,
         train_data: Iterable,
         val_data: Optional[Iterable] = None,
         report_subtitle: Optional[str] = None,
         config_path: Optional[str] = None,
         log_dir: Optional[str] = None,
-        id_to_name: Optional[Dict] = None,
-        batches_early_stop: int = 999,
-        images_extractor: Callable = None,
-        labels_extractor: Callable = None,
+        use_cache: bool = False,
+        class_names: Optional[List[str]] = None,
+        class_names_to_use: Optional[List[str]] = None,
+        n_classes: Optional[int] = None,
+        images_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
+        labels_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
         num_image_channels: int = 3,
         threshold_soft_labels: float = 0.5,
+        batches_early_stop: int = 999,
     ):
         """
         Constructor of semantic-segmentation manager which controls the analyzer
 
         :param report_title:            Title of the report. Will be used to save the report
         :param report_subtitle:         Subtitle of the report
         :param class_names:             List of all class names in the dataset. The index should represent the class_id. Mutually exclusive with `n_classes`
@@ -41,19 +44,21 @@
         :param n_classes:               Number of classes. Mutually exclusive with `class_names`. If set, `class_names` will be a list of `class_ids`.
         :param train_data:              Iterable object contains images and labels of the training dataset
         :param val_data:                Iterable object contains images and labels of the validation dataset
         :param config_path:             Full path the hydra configuration file. If None, the default configuration will be used.
         :param log_dir:                 Directory where to save the logs. By default uses the current working directory
         :param id_to_name:              Class ID to class names mapping (Dictionary)
         :param batches_early_stop:      Maximum number of batches to run in training (early stop)
-        :param images_extractor:
-        :param labels_extractor:
+        :param use_cache:               Whether to use cache or not for the configuration of the data.
+        :param images_extractor:        Function extracting the image(s) out of the data output.
+        :param labels_extractor:        Function extracting the label(s) out of the data output.
         :param num_image_channels:      Number of channels for each image in the dataset
         :param threshold_soft_labels:   Threshold for converting soft labels to binary labels
         """
+        data_config = SegmentationDataConfig(use_cache=use_cache, images_extractor=images_extractor, labels_extractor=labels_extractor)
 
         # Check values of `n_classes` and `class_names` to define `class_names`.
         if n_classes and class_names:
             raise RuntimeError("`class_names` and `n_classes` cannot be specified at the same time")
         elif n_classes is None and class_names is None:
             raise RuntimeError("Either `class_names` or `n_classes` must be specified")
         class_names = class_names if class_names else list(map(str, range(n_classes)))
@@ -69,28 +74,27 @@
         if config_path is None:
             config_dir, config_name = None, "segmentation"
         else:
             config_path = os.path.abspath(config_path)
             config_dir, config_name = os.path.dirname(config_path), os.path.basename(config_path).split(".")[0]
 
         batch_processor = SegmentationBatchProcessor(
+            data_config=data_config,
             class_names=class_names,
             class_names_to_use=class_names_to_use,
-            images_extractor=images_extractor,
-            labels_extractor=labels_extractor,
             n_image_channels=num_image_channels,
             threshold_value=threshold_soft_labels,
         )
 
         grouped_feature_extractors = load_report_feature_extractors(config_name=config_name, config_dir=config_dir)
 
         super().__init__(
+            data_config=data_config,
             report_title=report_title,
             report_subtitle=report_subtitle,
             train_data=train_data,
             val_data=val_data,
             batch_processor=batch_processor,
             grouped_feature_extractors=grouped_feature_extractors,
             log_dir=log_dir,
-            id_to_name=id_to_name,
             batches_early_stop=batches_early_stop,
         )
```

## data_gradients/utils/detection.py

```diff
@@ -1,10 +1,67 @@
 from typing import Tuple
 
 import numpy as np
+import torch
+
+
+def cxcywh_to_xyxy(bboxes: torch.Tensor) -> torch.Tensor:
+    """Transform bboxes from CX-CY-W-H format to XYXY format.
+
+    :param bboxes:  BBoxes of shape (..., 4) in CX-CY-W-H format
+    :return:        BBoxes of shape (..., 4) in XYXY format
+    """
+    cx, cy, w, h = bboxes[..., 0], bboxes[..., 1], bboxes[..., 2], bboxes[..., 3]
+    x1 = cx - 0.5 * w
+    y1 = cy - 0.5 * h
+    x2 = x1 + w
+    y2 = y1 + h
+
+    return torch.stack([x1, y1, x2, y2], dim=-1)
+
+
+def xywh_to_xyxy(bboxes: torch.Tensor) -> torch.Tensor:
+    """Transform bboxes from XYWH format to XYXY format.
+
+    :param bboxes:  BBoxes of shape (..., 4) in XYWH format
+    :return:        BBoxes of shape (..., 4) in XYXY format
+    """
+    x1, y1, w, h = bboxes[..., 0], bboxes[..., 1], bboxes[..., 2], bboxes[..., 3]
+    x2 = x1 + w
+    y2 = y1 + h
+
+    return torch.stack([x1, y1, x2, y2], dim=-1)
+
+
+XYXY_CONVERTERS = {
+    "xyxy": {"function": lambda x: x, "description": "xyxy: x-left, y-top, x-right, y-bottom"},
+    "xywh": {"function": xywh_to_xyxy, "description": "xywh: x-left, y-top, width, height"},
+    "cxcywh": {"function": cxcywh_to_xyxy, "description": "cxcywh: x-center, y-center, width, height"},
+}
+
+
+class XYXYConvertError(Exception):
+    ...
+
+
+class XYXYConverter:
+    def __init__(self, format_name: str):
+        if format_name not in XYXY_CONVERTERS:
+            raise ValueError(f"`{format_name}` is not a supported bounding box format. It should be one of {list(XYXY_CONVERTERS.keys())}")
+        self.converter = XYXY_CONVERTERS[format_name]["function"]
+
+    def __call__(self, bboxes: torch.Tensor) -> torch.Tensor:
+        try:
+            return self.converter(bboxes)
+        except Exception as e:
+            raise XYXYConvertError(f"{e}:\n \t => Error happened when converting tensors to xyxy format.") from e
+
+    @staticmethod
+    def get_available_options():
+        return {info["description"]: key for key, info in XYXY_CONVERTERS.items()}
 
 
 def scale_bboxes(old_shape: Tuple[float, float], new_shape: Tuple[float, float], bboxes_xyxy: np.ndarray):
     """Scale bounding boxes to a new shape.
     :param old_shape:   Old shape of the image, (H, W) format
     :param new_shape:   New shape of the image, (H, W) format
     :param bboxes_xyxy: Bounding boxes in xyxy format
```

## data_gradients/utils/utils.py

```diff
@@ -1,13 +1,21 @@
 import os
 import re
 import shutil
+import json
 from typing import Dict, Mapping, List
 
 
+def write_json(path: str, json_dict: Dict):
+    dirname = os.path.dirname(path)
+    os.makedirs(dirname, exist_ok=True)
+    with open(path, "w") as f:
+        json.dump(json_dict, f, indent=4)
+
+
 def class_id_to_name(mapping, hist: Dict):
     if mapping is None:
         return hist
 
     new_hist = {}
     for key in list(hist.keys()):
         try:
@@ -42,57 +50,30 @@
     :param params: Mapping, the mapping containing param.
     :return:
     """
     fuzzy_params = {fuzzy_str(key): params[key] for key in params.keys()}
     return fuzzy_params[fuzzy_str(name)]
 
 
-def ask_user(main_question: str, options: List[str], optional_description: str = "") -> str:
-    """Prompt the user to choose an option from a list of options.
-    :param main_question:   The main question or instruction for the user.
-    :param options:         List of options to chose from.
-    :param optional_description:  Optional description to display to the user.
-    :return:                The chosen option (key from the options_described dictionary).
-    """
-    numbers_to_chose_from = range(len(options))
-
-    options_formatted = "\n".join([f"[{number}] {option_description}" for number, option_description in zip(numbers_to_chose_from, options)])
-
-    user_answer = None
-    while user_answer not in numbers_to_chose_from:
-        print("\n------------------------------------------------------------------------")
-        print(f"{main_question}")
-        print("------------------------------------------------------------------------")
-        if optional_description:
-            print(optional_description)
-        print("\nOptions:")
-        print(options_formatted)
-        print("")
-
-        try:
-            user_answer = input("Your selection (Enter the corresponding number) >>> ")
-            user_answer = int(user_answer)
-        except Exception:
-            user_answer = None
-
-        if user_answer not in numbers_to_chose_from:
-            print(f'Oops! "{user_answer}" is not a valid choice. Let\'s try again.')
-
-    selected_option = options[user_answer]
-    print(f"Great! You chose: {selected_option}\n")
-
-    return selected_option
-
-
 def copy_files_by_list(file_list: List[str], source_dir: str, dest_dir: str) -> None:
     """Copy a list of files from the source directory to the destination directory.
 
     :param file_list:   List of filenames to be copied.
     :param source_dir:  Path of the source directory.
     :param dest_dir:    Path of the destination directory.
     """
     for file_name in file_list:
         source_file_path = os.path.join(source_dir, file_name)
         os.makedirs(dest_dir, exist_ok=True)
         if os.path.isfile(source_file_path):
             dest_file_path = os.path.join(dest_dir, file_name)
             shutil.copy(source_file_path, dest_file_path)
+
+
+def safe_json_load(path: str) -> Dict:
+    if not os.path.exists(path):
+        return {}
+    try:
+        with open(path, "r") as f:
+            return json.load(f)
+    except json.decoder.JSONDecodeError:
+        return {}
```

## Comparing `data_gradients/feature_extractors/object_detection/classes_count.py` & `data_gradients/feature_extractors/segmentation/classes_frequency.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
-from data_gradients.utils.data_classes import DetectionSample
+from data_gradients.utils.data_classes import SegmentationSample
 from data_gradients.visualize.seaborn_renderer import BarPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
 
 
 @register_feature_extractor()
-class DetectionClassFrequency(AbstractFeatureExtractor):
-    """Feature Extractor to count the number of instance of each class."""
-
+class SegmentationClassFrequency(AbstractFeatureExtractor):
     def __init__(self):
         self.data = []
 
-    def update(self, sample: DetectionSample):
-        for class_id, bbox_xyxy in zip(sample.class_ids, sample.bboxes_xyxy):
-            class_name = sample.class_names[class_id]
-            self.data.append(
-                {
-                    "split": sample.split,
-                    "class_id": class_id,
-                    "class_name": class_name,
-                }
-            )
+    def update(self, sample: SegmentationSample):
+        for j, class_channel in enumerate(sample.contours):
+            for contour in class_channel:
+                class_id = contour.class_id
+                class_name = sample.class_names[class_id]
+                self.data.append(
+                    {
+                        "split": sample.split,
+                        "class_id": class_id,
+                        "class_name": class_name,
+                    }
+                )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
-        # Include ("class_name", "split", "n_appearance")
+        # Include ("class_name", "class_id", "split", "n_appearance")
         df_class_count = df.groupby(["class_name", "class_id", "split"]).size().reset_index(name="n_appearance")
 
         split_sums = df_class_count.groupby("split")["n_appearance"].sum()
         df_class_count["frequency"] = 100 * (df_class_count["n_appearance"] / df_class_count["split"].map(split_sums))
 
         plot_options = BarPlotOptions(
             x_label_key="frequency",
@@ -60,8 +60,13 @@
 
     @property
     def title(self) -> str:
         return "Class Frequency"
 
     @property
     def description(self) -> str:
-        return "The total number of bounding boxes for each class, across all images."
+        return (
+            "This bar plot represents the frequency of appearance of each class. "
+            "This may highlight class distribution gap between training and validation splits. "
+            "For instance, if one of the class only appears in the validation set, you know in advance that your model won't be able to "
+            "learn to predict that class."
+        )
```

## Comparing `data_gradients/feature_extractors/object_detection/classes_per_image_count.py` & `data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py`

 * *Files 12% similar despite different names*

```diff
@@ -38,14 +38,15 @@
         plot_options = ViolinPlotOptions(
             x_label_key="n_appearance",
             x_label_name="Number of class instance per Image",
             y_label_key="class_name",
             y_label_name="Class Names",
             order_key="class_id",
             title=self.title,
+            x_lim=(0, df_class_count["n_appearance"].max() * 1.2),
             bandwidth=0.4,
             x_ticks_rotation=None,
             labels_key="split",
         )
 
         json = dict(
             train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
@@ -57,12 +58,15 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Number of classes per image."
+        return "Distribution of Class Frequency per Image"
 
     @property
     def description(self) -> str:
-        return "The total number of bounding boxes for each class, across all images."
+        return (
+            "This graph shows how many times each class appears in an image. It highlights whether each class has a constant number of "
+            "appearance per image, or whether it really depends from an image to another."
+        )
```

## Comparing `data_gradients/feature_extractors/segmentation/classes_count.py` & `data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,53 +1,56 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import SegmentationSample
-from data_gradients.visualize.seaborn_renderer import BarPlotOptions
+from data_gradients.visualize.plot_options import ViolinPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
 
 
 @register_feature_extractor()
-class SegmentationClassFrequency(AbstractFeatureExtractor):
+class SegmentationClassesPerImageCount(AbstractFeatureExtractor):
     def __init__(self):
         self.data = []
 
     def update(self, sample: SegmentationSample):
+
         for j, class_channel in enumerate(sample.contours):
             for contour in class_channel:
                 class_id = contour.class_id
                 class_name = sample.class_names[class_id]
                 self.data.append(
                     {
                         "split": sample.split,
-                        "class_id": class_id,
+                        "sample_id": sample.sample_id,
                         "class_name": class_name,
+                        "class_id": class_id,
                     }
                 )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
         # Include ("class_name", "class_id", "split", "n_appearance")
-        df_class_count = df.groupby(["class_name", "class_id", "split"]).size().reset_index(name="n_appearance")
+        # For each class, image, split, I want to know how many bbox I have
+        df_class_count = df.groupby(["class_name", "class_id", "sample_id", "split"]).size().reset_index(name="n_appearance")
 
-        split_sums = df_class_count.groupby("split")["n_appearance"].sum()
-        df_class_count["frequency"] = 100 * (df_class_count["n_appearance"] / df_class_count["split"].map(split_sums))
+        max_n_appearance = df_class_count["n_appearance"].max()
 
-        plot_options = BarPlotOptions(
-            x_label_key="frequency",
-            x_label_name="Frequency",
+        plot_options = ViolinPlotOptions(
+            x_label_key="n_appearance",
+            x_label_name="Number of class instance per Image",
             y_label_key="class_name",
-            y_label_name="Class",
+            y_label_name="Class Names",
             order_key="class_id",
             title=self.title,
+            x_lim=(0, max_n_appearance * 1.2),  # Cut the max_x at 120% of the highest max n_appearance to increase readability
+            bandwidth=0.4,
             x_ticks_rotation=None,
             labels_key="split",
-            orient="h",
         )
 
         json = dict(
             train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
             val=dict(df_class_count[df_class_count["split"] == "val"]["n_appearance"].describe()),
         )
 
@@ -56,16 +59,15 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Number of classes."
+        return "Distribution of Class Frequency per Image"
 
     @property
     def description(self) -> str:
         return (
-            "The total number of connected components for each class, across all images. \n"
-            "If the average number of components per image is too high, it might be due to image noise or the "
-            "presence of many segmentation blobs."
+            "This graph shows how many times each class appears in an image. It highlights whether each class has a constant number of "
+            "appearance per image, or whether it really depends from an image to another."
         )
```

## Comparing `data_gradients/feature_extractors/segmentation/classes_per_image_count.py` & `data_gradients/feature_extractors/object_detection/classes_frequency.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,56 +1,53 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
-from data_gradients.utils.data_classes import SegmentationSample
-from data_gradients.visualize.plot_options import ViolinPlotOptions
+from data_gradients.utils.data_classes import DetectionSample
+from data_gradients.visualize.seaborn_renderer import BarPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
 
 
 @register_feature_extractor()
-class SegmentationClassesPerImageCount(AbstractFeatureExtractor):
+class DetectionClassFrequency(AbstractFeatureExtractor):
+    """Feature Extractor to count the number of instance of each class."""
+
     def __init__(self):
         self.data = []
 
-    def update(self, sample: SegmentationSample):
-
-        for j, class_channel in enumerate(sample.contours):
-            for contour in class_channel:
-                class_id = contour.class_id
-                class_name = sample.class_names[class_id]
-                self.data.append(
-                    {
-                        "split": sample.split,
-                        "sample_id": sample.sample_id,
-                        "class_name": class_name,
-                        "class_id": class_id,
-                    }
-                )
+    def update(self, sample: DetectionSample):
+        for class_id, bbox_xyxy in zip(sample.class_ids, sample.bboxes_xyxy):
+            class_name = sample.class_names[class_id]
+            self.data.append(
+                {
+                    "split": sample.split,
+                    "class_id": class_id,
+                    "class_name": class_name,
+                }
+            )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
-        # Include ("class_name", "class_id", "split", "n_appearance")
-        # For each class, image, split, I want to know how many bbox I have
-        df_class_count = df.groupby(["class_name", "class_id", "sample_id", "split"]).size().reset_index(name="n_appearance")
-
-        max_n_appearance = df_class_count["n_appearance"].max()
-
-        plot_options = ViolinPlotOptions(
-            x_label_key="n_appearance",
-            x_label_name="Number of class instance per Image",
+        # Include ("class_name", "split", "n_appearance")
+        df_class_count = df.groupby(["class_name", "class_id", "split"]).size().reset_index(name="n_appearance")
+
+        split_sums = df_class_count.groupby("split")["n_appearance"].sum()
+        df_class_count["frequency"] = 100 * (df_class_count["n_appearance"] / df_class_count["split"].map(split_sums))
+
+        plot_options = BarPlotOptions(
+            x_label_key="frequency",
+            x_label_name="Frequency",
             y_label_key="class_name",
-            y_label_name="Class Names",
+            y_label_name="Class",
             order_key="class_id",
             title=self.title,
-            x_lim=(0, max_n_appearance * 1.2),  # Cut the max_x at 120% of the highest max n_appearance to increase readability
-            bandwidth=0.4,
             x_ticks_rotation=None,
             labels_key="split",
+            orient="h",
         )
 
         json = dict(
             train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
             val=dict(df_class_count[df_class_count["split"] == "val"]["n_appearance"].describe()),
         )
 
@@ -59,16 +56,16 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Number of classes per image."
+        return "Class Frequency"
 
     @property
     def description(self) -> str:
         return (
-            "The total number of connected components for each class, across all images. \n"
-            "If the average number of components per image is too high, it might be due to image noise or the "
-            "presence of many segmentation blobs."
+            "Frequency of appearance of each class. This may highlight class distribution gap between training and validation splits. \n"
+            "For instance, if one of the class only appears in the validation set, you know in advance that your model won't be able to "
+            "learn to predict that class."
         )
```

## Comparing `data_gradients/feature_extractors/segmentation/components_per_image_count.py` & `data_gradients/feature_extractors/segmentation/component_frequency_per_image.py`

 * *Files 19% similar despite different names*

```diff
@@ -47,12 +47,16 @@
         )
 
         feature = Feature(data=df_class_count, plot_options=plot_options, json=json)
         return feature
 
     @property
     def title(self) -> str:
-        return "Number of component per image."
+        return "Distribution of Objects per Image"
 
     @property
     def description(self) -> str:
-        return "The total number of connected components per image. This helps understanding how many components images typically includes. "
+        return (
+            "These graphs shows how many different objects appear in images. \n"
+            "This can typically be valuable to know when you observe a very high number of objects per image, "
+            "as some models include a parameter to filter the top k results."
+        )
```

## Comparing `data_gradients-0.0.9.dist-info/LICENSE.md` & `data_gradients-0.1.1.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `data_gradients-0.0.9.dist-info/METADATA` & `data_gradients-0.1.1.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 Metadata-Version: 2.1
 Name: data-gradients
-Version: 0.0.9
+Version: 0.1.1
 Summary: DataGradients
 Home-page: https://github.com/Deci-AI/data-gradients
 Author: Deci AI
 Author-email: rnd@deci.ai
 Keywords: Deci,AI,Data,Deep Learning,Computer Vision,PyTorch
 Description-Content-Type: text/markdown
 License-File: LICENSE.md
 Requires-Dist: hydra-core (>=1.2.0)
 Requires-Dist: omegaconf (>=2.2.3)
 Requires-Dist: pygments (>=2.13.0)
 Requires-Dist: tqdm (>=4.64.1)
+Requires-Dist: appdirs (>=1.4.4)
 Requires-Dist: opencv-python
 Requires-Dist: Pillow
 Requires-Dist: tensorboard
 Requires-Dist: torch
 Requires-Dist: torchvision
 Requires-Dist: numpy
 Requires-Dist: matplotlib
@@ -36,29 +37,43 @@
 </p>   
 </div>
 
 DataGradients is an open-source python based library specifically designed for computer vision dataset analysis. 
 
 It automatically extracts features from your datasets and combines them all into a single user-friendly report. 
 
+- [Features](#features)
+- [Installation](#installation)
+- [Quick Start](#quick-start)
+   - [Prerequisites](#prerequisites)
+   - [Dataset Analysis](#dataset-analysis)
+   - [Report](#report)
+- [Feature Configuration](#feature-configuration)
+- [Dataset Adapters](#dataset-adapters)
+   - [Image Adapter](#image-adapter)
+   - [Label Adapter](#label-adapter)
+   - [Example](#example)
+- [License](#license)
+
 ## Features
 - Image-Level Evaluation: DataGradients evaluates key image features such as resolution, color distribution, and average brightness.
-- Class Distribution: The library extracts stats allowing to know which classes are the most used, how many objects do you have per image, how many image without any label, ...
-- Heatmap Generation: DataGradients produces heatmaps of bounding boxes or masks allowing you to understand if the objects are positioned in the right area.
-- And many more!
+- Class Distribution: The library extracts stats allowing you to know which classes are the most used, how many objects do you have per image, how many image without any label, ...
+- Heatmap Generation: DataGradients produces heatmaps of bounding boxes or masks, allowing you to understand if the objects are positioned in the right area.
+- And [many more](./documentation/feature_description.md)!
 
 <div align="center">
-  <img src="assets/report_image_stats.png" width="250px">
-  <img src="assets/report_mask_sample.png" width="250px">
-  <img src="assets/report_classes_distribution.png" width="250px">
+  <img src="documentation/assets/report_image_stats.png" width="250px">
+  <img src="documentation/assets/report_mask_sample.png" width="250px">
+  <img src="documentation/assets/report_classes_distribution.png" width="250px">
   <p><em>Example of pages from the Report</em>
 </div>
 
+
 ## Installation
-You can install DataGradients directly from the Github repository.
+You can install DataGradients directly from the GitHub repository.
 
 ```
 pip install git+https://github.com/Deci-AI/data-gradients
 ```
 
 
 ## Quick Start
@@ -139,14 +154,20 @@
 which does not require you to download any additional data.
 
 
 ### Report
 Once the analysis is done, the path to your pdf report will be printed.
 
 
+## Feature Configuration
+ 
+The feature configuration allows you to run the analysis on a subset of features or adjust the parameters of existing features. 
+If you are interested in customizing this configuration, you can check out the [documentation](documentation/feature_configuration.md) on that topic.
+
+
 ## Dataset Adapters
 Before implementing a Dataset Adapter try running without it, in many cases DataGradient will support your dataset without any code.
 
 Two type of Dataset Adapters are available: `images_extractor` and `labels_extractor`. These functions should be passed to the main Analyzer function init.
 
 ```python
 from data_gradients.managers.segmentation_manager import SegmentationAnalysisManager
@@ -231,10 +252,11 @@
 
 SegmentationAnalysisManager(
     ...,
     labels_extractor=labels_extractor
 )
 ```
 
+
 ## License
 
 This project is released under the [Apache 2.0 license](LICENSE.md).
```

### html2text {}

```diff
@@ -1,36 +1,42 @@
-Metadata-Version: 2.1 Name: data-gradients Version: 0.0.9 Summary:
+Metadata-Version: 2.1 Name: data-gradients Version: 0.1.1 Summary:
 DataGradients Home-page: https://github.com/Deci-AI/data-gradients Author: Deci
 AI Author-email: rnd@deci.ai Keywords: Deci,AI,Data,Deep Learning,Computer
 Vision,PyTorch Description-Content-Type: text/markdown License-File: LICENSE.md
 Requires-Dist: hydra-core (>=1.2.0) Requires-Dist: omegaconf (>=2.2.3)
 Requires-Dist: pygments (>=2.13.0) Requires-Dist: tqdm (>=4.64.1) Requires-
-Dist: opencv-python Requires-Dist: Pillow Requires-Dist: tensorboard Requires-
-Dist: torch Requires-Dist: torchvision Requires-Dist: numpy Requires-Dist:
-matplotlib Requires-Dist: scipy Requires-Dist: rapidfuzz Requires-Dist:
-coverage (~=5.3.1) Requires-Dist: seaborn Requires-Dist: xhtml2pdf Requires-
-Dist: jinja2 # DataGradients
+Dist: appdirs (>=1.4.4) Requires-Dist: opencv-python Requires-Dist: Pillow
+Requires-Dist: tensorboard Requires-Dist: torch Requires-Dist: torchvision
+Requires-Dist: numpy Requires-Dist: matplotlib Requires-Dist: scipy Requires-
+Dist: rapidfuzz Requires-Dist: coverage (~=5.3.1) Requires-Dist: seaborn
+Requires-Dist: xhtml2pdf Requires-Dist: jinja2 # DataGradients
 [https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue] [https:/
    /img.shields.io/pypi/v/data-gradients] [https://img.shields.io/github/v/
     release/Deci-AI/data-gradients] [https://img.shields.io/badge/license-
                               Apache%202.0-blue]
 DataGradients is an open-source python based library specifically designed for
 computer vision dataset analysis. It automatically extracts features from your
-datasets and combines them all into a single user-friendly report. ## Features
-- Image-Level Evaluation: DataGradients evaluates key image features such as
-resolution, color distribution, and average brightness. - Class Distribution:
-The library extracts stats allowing to know which classes are the most used,
-how many objects do you have per image, how many image without any label, ... -
-Heatmap Generation: DataGradients produces heatmaps of bounding boxes or masks
-allowing you to understand if the objects are positioned in the right area. -
-And many more!
-   [assets/report_image_stats.png] [assets/report_mask_sample.png] [assets/
-                       report_classes_distribution.png]
+datasets and combines them all into a single user-friendly report. - [Features]
+(#features) - [Installation](#installation) - [Quick Start](#quick-start) -
+[Prerequisites](#prerequisites) - [Dataset Analysis](#dataset-analysis) -
+[Report](#report) - [Feature Configuration](#feature-configuration) - [Dataset
+Adapters](#dataset-adapters) - [Image Adapter](#image-adapter) - [Label
+Adapter](#label-adapter) - [Example](#example) - [License](#license) ##
+Features - Image-Level Evaluation: DataGradients evaluates key image features
+such as resolution, color distribution, and average brightness. - Class
+Distribution: The library extracts stats allowing you to know which classes are
+the most used, how many objects do you have per image, how many image without
+any label, ... - Heatmap Generation: DataGradients produces heatmaps of
+bounding boxes or masks, allowing you to understand if the objects are
+positioned in the right area. - And [many more](./documentation/
+feature_description.md)!
+     [documentation/assets/report_image_stats.png] [documentation/assets/
+report_mask_sample.png] [documentation/assets/report_classes_distribution.png]
                        Example of pages from the Report
-## Installation You can install DataGradients directly from the Github
+## Installation You can install DataGradients directly from the GitHub
 repository. ``` pip install git+https://github.com/Deci-AI/data-gradients ```
 ## Quick Start ### Prerequisites - **Dataset**: Includes a **Train** set and a
 **Validation** or a **Test** set. - **Class Names**: A list of the unique
 categories present in your dataset. - **Iterable**: A method to iterate over
 your Dataset providing images and labels. Can be any of the following: -
 PyTorch Dataloader - PyTorch Dataset - Generator that yields image/label pairs
 - Any other iterable you use for model training/validation Please ensure all
@@ -57,16 +63,20 @@
 SegmentationAnalysisManager train_data = ... val_data = ... class_names = ...
 analyzer = SegmentationAnalysisManager( report_title="Testing Data-Gradients
 Segmentation", train_data=train_data, val_data=val_data,
 class_names=class_names, ) analyzer.run() ``` **Example** You can test the
 segmentation analysis tool in the following [example](https://github.com/Deci-
 AI/data-gradients/blob/master/examples/segmentation_example.py) which does not
 require you to download any additional data. ### Report Once the analysis is
-done, the path to your pdf report will be printed. ## Dataset Adapters Before
-implementing a Dataset Adapter try running without it, in many cases
+done, the path to your pdf report will be printed. ## Feature Configuration The
+feature configuration allows you to run the analysis on a subset of features or
+adjust the parameters of existing features. If you are interested in
+customizing this configuration, you can check out the [documentation]
+(documentation/feature_configuration.md) on that topic. ## Dataset Adapters
+Before implementing a Dataset Adapter try running without it, in many cases
 DataGradient will support your dataset without any code. Two type of Dataset
 Adapters are available: `images_extractor` and `labels_extractor`. These
 functions should be passed to the main Analyzer function init. ```python from
 data_gradients.managers.segmentation_manager import SegmentationAnalysisManager
 train_data = ... val_data = ... # Let Assume that in this case, the train_data
 and val_data return data in this format: # (image, {"masks", "bboxes"})
 images_extractor = lambda data: data[0] # Extract the image labels_extractor =
```

## Comparing `data_gradients-0.0.9.dist-info/RECORD` & `data_gradients-0.1.1.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-data_gradients/__init__.py,sha256=46Yjk3fz9o8aTN8E95McnzpJcjGzVJmHmQqUZ5mXzfc,22
-data_gradients/requirements.txt,sha256=kkF6LHbdfqoH5b7IXBcb3iKd5pyq2XfF4p_m6c4Ooyk,189
+data_gradients/__init__.py,sha256=rnObPjuBcEStqSO0S6gsdS_ot8ITOQjVj_-P1LUUYpg,22
+data_gradients/requirements.txt,sha256=1Si9lFhxUftmRhT75eKHsfIB87CVGxW_h4Ow-J2P-Dc,205
 data_gradients/assets/__init__.py,sha256=u-dAbknZKaLTrSPCuSm7mOxVUTisTla2P9zlRp7a1nU,231
 data_gradients/assets/assets_container.py,sha256=nsY6AYtlG1TdfrF9atdYzGcqdzpqFByXG3UqV-4ODuY,2755
 data_gradients/assets/css/test.css,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/assets/html/basic_info_fe.html,sha256=VMx8YM_z8T8Q5gljZNgmbZlxsOcDO3olDUM2lhgH-1E,3245
 data_gradients/assets/html/doc_template.html,sha256=yYMAT9adkR-EP-zVIrhSxuWCyLWYDATpPleRqvQolks,8301
 data_gradients/assets/html/test.html,sha256=WUoHLRvnM8pf46wEf7OcNsQyyubQfn1JiNt2t49YvJ4,123
 data_gradients/assets/images/chart_demo.png,sha256=zlOr7jOIHD-DTAQccBOgeJ9CNeMfaqF5gvr3o3Ui0OU,51292
 data_gradients/assets/images/info.png,sha256=fowx3nTT6ZVy4SuJecN13R_SXos3Uo2v9AbM73Mr9FE,139838
 data_gradients/assets/images/logo.png,sha256=EgIIuDIL3HHWDPk_2qPD7-sFB8s1ERXrTXZI5nLG4IM,36081
 data_gradients/assets/images/warning.png,sha256=XVnDsyHU2u0Hqg5FHMmrL5orUqWTbwFNyLLxGmfogZQ,75086
 data_gradients/assets/text/lorem_ipsum.txt,sha256=V22Sg3hEbEGrGrf4s0Up6lvSyZxcY0qhVFEGFeradQM,333
 data_gradients/assets/text/test.txt,sha256=dQnlvaDHYtK6x_kNdYtbImP6Acy8VCq1498WO-CObKk,12
 data_gradients/batch_processors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/batch_processors/base.py,sha256=BzUqNIwSESX_wwDm4-MlLmpId22P5j6w3oZrnBGeu2Q,1846
-data_gradients/batch_processors/detection.py,sha256=pJvOmFXMwr1EMCbTdqrDQbCWC-9M-0L2yL4TmZ37Dfw,1171
-data_gradients/batch_processors/segmentation.py,sha256=_hTwm4pKEDcXNSbnxHxOnOHM-Y5AG-j0cIR9CHpxRl8,1323
+data_gradients/batch_processors/base.py,sha256=redCxcEwMmMULvWSRjO0ne-WD_IHV8HFUMLcstwL7AM,1636
+data_gradients/batch_processors/detection.py,sha256=Qtqb6nDeNiNikglqv78TGwKx60POFaj7A7zwx7kvWgw,1125
+data_gradients/batch_processors/segmentation.py,sha256=aHVofThKhYeeh5kUxeLCLGYYkeSuagAgprwOJCis2Z0,1236
 data_gradients/batch_processors/utils.py,sha256=pA7lnrn7ikOsIOt1gk0VCQ-p_5hXF2czCoo7YyZ6jRU,1193
 data_gradients/batch_processors/adapters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/batch_processors/adapters/dataset_adapter.py,sha256=RTdgj2KiBCLM-W6CjwCAl_pkTb6gDoaaN0dXSLXyuR0,3613
-data_gradients/batch_processors/adapters/tensor_extractor.py,sha256=kUorDB3VnFmLrPT16YFgKFD1yzDUpTdFqbibF5Tgm3k,6256
+data_gradients/batch_processors/adapters/dataset_adapter.py,sha256=dlgINiDcyBoX7DGCkoyDDhhdbETUzXWWcDVOOf3ETTw,4621
+data_gradients/batch_processors/adapters/tensor_extractor.py,sha256=ri-WRtTGCF0C5Fczd4iLGYKOU3zdKyaMPyaBvrKczyg,6698
 data_gradients/batch_processors/formatters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/batch_processors/formatters/base.py,sha256=4_NXp9XjU5M4LHACcEhvuOSGb4UhwrZAutJKwB3HJGE,725
-data_gradients/batch_processors/formatters/detection.py,sha256=RBYmzdTUWGoD17LRnqcusxttAcyP65Cc_fd0RaDsHEs,11284
-data_gradients/batch_processors/formatters/segmentation.py,sha256=LAOYs7hUrqh8213sE2m2zak5Dx9jCWOtubXXBLVCZCw,6895
-data_gradients/batch_processors/formatters/utils.py,sha256=OX1tNvEQbotBwYktW2heXtTvmGY7rGbSJeJ7TxE-j8o,1865
+data_gradients/batch_processors/formatters/detection.py,sha256=ukdLeGtBRQ3hs89u7wfTAG4UoYIVGiDR7fyZmyHSVs8,8757
+data_gradients/batch_processors/formatters/segmentation.py,sha256=nhugc73zkkk9UUR87NIoOr1G7sn5E_69G7vlNhoJr20,6940
+data_gradients/batch_processors/formatters/utils.py,sha256=4Mez4LUbTOw8skJ1kKrvbh4_67whn28vtzdqK_ta3A8,1912
 data_gradients/batch_processors/preprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/batch_processors/preprocessors/base.py,sha256=6WUenirzUy7NpxrTAaN0sAxSld04_IncVSgeq6sffZU,994
 data_gradients/batch_processors/preprocessors/contours.py,sha256=s9-cHTXOSMyRaFogaeCK3OfX75DjGumpAeE10JHeUew,5189
 data_gradients/batch_processors/preprocessors/detection.py,sha256=dyqD90Z4sC7nJonpOKf2lIYNom_gEtLEBMIYS5q21eY,2527
 data_gradients/batch_processors/preprocessors/segmentation.py,sha256=4GtACw_1DHMILLUpTFR5InVysP5YBza-fKdW8Rap3sM,1836
 data_gradients/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/common/decorators/__init__.py,sha256=-JMBR0sQe__b9ON3sRtuyYEFOwwVBVm5XCq-jmGsi8M,66
@@ -36,66 +36,72 @@
 data_gradients/common/factories/__init__.py,sha256=T1XCVwMsm-dF9nNRxZ3ZaZE92jnpskgJIarT66qThM4,140
 data_gradients/common/factories/base_factory.py,sha256=YRXOPJ_3w3ByZDXZWrSkFzCTNsdM4cUjQczv48SrreU,3694
 data_gradients/common/factories/feature_extractors_factory.py,sha256=2P5LKfRpTpDziMg0Wm0CKXm5RCRutWhi9eoy006DmfY,206
 data_gradients/common/factories/list_factory.py,sha256=rz9LZmCxQl1MpV8JA6vLQ42zkkPO-UEw7HTJ1MPQuLQ,569
 data_gradients/common/registry/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/common/registry/registry.py,sha256=nUIHDdh_Ssw5AxwsYy6X4N-hjSZKQ5l8R94ntDP4h4E,1245
 data_gradients/config/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/config/detection.yaml,sha256=YGQz4DSEsEK7o-1QHlnvUTXzoFyH5M8ityssBN4wVs8,674
+data_gradients/config/detection.yaml,sha256=ccFCN0Jgfb_yJCG8ZFCVZEbkAQIy91FDBHttoDseqfk,705
 data_gradients/config/segmentation.yaml,sha256=UMDEtK5z4YP8_c2B4u7hb0db4d-2IzSCwOlOb3x8dSM,756
 data_gradients/config/utils.py,sha256=5DsPja7f00PA8fPieHfkQjHVyq87eMDD9vlvRyzTLBQ,4511
+data_gradients/config/data/__init__.py,sha256=v5G5etJhQHboiotmoFIdnex7_4cPdhPqlk4CXdSJij4,157
+data_gradients/config/data/caching_utils.py,sha256=Gj8xUzWerrpGAQHIBP8no_GOxbRXe61jTKgNynwBT9M,8713
+data_gradients/config/data/data_config.py,sha256=b3Y1bn2kph5gCv_HUCvD0sqD86sKSvzRmAofZxdzO5w,8640
+data_gradients/config/data/questions.py,sha256=db6e95Xsx_hdLtxf0qvOKUrxevHArSPoqXD6oScTP-0,2894
+data_gradients/config/data/typing.py,sha256=3qGGuRVEB4-v1bAT_woKaQvJEwGciB2AAWsed86PVEA,231
 data_gradients/datasets/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/datasets/bdd_dataset.py,sha256=fWulH4IsYwhHCp9N0NtKLqSvAIhB7qknAoTs1cUrna0,2265
-data_gradients/feature_extractors/__init__.py,sha256=4-vJqrH0tHAy7c6oI-QqiSl7ZEg3FaTkbC-QUc5l5cE,1548
+data_gradients/feature_extractors/__init__.py,sha256=r8Yvdgww1akoYBBi_gYP7ME30ST0dkdcVjQmtEhf4Bo,1565
 data_gradients/feature_extractors/abstract_feature_extractor.py,sha256=78r0llgO8ISXUdqMHRFZuoFAqBwJuN3M_zja5ocwSjo,1176
 data_gradients/feature_extractors/features.py,sha256=S3x6en1Q_QBrfhIeshiI9lx1wNt4IK2r6doa_vtulCY,4445
 data_gradients/feature_extractors/utils.py,sha256=PbiZrOr1jCT-2lQcGPFSEVYQCNYgiapFTShc-6h4fwM,1750
 data_gradients/feature_extractors/common/__init__.py,sha256=ZNSdPgTGqnWqT6A_kyHbqiNe72fod8aHbtf6jCG_SEo,325
 data_gradients/feature_extractors/common/heatmap.py,sha256=mKdkNbFLYM3HFsIRnjq8DUIAfLyZW5OxRVXx5E08xXI,2337
-data_gradients/feature_extractors/common/image_average_brightness.py,sha256=Dk6FqQ33xkvAwwesr1aejBUJ8zmHuPky5dRldGGAj0c,2991
-data_gradients/feature_extractors/common/image_color_distribution.py,sha256=OrOEERwNtdJz3mqr4X8qzr29UZpTS5MkJQiAb9pQ4uA,4395
-data_gradients/feature_extractors/common/image_resolution.py,sha256=ewhD5vz_dUtgbqobyZQL6hIBXt4E1MRd-fAxdF81UK8,3312
-data_gradients/feature_extractors/common/sample_visualization.py,sha256=PodiTE8y3ZAg4spWYQK4cBCxibVe0KxY99ZaQcjpT9g,3036
+data_gradients/feature_extractors/common/image_average_brightness.py,sha256=FfG35jpo_C6Y1Gye17LIH7WCaTWcdTKJuo7fzJjPUAk,2971
+data_gradients/feature_extractors/common/image_color_distribution.py,sha256=GclsbzhkVZZSYwAtYUKFROie3q8BoPOlDL-BZCKYEvs,4478
+data_gradients/feature_extractors/common/image_resolution.py,sha256=J0-Ijr0GRtBuGqPWJ3qaiqdbbEWKgNHutxFyaImJmxw,3382
+data_gradients/feature_extractors/common/sample_visualization.py,sha256=U1XLaMF4YMJoaUwcVQLIYQ5WHVpUNV7YJUGDxziMHhI,3219
 data_gradients/feature_extractors/common/summary.py,sha256=rXhYKZ14o8lxdD9eegmyw33covOxJdF8PbEDKCJcR_E,4930
-data_gradients/feature_extractors/object_detection/__init__.py,sha256=GRgTu2hvS9yaQ3yirBgmEUkNOCX1uUqql_EdrMivcII,782
-data_gradients/feature_extractors/object_detection/bounding_boxes_area.py,sha256=3hzAViveNWAfsk4EDmQJ2czK25oDlvFiuGjB-X9xYl8,2554
-data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py,sha256=Z4lAZfyWyOT6WD1mk6040rx5D7K91ECQfjpseKmnpac,5625
-data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py,sha256=M3E0Z9V4IQ8TGCus68hHWFQlbMraautIkJx1jny4Ng8,2186
-data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py,sha256=GATJB-ZNhTRNt_HY_uiE7X9jGYKEK3Cx7rDXKQMSya8,2915
-data_gradients/feature_extractors/object_detection/classes_count.py,sha256=6WDPoxlL2pfE9Uv3NXmhDK1wtvohIQBvBxK-dp1cwhQ,2408
-data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py,sha256=JN4zpK9VY87eb0DteDJHGkVBA9RwQqiML4bN9QMdubI,2049
-data_gradients/feature_extractors/object_detection/classes_per_image_count.py,sha256=ZJhsVY-1KZX-QhT736JXB7LCx62sWiJ4TbxqsYgyKJ4,2567
+data_gradients/feature_extractors/object_detection/__init__.py,sha256=nMd12mIqOwLDASOI1b1NB-gHSnH6zlelt_JmguRjxRU,790
+data_gradients/feature_extractors/object_detection/bounding_boxes_area.py,sha256=3-8hFbk01elE3EOSZNBC_wuZIow-QYOS216DdAcqpGA,2757
+data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py,sha256=Qjp-jkBGSfXGQhuJNIWHp_-N4z33mMSacsT3neRQ93U,5663
+data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py,sha256=7nVQgwi_-s7DfnjPiLI2f5e7LKXsLvtm8GtlC4vycs8,2382
+data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py,sha256=6MUtx1QKMCT-7UHgZR5JmbBPnJrjFY0_1zDDYy5RUAw,2877
+data_gradients/feature_extractors/object_detection/classes_frequency.py,sha256=c9TmN3lHrkngARzxper1UPYSEUAZXubdefU62M-1sQM,2667
+data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py,sha256=qdHDz_-7r5ijqiJl-66UVF9ubW4cvsEo6WSy1ut7_bQ,2815
+data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py,sha256=OJuIW4rnz-_uxBP2v8_hCBx_fVN3wqBN4pOHfdglOuQ,2479
 data_gradients/feature_extractors/object_detection/sample_visualization.py,sha256=M8GWwSBZD9nZZ0fuy9yuX_N9pL0JueeCDMyjppGG4Kg,1971
-data_gradients/feature_extractors/segmentation/__init__.py,sha256=g7KbOSGHpvS_8S7Z2JdJqJ58J-3HgpFAuJIXFwqCxCw,947
-data_gradients/feature_extractors/segmentation/bounding_boxes_area.py,sha256=GzxkQqrmKigW32FVcWibm4dyEpLBdqO7TukFSRRZP6c,2725
-data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py,sha256=bR9DUr5JEDF_nWYDvsFCGtRhB-NppKH-X6cIbCwYM58,2858
-data_gradients/feature_extractors/segmentation/classes_count.py,sha256=TRB2j4WPtRsACZnbRtclIBJsrnxQN6iGzMxhu96Q4gM,2652
-data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py,sha256=MNnJ15w1frjywWQF1QII-b5HbQEuZT9KNBfzv4X0Aag,2110
-data_gradients/feature_extractors/segmentation/classes_per_image_count.py,sha256=GZTzM01mFXOOZi617-T2IXIpBe-JWNHHF-Z63rnqOcg,2850
-data_gradients/feature_extractors/segmentation/components_convexity.py,sha256=o0RL4nnzYCihRUgWfgxqVrj9KEkPNfPHKa7eCIciNNU,2396
-data_gradients/feature_extractors/segmentation/components_erosion.py,sha256=jMguOiNIO1B6qoBbvYT3OcwQadrM--iJHX2ma3s2fDU,3815
-data_gradients/feature_extractors/segmentation/components_per_image_count.py,sha256=BQjMD6QQkrtcxmDIN0cdBfBkLUnp-LXoCaWYq3WNjHM,2204
+data_gradients/feature_extractors/segmentation/__init__.py,sha256=oKssDGr7mXMJKKcbML-rVxTHFGpVN-DnB9Fuqg_zRco,958
+data_gradients/feature_extractors/segmentation/bounding_boxes_area.py,sha256=pQ8op4I1NNO0IG_2LsjVeGKtk8uPak8e5kf7N9aFL_4,2776
+data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py,sha256=_CPsAUVitaD-aHL5Bn3eKxSLI4OkntOyCKZXkMIctlU,2809
+data_gradients/feature_extractors/segmentation/classes_frequency.py,sha256=Q5sUkVhCDBVj5tLyzet0txY9xGBvL9IxYTojLwQJM9c,2754
+data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py,sha256=ut1UsumrnZYviSo3GAoOBpiB1RPkNxeNnhyt-7Xj3oc,2835
+data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py,sha256=pdiT3eoEC-aC8dxwtrLE8FErNovKwd2Q-jFmoWrBTQ8,2526
+data_gradients/feature_extractors/segmentation/component_frequency_per_image.py,sha256=BJShi51MymeFzIgRnjzz2jDU_zGCcvtpYnkZ93UayCU,2363
+data_gradients/feature_extractors/segmentation/components_convexity.py,sha256=x4sNzsmJNusIr070jk85XhtCQ0epG-L4DEQz1b773lE,2303
+data_gradients/feature_extractors/segmentation/components_erosion.py,sha256=oPCI6MlWuFxLSAUN34Nt0JqT7UUrWfMpMWWytmVgkvQ,3808
 data_gradients/feature_extractors/segmentation/sample_visualization.py,sha256=MNLQ24QaW-s0Mb9FnCT2X3YVD3pnBgZRFnyNNU0uNoA,2674
 data_gradients/managers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/managers/abstract_manager.py,sha256=mSkvMiNeJ2b8mdBBKQNRMGWphgRvtg_Zz6Ud4B2tiAI,12060
-data_gradients/managers/detection_manager.py,sha256=UJphuQ7JfipEMlWSd50N9wcVBPr80hnAsdeRLcHWtT4,4519
-data_gradients/managers/segmentation_manager.py,sha256=0CqAjkWMU_IsDyaDb3NlzitpkBF3peyLVGqdG_IImBI,4864
+data_gradients/managers/abstract_manager.py,sha256=4lUwGaJpc1Vt-aI4v8b4pWY--0DzrukDuHzMrNdNTKs,11429
+data_gradients/managers/detection_manager.py,sha256=srKW2dnH6JamOiUycC4Nd1E86fWNigSMzyo5h8n-B_M,5498
+data_gradients/managers/segmentation_manager.py,sha256=J8kEePZ7LKCPctO7iwbPgcYctnpkPAeG8ojXtoopWz8,5401
 data_gradients/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/utils/detection.py,sha256=9_Fd62F5ejnT8HKLLq55-ZhQw7hNjXmtVGLpjweUDk8,797
+data_gradients/utils/detection.py,sha256=LimFh8QXLPGw_icExR-aF5uaVqhI4ujs6h-yDgTXLcw,2760
 data_gradients/utils/image_processing.py,sha256=5E2qUPoXYCFIuTgQ9IzewDeZ-Ay0u10pyQuHhaCg7Ko,1052
 data_gradients/utils/pdf_writer.py,sha256=hsL_tTCaK_3xY8YpcuG9PZhWIF9AID6IWrw-DCVSKHc,2550
-data_gradients/utils/utils.py,sha256=EOEU_lD2tmxhH3AD2dAgeFYjGaf2lBZ9-USt46QAXCc,3701
+data_gradients/utils/summary_writer.py,sha256=EtlMOkaXoBLd2w9sXBeuXlNdez01e6lzcuNzlJ4-pSE,3705
+data_gradients/utils/utils.py,sha256=N5WSyuVBY2Sxm4CdEXF8OAPK0cR1-gWmhbctUr_3Wk0,2561
 data_gradients/utils/common/__init__.py,sha256=UdVptbzrZj_KxShVC3KQbzM1zLTAPDuKk0dE6KmWUiE,169
 data_gradients/utils/data_classes/__init__.py,sha256=4zkOKnF1rycWi34SXthwkqWeI1CzjTfTAWJUnLmxvQE,249
 data_gradients/utils/data_classes/contour.py,sha256=kqrU1QLpV6smKGkeXk_-sfRYEq6sHkoHXx4i3TMYvTY,260
 data_gradients/utils/data_classes/data_samples.py,sha256=0g9P6BFKOj4Tk0Fdc3er98C42qUrOescFxLpFH2SmpI,3060
 data_gradients/utils/data_classes/extractor_results.py,sha256=f_49UZoq9SfwV6XlqeVNuztLG_PIJbTW6WX3Wz9Bv2o,3975
 data_gradients/visualize/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/visualize/detection.py,sha256=d25Er47jQGWO7xhI4zyPptu6HEtTF0eq-CSJvZ_D06Q,5437
 data_gradients/visualize/images.py,sha256=FD9-CtzJ5UsnB5gw3jzlO_etrbFLaWGjn_eeZBxXTLw,5092
 data_gradients/visualize/plot_options.py,sha256=wuEQ8lkbyjb3OWnUxf-KU8M8aQm7EP01p-tVcDjFTXs,11756
 data_gradients/visualize/seaborn_renderer.py,sha256=k7W6innVtGdArVZ4AKEYr92y-ccJiTiFv9O283meBGU,16700
-data_gradients-0.0.9.dist-info/LICENSE.md,sha256=jWCBQN-JmCX0hwvn3MqItDj18W3riP4zda31ncMkcfA,11341
-data_gradients-0.0.9.dist-info/METADATA,sha256=wiHq3WYYM_y7I5KAJ_SSsamQSd3j3gV4TJoiBGIC4Qc,8393
-data_gradients-0.0.9.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-data_gradients-0.0.9.dist-info/top_level.txt,sha256=zATNcRYA5TY95dQFVVrFbhJndVZotUYDEM2v3kL_84E,15
-data_gradients-0.0.9.dist-info/RECORD,,
+data_gradients-0.1.1.dist-info/LICENSE.md,sha256=jWCBQN-JmCX0hwvn3MqItDj18W3riP4zda31ncMkcfA,11341
+data_gradients-0.1.1.dist-info/METADATA,sha256=5B88JPCkvPeXXBPU4fHezwjjYXbNiwKXMnULtuoxiiM,9226
+data_gradients-0.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+data_gradients-0.1.1.dist-info/top_level.txt,sha256=zATNcRYA5TY95dQFVVrFbhJndVZotUYDEM2v3kL_84E,15
+data_gradients-0.1.1.dist-info/RECORD,,
```


# Comparing `tmp/lyscripts-0.6.9.tar.gz` & `tmp/lyscripts-0.7.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "lyscripts-0.6.9.tar", last modified: Wed Jun 21 08:33:34 2023, max compression
+gzip compressed data, was "lyscripts-0.7.0.tar", last modified: Mon Jun 26 14:39:51 2023, max compression
```

## Comparing `lyscripts-0.6.9.tar` & `lyscripts-0.7.0.tar`

### file list

```diff
@@ -1,80 +1,81 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.435683 lyscripts-0.6.9/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.427682 lyscripts-0.6.9/.chglog/
--rwxr-xr-x   0 runner    (1001) docker     (123)     1043 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.chglog/CHANGELOG.tpl.md
--rwxr-xr-x   0 runner    (1001) docker     (123)      814 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.chglog/config.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.423683 lyscripts-0.6.9/.github/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.427682 lyscripts-0.6.9/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (123)      667 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.github/workflows/build.yml
--rw-r--r--   0 runner    (1001) docker     (123)      791 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.github/workflows/docs.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1205 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.github/workflows/tests.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1860 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)     1611 2023-06-21 08:33:17.000000 lyscripts-0.6.9/.pre-commit-config.yaml
--rw-r--r--   0 runner    (1001) docker     (123)    10722 2023-06-21 08:33:17.000000 lyscripts-0.6.9/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (123)     1069 2023-06-21 08:33:17.000000 lyscripts-0.6.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     7768 2023-06-21 08:33:34.435683 lyscripts-0.6.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     7214 2023-06-21 08:33:17.000000 lyscripts-0.6.9/README.md
--rw-r--r--   0 runner    (1001) docker     (123)    18415 2023-06-21 08:33:17.000000 lyscripts-0.6.9/favicon.png
--rw-r--r--   0 runner    (1001) docker     (123)   105407 2023-06-21 08:33:17.000000 lyscripts-0.6.9/github-social-card.png
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.427682 lyscripts-0.6.9/lyscripts/
--rw-r--r--   0 runner    (1001) docker     (123)     3319 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      268 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)      160 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts/_version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.427682 lyscripts-0.6.9/lyscripts/app/
--rw-r--r--   0 runner    (1001) docker     (123)      617 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/app/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10498 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/app/prevalence.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.431683 lyscripts-0.6.9/lyscripts/data/
--rw-r--r--   0 runner    (1001) docker     (123)     1759 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1024 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4665 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/clean.py
--rw-r--r--   0 runner    (1001) docker     (123)    13440 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/enhance.py
--rw-r--r--   0 runner    (1001) docker     (123)     5525 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/generate.py
--rw-r--r--   0 runner    (1001) docker     (123)     2808 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/join.py
--rw-r--r--   0 runner    (1001) docker     (123)    14049 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/lyproxify.py
--rw-r--r--   0 runner    (1001) docker     (123)     3802 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/split.py
--rw-r--r--   0 runner    (1001) docker     (123)     1124 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/data/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8517 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/evaluate.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.431683 lyscripts-0.6.9/lyscripts/plot/
--rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/plot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      878 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/plot/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5504 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/plot/corner.py
--rw-r--r--   0 runner    (1001) docker     (123)     4230 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/plot/histograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/plot/thermo_int.py
--rw-r--r--   0 runner    (1001) docker     (123)    10042 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/plot/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.431683 lyscripts-0.6.9/lyscripts/predict/
--rw-r--r--   0 runner    (1001) docker     (123)     1132 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/predict/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      794 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/predict/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14134 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/predict/prevalences.py
--rw-r--r--   0 runner    (1001) docker     (123)     6647 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/predict/risks.py
--rw-r--r--   0 runner    (1001) docker     (123)     1956 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/predict/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    18888 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/sample.py
--rw-r--r--   0 runner    (1001) docker     (123)     4859 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/temp_schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)    21030 2023-06-21 08:33:17.000000 lyscripts-0.6.9/lyscripts/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.427682 lyscripts-0.6.9/lyscripts.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     7768 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)      193 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       10 2023-06-21 08:33:34.000000 lyscripts-0.6.9/lyscripts.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-06-21 08:33:17.000000 lyscripts-0.6.9/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-21 08:33:34.435683 lyscripts-0.6.9/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-21 08:33:17.000000 lyscripts-0.6.9/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.431683 lyscripts-0.6.9/tests/
--rw-r--r--   0 runner    (1001) docker     (123)     2433 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/_sample.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.431683 lyscripts-0.6.9/tests/plot/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.431683 lyscripts-0.6.9/tests/plot/baseline/
--rw-r--r--   0 runner    (1001) docker     (123)    12265 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/plot/baseline/sine.png
--rw-r--r--   0 runner    (1001) docker     (123)    12916 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/plot/baseline/sine.svg
--rw-r--r--   0 runner    (1001) docker     (123)    12629 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/plot/baseline/sine_svg.png
--rw-r--r--   0 runner    (1001) docker     (123)    36976 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/plot/baseline/test_draw.png
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.435683 lyscripts-0.6.9/tests/plot/data/
--rw-r--r--   0 runner    (1001) docker     (123)    86144 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/plot/data/beta_samples.hdf5
--rw-r--r--   0 runner    (1001) docker     (123)     8088 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/plot/plot_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-21 08:33:34.435683 lyscripts-0.6.9/tests/predict/
--rw-r--r--   0 runner    (1001) docker     (123)     1156 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/predict/predict_utils_test.py
--rw-r--r--   0 runner    (1001) docker     (123)     3291 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/predict/prevalences_test.py
--rw-r--r--   0 runner    (1001) docker     (123)      504 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/run_doctests.py
--rw-r--r--   0 runner    (1001) docker     (123)       31 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/test.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      491 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/test_data.csv
--rw-r--r--   0 runner    (1001) docker     (123)     2831 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/test_params.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1161 2023-06-21 08:33:17.000000 lyscripts-0.6.9/tests/utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.803161 lyscripts-0.7.0/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.795160 lyscripts-0.7.0/.chglog/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1043 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.chglog/CHANGELOG.tpl.md
+-rwxr-xr-x   0 runner    (1001) docker     (123)      814 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.chglog/config.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.791160 lyscripts-0.7.0/.github/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.795160 lyscripts-0.7.0/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)      667 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.github/workflows/build.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      791 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.github/workflows/docs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1205 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.github/workflows/tests.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1860 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)     1611 2023-06-26 14:39:33.000000 lyscripts-0.7.0/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)    11220 2023-06-26 14:39:33.000000 lyscripts-0.7.0/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (123)     1069 2023-06-26 14:39:33.000000 lyscripts-0.7.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     7768 2023-06-26 14:39:51.799160 lyscripts-0.7.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     7214 2023-06-26 14:39:33.000000 lyscripts-0.7.0/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)    18415 2023-06-26 14:39:33.000000 lyscripts-0.7.0/favicon.png
+-rw-r--r--   0 runner    (1001) docker     (123)   105407 2023-06-26 14:39:33.000000 lyscripts-0.7.0/github-social-card.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.795160 lyscripts-0.7.0/lyscripts/
+-rw-r--r--   0 runner    (1001) docker     (123)     3838 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      268 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      160 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts/_version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.795160 lyscripts-0.7.0/lyscripts/app/
+-rw-r--r--   0 runner    (1001) docker     (123)      617 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/app/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10498 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/app/prevalence.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/lyscripts/data/
+-rw-r--r--   0 runner    (1001) docker     (123)     1759 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1024 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4723 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/clean.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13388 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/enhance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5409 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/generate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/join.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13447 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/lyproxify.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3636 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/split.py
+-rw-r--r--   0 runner    (1001) docker     (123)      846 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/data/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5752 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8604 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/evaluate.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/lyscripts/plot/
+-rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/plot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      878 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/plot/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5455 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/plot/corner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4114 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/plot/histograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5937 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/plot/thermo_int.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9937 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/plot/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/lyscripts/predict/
+-rw-r--r--   0 runner    (1001) docker     (123)     1132 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/predict/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      794 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/predict/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14387 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/predict/prevalences.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6860 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/predict/risks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1956 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/predict/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18922 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/sample.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4822 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/temp_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14643 2023-06-26 14:39:33.000000 lyscripts-0.7.0/lyscripts/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.795160 lyscripts-0.7.0/lyscripts.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     7768 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       45 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      193 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       10 2023-06-26 14:39:51.000000 lyscripts-0.7.0/lyscripts.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-06-26 14:39:33.000000 lyscripts-0.7.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-26 14:39:51.803161 lyscripts-0.7.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-26 14:39:33.000000 lyscripts-0.7.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)     2433 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/_sample.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/tests/plot/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/tests/plot/baseline/
+-rw-r--r--   0 runner    (1001) docker     (123)    12265 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/plot/baseline/sine.png
+-rw-r--r--   0 runner    (1001) docker     (123)    12916 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/plot/baseline/sine.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    12629 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/plot/baseline/sine_svg.png
+-rw-r--r--   0 runner    (1001) docker     (123)    36976 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/plot/baseline/test_draw.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/tests/plot/data/
+-rw-r--r--   0 runner    (1001) docker     (123)    86144 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/plot/data/beta_samples.hdf5
+-rw-r--r--   0 runner    (1001) docker     (123)     8094 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/plot/plot_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-26 14:39:51.799160 lyscripts-0.7.0/tests/predict/
+-rw-r--r--   0 runner    (1001) docker     (123)     1156 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/predict/predict_utils_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3291 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/predict/prevalences_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)      504 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/run_doctests.py
+-rw-r--r--   0 runner    (1001) docker     (123)       31 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/test.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      491 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/test_data.csv
+-rw-r--r--   0 runner    (1001) docker     (123)     2831 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/test_params.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1161 2023-06-26 14:39:33.000000 lyscripts-0.7.0/tests/utils_test.py
```

### Comparing `lyscripts-0.6.9/.chglog/CHANGELOG.tpl.md` & `lyscripts-0.7.0/.chglog/CHANGELOG.tpl.md`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/.chglog/config.yml` & `lyscripts-0.7.0/.chglog/config.yml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/.github/workflows/build.yml` & `lyscripts-0.7.0/.github/workflows/build.yml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/.github/workflows/docs.yml` & `lyscripts-0.7.0/.github/workflows/docs.yml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/.github/workflows/tests.yml` & `lyscripts-0.7.0/.github/workflows/tests.yml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/.gitignore` & `lyscripts-0.7.0/.gitignore`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/.pre-commit-config.yaml` & `lyscripts-0.7.0/.pre-commit-config.yaml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/CHANGELOG.md` & `lyscripts-0.7.0/CHANGELOG.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+<a name="0.7.0"></a>
+## [0.7.0] - 2023-06-26
+
+### Bug Fixes
+- add modalities from params in synthetic data generation
+
+### Features
+- add extensible & versatile logging decorator
+- add `--log-level` option to top-level lyscripts command
+- add log-level to `log_state` decorator
+
+### Other
+- all commands now use the logging library for status updates/ouputs. This fixes [#2].
+
+
 <a name="0.6.9"></a>
 ## [0.6.9] - 2023-06-21
 
 ### Bug Fixes
 - change the indentation length in the generated markdown data documentation to 4 spaces. Fixes [#41].
 
 
@@ -260,15 +275,16 @@
 - set up git-chglog for creating changelogs
 - add pre-commit hook to check commit msg
 
 
 <a name="0.5.3"></a>
 ## [0.5.3] - 2022-08-22
 
-[Unreleased]: https://github.com/rmnldwg/lyscripts/compare/0.6.9...HEAD
+[Unreleased]: https://github.com/rmnldwg/lyscripts/compare/0.7.0...HEAD
+[0.7.0]: https://github.com/rmnldwg/lyscripts/compare/0.6.9...0.7.0
 [0.6.9]: https://github.com/rmnldwg/lyscripts/compare/0.6.8...0.6.9
 [0.6.8]: https://github.com/rmnldwg/lyscripts/compare/0.6.7...0.6.8
 [0.6.7]: https://github.com/rmnldwg/lyscripts/compare/0.6.6...0.6.7
 [0.6.6]: https://github.com/rmnldwg/lyscripts/compare/0.6.5...0.6.6
 [0.6.5]: https://github.com/rmnldwg/lyscripts/compare/0.6.4...0.6.5
 [0.6.4]: https://github.com/rmnldwg/lyscripts/compare/0.6.3...0.6.4
 [0.6.3]: https://github.com/rmnldwg/lyscripts/compare/0.6.2...0.6.3
@@ -281,14 +297,15 @@
 [0.5.8]: https://github.com/rmnldwg/lyscripts/compare/0.5.7...0.5.8
 [0.5.7]: https://github.com/rmnldwg/lyscripts/compare/0.5.6...0.5.7
 [0.5.6]: https://github.com/rmnldwg/lyscripts/compare/0.5.5...0.5.6
 [0.5.5]: https://github.com/rmnldwg/lyscripts/compare/0.5.4...0.5.5
 [0.5.4]: https://github.com/rmnldwg/lyscripts/compare/0.5.3...0.5.4
 [0.5.3]: https://github.com/rmnldwg/lyscripts/compare/0.5.2...0.5.3
 
+[#2]: https://github.com/rmnldwg/lyscripts/issues/2
 [#5]: https://github.com/rmnldwg/lyscripts/issues/5
 [#8]: https://github.com/rmnldwg/lyscripts/issues/8
 [#11]: https://github.com/rmnldwg/lyscripts/issues/11
 [#13]: https://github.com/rmnldwg/lyscripts/issues/13
 [#15]: https://github.com/rmnldwg/lyscripts/issues/15
 [#16]: https://github.com/rmnldwg/lyscripts/issues/16
 [#17]: https://github.com/rmnldwg/lyscripts/issues/17
```

### Comparing `lyscripts-0.6.9/LICENSE` & `lyscripts-0.7.0/LICENSE`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/PKG-INFO` & `lyscripts-0.7.0/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lyscripts
-Version: 0.6.9
+Version: 0.7.0
 Summary: Package containing scripts used in lynference pipelines
 Author-email: Roman Ludwig <roman.ludwig@usz.ch>
 License: MIT
 Project-URL: source, https://github.com/rmnldwg/lyscripts
 Project-URL: documentation, https://rmnldwg.github.io/lyscripts
 Keywords: scripts,lymph,inference
 Classifier: Programming Language :: Python :: 3
```

### Comparing `lyscripts-0.6.9/README.md` & `lyscripts-0.7.0/README.md`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/favicon.png` & `lyscripts-0.7.0/favicon.png`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/github-social-card.png` & `lyscripts-0.7.0/github-social-card.png`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/__init__.py` & `lyscripts-0.7.0/lyscripts/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,31 +1,36 @@
 """
 .. include:: ../README.md
 """
 import argparse
+import logging
 import re
 
 from rich.containers import Lines
+from rich.logging import RichHandler
 from rich.text import Text
 from rich_argparse import RichHelpFormatter
 
+from lyscripts import app, data, evaluate, plot, predict, sample, temp_schedule
+from lyscripts._version import version
 from lyscripts.utils import report
 
-from . import app, data, evaluate, plot, predict, sample, temp_schedule
-from ._version import version
-
 __version__ = version
 __description__ = "Package containing scripts used in lynference pipelines"
 __author__ = "Roman Ludwig"
 __email__ = "roman.ludwig@usz.ch"
 __uri__ = "https://github.com/rmnldwg/lyscripts"
 
 # nopycln: file
 
 
+logger = logging.getLogger(__name__)
+logger.addHandler(logging.NullHandler())
+
+
 class RichDefaultHelpFormatter(
     RichHelpFormatter,
     argparse.ArgumentDefaultsHelpFormatter,
 ):
     """
     Empty class that combines the functionality of displaying the default value with
     the beauty of the `rich` formatter
@@ -44,35 +49,38 @@
             for line in wrapped_par:
                 text_lines.append(line)
 
             text_lines.append(text_cls("\n"))
 
         return text_cls("\n").join(indent + line for line in text_lines) + "\n\n"
 
+
 RichDefaultHelpFormatter.styles["argparse.syntax"] = "red"
 RichDefaultHelpFormatter.styles["argparse.formula"] = "green"
 RichDefaultHelpFormatter.highlights.append(
     r"\$(?P<formula>[^$]*)\$"
 )
 RichDefaultHelpFormatter.styles["argparse.bold"] = "bold"
 RichDefaultHelpFormatter.highlights.append(
     r"\*(?P<bold>[^*]*)\*"
 )
 RichDefaultHelpFormatter.styles["argparse.italic"] = "italic"
 RichDefaultHelpFormatter.highlights.append(
     r"_(?P<italic>[^_]*)_"
 )
 
+
 def exit_cli(args: argparse.Namespace):
     """Exit the cmd line tool"""
     if args.version:
         report.print("lyscripts ", __version__)
     else:
         report.print("No command chosen. Exiting...")
 
+
 def main():
     """
     Utility for performing common tasks w.r.t. the inference and prediction tasks one
     can use the `lymph` package for.
     """
     parser = argparse.ArgumentParser(
         prog="lyscripts",
@@ -80,22 +88,36 @@
         formatter_class=RichDefaultHelpFormatter,
     )
     parser.set_defaults(run_main=exit_cli)
     parser.add_argument(
         "-v", "--version", action="store_true",
         help="Display the version of lyscripts"
     )
+    parser.add_argument(
+        "--log-level", default="INFO",
+        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
+    )
 
     subparsers = parser.add_subparsers()
 
     # the individual scripts add `ArgumentParser` instances and their arguments to
     # this `subparsers` object
     app._add_parser(subparsers, help_formatter=parser.formatter_class)
     data._add_parser(subparsers, help_formatter=parser.formatter_class)
     evaluate._add_parser(subparsers, help_formatter=parser.formatter_class)
     plot._add_parser(subparsers, help_formatter=parser.formatter_class)
     predict._add_parser(subparsers, help_formatter=parser.formatter_class)
     sample._add_parser(subparsers, help_formatter=parser.formatter_class)
     temp_schedule._add_parser(subparsers, help_formatter=parser.formatter_class)
 
     args = parser.parse_args()
+
+    handler = RichHandler(
+        console=report,
+        show_time=False,
+        markup=True,
+    )
+    handler.setFormatter(logging.Formatter("%(message)s"))
+    logger.addHandler(handler)
+    logger.setLevel(args.log_level)
+
     args.run_main(args)
```

### Comparing `lyscripts-0.6.9/lyscripts/app/__init__.py` & `lyscripts-0.7.0/lyscripts/app/__init__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/app/prevalence.py` & `lyscripts-0.7.0/lyscripts/app/prevalence.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/data/__init__.py` & `lyscripts-0.7.0/lyscripts/data/__init__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/data/__main__.py` & `lyscripts-0.7.0/lyscripts/data/__main__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/data/clean.py` & `lyscripts-0.7.0/lyscripts/data/clean.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,29 @@
 """
 Transform the enhanced lyDATA CSV files into a format that can be used by the lymph
 model using this package's utilities.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 import warnings
 from pathlib import Path
 from typing import Any, Dict, Optional
 
 import pandas as pd
 
 from lyscripts.data.utils import load_csv_table, save_table_to_csv
-from lyscripts.utils import load_yaml_params, report
+from lyscripts.utils import load_yaml_params
 
 warnings.simplefilter(action="ignore", category=FutureWarning)
 
 
+logger = logging.getLogger(__name__)
+
+
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
     """
@@ -125,23 +130,21 @@
 
     OPTIONAL ARGUMENTS:
       -h, --help            show this help message and exit
       -p, --params PARAMS   Path to the params file to use for the transformation.
                             (default: ./params.yaml)
     ```
     """
-    params = load_yaml_params(args.params)
-
-    input_table = load_csv_table(args.input, header_row=[0,1,2])
+    params = load_yaml_params(args.params, logger=logger)
+    input_table = load_csv_table(args.input, header_row=[0,1,2], logger=logger)
 
-    with report.status("Prepare table for use with lymph model..."):
-        lymph_table = lyprox_to_lymph(input_table, class_name=params["model"]["class"])
-        report.success("Prepared table for use with lymph model.")
+    lymph_table = lyprox_to_lymph(input_table, class_name=params["model"]["class"])
+    logger.info("Prepared table for use with lymph model.")
 
-    save_table_to_csv(args.output, lymph_table)
+    save_table_to_csv(args.output, lymph_table, logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/data/enhance.py` & `lyscripts-0.7.0/lyscripts/data/enhance.py`

 * *Files 3% similar despite different names*

```diff
@@ -7,29 +7,36 @@
 their respective sensitivity and specificity.
 
 2. Complete sub- & super-level fields. This means that if a dataset reports LNLs IIa
 and IIb separately, this script will add the column for LNL II and fill it with the
 correct values. Conversely, if e.g. LNL II is reported to be healthy, we can assume
 the sublevels IIa and IIb would have been reported as healthy, too.
 """
+# pylint: disable=singleton-comparison,logging-fstring-interpolation
 import argparse
+import logging
 import warnings
 from functools import lru_cache
 from pathlib import Path
 from typing import List, Optional, Tuple
 
 import numpy as np
 import pandas as pd
-from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn
 
 from lyscripts.data.utils import load_csv_table, save_table_to_csv
-from lyscripts.utils import get_modalities_subset, load_yaml_params, report
+from lyscripts.decorators import log_state
+from lyscripts.utils import (
+    CustomProgress,
+    get_modalities_subset,
+    load_yaml_params,
+    report,
+)
 
 warnings.simplefilter(action="ignore", category=pd.errors.PerformanceWarning)
-# pylint: disable=singleton-comparison
+logger = logging.getLogger(__name__)
 
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
@@ -94,14 +101,18 @@
     """
     has_sublvls = all(lnl+sub in data_frame for sub in sub_ids)
     if not has_sublvls:
         return None
     return data_frame[[lnl+sub for sub in sub_ids]].values
 
 
+@log_state(
+    success_msg="Inferred super-level's involvement from sub-levels",
+    logger=logger,
+)
 def infer_superlvl_from_sublvls(
     table: pd.DataFrame,
     modalities: List[str],
     lnls_with_sub: List[str],
     sublvls: Optional[List[str]] = None,
 ) -> pd.DataFrame:
     """
@@ -265,20 +276,14 @@
 
 CONSENSUS_FUNCS = {
     "max_llh": maxllh_consensus,
     "rank": rank_consensus,
     "logic_or": lambda obs, *_args, **_kwargs: or_consensus(obs),
     "logic_and": lambda obs, *_args, **_kwargs: and_consensus(obs),
 }
-PROGRESS = Progress(
-    SpinnerColumn(),
-    *Progress.get_default_columns(),
-    TimeElapsedColumn(),
-    console=report,
-)
 
 
 def main(args: argparse.Namespace):
     """
     Below is the help output (call with `lyscripts enhance --help`)
 
     ```
@@ -320,78 +325,72 @@
                             Indicate what kinds of sublevels exist (default: ['a',
                             'b'])
       --lnls-with-sub LNLS_WITH_SUB [LNLS_WITH_SUB ...]
                             List of LNLs where sublevel reporting has been performed
                             or is common (default: ['I', 'II', 'V'])
     ```
     """
-    input_table = load_csv_table(args.input, header_row=[0,1,2])
+    input_table = load_csv_table(args.input, header_row=[0,1,2], logger=logger)
+    params = load_yaml_params(args.params, logger=logger)
 
-    params = load_yaml_params(args.params)
     modalities = get_modalities_subset(
         defined_modalities=params["modalities"],
         selection=args.modalities,
     )
 
     available_mod_keys = set(
         input_table.columns.get_level_values(0)
     ).intersection(
         modalities.keys()
     )
     available_mods = {key: modalities[key] for key in available_mod_keys}
     lnl_union = set().union(
         *[input_table[mod,"ipsi"].columns for mod in available_mod_keys]
     )
-
     consensus = pd.DataFrame(
         index=input_table.index,
         columns=pd.MultiIndex.from_product(
             [args.consensus, ["ipsi", "contra"], lnl_union]
         )
     )
 
-    with PROGRESS:
-        enhance_task = PROGRESS.add_task(
-            description="Compute consensus of modalities...",
+    with CustomProgress(console=report) as report_progress:
+        enhance_task = report_progress.add_task(
+            description=f"Compute {args.consensus} consensus of modalities...",
             total=2 * len(input_table),
         )
         for side in ["ipsi", "contra"]:
             # go through patients and LNLs and compute consensus for each
             for p,patient in input_table.iterrows():
                 for lnl in lnl_union:
                     observations = get_lnl_observations(
                         patient, side, lnl, available_mods
                     )
                     for cons in args.consensus:
                         consensus[cons, side, lnl].iloc[p] = CONSENSUS_FUNCS[cons](
                             observations, available_mods.values()
                         )
-                PROGRESS.update(enhance_task, advance=1)
+                report_progress.update(enhance_task, advance=1)
         table_with_consensus = input_table.join(consensus)
 
-    report.success(
-        "Computed consensus of observations according to "
-        f"the methods {args.consensus}"
-    )
 
-    with report.status("Fixing sub- & super level fields..."):
-        data_modalities = set(
-            table_with_consensus.columns.get_level_values(0)
-        ).intersection(
-            [*modalities.keys(), *args.consensus]
-        )
-        consensus_and_fixed_sublvlvs = infer_superlvl_from_sublvls(
-            table_with_consensus,
-            data_modalities,
-            lnls_with_sub=args.lnls_with_sub,
-            sublvls=args.sublvls,
-        )
-        report.success("Fixed sub- & super level fields.")
+    data_modalities = set(
+        table_with_consensus.columns.get_level_values(0)
+    ).intersection(
+        [*modalities.keys(), *args.consensus]
+    )
+    consensus_and_fixed_sublvlvs = infer_superlvl_from_sublvls(
+        table_with_consensus,
+        data_modalities,
+        lnls_with_sub=args.lnls_with_sub,
+        sublvls=args.sublvls,
+    )
+    logger.info("Fixed sub- & super level fields.")
 
-    save_table_to_csv(args.output, consensus_and_fixed_sublvlvs)
+    save_table_to_csv(args.output, consensus_and_fixed_sublvlvs, logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/data/generate.py` & `lyscripts-0.7.0/lyscripts/data/generate.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,22 @@
 """
 Generate synthetic patient data for testing and validation purposes.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 
 import emcee
 import numpy as np
 
 from lyscripts.data.utils import save_table_to_csv
-from lyscripts.utils import create_model_from_config, load_yaml_params, report
+from lyscripts.utils import create_model_from_config, load_yaml_params
+
+logger = logging.getLogger(__name__)
 
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
@@ -90,64 +94,65 @@
       --load-theta {mean,max_llh}
                             Use either the mean or the maximum likelihood estimate
                             from drawn samples (default: mean)
       --samples SAMPLES     Path to the samples if a method to load them was chosen
                             (default: ./models/samples.hdf5)
     ```
     """
-    params = load_yaml_params(args.params)
-    model = create_model_from_config(params)
+    params = load_yaml_params(args.params, logger=logger)
+    model = create_model_from_config(params, logger=logger)
     ndim = len(model.spread_probs) + model.diag_time_dists.num_parametric
 
     if args.set_theta is not None:
-        with report.status("Assign given parameters to model..."):
-            if len(args.set_theta) != ndim:
-                raise ValueError(
-                    f"Model takes {ndim} parameters, but{len(args.set_theta)} were provided"
-                )
-            theta = np.array(args.set_theta)
-            model.check_and_assign(theta)
-            report.print(theta)
-            report.success("Assigned given parameters to model")
-    else:
-        with report.status(f"Load samples and choose their {args.load_theta} value..."):
-            backend = emcee.backends.HDFBackend(
-                args.samples,
-                read_only=True,
-                name="mcmc"
+        if len(args.set_theta) != ndim:
+            raise ValueError(
+                f"Model takes {ndim} parameters, but{len(args.set_theta)} were provided"
             )
-            chain = backend.get_chain(flat=True)
-            log_probs = backend.get_blobs(flat=True)
+        theta = np.array(args.set_theta)
+        model.check_and_assign(theta)
+        logger.debug(theta)
+        logger.info("Assigned given parameters to model")
+
+    else:
+        backend = emcee.backends.HDFBackend(
+            args.samples,
+            read_only=True,
+            name="mcmc"
+        )
+        chain = backend.get_chain(flat=True)
+        log_probs = backend.get_blobs(flat=True)
 
-            if args.load_theta == "mean":
-                theta = np.mean(chain, axis=0)
-            elif args.load_theta == "max_llh":
-                max_llh_idx = np.argmax(log_probs)
-                theta = chain[max_llh_idx]
-            else:
-                raise ValueError("Only 'mean' and 'max_llh' are supported")
-
-            model.check_and_assign(theta)
-            report.print(theta)
-            report.success(f"Loaded samples and assigned their {args.load_theta} value")
-
-    with report.status(f"Generate synthetic data of {args.num} patients..."):
-        synthetic_data = model.generate_dataset(
-            num_patients=args.num,
-            stage_dist=params["synthetic"]["t_stages_dist"],
-            ext_prob=params["synthetic"]["midline_ext_prob"],
+        if args.load_theta == "mean":
+            theta = np.mean(chain, axis=0)
+        elif args.load_theta == "max_llh":
+            max_llh_idx = np.argmax(log_probs)
+            theta = chain[max_llh_idx]
+        else:
+            raise ValueError("Only 'mean' and 'max_llh' are supported")
+
+        model.check_and_assign(theta)
+        logger.debug(theta)
+        logger.info(f"Loaded samples and assigned their {args.load_theta} value")
+
+    model.modalities = params["synthetic"]["modalities"]
+    logger.debug(f"Assigned modalities for synthetic data: {model.modalities}")
+
+    synthetic_data = model.generate_dataset(
+        num_patients=args.num,
+        stage_dist=params["synthetic"]["t_stages_dist"],
+        ext_prob=params["synthetic"]["midline_ext_prob"],
+    )
+    if len(synthetic_data) != args.num:
+        logger.error(
+            f"Length of generated data ({len(synthetic_data)}) does not match "
+            f"target length ({args.num})"
         )
-        if len(synthetic_data) != args.num:
-            raise RuntimeError(
-                f"Length of generated data ({len(synthetic_data)}) does not match "
-                f"target length ({args.num})"
-            )
-        report.success(f"Created synthetic data of {args.num} patients.")
+    logger.info(f"Created synthetic data of {args.num} patients.")
 
-    save_table_to_csv(args.output, synthetic_data)
+    save_table_to_csv(args.output, synthetic_data, logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/data/join.py` & `lyscripts-0.7.0/lyscripts/data/join.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 """
 Join datasets from different sources (but of the same format) into one.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 import warnings
 from pathlib import Path
 
 import pandas as pd
 
 from lyscripts.data.utils import load_csv_table, save_table_to_csv
-from lyscripts.utils import report
 
 warnings.simplefilter(action="ignore", category=FutureWarning)
 
 
+logger = logging.getLogger(__name__)
+
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
@@ -69,23 +72,23 @@
                             concatenate. (default: None)
       -o, --output OUTPUT   Location to store the concatenated CSV file. (default:
                             None)
     ```
     """
     concatenated_table = pd.DataFrame()
     for input_path in args.inputs:
-        input_table = load_csv_table(input_path, header_row=[0,1,2])
+        input_table = load_csv_table(input_path, header_row=[0,1,2], logger=logger)
         concatenated_table = pd.concat(
             [concatenated_table, input_table],
             ignore_index=True
         )
-        report.print(f"+ concatenated data from {input_path}")
-    report.success(f"Read & concatenated all {len(args.inputs)} CSV files")
+        logger.info(f"+ concatenated data from {input_path}")
+    logger.info(f"Read & concatenated all {len(args.inputs)} CSV files")
 
-    save_table_to_csv(args.output, concatenated_table)
+    save_table_to_csv(args.output, concatenated_table, logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/data/lyproxify.py` & `lyscripts-0.7.0/lyscripts/data/lyproxify.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,34 +3,34 @@
 
 To do so, it needs a dictionary that defines a mapping from raw columns to the LyProX
 style data format. See the documentation of the `transform_to_lyprox` function for
 more information.
 
 [LyProX]: https://lyprox.org
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
 import importlib.util
+import logging
 import warnings
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
 import pandas as pd
 
 from lyscripts.data.utils import load_csv_table, save_table_to_csv
-from lyscripts.utils import (
-    delete_private_keys,
-    flatten,
-    raise_if_args_none,
-    report,
-    report_state,
-)
+from lyscripts.decorators import log_state
+from lyscripts.utils import delete_private_keys, flatten
 
 warnings.simplefilter(action="ignore", category=FutureWarning)
 
 
+logger = logging.getLogger(__name__)
+
+
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
     """
@@ -171,22 +171,17 @@
                 i += 1
 
             md_docs += generate_markdown_docs(value, depth + 1, indent_len)
 
     return md_docs
 
 
-@report_state(
-    status_msg="Transform raw data to LyProX style table...",
-    success_msg="Transformed raw data to LyProX style table.",
-    stop_on_exc=True
-)
-@raise_if_args_none(
-    message="Must provide raw data and mapping instruction module",
-    level="warning",
+@log_state(
+    success_msg="Transformed raw data to LyProX style table",
+    logger=logger,
 )
 def transform_to_lyprox(
     raw: pd.DataFrame,
     column_map: Dict[Tuple, Dict[str, Any]]
 ) -> pd.DataFrame:
     """
     Transform `raw` data frame into table that can be uploaded directly to [LyProX].
@@ -249,20 +244,18 @@
                 raise ParsingError(
                     f"Column {multi_idx_col} has neither a `default` value nor `func` "
                     "describing how to fill this column."
                 )
     return processed
 
 
-@report_state(
-    status_msg="Transform absolute side reporting to tumor-relative...",
-    success_msg="Transformed absolute side reporting to tumor-relative.",
-    stop_on_exc=True,
+@log_state(
+    success_msg="Transformed absolute side reporting to tumor-relative",
+    logger=logger,
 )
-@raise_if_args_none(message="Missing data table", level="warning")
 def leftright_to_ipsicontra(data: pd.DataFrame):
     """
     Change absolute side reporting to tumor-relative.
 
     Transform reporting of LNL involvement by absolute side (right & left) to a
     reporting relative to the tumor (ipsi- & contralateral). The table `data` should
     already be in the format LyProX requires, except for the side-reporting of LNL
@@ -284,33 +277,31 @@
     data = pd.concat(
                 [left_data, right_data], ignore_index=True
             )
     assert len_before == len(data), "Number of patients changed"
     return data
 
 
-@report_state(
-    status_msg="Exclude patients based on provided exclusion criteria...",
-    success_msg="Excluded patients based on provided exclusion criteria.",
-    stop_on_exc=True,
+@log_state(
+    success_msg="Excluded patients based on provided criteria",
+    logger=logger,
 )
-@raise_if_args_none(message="Raw data and exclusion criteria needed", level="warning")
 def exclude_patients(raw: pd.DataFrame, exclude: List[Tuple[str, Any]]):
     """
     Exclude patients in the `raw` data based on a list of what to `exclude`. This
     list contains tuples `(column, check)`. The `check` function will then exclude
     any patients from the cohort where `check(raw[column])` evaluates to `True`.
 
     Example:
     >>> exclude = [("age", lambda s: s > 50)]
     >>> table = pd.DataFrame({
     ...     "age":        [43, 82, 18, 67],
     ...     "T-category": [ 3,  4,  2,  1],
     ... })
-    >>> exclude_patients(table, exclude, verbose=False)
+    >>> exclude_patients(table, exclude)
        age  T-category
     0   43           3
     2   18           2
     """
     for column, check in exclude:
         exclude = check(raw[column])
         raw = raw.loc[~exclude]
@@ -352,36 +343,33 @@
                             at 0 _after_ the `header-rows`. (default: [])
       --drop-cols DROP_COLS [DROP_COLS ...]
                             Delete columns of specified indices. (default: [])
       --add-index           If the data doesn't contain an index, add one by
                             enumerating the patients (default: False)
     ```
     """
-    raw: pd.DataFrame = load_csv_table(args.input, header_row=args.header_rows)
+    raw: pd.DataFrame = load_csv_table(args.input, header_row=args.header_rows, logger=logger)
     raw = clean_header(raw, num_cols=raw.shape[1], num_header_rows=len(args.header_rows))
 
-    with report.status("Trim rows and columns..."):
-        cols_to_drop = raw.columns[args.drop_cols]
-        trimmed = raw.drop(cols_to_drop, axis="columns")
-        trimmed = trimmed.drop(index=args.drop_rows)
-        trimmed = trimmed.dropna(axis="index", how="all")
-        report.success("Trimmed rows and columns.")
-
-    with report.status("Import mapping instructions..."):
-        spec = importlib.util.spec_from_file_location("map_module", args.mapping)
-        mapping = importlib.util.module_from_spec(spec)
-        spec.loader.exec_module(mapping)
-        report.success(f"Imported mapping instructions from {args.mapping}")
+    cols_to_drop = raw.columns[args.drop_cols]
+    trimmed = raw.drop(cols_to_drop, axis="columns")
+    trimmed = trimmed.drop(index=args.drop_rows)
+    trimmed = trimmed.dropna(axis="index", how="all")
+    logger.info(f"Dropped rows {args.drop_rows} and columns {cols_to_drop}.")
+
+    spec = importlib.util.spec_from_file_location("map_module", args.mapping)
+    mapping = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(mapping)
+    logger.info(f"Imported mapping instructions from {args.mapping}")
 
     reduced = exclude_patients(trimmed, mapping.EXCLUDE)
 
     if args.add_index:
-        with report.status("Add index column to data..."):
-            reduced.insert(0, ("patient", "#", "id"), list(range(len(reduced))))
-            report.success("Added index column to data.")
+        reduced.insert(0, ("patient", "#", "id"), list(range(len(reduced))))
+        logger.info("Added index column to data.")
 
     processed = transform_to_lyprox(reduced, mapping.COLUMN_MAP)
 
     if ("tumor", "1", "side") in processed.columns:
         processed = leftright_to_ipsicontra(processed)
 
-    save_table_to_csv(args.output, processed)
+    save_table_to_csv(args.output, processed, logger=logger)
```

### Comparing `lyscripts-0.6.9/lyscripts/data/split.py` & `lyscripts-0.7.0/lyscripts/data/split.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,21 +1,24 @@
 """
 Split the full dataset into cross-validation folds according to the
 content of the params.yaml file.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 import warnings
 from pathlib import Path
 
 import numpy as np
 import pandas as pd
 
-from lyscripts.utils import load_yaml_params, report
+from lyscripts.utils import load_yaml_params
 
 warnings.simplefilter(action="ignore", category=FutureWarning)
+logger = logging.getLogger(__name__)
 
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
@@ -69,52 +72,50 @@
     OPTIONAL ARGUMENTS:
       -h, --help            show this help message and exit
       -p, --params PARAMS   Path to parameter YAML file. (default: ./params.yaml)
     ```
     """
     params = load_yaml_params(args.params)
 
-    with report.status("Reading in concatenated CSV file..."):
-        header = [0,1] if params["model"]["class"] == "Unilateral" else [0,1,2]
-        concatenated_df = pd.read_csv(args.input, header=header)
-        report.success(f"Read in concatenated CSV file from {args.input}")
-
-    with report.status("Split data into sets for training & "):
-        args.output.mkdir(exist_ok=True)
-        shuffled_df = concatenated_df.sample(
-            frac=1.,
-            replace=False,
-            random_state=params["cross-validation"]["seed"]
-        ).reset_index(drop=True)
-
-        split_dfs = np.array_split(
-            shuffled_df,
-            len(params["cross-validation"]["folds"])
+    header = [0,1] if params["model"]["class"] == "Unilateral" else [0,1,2]
+    concatenated_df = pd.read_csv(args.input, header=header)
+    logger.info(f"Read in concatenated CSV file from {args.input}")
+
+    args.output.mkdir(exist_ok=True)
+    shuffled_df = concatenated_df.sample(
+        frac=1.,
+        replace=False,
+        random_state=params["cross-validation"]["seed"]
+    ).reset_index(drop=True)
+
+    split_dfs = np.array_split(
+        shuffled_df,
+        len(params["cross-validation"]["folds"])
+    )
+    for fold_id, split_pattern in params["cross-validation"]["folds"].items():
+        # Concatenate training and evaluation DataFrames from the split DataFrames
+        # using the split pattern defined in the params file.
+        eval_df = pd.concat(
+            [split_dfs[k] for k,sym in enumerate(split_pattern) if sym == "e"],
+            ignore_index=True
         )
-        for fold_id, split_pattern in params["cross-validation"]["folds"].items():
-            # Concatenate training and evaluation DataFrames from the split DataFrames
-            # using the split pattern defined in the params file.
-            eval_df = pd.concat(
-                [split_dfs[k] for k,sym in enumerate(split_pattern) if sym == "e"],
-                ignore_index=True
-            )
-            train_df = pd.concat(
-                [split_dfs[k] for k,sym in enumerate(split_pattern) if sym == "t"],
-                ignore_index=True
-            )
-
-            eval_df.to_csv(args.output / f"{fold_id}_eval.csv", index=None)
-            train_df.to_csv(args.output / f"{fold_id}_train.csv", index=None)
-            report.print(f"+ split data into train & eval sets for round {fold_id}")
-
-        report.success(
-            "Split data into train & eval sets for all "
-            f"{len(params['cross-validation']['folds'])} folds"
+        train_df = pd.concat(
+            [split_dfs[k] for k,sym in enumerate(split_pattern) if sym == "t"],
+            ignore_index=True
         )
 
+        eval_df.to_csv(args.output / f"{fold_id}_eval.csv", index=None)
+        train_df.to_csv(args.output / f"{fold_id}_train.csv", index=None)
+        logger.info(f"+ split data into train & eval sets for round {fold_id}")
+
+    logger.info(
+        "Split data into train & eval sets for all "
+        f"{len(params['cross-validation']['folds'])} folds"
+    )
+
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
     args.run_main(args)
```

### Comparing `lyscripts-0.6.9/lyscripts/data/utils.py` & `lyscripts-0.7.0/lyscripts/data/utils.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,38 +2,29 @@
 Utilities related to the commands for data cleaning and processing.
 """
 from pathlib import Path
 from typing import List
 
 import pandas as pd
 
-from lyscripts.utils import (
+from lyscripts.decorators import (
     check_input_file_exists,
     check_output_dir_exists,
-    raise_if_args_none,
-    report_state,
+    log_state,
 )
 
 
-@report_state(
-    status_msg="Save processed CSV file...",
-    success_msg="Saved processed CSV file.",
-)
+@log_state(success_msg="Saved processed CSV file")
 @check_output_dir_exists
-@raise_if_args_none(message="Specify table to save", level="warning")
 def save_table_to_csv(output_path: Path, table: pd.DataFrame):
     """Save a `pd.DataFrame` to `output_path`."""
     table.to_csv(output_path, index=None)
 
 
-@report_state(
-    status_msg="Load input CSV file...",
-    success_msg="Loaded input CSV file.",
-)
+@log_state(success_msg="Loaded input CSV file")
 @check_input_file_exists
-@raise_if_args_none(message="Header rows must be specified", level="warning")
 def load_csv_table(input_path: Path, header_row: List[int]) -> pd.DataFrame:
     """
     Load a CSV table from `input_path` into a `pd.DataFrame` where the list `header`
     defines which rows make up the column names.
     """
     return pd.read_csv(input_path, header=header_row)
```

### Comparing `lyscripts-0.6.9/lyscripts/evaluate.py` & `lyscripts-0.7.0/lyscripts/evaluate.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,31 +1,34 @@
 """
 Evaluate the performance of the trained model by computing quantities like the
 Bayesian information criterion (BIC) or (if thermodynamic integration was performed)
 the actual evidence (with error) of the model.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
 import json
+import logging
 from pathlib import Path
 from typing import Tuple
 
 import emcee
 import h5py
 import lymph
 import numpy as np
 import pandas as pd
 from scipy.integrate import trapezoid
 
 from lyscripts.utils import (
     create_model_from_config,
     load_data_for_model,
     load_yaml_params,
-    report,
 )
 
+logger = logging.getLogger(__name__)
+
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
@@ -108,14 +111,46 @@
     for i in range(num):
         rand_idx = np.random.choice(log_probs.shape[1], size=log_probs.shape[0])
         drawn_accuracy = log_probs[np.arange(log_probs.shape[0]),rand_idx].copy()
         integrals[i] = trapezoid(y=drawn_accuracy, x=temp_schedule)
     return np.mean(integrals), np.std(integrals)
 
 
+def compute_ti_results(
+    metrics: dict,
+    params: dict,
+    ndim: int,
+    h5_file: Path,
+    model: Path,
+) -> Tuple[np.ndarray, np.ndarray]:
+    """Compute the results in case of a thermodynamic integration run."""
+    temp_schedule = params["sampling"]["temp_schedule"]
+    num_temps = len(temp_schedule)
+
+    if num_temps != len(h5_file["ti"]):
+        raise RuntimeError(
+            f"Parameters suggest temp schedule of length {num_temps}, "
+            f"but stored are {len(h5_file['ti'])}"
+        )
+
+    nwalker = ndim * params["sampling"]["walkers_per_dim"]
+    nsteps = params["sampling"]["nsteps"]
+    ti_log_probs = np.zeros(shape=(num_temps, nsteps * nwalker))
+
+    for i, round in enumerate(h5_file["ti"]):
+        reader = emcee.backends.HDFBackend(model, name=f"ti/{round}", read_only=True)
+        ti_log_probs[i] = reader.get_blobs(flat=True)
+
+    evidence, evidence_std = compute_evidence(temp_schedule, ti_log_probs)
+    metrics["evidence"] = evidence
+    metrics["evidence_std"] = evidence_std
+
+    return temp_schedule, ti_log_probs
+
+
 def main(args: argparse.Namespace):
     """
     To evaluate the performance of a sampling round, this program follows these steps:
 
     1. Read in the paramter file `params.yaml` and the data that was used for inference
     2. Recreate an instance of the model that was used during the training stage
     3. If thermodynamic integration (TI) [^2] was performed, compute the accuracy for
@@ -150,87 +185,70 @@
     --metrics METRICS    Path to metrics file (default: ./metrics.json)
     ```
 
     [^2]: https://doi.org/10.1007/s11571-021-09696-9
     """
     metrics = {}
 
-    params = load_yaml_params(args.params)
-    model = create_model_from_config(params)
+    params = load_yaml_params(args.params, logger=logger)
+    model = create_model_from_config(params, logger=logger)
     ndim = len(model.spread_probs) + model.diag_time_dists.num_parametric
     is_uni = isinstance(model, lymph.Unilateral)
 
     data = load_data_for_model(args.data, header_rows=[0,1] if is_uni else [0,1,2])
-
     h5_file = h5py.File(args.model, mode="r")
-    # check if TI has been performed
+
+    # if TI has been performed, compute the accuracy for every step
     if "ti" in h5_file:
-        with report.status("Compute results of thermodynamic integration..."):
-            temp_schedule = params["sampling"]["temp_schedule"]
-            num_temps = len(temp_schedule)
-            if num_temps != len(h5_file["ti"]):
-                raise RuntimeError(
-                    f"Parameters suggest temp schedule of length {num_temps}, "
-                    f"but stored are {len(h5_file['ti'])}"
-                )
-            nwalker = ndim * params["sampling"]["walkers_per_dim"]
-            nsteps = params["sampling"]["nsteps"]
-            ti_log_probs = np.zeros(shape=(num_temps, nsteps * nwalker))
-            for i,round in enumerate(h5_file["ti"]):
-                reader = emcee.backends.HDFBackend(
-                    args.model,
-                    name=f"ti/{round}",
-                    read_only=True,
-                )
-                ti_log_probs[i] = reader.get_blobs(flat=True)
-
-            evidence, evidence_std = compute_evidence(temp_schedule, ti_log_probs)
-            metrics["evidence"] = evidence
-            metrics["evidence_std"] = evidence_std
-            report.success(
-                f"Computed results of thermodynamic integration with {num_temps} steps"
-            )
-
-        with report.status("Plot  vs accuracy..."):
-            args.plots.parent.mkdir(exist_ok=True)
-
-            tmp_df = pd.DataFrame(
-                np.array([
-                    temp_schedule,
-                    np.mean(ti_log_probs, axis=1),
-                    np.std(ti_log_probs, axis=1)
-                ]).T,
-                columns=["", "accuracy", "std"],
-            )
-            tmp_df.to_csv(args.plots, index=False)
-            report.success(f"Plotted  vs accuracy at {args.plots}")
-
-    with report.status("Open samples from emcee backend..."):
-        backend = emcee.backends.HDFBackend(args.model, read_only=True, name="mcmc")
-        # use blobs, because also for TI, this is the unscaled log-prob
-        final_log_probs = backend.get_blobs()
-        report.success(f"Opened samples from emcee backend from {args.model}")
-
-    with report.status("Write out metrics..."):
-        # store metrics in JSON file
-        args.metrics.parent.mkdir(parents=True, exist_ok=True)
-        args.metrics.touch(exist_ok=True)
-
-        # further populate metrics dictionary
-        metrics["BIC"] = comp_bic(
-            final_log_probs, ndim, len(data),
+        temp_schedule, ti_log_probs = compute_ti_results(
+            metrics=metrics,
+            params=params,
+            ndim=ndim,
+            h5_file=h5_file,
+            model=args.model,
+        )
+        logger.info(
+            "Computed results of thermodynamic integration with "
+            f"{len(temp_schedule)} steps"
         )
-        metrics["max_llh"] = np.max(final_log_probs)
-        metrics["mean_llh"] = np.mean(final_log_probs)
 
-        # write out metrics again
-        with open(args.metrics, mode="w") as metrics_file:
-            json.dump(metrics, metrics_file)
+        # store inverse temperatures and log-probs in CSV file
+        args.plots.parent.mkdir(exist_ok=True)
+
+        beta_vs_accuracy = pd.DataFrame(
+            np.array([
+                temp_schedule,
+                np.mean(ti_log_probs, axis=1),
+                np.std(ti_log_probs, axis=1)
+            ]).T,
+            columns=["", "accuracy", "std"],
+        )
+        beta_vs_accuracy.to_csv(args.plots, index=False)
+        logger.info(f"Plotted  vs accuracy at {args.plots}")
+
+
+    # use blobs, because also for TI, this is the unscaled log-prob
+    backend = emcee.backends.HDFBackend(args.model, read_only=True, name="mcmc")
+    final_log_probs = backend.get_blobs()
+    logger.info(f"Opened samples from emcee backend from {args.model}")
+
+    # store metrics in JSON file
+    args.metrics.parent.mkdir(parents=True, exist_ok=True)
+    args.metrics.touch(exist_ok=True)
+
+    metrics["BIC"] = comp_bic(
+        final_log_probs, ndim, len(data),
+    )
+    metrics["max_llh"] = np.max(final_log_probs)
+    metrics["mean_llh"] = np.mean(final_log_probs)
+
+    with open(args.metrics, mode="w", encoding="utf-8") as metrics_file:
+        json.dump(metrics, metrics_file)
 
-        report.success(f"Wrote out metrics to {args.metrics}")
+    logger.info(f"Wrote out metrics to {args.metrics}")
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/plot/__init__.py` & `lyscripts-0.7.0/lyscripts/plot/__init__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/plot/__main__.py` & `lyscripts-0.7.0/lyscripts/plot/__main__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/plot/corner.py` & `lyscripts-0.7.0/lyscripts/plot/corner.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 """
 Generate a corner plot of the drawn samples.
 
 A corner plot is a combination of 1D and 2D marginals of probability distributions.
 The library I use for this is built on `matplotlib` and is called
 [`corner`](https://github.com/dfm/corner.py).
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 from typing import List, Union
 
 import corner
 import emcee
 import lymph
 
 from lyscripts.plot.utils import save_figure
-from lyscripts.utils import load_yaml_params, model_from_config, report
+from lyscripts.utils import load_yaml_params, model_from_config
+
+logger = logging.getLogger(__name__)
 
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
@@ -130,38 +134,36 @@
         output                Path to output corner plot (SVG).
 
     OPTIONAL ARGUMENTS:
         -h, --help            show this help message and exit
         -p, --params PARAMS   Path to parameter file (default: ./params.yaml)
     ```
     """
-    params = load_yaml_params(args.params)
+    params = load_yaml_params(args.params, logger=logger)
+
+    backend = emcee.backends.HDFBackend(args.model, read_only=True)
+    logger.info(f"Opened model as emcee backend from {args.model}")
 
-    with report.status("Open model as emcee backend..."):
-        backend = emcee.backends.HDFBackend(args.model, read_only=True)
-        report.success(f"Opened model as emcee backend from {args.model}")
-
-    with report.status("Plot corner plot..."):
-        model = model_from_config(
-            graph_params=params["graph"],
-            model_params=params["model"],
-        )
-        labels = get_param_labels(model)
-        labels = [label.replace("->", "") for label in labels]
-
-        chain = backend.get_chain(flat=True)
-        if len(labels) != chain.shape[1]:
-            raise RuntimeError(f"length labels: {len(labels)}, shape chain: {chain.shape}")
-        fig = corner.corner(
-            chain,
-            labels=labels,
-            show_titles=True,
-        )
+    model = model_from_config(
+        graph_params=params["graph"],
+        model_params=params["model"],
+    )
+    labels = get_param_labels(model)
+    labels = [label.replace("->", "") for label in labels]
+
+    chain = backend.get_chain(flat=True)
+    if len(labels) != chain.shape[1]:
+        raise RuntimeError(f"length labels: {len(labels)}, shape chain: {chain.shape}")
+    fig = corner.corner(
+        chain,
+        labels=labels,
+        show_titles=True,
+    )
 
-    save_figure(args.output, fig, formats=["png", "svg"])
+    save_figure(args.output, fig, formats=["png", "svg"], logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/plot/histograms.py` & `lyscripts-0.7.0/lyscripts/plot/histograms.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 """
 Plot computed risks and prevalences into a beautiful histogram.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 
 import matplotlib.pyplot as plt
 
 from lyscripts.plot.utils import (
     COLOR_CYCLE,
     Histogram,
     Posterior,
     draw,
     get_size,
     save_figure,
     use_mpl_stylesheet,
 )
-from lyscripts.utils import report
+
+logger = logging.getLogger(__name__)
 
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
@@ -91,49 +94,47 @@
                               into one plot (default: None)
         --title TITLE         Title of the plot (default: None)
         --bins BINS           Number of bins to put the computed values into (default:
                               60)
         --mplstyle MPLSTYLE   Path to the MPL stylesheet (default: ./.mplstyle)
     ```
     """
-    use_mpl_stylesheet(args.mplstyle)
+    use_mpl_stylesheet(args.mplstyle, logger=logger)
 
-    with report.status("Add content to figure..."):
-        contents = []
-        for name in args.names:
-            color = next(COLOR_CYCLE)
-            contents.append(Histogram.from_hdf5(
+    contents = []
+    for name in args.names:
+        color = next(COLOR_CYCLE)
+        contents.append(Histogram.from_hdf5(
+            filename=args.input,
+            dataname=name,
+            kwargs={"color": color},
+        ))
+        logger.info(f"Added histogram {name} to figure")
+        try:
+            contents.append(Posterior.from_hdf5(
                 filename=args.input,
                 dataname=name,
                 kwargs={"color": color},
             ))
-            report.success(f"Added histogram {name} to figure")
-            try:
-                contents.append(Posterior.from_hdf5(
-                    filename=args.input,
-                    dataname=name,
-                    kwargs={"color": color},
-                ))
-            except KeyError:
-                report.info(f"No observation data available for dataset {name}")
-            else:
-                report.success(f"Added posterior PDF for data {name} to figure")
-
-    with report.status("Draw figure..."):
-        fig, ax = plt.subplots(figsize=get_size())
-        draw(
-            axes=ax,
-            contents=contents,
-            hist_kwargs={"nbins": args.bins},
-            percent_lims=(5., 5.)
-        )
-        ax.legend()
-        report.success("Drawn figure")
+        except KeyError:
+            logger.warning(f"No observation data available for dataset {name}")
+        else:
+            logger.info(f"Added posterior PDF for data {name} to figure")
+
+    fig, ax = plt.subplots(figsize=get_size())
+    draw(
+        axes=ax,
+        contents=contents,
+        hist_kwargs={"nbins": args.bins},
+        percent_lims=(5., 5.)
+    )
+    ax.legend()
+    logger.info("Drawn figure")
 
-    save_figure(fig, args.output, formats=["png", "svg"])
+    save_figure(fig, args.output, formats=["png", "svg"], logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/plot/thermo_int.py` & `lyscripts-0.7.0/lyscripts/plot/thermo_int.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,32 +1,35 @@
 """
 Plot how the accuracy develops over the course of a thermodynamic integration run.
 
 This can also be used to compare how the accuracy of different models develops during
 thermdynamic integration.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 
 import matplotlib
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 from cycler import cycler
 
 from lyscripts.plot.utils import (
     COLORS,
     get_size,
     save_figure,
     use_mpl_stylesheet,
 )
-from lyscripts.utils import report
 
+logger = logging.getLogger(__name__)
 LINE_CYCLER = cycler(linestyle=["-", "--"]) * cycler(color=list(COLORS.values()))
 
+
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
     """
@@ -113,74 +116,75 @@
         --mplstyle MPLSTYLE   Path to the MPL stylesheet (default: ./.mplstyle)
     ```
 
     [^1]: https://doi.org/10.1007/s11571-021-09696-9
     """
     use_mpl_stylesheet(args.mplstyle)
 
-    with report.status("Load CSV file(s)..."):
-        accuracy_series = []
-        min_acc = np.inf
-        max_acc = -np.inf
-        for input in args.inputs:
-            tmp = pd.read_csv(input)
-            min_acc = np.min([min_acc, *tmp["accuracy"]])
-            max_acc = np.max([max_acc, *tmp["accuracy"]])
-            accuracy_series.append(tmp)
-            report.print(f"+ read in {input}")
-        report.success("Loaded CSV file(s)")
-
-    with report.status("Prepare figure..."):
-        fig, ax = plt.subplots(figsize=get_size())
-        if args.title is not None:
-            fig.suptitle(args.title)
-
-        ax.set_xlabel("inverse temperature $\\beta$")
-        xticks = np.linspace(0., 1., 7)
-        xticklabels = [f"{x**args.power:.2g}" for x in xticks]
-        ax.set_xticks(ticks=xticks, labels=xticklabels)
-        ax.set_xlim(left=0., right=1.)
-
-        ax.set_ylabel("accuracy $\\mathcal{A}(\\beta)$")
-        ax.set_yscale("symlog")
-        ax.get_yaxis().set_major_locator(matplotlib.ticker.MultipleLocator(800))
-        ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
-        ax.ticklabel_format(axis="y", style="sci", scilimits=(2,2))
-        report.success("Prepared figure")
-
-    with report.status("Plot series..."):
-        for i,series in enumerate(accuracy_series):
-            last_acc = series['accuracy'].values[-1]
-            try:
-                label = args.labels[i] + " $\\mathcal{A}(1)$ = " + f"{last_acc:g}"
-            except IndexError:
-                label = None,
-            if "stddev" in series:
-                ax.errorbar(
-                    series[""]**(1./args.power),
-                    series["accuracy"],
-                    yerr=series["stddev"],
-                    label=label,
-                )
-            else:
-                ax.plot(
-                    series[""]**(1./args.power),
-                    series["accuracy"],
-                    label=label,
-                )
-        if len(args.labels) > 0:
-            ax.legend()
-        report.success("Plotted series")
+    accuracy_series = []
+    min_acc = np.inf
+    max_acc = -np.inf
+    for input in args.inputs:
+        tmp = pd.read_csv(input)
+        min_acc = np.min([min_acc, *tmp["accuracy"]])
+        max_acc = np.max([max_acc, *tmp["accuracy"]])
+        accuracy_series.append(tmp)
+        logger.info(f"+ read in {input}")
+    logger.info("Loaded CSV file(s)")
+
+
+    fig, ax = plt.subplots(figsize=get_size())
+    if args.title is not None:
+        fig.suptitle(args.title)
+
+    ax.set_xlabel("inverse temperature $\\beta$")
+    xticks = np.linspace(0., 1., 7)
+    xticklabels = [f"{x**args.power:.2g}" for x in xticks]
+    ax.set_xticks(ticks=xticks, labels=xticklabels)
+    ax.set_xlim(left=0., right=1.)
+
+    ax.set_ylabel("accuracy $\\mathcal{A}(\\beta)$")
+    ax.set_yscale("symlog")
+    ax.get_yaxis().set_major_locator(matplotlib.ticker.MultipleLocator(800))
+    ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
+    ax.ticklabel_format(axis="y", style="sci", scilimits=(2,2))
+    logger.info("Prepared figure")
+
+
+    for i, series in enumerate(accuracy_series):
+        last_acc = series['accuracy'].values[-1]
+        try:
+            label = args.labels[i] + " $\\mathcal{A}(1)$ = " + f"{last_acc:g}"
+        except IndexError:
+            label = None
+
+        if "stddev" in series:
+            ax.errorbar(
+                series[""]**(1./args.power),
+                series["accuracy"],
+                yerr=series["stddev"],
+                label=label,
+            )
+        else:
+            ax.plot(
+                series[""]**(1./args.power),
+                series["accuracy"],
+                label=label,
+            )
+
+    if len(args.labels) > 0:
+        ax.legend()
+    logger.info("Plotted series")
+
 
     if args.show:
-        with report.status("Display the plot..."):
-            plt.show()
-            report.success("Showed the plot")
+        plt.show()
+        logger.info("Showed the plot")
     else:
-        save_figure(fig, args.output, formats=["png", "svg"])
+        save_figure(fig, args.output, formats=["png", "svg"], logger=logger)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/plot/utils.py` & `lyscripts-0.7.0/lyscripts/plot/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,18 +9,18 @@
 import h5py
 import matplotlib.pyplot as plt
 import numpy as np
 import scipy as sp
 from matplotlib.axes._axes import Axes as MPLAxes
 from matplotlib.figure import Figure
 
-from lyscripts.utils import (
+from lyscripts.decorators import (
     check_input_file_exists,
     check_output_dir_exists,
-    report_state,
+    log_state,
 )
 
 # define USZ colors
 COLORS = {
     "blue": '#005ea8',
     "orange": '#f17900',
     "green": '#00afa5',
@@ -274,28 +274,22 @@
             tmp_plot_kwargs.update(content.kwargs)
             axes.plot(x, content.pdf(x), **tmp_plot_kwargs)
 
     axes.set_xlim(*xlims)
     return axes
 
 
-@report_state(
-    status_msg="Load MPL stylesheet...",
-    success_msg="Loaded MPL stylesheet.",
-)
+@log_state(success_msg="Loaded MPL stylesheet")
 @check_input_file_exists
 def use_mpl_stylesheet(file_path: Union[str, Path]):
     """Load a `.mplstyle` stylesheet from `file_path`."""
     plt.style.use(file_path)
 
 
-@report_state(
-    status_msg="Save matplotlib figure...",
-    success_msg="Saved matplotlib figure.",
-)
+@log_state(success_msg="Saved matplotlib figure")
 @check_output_dir_exists
 def save_figure(
     output_path: Union[str, Path],
     figure: Figure,
     formats: Optional[List[str]],
 ):
     """Save a `figure` to `output_path` in every one of the provided `formats`."""
```

### Comparing `lyscripts-0.6.9/lyscripts/predict/__init__.py` & `lyscripts-0.7.0/lyscripts/predict/__init__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/predict/__main__.py` & `lyscripts-0.7.0/lyscripts/predict/__main__.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/predict/prevalences.py` & `lyscripts-0.7.0/lyscripts/predict/prevalences.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,36 +5,41 @@
 This essentially amounts to computing the data likelihood under the model and comparing
 it to the empirical likelihood of a given pattern of lymphatic progression.
 
 Like `lyscripts.predict.risks`, the computation of the prevalences can be done for
 different scenarios. How to define these scenarios can be seen in the
 [`lynference`](https://github.com/rmnldwg/lynference) repository.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 from typing import Dict, Generator, List, Optional
 
 import h5py
 import lymph
 import numpy as np
 import pandas as pd
 from rich.progress import track
 
+from lyscripts.decorators import log_state
 from lyscripts.predict.utils import complete_pattern
 from lyscripts.utils import (
     LymphModel,
     create_model_from_config,
     flatten,
     get_lnls,
     load_data_for_model,
     load_hdf5_samples,
     load_yaml_params,
     report,
 )
 
+logger = logging.getLogger(__name__)
+
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
@@ -179,14 +184,15 @@
     with_midline_ext["info", "tumor", "midline_extension"] = True
     without_midline_ext = patient_row.copy()
     without_midline_ext["info", "tumor", "midline_extension"] = False
 
     return with_midline_ext.append(without_midline_ext).reset_index()
 
 
+@log_state(logger=logger)
 def compute_observed_prevalence(
     pattern: Dict[str, Dict[str, bool]],
     data: pd.DataFrame,
     lnls: List[str],
     t_stage: str = "early",
     modality: str = "max_llh",
     midline_ext: Optional[bool] = None,
@@ -276,14 +282,15 @@
             given_params=given_params,
             log=False,
         )
 
     return prevalence
 
 
+@log_state(logger=logger)
 def generate_predicted_prevalences(
     pattern: Dict[str, Dict[str, bool]],
     model: LymphModel,
     samples: np.ndarray,
     t_stage: str = "early",
     midline_ext: Optional[bool] = None,
     midline_ext_prob: float = 0.3,
@@ -349,20 +356,20 @@
 
     OPTIONAL ARGUMENTS:
     -h, --help       show this help message and exit
     --thin THIN      Take only every n-th sample (default: 1)
     --params PARAMS  Path to parameter file (default: ./params.yaml)
     ```
     """
-    params = load_yaml_params(args.params)
-    model = create_model_from_config(params)
-    samples = load_hdf5_samples(args.model)
+    params = load_yaml_params(args.params, logger=logger)
+    model = create_model_from_config(params, logger=logger)
+    samples = load_hdf5_samples(args.model, logger=logger)
 
     header_rows = [0,1] if isinstance(model, lymph.Unilateral) else [0,1,2]
-    data = load_data_for_model(args.data, header_rows)
+    data = load_data_for_model(args.data, header_rows, logger=logger)
 
     args.output.parent.mkdir(exist_ok=True)
     num_prevalences = len(params["prevalences"])
     with h5py.File(args.output, mode="w") as prevalences_storage:
         for i,scenario in enumerate(params["prevalences"]):
             prevs_gen = generate_predicted_prevalences(
                 model=model,
@@ -392,15 +399,15 @@
                     prevs_h5dset.attrs[key] = val
                 except TypeError:
                     pass
 
             prevs_h5dset.attrs["num_match"] = num_match
             prevs_h5dset.attrs["num_total"] = num_total
 
-        report.success(
+        logger.info(
             f"Computed prevalences of {num_prevalences} scenarios stored at "
             f"{args.output}"
         )
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
```

### Comparing `lyscripts-0.6.9/lyscripts/predict/risks.py` & `lyscripts-0.7.0/lyscripts/predict/risks.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,33 +2,38 @@
 Predict risks of involvements using the samples that were drawn during the inference
 process and scenarios as defined in a YAML file.
 
 The structure of these scenarios can is similar to how scenarios are defined for the
 `lyscripts.predict.prevalences` script. Examples can be seen in an actual `params.yaml`
 file over in the [`lynference`](https://github.com/rmnldwg/lynference) repository.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 from typing import Dict, Generator, List, Optional
 
 import h5py
 import lymph
 import numpy as np
 from rich.progress import track
 
+from lyscripts.decorators import log_state
 from lyscripts.predict.utils import complete_pattern
 from lyscripts.utils import (
     LymphModel,
     create_model_from_config,
     get_lnls,
     load_hdf5_samples,
     load_yaml_params,
     report,
 )
 
+logger = logging.getLogger(__name__)
+
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
@@ -37,14 +42,15 @@
         Path(__file__).name.replace(".py", ""),
         description=__doc__,
         help=__doc__,
         formatter_class=help_formatter,
     )
     _add_arguments(parser)
 
+
 def _add_arguments(parser: argparse.ArgumentParser):
     """
     Add arguments needed to run this script to a `subparsers` instance
     and run the respective main function when chosen.
     """
     parser.add_argument(
         "model", type=Path,
@@ -62,14 +68,15 @@
         "--params", default="./params.yaml", type=Path,
         help="Path to parameter file"
     )
 
     parser.set_defaults(run_main=main)
 
 
+@log_state(logger=logger)
 def predicted_risk(
     involvement: Dict[str, Dict[str, bool]],
     model: LymphModel,
     samples: np.ndarray,
     t_stage: str,
     midline_ext: bool = False,
     given_diagnosis: Optional[Dict[str, Dict[str, bool]]] = None,
@@ -154,17 +161,17 @@
 
     OPTIONAL ARGUMENTS:
     -h, --help       show this help message and exit
     --thin THIN      Take only every n-th sample (default: 1)
     --params PARAMS  Path to parameter file (default: ./params.yaml)
     ```
     """
-    params = load_yaml_params(args.params)
-    model = create_model_from_config(params)
-    samples = load_hdf5_samples(args.model)
+    params = load_yaml_params(args.params, logger=logger)
+    model = create_model_from_config(params, logger=logger)
+    samples = load_hdf5_samples(args.model, logger=logger)
 
     args.output.parent.mkdir(exist_ok=True)
     num_risks = len(params["risks"])
     with h5py.File(args.output, mode="w") as risks_storage:
         for i,scenario in enumerate(params["risks"]):
             risks_gen = predicted_risk(
                 model=model,
@@ -184,15 +191,15 @@
                 data=risks_arr,
             )
             for key,val in scenario.items():
                 try:
                     risks_h5dset.attrs[key] = val
                 except TypeError:
                     pass
-        report.success(
+        logger.info(
             f"Computed risks of {num_risks} scenarios stored at {args.output}"
         )
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
```

### Comparing `lyscripts-0.6.9/lyscripts/predict/utils.py` & `lyscripts-0.7.0/lyscripts/predict/utils.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/lyscripts/sample.py` & `lyscripts-0.7.0/lyscripts/sample.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,15 +5,17 @@
 This is the central script performing for our project on modelling lymphatic spread
 in head & neck cancer. We use it for model comparison via the thermodynamic
 integration functionality and use the sampled parameter estimates for risk
 predictions. This risk estimate may in turn some day guide clinicians to make more
 objective decisions with respect to defining the *elective clinical target volume*
 (CTV-N) in radiotherapy.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 import os
 import warnings
 from multiprocessing import Pool
 from pathlib import Path
 from typing import Any, Callable, Dict, Optional, Union
 
 import emcee
@@ -26,14 +28,16 @@
     CustomProgress,
     get_modalities_subset,
     load_yaml_params,
     model_from_config,
     report,
 )
 
+logger = logging.getLogger(__name__)
+
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
     Add an `ArgumentParser` to the subparsers action.
@@ -116,18 +120,26 @@
         log_prob_fn,
         pool=None,
         backend=None,
         random_state=None,
     ):
         """Initialize sampler with sane defaults."""
         moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]
+
         if random_state is not None:
             self.random_state = random_state.get_state()
+            try:
+                logger.debug(f"Using random state {self.random_state}")
+            except AttributeError as attr_err:
+                logger.warning("Could not get random state", exc_info=attr_err)
+
+
         super().__init__(nwalkers, ndim, log_prob_fn, pool, moves, backend=backend)
 
+
     def run_sampling(
         self,
         min_steps: int = 0,
         max_steps: int = 10000,
         thin_by: int = 1,
         initial_state: Optional[Union[emcee.State, np.ndarray]] = None,
         check_interval: int = 100,
@@ -199,14 +211,21 @@
             accept_rates.append(100. * np.mean(self.acceptance_fraction))
 
             # ...compute the autocorrelation time and store it in an array.
             new_acor = self.get_autocorr_time(tol=0)
             iterations.append(self.iteration)
             acor_times.append(np.mean(new_acor))
 
+            logger.debug(
+                f"Acceptance rate at {self.iteration}: {accept_rates[-1]:.2f}%"
+            )
+            logger.debug(
+                f"Autocorrelation time at {self.iteration}: {acor_times[-1]:.2f}"
+            )
+
             # check convergence
             is_converged = self.iteration >= min_steps
             is_converged &= np.all(new_acor * trust_threshold < self.iteration)
             rel_acor_diff = np.abs(old_acor - new_acor) / new_acor
             is_converged &= np.all(rel_acor_diff < rel_acor_threshold)
 
             # if it has converged, stop
@@ -219,29 +238,28 @@
         acor_times.append(np.mean(self.get_autocorr_time(tol=0)))
         accept_rates.append(100. * np.mean(self.acceptance_fraction))
         accept_rate_str = f"acceptance rate was {accept_rates[-1]:.2f}%"
 
         if progress_desc is not None:
             report_progress.stop()
             if is_converged:
-                report.success(
-                    progress_desc, "converged,", accept_rate_str
-                )
+                logging.info(f"{progress_desc} converged, {accept_rate_str}")
             else:
-                report.info(
-                    progress_desc, "finished: Max. steps reached,", accept_rate_str
+                logging.info(
+                    f"{progress_desc} finished: Max. steps reached, {accept_rate_str}"
                 )
 
         return {
             "iterations": iterations,
             "acor_times": acor_times,
             "accept_rates": accept_rates,
             "final_state": coords,
         }
 
+
 def run_mcmc_with_burnin(
     nwalkers: int,
     ndim: int,
     log_prob_fn: Callable,
     nsteps: int,
     persistent_backend: emcee.backends.HDFBackend,
     sampling_kwargs: Optional[dict] = None,
@@ -394,63 +412,61 @@
                             multiprocessing will not be used. (default: None)
       --seed SEED           Seed value to reproduce the same sampling round. (default:
                             42)
     ```
 
     [^1]: https://doi.org/10.1007/s11571-021-09696-9
     """
-    params = load_yaml_params(args.params)
+    params = load_yaml_params(args.params, logger=logger)
 
-    with report.status("Read in training data..."):
-        # Only read in two header rows when using the Unilateral model
-        is_unilateral = params["model"]["class"] == "Unilateral"
-        header = [0, 1] if is_unilateral else [0, 1, 2]
-        inference_data = pd.read_csv(args.input, header=header)
-        report.success(f"Read in training data from {args.input}")
-
-    with report.status("Set up model & load data..."):
-        global MODEL  # ugly, but necessary for pickling
-        MODEL = model_from_config(
-            graph_params=params["graph"],
-            model_params=params["model"],
-            modalities_params=get_modalities_subset(
-                defined_modalities=params["modalities"],
-                selection=args.modalities,
-            ),
-        )
-        MODEL.patient_data = inference_data
-        ndim = len(MODEL.spread_probs) + MODEL.diag_time_dists.num_parametric
-        nwalkers = ndim * params["sampling"]["walkers_per_dim"]
-        thin_by = params["sampling"]["thin_by"]
-        report.success(
-            f"Set up {type(MODEL)} model with {ndim} parameters and loaded "
-            f"{len(inference_data)} patients"
-        )
+    # Only read in two header rows when using the Unilateral model
+    is_unilateral = params["model"]["class"] == "Unilateral"
+    header = [0, 1] if is_unilateral else [0, 1, 2]
+    inference_data = pd.read_csv(args.input, header=header)
+    logger.info(f"Read in training data from {args.input}")
+
+    global MODEL  # ugly, but necessary for pickling
+    MODEL = model_from_config(
+        graph_params=params["graph"],
+        model_params=params["model"],
+        modalities_params=get_modalities_subset(
+            defined_modalities=params["modalities"],
+            selection=args.modalities,
+        ),
+    )
+    MODEL.patient_data = inference_data
+    ndim = len(MODEL.spread_probs) + MODEL.diag_time_dists.num_parametric
+    nwalkers = ndim * params["sampling"]["walkers_per_dim"]
+    thin_by = params["sampling"]["thin_by"]
+    logger.info(
+        f"Set up {type(MODEL)} model with {ndim} parameters and loaded "
+        f"{len(inference_data)} patients"
+    )
 
     if args.ti:
         global INV_TEMP
-        with report.status("Prepare thermodynamic integration..."):
-            # make sure path to output file exists
-            args.output.parent.mkdir(parents=True, exist_ok=True)
-
-            # set up sampling params
-            temp_schedule = params["sampling"]["temp_schedule"]
-            nsteps = params["sampling"]["nsteps"]
-            burnin = params["sampling"]["burnin"]
-            report.success("Prepared thermodynamic integration.")
+
+        # make sure path to output file exists
+        args.output.parent.mkdir(parents=True, exist_ok=True)
+
+        # set up sampling params
+        temp_schedule = params["sampling"]["temp_schedule"]
+        nsteps = params["sampling"]["nsteps"]
+        burnin = params["sampling"]["burnin"]
+        logger.info("Prepared thermodynamic integration.")
 
         # record some information about each burnin phase
         x_axis = temp_schedule.copy()
         plots = {
             "acor_times": [],
             "accept_rates": [],
         }
         for i,inv_temp in enumerate(temp_schedule):
             INV_TEMP = inv_temp
-            report.print(f"TI round {i+1}/{len(temp_schedule)} with  = {INV_TEMP}")
+            logger.info(f"TI round {i+1}/{len(temp_schedule)} with  = {INV_TEMP}")
 
             # set up backend
             hdf5_backend = emcee.backends.HDFBackend(args.output, name=f"ti/{i+1:0>2d}")
 
             burnin_info = run_mcmc_with_burnin(
                 nwalkers, ndim, log_prob_fn, nsteps,
                 persistent_backend=hdf5_backend,
@@ -464,25 +480,24 @@
             plots["acor_times"].append(burnin_info["acor_times"][-1])
             plots["accept_rates"].append(burnin_info["accept_rates"][-1])
 
         # copy last sampling round over to a group in the HDF5 file called "mcmc"
         # because that is what other scripts expect to see, e.g. for plotting risks
         h5_file = h5py.File(args.output, "r+")
         h5_file.copy(f"ti/{len(temp_schedule):0>2d}", h5_file, name="mcmc")
-        report.success("Finished thermodynamic integration.")
+        logger.info("Finished thermodynamic integration.")
 
     else:
-        with report.status("Prepare sampling params & backend..."):
-            # make sure path to output file exists
-            args.output.parent.mkdir(parents=True, exist_ok=True)
+        # make sure path to output file exists
+        args.output.parent.mkdir(parents=True, exist_ok=True)
 
-            # prepare backend
-            hdf5_backend = emcee.backends.HDFBackend(args.output, name="mcmc")
+        # prepare backend
+        hdf5_backend = emcee.backends.HDFBackend(args.output, name="mcmc")
 
-            report.success(f"Prepared sampling params & backend at {args.output}")
+        logger.info(f"Prepared sampling params & backend at {args.output}")
 
         burnin_info = run_mcmc_with_burnin(
             nwalkers, ndim, log_prob_fn,
             nsteps=params["sampling"]["nsteps"],
             persistent_backend=hdf5_backend,
             sampling_kwargs=params["sampling"]["kwargs"],
             thin_by=thin_by,
@@ -492,27 +507,24 @@
         )
         x_axis = np.array(burnin_info["iterations"])
         plots = {
             "acor_times": burnin_info["acor_times"],
             "accept_rates": burnin_info["accept_rates"]
         }
 
-    with report.status("Store plots about burnin phases..."):
-        args.plots.mkdir(parents=True, exist_ok=True)
-
-        for name, y_axis in plots.items():
-            tmp_df = pd.DataFrame(
-                np.array([x_axis, y_axis]).T,
-                columns=["x", name],
-            )
-            tmp_df.to_csv(args.plots/(name + ".csv"), index=False)
+    args.plots.mkdir(parents=True, exist_ok=True)
 
-        report.success(
-            f"Stored {len(plots)} plots about burnin phases at {args.plots}"
+    for name, y_axis in plots.items():
+        tmp_df = pd.DataFrame(
+            np.array([x_axis, y_axis]).T,
+            columns=["x", name],
         )
+        tmp_df.to_csv(args.plots/(name + ".csv"), index=False)
+
+    logger.info(f"Stored {len(plots)} plots about burnin phases at {args.plots}")
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/temp_schedule.py` & `lyscripts-0.7.0/lyscripts/temp_schedule.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,22 +7,24 @@
 expected log-likelihood is very steep. Hence, the inverse temparature $\\beta$ must be
 more densely spaced in the beginning.
 
 This can be achieved by using a power sequence: Generate $n$ linearly spaced points in
 the interval $[0, 1]$ and then transform each point by computing $\\beta_i^k$ where
 $k$ could e.g. be 5.
 """
+# pylint: disable=logging-fstring-interpolation
 import argparse
+import logging
 from pathlib import Path
 from typing import Callable, List, Union
 
 import numpy as np
 import yaml
 
-from lyscripts.utils import report
+logger = logging.getLogger(__name__)
 
 
 def _add_parser(
     subparsers: argparse._SubParsersAction,
     help_formatter,
 ):
     """
@@ -63,27 +65,30 @@
     def inner(*args) -> Union[np.ndarray, List[float]]:
         res = func(*args)
         if isinstance(res, np.ndarray):
             return res.tolist()
         return res
     return inner
 
+
 @tolist
 def geometric_schedule(n: int, *_a) -> np.ndarray:
     """Create a geometric sequence of `n` numbers from 0. to 1."""
     log_seq = np.logspace(0., 1., n)
     shifted_seq = log_seq - 1.
     geom_seq = shifted_seq / 9.
     return geom_seq
 
+
 @tolist
 def linear_schedule(n: int, *_a) -> np.ndarray:
     """Create a linear sequence of `n` numbers from 0. to 1."""
     return np.linspace(0., 1., n)
 
+
 @tolist
 def power_schedule(n: int, power: float, *_a) -> np.ndarray:
     """Create a power sequence of `n` numbers from 0. to 1."""
     lin_seq = np.linspace(0., 1., n)
     power_seq = lin_seq**power
     return power_seq
 
@@ -123,20 +128,19 @@
                                         temperature. (default: geometric)
     --num NUM                          Number of inverse temperatures in the schedule
                                         (default: 32)
     --pow POW                          If a power schedule is chosen, use this as
                                         power (default: 4)
     ```
     """
-    with report.status(f"Create {args.method} sequence of length {args.num}..."):
-        func = SCHEDULES[args.method]
-        schedule = func(args.num, args.pow)
-        yaml_output = yaml.dump({"temp_schedule": schedule})
-        report.success(f"Created {args.method} sequence of length {args.num}")
-        report.print(yaml_output)
+    func = SCHEDULES[args.method]
+    schedule = func(args.num, args.pow)
+    yaml_output = yaml.dump({"temp_schedule": schedule})
+    logger.info(f"Created {args.method} sequence of length {args.num}")
+    logger.debug(yaml_output)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description=__doc__)
     _add_arguments(parser)
 
     args = parser.parse_args()
```

### Comparing `lyscripts-0.6.9/lyscripts/utils.py` & `lyscripts-0.7.0/lyscripts/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -2,36 +2,38 @@
 This module contains frequently used functions and decorators that are used throughout
 the subcommands to load e.g. YAML specifications or model definitions.
 
 It also contains helpers for reporting the script's progress via a slightly customized
 `rich` console and a custom `Exception` called `LyScriptsWarning` that can propagate
 occuring issues to the right place.
 """
-import sys
-from functools import wraps
 from pathlib import Path
 from typing import Any, BinaryIO, Callable, Dict, List, Optional, TextIO, Union
 
 import h5py
 import lymph
 import numpy as np
 import pandas as pd
 import yaml
 from emcee.backends import HDFBackend
 from rich.console import Console
 from rich.progress import (
-    BarColumn,
-    MofNCompleteColumn,
     Progress,
+    SpinnerColumn,
     TextColumn,
     TimeElapsedColumn,
-    TimeRemainingColumn,
 )
 from scipy.special import factorial
 
+from lyscripts.decorators import (
+    check_input_file_exists,
+    log_state,
+    provide_file,
+)
+
 try:
     import streamlit
     from streamlit.runtime.scriptrunner import get_script_run_ctx
     streamlit.status = streamlit.spinner
 except ImportError:
     def get_script_run_ctx() -> bool:
         """A mock for the `get_script_run_ctx` function of `streamlit`."""
@@ -77,46 +79,61 @@
             return getattr(streamlit, func_name)(" ".join(objects))
 
         return func(self, *objects, **kwargs)
 
     return inner
 
 
+def inject_lvl_and_symbol(objects, level = "INFO", symbol = None, width = 8):
+    """Nicely format the `objects` to be printed with a `level` and `symbol`."""
+    prefix = "[blue]" + level.ljust(width) + "[/blue]"
+    if symbol is not None:
+        objects = [prefix, symbol, *objects]
+    else:
+        objects = [prefix, *objects]
+    return objects
+
+
 class LyScriptsReport(Console):
     """
     Small extension to the `Console` class of the rich package.
     """
-
     @redirect_to_streamlit
     def status(self, *objects, **kwargs):
         """Re-implement `status` method to allow decoration."""
         return super().status(*objects, **kwargs)
 
     @redirect_to_streamlit
     def success(self, *objects, **kwargs) -> None:
         """Prefix a bold green check mark to any output."""
-        objects = [CHECK, *objects]
+        objects = inject_lvl_and_symbol(objects, symbol=CHECK)
         return super().print(*objects, **kwargs)
 
     @redirect_to_streamlit
     def info(self, *objects, **kwargs) -> None:
         """Prefix a bold yellow circle to any output."""
-        objects = [CIRCL, *objects]
+        objects = inject_lvl_and_symbol(objects, symbol=CIRCL)
+        return super().print(*objects, **kwargs)
+
+    @redirect_to_streamlit
+    def add(self, *objects, **kwargs) -> None:
+        """Prefix a bold yellow circle to any output."""
+        objects = inject_lvl_and_symbol(objects, symbol="+")
         return super().print(*objects, **kwargs)
 
     @redirect_to_streamlit
     def warning(self, *objects, **kwargs) -> None:
         """Prefix a bold yellow triangle to any output."""
-        objects = [WARN, *objects]
+        objects = inject_lvl_and_symbol(objects, symbol=WARN)
         return super().print(*objects, **kwargs)
 
     @redirect_to_streamlit
     def error(self, *objects, **kwargs) -> None:
         """Prefix a bold red cross to any output."""
-        objects = [CROSS, *objects]
+        objects = inject_lvl_and_symbol(objects, symbol=CROSS)
         return super().print(*objects, **kwargs)
 
     def exception(self, exception, **kwargs) -> None:
         """Display a traceback either via `streamlit` or in the console."""
         if is_streamlit_running():
             return streamlit.exception(exception)
         else:
@@ -124,198 +141,23 @@
 
 report = LyScriptsReport()
 
 
 class CustomProgress(Progress):
     """Small wrapper around rich's `Progress` initializing my custom columns."""
     def __init__( self, **kwargs: dict):
+        prefix = " ".join(inject_lvl_and_symbol([]))
         columns = [
-            TextColumn("[progress.description]{task.description}"),
-            BarColumn(),
-            MofNCompleteColumn(),
-            TimeRemainingColumn(),
+            TextColumn(prefix),
+            SpinnerColumn(finished_text=CHECK),
+            *Progress.get_default_columns(),
             TimeElapsedColumn(),
         ]
         super().__init__(*columns, **kwargs)
 
-report_progress = CustomProgress()
-
-
-def report_state(
-    status_msg: str,
-    success_msg: str,
-    stop_on_exc: bool = False,
-    verbose: bool = True,
-) -> Callable:
-    """
-    Outermost decorator that catches and gracefully reports exceptions that occur.
-
-    During the execution of the decorated function, it will display the `status_msg`.
-    When successful, the `success_msg` will finally be printed. And if the decorated
-    function raises a `LyScriptsError`, then that exception's message will be passed on
-    to the methods of the reporting class/module.
-
-    If `stop_on_exc` is set to `True`, the program exits when catching an error. And
-    lastly, with `verbose=False`, one can turn off the reporting entirely which may
-    in turn be overrridden when calling the decorated function with `verbose=True` and
-    vice versa.
-    """
-    def assembled_decorator(func: Callable) -> Callable:
-        """The decorator that gets returned by `report_state`."""
-        default_verbosity = verbose
-
-        @wraps(func)
-        def inner(*args, **kwargs):
-            """The wrapped function."""
-            verbose = kwargs.pop("verbose", default_verbosity)
-            try:
-                report.quiet = not verbose
-            except AttributeError:
-                pass
-
-            with report.status(status_msg):
-                try:
-                    result = func(*args, **kwargs)
-                except LyScriptsWarning as ly_err:
-                    msg = getattr(ly_err, "message", repr(ly_err))
-                    level = getattr(ly_err, "level", "info")
-                    getattr(report, level)(msg)
-                except Exception as exc:
-                    report.exception(exc)
-                    if stop_on_exc:
-                        sys.exit(1)
-                else:
-                    report.success(success_msg)
-                    return result
-
-                return None
-
-        return inner
-
-    return assembled_decorator
-
-
-def raise_if_args_none(message: str, level: str = "warning") -> Callable:
-    """
-    Call to create a decorator that raises a `LyScriptsWarning` with the specified
-    `message` and `level` when one of the positional arguments to the decorated
-    function is `None`.
-
-    This should be the second decorator after `report_state`, since that outermost
-    decorator catches the raised `LyScriptsWarning` and handles it appropriately.
-    """
-    def assembled_decorator(func: Callable) -> Callable:
-        """The decorator that gets returned by `raise_if_args_none`."""
-        @wraps(func)
-        def inner(*args, **kwargs) -> Any:
-            """The wrapped function."""
-            for arg in args:
-                if arg is None:
-                    raise LyScriptsWarning(message, level)
-
-            return func(*args, **kwargs)
-
-        return inner
-
-    return assembled_decorator
-
-
-def check_input_file_exists(loading_func: Callable) -> Callable:
-    """
-    Decorator that checks if the file path provided to the `loading_func` exists and
-    throws a `FileNotFoundError` if it does not.
-
-    The purpose of this deorator is to provide a consistent error message to the
-    `report_func_state`, since some libraries throw other errors when a file is not
-    found.
-    """
-    @wraps(loading_func)
-    def inner(file_path: str, *args, **kwargs) -> Any:
-        """Wrapped loading function."""
-        file_path = Path(file_path)
-        if not file_path.is_file():
-            raise LyScriptsWarning(f"No file found at {file_path}", level="warning")
-
-        return loading_func(file_path, *args, **kwargs)
-
-    return inner
-
-
-def provide_file(is_binary: bool) -> Callable:
-    """
-    Create a decorator that can make sure a decorated function is provided with a
-    file-like object, even when it is called with the file path only.
-
-    This means, the assembled decorator checks the argument type and, if necessary,
-    opens the file to call the decorated function. The provided file is either a text
-    file of - if `is_binary` is set to `True` - a binary file.
-    """
-    def assembled_decorator(loading_func: Callable) -> Callable:
-        """
-        The assembled decorator that provides the decorated function with either a
-        test or binary file.
-        """
-        @wraps(loading_func)
-        def inner(file_or_path: Union[str, Path, TextIO, BinaryIO], *args, **kwargs):
-            """The wrapped function."""
-            if isinstance(file_or_path, (str, Path)):
-                file_path = Path(file_or_path)
-                if not file_path.is_file():
-                    raise FileNotFoundError(f"No file found at {file_path}")
-
-                if is_binary:
-                    with open(file_path, mode="rb") as bin_file:
-                        return loading_func(bin_file, *args, **kwargs)
-                else:
-                    with open(file_path, mode="r", encoding="utf-8") as txt_file:
-                        return loading_func(txt_file, *args, **kwargs)
-
-            return loading_func(file_or_path, *args, **kwargs)
-
-        return inner
-
-    return assembled_decorator
-
-
-def provide_text_file(loading_func: Callable) -> Callable:
-    """
-    Decorator that makes sure the `loading_func` is provided with a file-like object
-    regardless of whether the input is a `str`, `Path` or already file-like.
-    """
-    @wraps(loading_func)
-    def inner(file_or_path: Union[str, Path, TextIO], *args, **kwargs) -> Any:
-        """Wrapped loading function."""
-        if isinstance(file_or_path, (str, Path)):
-            file_path = Path(file_or_path)
-            if not file_path.is_file():
-                raise FileNotFoundError(f"No file found at {file_path}")
-
-            with open(file_path, mode="r", encoding="utf-8") as file:
-                return loading_func(file, *args, **kwargs)
-
-        return loading_func(file_or_path, *args, **kwargs)
-
-    return inner
-
-
-def check_output_dir_exists(saving_func: Callable) -> Callable:
-    """
-    Decorator to make sure the parent directory of the file that is supposed to be
-    saved does exist.
-    """
-    @wraps(saving_func)
-    def inner(file_path: str, *args, **kwargs) -> Any:
-        """Wrapped saving function."""
-        file_path = Path(file_path)
-        file_path.parent.mkdir(parents=True, exist_ok=True)
-
-        return saving_func(file_path, *args, **kwargs)
-
-    return inner
-
 
 def binom_pmf(k: Union[List[int], np.ndarray], n: int, p: float):
     """Binomial PMF"""
     if p > 1. or p < 0.:
         raise ValueError("Binomial prob must be btw. 0 and 1")
     q = (1. - p)
     binom_coeff = factorial(n) / (factorial(k) * factorial(n - k))
@@ -388,22 +230,16 @@
     )
 
     return model
 
 
 LymphModel = Union[lymph.Unilateral, lymph.Bilateral, lymph.MidlineBilateral]
 
-@report_state(
-    status_msg="Create model based on YAML parameters...",
-    success_msg="Created model based on YAML parameters",
-)
-@raise_if_args_none(
-    message="No valid params provided",
-    level="warning",
-)
+
+@log_state(success_msg="Model created from YAML config")
 def create_model_from_config(params: Dict[str, Any]) -> LymphModel:
     """Create a model instance as defined by some YAML params."""
     if "graph" in params:
         graph = graph_from_config(params["graph"])
     else:
         raise LyScriptsWarning("No graph definition found in YAML file", level="error")
 
@@ -425,18 +261,14 @@
 
     if "modalities" in params:
         model.modalities = params["modalities"]
 
     return model
 
 
-@raise_if_args_none(
-    message="No valid model provided",
-    level="warning",
-)
 def get_lnls(model: LymphModel) -> List[str]:
     """Extract the list of LNLs from a model instance. E.g.:
     >>> graph = {
     ...     ("tumor", "T"): ["II", "III"],
     ...     ("lnl", "II"): ["III"],
     ...     ("lnl", "III"): [],
     ... }
@@ -571,22 +403,15 @@
         try:
             selected_modalities[mod] = defined_modalities[mod]
         except KeyError as key_err:
             raise KeyError(f"Modality {mod} has not been defined yet") from key_err
     return selected_modalities
 
 
-@report_state(
-    status_msg="Read in patient data for model...",
-    success_msg="Read in patient data for model.",
-)
-@raise_if_args_none(
-    message="No patient data file(path) provided.",
-    level="warning",
-)
+@log_state(success_msg="Loaded patient data")
 @provide_file(is_binary=False)
 def load_data_for_model(
     file: TextIO,
     header_rows: List[int],
 ) -> pd.DataFrame:
     """
     Load patient data from a CSV file stored at `file` and consider the row
@@ -594,50 +419,33 @@
 
     This CSV file should already be preprocessed and in the format that the `lymph`
     models understand.
     """
     return pd.read_csv(file, header=header_rows)
 
 
-@report_state(
-    status_msg="Load YAML params...",
-    success_msg="Loaded YAML params.",
-)
-@raise_if_args_none(
-    message="No YAML file(path) provided.",
-    level="warning",
-)
+@log_state(success_msg="Loaded YAML params")
 @provide_file(is_binary=False)
 def load_yaml_params(file: Path) -> dict:
     """Load parameters from a YAML `file`."""
     return yaml.safe_load(file)
 
 
-@report_state(
-    status_msg="Load HDF5 samples from MCMC run...",
-    success_msg="Loaded HDF5 samples from MCMC run.",
-)
+@log_state(success_msg="Loaded HDF5 samples from MCMC run")
 @check_input_file_exists
 def load_model_samples(file_path: Path) -> np.ndarray:
     """
     Load samples produced by an MCMC sampling process that are stored at
     `file_path` in an HDF5 format.
     """
     backend = HDFBackend(file_path, read_only=True)
     return backend.get_chain(flat=True)
 
 
-@report_state(
-    status_msg="Load HDF5 samples...",
-    success_msg="Loaded HDF5 samples.",
-)
-@raise_if_args_none(
-    message="No HDF5 file provided.",
-    level="warning",
-)
+@log_state(success_msg="Loaded HDF5 samples")
 @provide_file(is_binary=True)
 def load_hdf5_samples(file: BinaryIO, name: str = "mcmc/chain") -> np.ndarray:
     """
     Load a chain of samples from an uploaded HDF5 `file` that is stored in the dataset
     called `name`.
     """
     with h5py.File(file, mode="r") as h5file:
```

### Comparing `lyscripts-0.6.9/lyscripts.egg-info/PKG-INFO` & `lyscripts-0.7.0/lyscripts.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lyscripts
-Version: 0.6.9
+Version: 0.7.0
 Summary: Package containing scripts used in lynference pipelines
 Author-email: Roman Ludwig <roman.ludwig@usz.ch>
 License: MIT
 Project-URL: source, https://github.com/rmnldwg/lyscripts
 Project-URL: documentation, https://rmnldwg.github.io/lyscripts
 Keywords: scripts,lymph,inference
 Classifier: Programming Language :: Python :: 3
```

### Comparing `lyscripts-0.6.9/lyscripts.egg-info/SOURCES.txt` & `lyscripts-0.7.0/lyscripts.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 .chglog/config.yml
 .github/workflows/build.yml
 .github/workflows/docs.yml
 .github/workflows/tests.yml
 lyscripts/__init__.py
 lyscripts/__main__.py
 lyscripts/_version.py
+lyscripts/decorators.py
 lyscripts/evaluate.py
 lyscripts/sample.py
 lyscripts/temp_schedule.py
 lyscripts/utils.py
 lyscripts.egg-info/PKG-INFO
 lyscripts.egg-info/SOURCES.txt
 lyscripts.egg-info/dependency_links.txt
```

### Comparing `lyscripts-0.6.9/pyproject.toml` & `lyscripts-0.7.0/pyproject.toml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/_sample.py` & `lyscripts-0.7.0/tests/_sample.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/plot/baseline/sine.png` & `lyscripts-0.7.0/tests/plot/baseline/sine.png`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/plot/baseline/sine.svg` & `lyscripts-0.7.0/tests/plot/baseline/sine.svg`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/plot/baseline/sine_svg.png` & `lyscripts-0.7.0/tests/plot/baseline/sine_svg.png`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/plot/baseline/test_draw.png` & `lyscripts-0.7.0/tests/plot/baseline/test_draw.png`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/plot/data/beta_samples.hdf5` & `lyscripts-0.7.0/tests/plot/data/beta_samples.hdf5`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/plot/plot_utils_test.py` & `lyscripts-0.7.0/tests/plot/plot_utils_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -219,10 +219,10 @@
 
     # assert mpl_comp.compare_images(
     #     expected="./tests/plot/baseline/sine.svg",
     #     actual="./tests/plot/results/sine.svg",
     #     tol=0.,
     # ) is None, "SVG of figure was not stored correctly."
 
-    assert save_figure_capture.out == expected_output, (
-        "The output during the save figure procedure was wrong."
-    )
+    # assert save_figure_capture.out == expected_output, (
+    #     "The output during the save figure procedure was wrong."
+    # )
```

### Comparing `lyscripts-0.6.9/tests/predict/predict_utils_test.py` & `lyscripts-0.7.0/tests/predict/predict_utils_test.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/predict/prevalences_test.py` & `lyscripts-0.7.0/tests/predict/prevalences_test.py`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/test_params.yaml` & `lyscripts-0.7.0/tests/test_params.yaml`

 * *Files identical despite different names*

### Comparing `lyscripts-0.6.9/tests/utils_test.py` & `lyscripts-0.7.0/tests/utils_test.py`

 * *Files identical despite different names*

